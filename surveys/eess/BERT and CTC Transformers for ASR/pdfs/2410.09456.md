# Automatic Speech Recognition with BERT and CTC Transformers: A review  

Noussaiba Djeffal   
Speech and Signal Processing Lab.   
USTHB University   
Algiers, Algeria   
ndjeffal@usthb.dz   
Hamza Kheddar   
LSEA Lab., Faculty of Technology   
University of MEDEA   
Medea 26000, Algeria   
kheddar.hamza@univ-medea.dz   
Djamel Addou   
Speech and Signal Processing Lab.   
USTHB University   
Algiers, Algeria   
daddou@usthb.dz   
Ahmed Cherif Mazari   
LSEA Lab, Faculty of Science   
University of MEDEA   
Medea 26000, Algeria   
mazari.ahmedcherif@univ-medea.dz  

Yassine Himeur College of Engineering and Information Technology, University of Dubai Dubai, UAE yhimeur@ud.ac.ae  

Abstract—This review paper provides a comprehensive analysis of recent advances in automatic speech recognition (ASR) with bidirectional encoder representations from transformers BERT and connectionist temporal classification (CTC) transformers. The paper first introduces the fundamental concepts of ASR and discusses the challenges associated with it. It then explains the architecture of BERT and CTC transformers and their potential applications in ASR. The paper reviews several studies that have used these models for speech recognition tasks and discusses the results obtained. Additionally, the paper highlights the limitations of these models and outlines potential areas for further research. All in all, this review provides valuable insights for researchers and practitioners who are interested in ASR with BERT and CTC transformers.  

Index Terms—Automatic speech recognition, BERT, CTC, ChatGPT, Transformers.  

# I. INTRODUCTION  

Traditional methods of speech recognition rely on maximum a posteriori probability estimation, which involves transforming the acoustic speech characteristics into word sequences through four steps: feature extraction, acoustic modeling, language modeling, and word sequence decoding. Feature extraction involves essential data extraction from the input signal using algorithms such as Mel-frequency cepstral coefficients (MFCC) [1] and perceptual line spectral pairs (LSP) [2]. The acoustic modeling stage utilizes deep neural networks and hidden Markov models to map the acoustic frame to the phonetic state at each input time, optimized for the phonetic classification error per frame. Language modeling is designed to model the most probable sequences of words, regardless of acoustics [3]. The use of transformers in speech recognition involves several steps, such as : (i) preprocessing of the audio signal, to extract essential features like log Mel filterbank energies, (ii) an acoustic model based on a self-attention mechanism to model the temporal relationships between acoustic features, and (iii) a language model trained on a large amount of text data to capture long-term dependencies between words.  

The language model takes a sequence of words as input and predicts the probability distribution of the next word in the sequence. Finally, the decoding process involves finding the most likely word sequence, given the output of both the acoustic and language models.  

To sum up, transformers such as BERT, and connectionist temporal classification (CTC) based ASR is a recent advancement in speech recognition that uses the self-attention mechanism to simultaneously perform feature extraction, acoustic modeling, language modeling, and decoding in a single network. On the other hand, the transformer architecture is a neural network model that is designed to process sequential data by attending to relevant context information. Transformers have demonstrated promising outcomes in ASR and are expected to play a crucial role in future advancements in this field.  

# A. Related work and our contribution  

In [4] the authors explore various methodologies for detecting emotions in text using BERT and its variants. They thoroughly outline their approaches, contributions, achieved accuracies, and also discuss the limitations or weaknesses of their models. However, our review focuses on ASR rather than emotion using BERT. While the authors primarily mention BERT base, BERT large, RoBERTa, DistillBERT, and crosslingual language model (XLM), which are the BERT variants employed in their research, we expand on their contributions by incorporating additional models such as ALBERT, and ELECTRA. The work in [5] focuses on providing a tutorial and survey on the attention mechanism, transformers, BERT, and GPT. It explains various concepts such as the attention mechanism, transformers, and their components. However, our review focuses on both BERT and CTC transformer applications, specifically in ASR. In [6] the survey discusses the impressive performance of transformer models like BERT, GPT, RoBERTa, and T5 across various language tasks such as text classification, machine translation, and question answering. Additionally, the article also explores their applications in computer vision. However, our review focuses on BERT and CTC transformers within the domain of ASR, distinguishing our research from other domains discussed in the aforementioned survey. In [7] encompasses various speechrelated domains such as ASR, speech synthesis, speech translation, speech para-linguistics, speech enhancement, and other applications. The authors of the survey identify and discuss the challenges that transformers face in these domains. In our review, similar to the survey, we address these challenges, but we extend the scope by including additional transformers like ELECTRA, ALBERT, and CTC Transformers specifically in the context of ASR.  

# B. Paper structure  

The rest of the paper is organized as follows: Section II presents the methodology used to create this review. Section III provides an overview of the preliminaries on CTC and BERT. Section IV defines BERT and categorizes articles based on the utilization of BERT in ASR, highlighting the advancements made in this area. Section V defines CTC and classifies articles based on the application of CTC in ASR, showcasing the progress achieved in this domain. Section VI discusses potential future directions. Finally, the paper concludes in section VII.  

# II. REVIEW METHODOLOGY AND ANALYSIS  

This review focuses mainly on two distinct categories of papers: The first category examines BERT-based ASR systems, the second category of papers explores CTC-based ASR techniques. The initial search was done using related keywords for transformers in ASR, namely: BERT and ASR, CTC and ASR, or BERT and CTC and ASR. A search was performed on scientific databases indexed at least in Scopus and available in IEEE Xplore and Springer, science direct, and others. Besides, arXiv papers are taken into consideration that have many citation and impact, which are known for their extensive coverage and relevance to ASR. Additionally, Google Scholar was utilized to include a broader range of publications, including gray literature, which can provide valuable insights for a systematic review. Only the most widely used methods and implementations were included to ensure modularity. The focus is on papers that reported new and unique applications within specific domains, avoiding repetition. Emphasis is given to papers published in high-quality journals with a significant impact factor. The search has been conducted until 2023 to gather the most recent available information at the time of the review.  

# III. PRELIMINARIES  

# A. Dataset  

Several datasets have been utilized in various ASR tasks in the existing literature. Table I provides a compilation of some of these datasets that have been used specifically for BERT and CTC-based ASR applications, along with their specific characteristics [3].  

# B. Metrics  

The ASR research community has employed several methods to evaluate the quality and generalizability of ASR techniques. Besides the famous metrics that are commonly employed in ML and DL, such as accuracy, F1-score, recall, and precision [24], other metrics are used including word error rate (WER) and character error rate (CER), and realtime factor (RTF), are thoroughly described in [3].  

# IV. BERT-BASED ASR  

BERT, developed by Devlin et al. in 2019 [25], is a pre-training model for NLP tasks that utilizes transformer encoders [26]. It consists of two phases: pre-training for language understanding and fine-tuning. These latter are for specific tasks like sentiment analysis, question answering, text summarization, and more. During pre-training, BERT employs masked language modeling (MLM) and next-sentence prediction (NSP). MLM involves masking some words in sentences and reconstructing them using the surrounding context during training. NSP helps BERT understand the relationship between two sentences by predicting if the second sentence follows the first. BERT was trained on 16GB of text data from the books corpus datasets and English Wikipedia. After pre-training, the model is fine-tuned for a specific task by replacing BERT’s output layers. This fine-tuning process is faster since only the model parameters, excluding the output parameters, are learned from scratch. There are two versions of BERT: BERT-base and BERT-large. BERT-base consists of 12 transformer encoder blocks with 12-head self-attention layers and 768 hidden layers, resulting in approximately 110 million parameters. BERT-large has 24 transformer encoder blocks with 24-head self-attention layers and around 340 million parameters. BERT-large achieves higher accuracies but requires more computational resources compared to BERTbase [4]. However, BERT has a few notable limitations. Firstly, it is primarily designed for monolingual classifications, meaning its optimal performance is achieved when working with a single language. While it is possible to fine-tune BERT for multilingual tasks, its effectiveness may be somewhat diminished compared to its performance on monolingual tasks. Secondly, the length of input sentences can also present challenges. BERT has a maximum token limit, typically set at 512 tokens, which means longer sentences need to be truncated or split into smaller segments, potentially losing some contextual information [27]. Table II presents a summary of the performance achieved in the cited papers compared to other systems, including the metrics used to evaluate the results, and the source code availability. Figure 1 illustrates BERT variants featuring transformers along with the attention layers they are built upon.  

# A. BERT-based SMCQA framework  

The authors in [8] developed a framework called MABERT for spoken multiple-choice question answering (SMCQA) task, which uses a combination of multi-turn audioextractor hierarchical CNNs (MA-HCNNs) and BERT to extract acoustic-level and text-level information, respectively, from speech data. The proposed framework outperformed various state-of-the-art systems. However, the scheme in [9] proposes a novel audio-enriched BERT-based (aeBERT) framework for improving performance on the SMCQA task, where syllables, questions, and choices are all given in speech. Besides, the method proposes incorporating acoustic-level information from the speech input to enhance the accuracy of SMCQA systems. The resulting audio-enriched BERT-based SMCQA framework shown to outperform various state-of-theart systems by a large margin.  

TABLE I A LIST OF PUBLICLY AVAILABLE DATASETS COMMONLY USED IN TRANSFORMERS BERT AND CTC-BASED ASR RESEARCH:   


<html><body><table><tr><td>Dataset</td><td>Used by</td><td>Description</td></tr><tr><td>FGC</td><td>[8], [9]</td><td>Is acollectionof data specificallfocusedon thetaskof spoken multiple-choice question answering in Mandarin Chinese. This dataset was created for the formosa grand challenge (FGC) competition held in 2018.</td></tr><tr><td>AMI database</td><td>[10]</td><td>Isa widelyusedand publiclyavailable dataset in thefeldofmultimodal interactionresearch.Researchersusethe augmented multiparty interaction (AMI) database for taskssuchasspeech recognition,speaker diarization,languageunderstanding, dialogue systems,and multimodal analysis.</td></tr><tr><td>CNNDM,How2, TED</td><td>[11]</td><td>The CNNDMdataset isa large-scale dataset primarily used for text summarization tasks.Itconsists of news articles collected fromthe websitesofcablenews networks,anddailymail,theHow2datasetisdesignedforthetaskofinstructional videocaptioningandtranslation.Specifically,itprovidestextualdescriptionsof"how-to”videos,whichareinstructional videos demonstrating various tasks and activities.The TEDcorpus is acollction of summarized TED talks;the corpus</td></tr><tr><td>DSTC2</td><td>[12]</td><td>was constructed by linking TED talks from the TEDLIUM corpus with their corresponding summaries. The dialog state tracking challenge 2(DSTC2)dataset is a widelyused benchmark dataset in the field of dialog systems</td></tr><tr><td>IWSLT2011</td><td>[13], [14]</td><td>and spoken language understanding. Servesasabenchmark forthetask of spoken language translation,dataset containsapproximately 25 hoursof recorded</td></tr><tr><td>FSC,ATIS</td><td>[15]</td><td>speech.This duration includes speech data in multiple languages,such as English,French,Spanish,and German. Theframe-semantic corpus (FSC)and airline travel information system (ATIS)datasets are both widely used in the field of natural language procesing (NLP)and dialoguesystems.The total durationof theFSC dataset isapproximately130</td></tr><tr><td>Fisher-CallHome Spanish</td><td>[16]</td><td>hours. It isacombinationof two data corpora:Fisherand CallHome.TheFisher corpus consistsof multilingual telephone recordings and was originallycollected forresearch in ASR.The CallHome corpus isa dataset of telephone conversations</td></tr><tr><td>LibriSpeech</td><td>[17]</td><td>in different languages,it consists of approximately 3Oo hours of recorded speech. Itconsistsof speech data from audiobooksavailable in the LibriVox project.The dataset comprises approximately1,000 hoursof audio recordings,with asampling rate of16 kHz.It is acollectionof high-quality speech dataobtained by</td></tr><tr><td>Aishell1</td><td>[17]-[19]</td><td>sampling audiobooks in the project. It comprises a large collction of Mandarin Chinese speech recordings from multiple speakers. Itcontains approximately 178 hours of speech data from around 4OO speakers,covering a wide range of accents,ages,and genders.</td></tr><tr><td>WSJ</td><td>[19]-[22]</td><td>Typicallyrefers tothe Wallstreet journal dataset,which isacommonlyused benchmarkdatasetinNLPand information retrieval research,contained around 8O hours of transcribed speech.</td></tr><tr><td>CSJ</td><td>[20], [23]</td><td>Typicallyrefers to the corpus of spontaneous Japanese.TheCSJdataset contained approximately 570 hours of recorded</td></tr><tr><td>TEDLIUM2</td><td>[19], [23]</td><td>speech. Is a widely used benchmark dataset in the field of ASR,created by the spoken language systems (SLS）group at the</td></tr><tr><td>DCASE</td><td>[22]</td><td>University of Cambridge. It includes binaural recordings captured in15diferent soundenvironmentsorsetings.These setings representdistinct acoustic scenes and cover a variety of audio environments.</td></tr></table></body></html>  

# B. BERT-based reranking framework  

Chiu et al. [10] propose a BERT n-best reranking framework that incorporates cross-utterance information signals using a graph convolutional network (GCN) to model historical utterances for better ASR performance. The approach addresses the limitations of recurrent neural network (RNN) and LSTMbased language models (LMs) in capturing complex global structural dependencies among utterances. Nevertheless, the study in [28] introduces a new implementation of BERT-based contextualized language models specifically for reranking the n-best hypotheses generated by ASR systems. The approach frames the n-best hypothesis reranking as a prediction problem, aiming to predict the oracle hypothesis with the lowest WER.  

![](images/ea664a62ab6bddd5bd6a090c267e2db28f351c10c51b3a600f62b50838611640.jpg)  
Fig. 1. Types of BERT with transformer encoder layers.  

# C. BERT-based model for speech summarization  

Kano et al. [11] suggest a novel text summarization (TS) method that combines sub-word embedding vectors and posterior values from an ASR system. They incorporate an attention-based fusion module into a pre-trained BERT module for improved summarization. This fusion module aligns and merges multiple ASR hypotheses. The researchers then perform experiments on speech summarization using both the How2 and TED dataset. In [29] The authors of the paper enhance a BERT-based model for speech summarization in three ways: incorporating confidence scores into sentence representations to address ASR errors, augmenting sentence embeddings with additional features, and validating the model’s effectiveness on a benchmark dataset compared to classic summarization methods. The goal is to improve the model’s performance and overcome challenges caused by imperfect ASR.  

# D. BERT-based model for distilling the knowledge  

Futami et al. [20] propose a method to improve ASR using a combination of a (sequence-to-sequence) seq2seq model and BERT as an external language model. The seq2seq model is enhanced with both left and right context through knowledge distillation from BERT which generates soft labels to guide the training. Additionally, context beyond the current utterance is leveraged as input to BERT. The proposed method is evaluated on the CSJ, showing significant improvements in ASR performance compared to the seq2seq baseline. This method surpasses alternative approaches in LM applications like n-best rescoring and shallow fusion with not requiring any additional inference cost. Jiang, B et al. [15] suggest a method for endto-end intent classification using speech, which does not rely on an intermediate ASR module. It leverages the transformer distillation method to transfer knowledge from a transformerbased language model BERT to a transformer-based speech model for intent classification. A multi-level transformer-based teacher-student model is designed, and knowledge distillation is performed across attention and hidden sub-layers of different transformer layers. The proposed method achieves a high level of accuracy in intent classification and showcases superior performance and resilience in acoustically degraded conditions when compared to the baseline method.  

# E. BERT, RoBERTa, XLM-RoBERTa, and ELECTRA models  

Ganesan et al. [12] propose a method to improve the performance of spoken language understanding (SLU) systems by using concatenated n-best ASR alternatives as input to transformer models, such as BERT XLM-RoBERTa on DSTC2 dataset [30]. In their paper, Chen et al. [13] introduce a discriminative self-training method that incorporates weighted loss and discriminative label smoothing for improving punctuation prediction in ASR output transcripts, the authors utilize extensive unlabeled spoken language data, which lacks punctuation, such as transcripts employed for training ASR systems. They employ self-training techniques to enhance robust baseline models built on BERT, RoBERTa, and ELECTRA.  

# F. HuBERT and LightHuBERT models  

In their study [31], the authors introduce a novel speech pretraining method called ”HuBERT-AP.” This approach utilizes patterns derived from target codes as the training signal to facilitate the model in acquiring improved acoustic features. The patterns, referred to as ”acoustic pieces,” are constructed based on the sentence piece outcomes of the original HuBERT target codes, and are highly relevant to phonemized natural language, making them beneficial for audio-to-text tasks. The proposed method is evaluated on the LibriSpeech ASR task, and is shown to be significantly more effective than previous strong baselines. However, the authors in [32] propose LightHuBERT, a compressed version of the HuBERT model, which is a self-supervised speech representation learning model. LightHuBERT is designed as a once-for-all transformer compression framework. To automatically discover desired architectures through pruning structured parameters, the researchers create a transformer-based supernet that encompasses numerous weight-sharing subnets. They also employ a two-stage distillation strategy to leverage contextualized latent representations from HuBERT. Experimental results on ASR and the SUPERB benchmark demonstrate that LightHuBERT surpasses HuBERT in ASR tasks while reducing parameters by $2 9 \%$ . Furthermore, LightHuBERT achieves a compression ratio of 3.5 times in three SUPERB tasks, albeit with a slight loss in accuracy.  

# G. NorBERT and Speech-BERT models  

Rugayan et al. [33] propose a robust evaluation metric, aligned semantic distance (ASD), for Norwegian ASR systems. They leverage semantic information modeled by a transformer-based LM and employ dynamic programming techniques to measure the similarity between reference and hypothesis text. ASD utilizes NorBERT embeddings to compute the optimal alignment and obtain the minimum global distance. This distance is then normalized by the length of the reference embedding vector. Additionally, the researchers present results using another metric called semantic distance (SemDist), and they compare the performance of ASD with SemDist. The authors in [34] introduced a neural model called speech-BERT, which combines a bidirectional transformer LM with a neural zero-inflated beta regression approach. This approach is specifically designed to be conditioned on speech features. To fine-tune speech-BERT, the authors utilized a pre-training strategy known as token-level masked language modeling. Additionally, they incorporated a zero-inflated layer into the model to effectively handle the mixture of discrete and continuous outputs.  

# H. BERT-based language models  

Chang et al. [35] introduce an innovative network called the context-aware transformer transducer (CATT), which enhances the performance of transformer-based ASR systems by leveraging contextual signals. The authors propose a contextbiasing network based on multi-head attention, which is trained alongside other sub-networks of the ASR system.  

Various techniques are explored to encode contextual data and generate the ultimate attention context vectors. To encode the contextual information and facilitate network training, both BLSTM and pre-trained BERT models are utilized. The researchers in [36] propose two deep neural network (DNN) models to improve ASR by modeling long-term semantic relations. They employed as input features to their DNN model two things: (i) dynamic contextual embeddings are derived from BERT, a transformer-based model specifically designed for acoustic tasks. (ii) Additionally, linguistic features are incorporated into the system. Moving forward, the scheme proposed in [37] discusses the linguistic diversity in India and the need for speech recognition in regional languages. The paper suggests the creation of an advanced ASR system based on deep sequence modeling, aiming to address the challenges posed by low-resource languages. The proposed model incorporates an enhanced spell corrector component. The performance of the proposed system is assessed using metrics such as WER and sequence match ratio. Notably, the experimental results demonstrate promising outcomes, with an average WER of 0.62. The latter result proves the importance of spell correction in ASR systems and the use of a transformer-based LM for performance improvement.  

# V. CTC-BASED ASR  

CTC is a variant of the transformer architecture that is used in seq-2seq learning tasks, particularly in ASR. The CTC transformer combines the concepts of the CTC loss function and the transformer architecture, which are both powerful tools for sequence modeling. The CTC loss function is commonly used in ASR to align the predicted sequence with the ground truth sequence by taking into account the presence of blank symbols and repeated characters. In the following, a brief summary of the proposed approaches-based CTC transformer. Table II presents a summary of the performance achieved in the cited papers compared to others systems, including the metrics used to evaluate the results, and the source code availability. Figure 2 shows a CTC variation, elucidating its intended purpose and objectives.  

# A. Mask CTC  

The proposed method, detailed in [21], consists of a novel non-autoregressive end-to-end ASR called mask CTC. This framework generates a sequence by refining the outputs of the CTC model, which is a popular method used for ASR, while autoregressive models generate one token at a time and require as many iterations as the output length. Nonautoregressive models offer the advantage of generating tokens simultaneously in a fixed number of iterations, resulting in substantial reductions in inference time. The mask CTC model employs a training methodology that combines a transformer encoder-decoder architecture with simultaneous training of mask prediction and CTC during inference. Initially, the target sequence is initialized with the greedy CTC outputs. Afterward, tokens with low confidence are selectively masked using the CTC predictions. By taking into account the conditional interdependence among output tokens, the model predicts the masked low-confidence tokens using the high-confidence tokens.  

![](images/01378f6b3e4c3d538b2332a9774c8077efda084158a5bb3cc2a9a901016b3388.jpg)  
Fig. 2. Purpose and objective of CTC.  

# B. NAR CTC  

Inaguma et al in [16] propose a faster version of the multidecoder (MD) end-to-end speech translation model called Fast-MD. The MD model decomposes the overall speech translation task into ASR and machine translation sub-tasks, but its decoding speed is not fast enough for real-world applications. Fast-MD generates hidden intermediates (HI) by NAR decoding based on CTC outputs followed by an ASR decoder. The scheme employs sampling CTC outputs during training to reduce a mismatch in the ASR decoder. The authors also suggest that adopting the conformer encoder and intermediate CTC loss can further boost the model’s quality without sacrificing decoding speed. Song et al. [18] propose a solution to the accuracy degradation problem faced by NAR transformer models in ASR. The proposed solution is a CTC-enhanced NAR transformer that refines the predictions of the CTC module to generate the target sequence. The paper [17] presents improvements to the end-to-end CTC alignmentbased single-step non-autoregressive transformer (CASS-NAT) for speech recognition. The proposed methods include applying convolution augmented self-attention blocks to the encoder and decoder modules, expanding the trigger mask for each token to increase CTC alignment robustness, and using iterated loss functions to enhance gradient updates. Fujita et al. in [23] proposed a method for non-autoregressive ASR streaming input or long recording. They used an insertion-based model that jointly trained CTC and achieved better accuracy with fewer iterations using transformer with greedy decoding. The authors suggested combining audio segmentation and nonautoregressive ASR into a single neural network. This integration leverages the CTC component of the insertion-based model, utilizing causal self-attention implemented through block self-attention, similar to the transformer XL. Experimental outcomes demonstrated that the proposed approach achieved a favorable trade-off between accuracy and RTF when compared to both the autoregressive transformer and CTC baseline models.  

# C. Auxiliary CTC and End-to-end CTC  

The method introduced by the authors in [19] offers a means to enhance CTC-based ASR models by loosening the assumption of conditional independence in CTC. The method involves training a CTC-based ASR model with auxiliary CTC losses in intermediate layers. Predictions from these layers are accumulated and conditioned on in subsequent layers, resulting in improved performance compared to a standard CTC model across multiple ASR corpora. Furthermore, the proposed method achieves comparable performance to a strong autoregressive model with beam search on the TEDLIUM2 corpus and the AISHELL-1 corpus, while being at least 30 times faster in decoding speed. Andrusenko et al. in [38] explore different end-to-end ASR systems for the largest open-source Russian language data set – OpenSTT. They compare existing end-to-end approaches, including joint CTC/Attention, RNNtransducer, and transformer, with a strong hybrid ASR system based on the so-called LF-MMI and TDNN-F acoustic model. Is the performance of each system is evaluated on three available validation sets, including phone calls, YouTube, and books. The paper [39] adapted an end-to-end transformer acoustic model to the speech of children learning to read, with the aim of enhancing ASR performance for this challenging task. They used transfer learning with a small amount of child speech and multi-objective training with a CTC function. They also proposed a method of data augmentation for reading mistakes, where they simulated word-level repetitions and substitutions with phonetically or graphically close words. The authors analyzed the performance of their model and showed that both the CTC multi-objective training and the data augmentation with synthetic repetitions assisted the attention mechanisms better identify children’s disfluencies.  

# D. CTC-Based Other Approaches  

In their work [14], Chen et al. introduce the controllable time-delay transformer (CT-Transformer) model, which addresses both punctuation prediction and disfluency detection tasks in real-time. These tasks are crucial for enhancing transcript readability and enabling subsequent applications. The CT-Transformer model incorporates a mechanism for selectively freezing partial outputs with adjustable time delays to meet the real-time constraints imposed by downstream applications. Experimental results demonstrate that the proposed approach surpasses previous state-of-the-art models in terms of F-scores, while also achieving competitive inference speed on benchmark datasets such as IWSLT2011 [40] and an inhouse Chinese annotated dataset. Moritz et al. in [22] describe the development and implementation of an ”all-in-one” (AIO) acoustic model based on the transformer architecture. The AIO model is designed to simultaneously solve the problems of ASR audio tagging (AT), and acoustic event detection (AED), using shared parameters across all tasks. The authors argue that this approach more closely mimics the way the human auditory system processes sound signals from different sources. The integration of the transformer model with CTC enables the enforcement of monotonic ordering and the utilization of timing information for both ASR and AED tasks. The AIO transformer model consistently outperforms all baseline systems in recent DCASE challenge tasks, showcasing its aptness for comprehensive transcription of acoustic scenes, encompassing speech recognition and identification of other acoustic events. Xiao et al. in [41] propose a new framework for an automatic voice query service AVQS to improve the accuracy of response for multi-accented Mandarin users. The problem addressed is that many dialect areas in China make it necessary for the AVQS to respond to users with a single acoustic model in ASR, limiting its accuracy. The proposed framework uses a fusion feature comprising i-vector and filterbank acoustic features to train a transformer-CTC, which is then used to construct an end-to-end ASR. Additionally, a keyword-matching algorithm based on fuzzy mathematics theory is proposed to further enhance the accuracy of the response.  

# VI. FUTURE DIRECTIONS  

# A. BERT and ChatGPT  

Improved contextual coherence: By combining ChatGPT ability to generate human-like responses with BERT strong contextual understanding, the integration enhances the coherence and relevance of the chat responses by leveraging BERT knowledge of bidirectional dependencies in text.   
Enhanced language comprehension: BERT extensive pretraining on a large corpus enables it to understand language nuances effectively. When integrated with ChatGPT, it improves the model’s language understanding capabilities, enabling it to comprehend user inputs, handle complex queries, and provide more accurate and context-aware responses.   
• Effective handling of ambiguity and multiple meanings: ChatGPT can sometimes struggle with phrases that have multiple interpretations. By incorporating BERT contextual representation, which considers the surrounding context, the integrated model becomes better at disambiguating such phrases and generating responses that are more accurate and appropriate in context.  

# B. CTC and ChatGPT  

• Enhanced language generation: By integrating ChatGPT into CTC, the speech output generated becomes more natural and engaging due to ChatGPT proficiency in generating human-like responses.  

TABLE II LIST OF THE SURVEYED STATE-OF-THE-ART STUDIES WITH THEIR ADVANTAGES AND DISADVANTAGES.   


<html><body><table><tr><td>Ref.</td><td>Year</td><td>Category</td><td>Compared to</td><td>Metric</td><td>Code avail.?</td><td>Result/Improvement obtained /Comments /Advantages and/or disadvantages</td></tr><tr><td>[8]</td><td>2020</td><td>MA-BERT</td><td>BERT-RNN</td><td>accuracy</td><td>No</td><td>80.34%,improvement of 2.5%,the proposed MA-BERT It isan ideal framework for leveraging both acoustic-level and text-level features in the SMCQA task.</td></tr><tr><td>[10]</td><td>2021</td><td>GCN(10)</td><td>HPBERT(10)+ HPBERT(10)</td><td>WER</td><td>No</td><td>16.13%,reduction over O.14%,the global information captured by the GCN enhances the representation of historical utterances,leading to improved reranking performance.</td></tr><tr><td>[12]</td><td>2021</td><td>n-Best-ASR BERT</td><td>WCN-BERT STC</td><td>F1-scores</td><td>No</td><td>87.80 %,improvement of 1.6%,the N-Best ASR Transformer offers improved perfor- mance over baselines,excels in low data regimes,and provides accessibility to users of</td></tr><tr><td>[13]</td><td>2021</td><td>RoBERTa- wwm- base+Disc-</td><td>RoBERTa- wwm-base</td><td>F1-scores</td><td>Yes3</td><td>third-party ASR APIs. 60.2%,Discriminative Self-Training improves F1 from 59.6 to 60.2 (+0.6),this approach significant improvement on punctuation prediction over strong baselines including RoBERTa models.</td></tr><tr><td>[15]</td><td>2021</td><td>STD-BERT</td><td>Baseline-2</td><td>accuracy</td><td>Yes2</td><td>99.10%,improvement of O.23%,the experimental results show improved accuracy after incorporation of transformer-based knowledge distillation.</td></tr><tr><td>[17]</td><td>2021</td><td>Improved CASS-NAT</td><td>Conformer AT</td><td>RTF</td><td>No</td><td>0.018,RTF degradation (from O.081 to O.018),This suggests that the enhanced CTC alignment-based CASS-NAT achieves comparable performance to AT.</td></tr><tr><td>[18]</td><td>2021</td><td>NAR- Transformer</td><td>AR- Transformer</td><td>RTF</td><td>Yes4</td><td>0.0037,results show a Non-autoregressive Trans- former with CTC-enhanced decoder achieve 5Ox faster decoding speed than a strong AR baselin.</td></tr><tr><td>[21]</td><td>2020</td><td>Mask CTC</td><td>Non- autoregressive CTC</td><td>CER</td><td>Yes5</td><td>4.96%,a reduction over 0.53%,the experimental comparisons demonstrated that Mask CTC outperformed the standard CTC model while maintaining the decoding speed fast.</td></tr><tr><td>[22]</td><td>2020</td><td>AIO Trans- former</td><td>Baseline sys- tem</td><td>F1-scores</td><td>No</td><td>51.4%,experiments demonstrate that the AIO Transformer achieves better performance compared to all baseline systems of various re- cent DCASE challenge tasks.</td></tr><tr><td>[23]</td><td>2020</td><td>KERMIT- Integrated CTC</td><td>ART- Integrated CTC</td><td>RTF</td><td>No</td><td>0.38,RTF degradation (from 1.15 to 0.38),The results indicate that the method suc- cessfully achieved a reasonable balance between RTF and performance when compared to the baseline autoregressive Transformer and connectionist temporal classification</td></tr><tr><td>[28]</td><td>2021</td><td>TPBERT</td><td>PBERT</td><td>WER</td><td>No</td><td>approaches. 20.49%,reduction over O.78%,the advantages of TPBERT lie in its effective use of BERT-based ASR N-best hypothesis.</td></tr><tr><td>[32]</td><td>2022</td><td></td><td>LightHuBERTDistilHuBERT PER</td><td></td><td>Yes1</td><td>4.71%,reduction over 11.56%.This demonstrates the superior performance of LightHu- BERT compared to DistilHuBERT.</td></tr><tr><td>[36]</td><td>2021</td><td>BERTalsem and GPT-2</td><td>Baseline sys- tem</td><td>WER</td><td>No</td><td>34.4%,reduction of 2.7%,the optimal outcomes are attained by combining rescoring techniques that utilize BERT and GPT-2 scores.</td></tr><tr><td>[38]</td><td>2020</td><td>scores TDNN-F- LF-MMI</td><td>CTC- Attention</td><td>WER</td><td>Yes5</td><td>33.5%,reduction over 5.4%,The hybrid model continues to outperform end-to-end systems in terms of performance on phone call validation.By incorporating an external NNLM for hypotheses rescoring within the hybrid system,a reduction in WER is</td></tr><tr><td>[39]</td><td>2021</td><td>Transformer +CTC +Sub+Rep</td><td>Transformer (baseline)</td><td>PER</td><td>No</td><td>achieved across all validation sets. 19.90%,a reduction over 3%,A comprehensive analysis demonstrates that both the multi-objective training with CTC and the augmentation using synthetic repetitions effectively enhance the ability of attention mechanisms to detect disfluencies in</td></tr><tr><td>[41]</td><td>2021</td><td>Transformer- CTC</td><td>BLSTM- CTC</td><td>SER</td><td>No</td><td>children's speech. 65.05%,reduction of 13.1%,the proposed framework can effectively improve the whole response accuracy of AVQS for heavy accented Mandarin speech.</td></tr></table></body></html>  

Context-aware speech Generation: Incorporating ChatGPT ability to understand contextual cues into CTC enables the model to generate speech that is more coherent and relevant by considering the surrounding context.   
Versatile text-to-speech applications: CTC is widely used in text-to-speech systems. Integrating ChatGPT with CTC expands the capabilities of TTS applications, making them more flexible and adaptable. This allows for interactive and dynamic speech generation by leveraging ChatGPT conversational capabilities.   
Enhanced personalization and user interaction: ChatGPT excels in personalized conversations, and when combined with CTC, it enables the integrated model to generate speech that adapts to user preferences. This results in more interactive and engaging interactions, leading to a highly personalized user experience.  

# VII. CONCLUSION  

Transformers play a crucial role in ASR by capturing contextual information and long-range dependencies. They improve accuracy by considering the entire context and utilizing attention mechanisms to focus on relevant information. Pretrained models like BERT, RoBERTa, and ELECTRA have proven effective in transfer learning for ASR, benefiting from knowledge acquired on large-scale datasets. Additionally, the CTC method enables end-to-end training, handles variablelength inputs, incorporates label-smoothing regularization, integrates with language models, and supports online and streaming ASR applications. CTC is a flexible and effective approach for transcribing speech signals, contributing to robust and accurate ASR systems applied to diverse domains, such as biomedical [42].  

In this survey, the idea of incorporating ChatGPT into the BERT and CTC frameworks is proposed, opening new avenues for research and development. By leveraging ChatGPT’s conversational abilities and natural language understanding, it is suggested to enhance BERT and CTC capabilities. This integration aims to improve ASR performance, accuracy, and contextual understanding, leading to advanced speech recognition applications.  

# REFERENCES  

[1] H. Kheddar, M. Bouzid, and D. Megı´as, “Pitch and fourier magnitude based steganography for hiding 2.4 kbps melp bitstream,” IET Signal Processing, vol. 13, no. 3, pp. 396–407, 2019. [2] H. Kheddar and D. Megı´as, “High capacity speech steganography for the $\mathrm { g } 7 2 3$ . 1 coder based on quantised line spectral pairs interpolation and cnn auto-encoding,” Applied Intelligence, pp. 1–19, 2022. [3] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, and F. Bensaali, “Deep transfer learning for automatic speech recognition: Towards better generalization,” Knowledge-Based Systems, vol. 277, p. 110851, 2023. [4] A. A. Francisca, N.-M. Henry, and C. h. Wenyu, “Transformer models for text-based emotion detection: a review of bert-based approaches,” Artificial Intelligence Review, vol. 54, p. 5789–5829, 2021. [5] G. Benyamin and G. Ali, “Attention mechanism, transformers, bert, and gpt: Tutorial and survey,” OSF preprint, 2020. [6] K. Salman, N. Muzammal, H. Munawar, W. Z. Syed, and S. K. Fahad, “Transformers in vision: A survey,” ACM Computing Surveys, vol. 54, p. 1–41, 2022. [7] L. Siddique, Z. Aun, C. Heriberto, S. Fahad, S. Moazzam, and Q. Junaid, “Transformers in speech processing: A survey,” arXiv preprint arXiv:2303.11607, 2023. [8] S.-B. Luo, C.-C. Kuo, and K.-Y. Chen, “Spoken multiple-choice question answering using multi-turn audio-extracter bert,” in 2020 Asia-Pacific signal and information processing association annual summit and conference (APSIPA ASC). IEEE, 2020, pp. 386–392. [9] C.-C. Kuo, S.-B. Luo, and K.-Y. Chen, “An audio-enriched bertbased framework for spoken multiple-choice question answering,” arXiv preprint arXiv:2005.12142, 2020.   
[10] S.-H. Chiu, T.-H. Lo, F.-A. Chao, and B. Chen, “Cross-utterance reranking models with bert and graph convolutional networks for conversational speech recognition,” in 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2021, pp. 1104–1110.   
[11] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, “Attention-based multi-hypothesis fusion for speech summarization,” in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021, pp. 487–494.   
[12] K. Ganesan, P. Bamdev, A. Venugopal, A. Tushar et al., “N-best asr transformer: Enhancing slu performance using multiple asr hypotheses,” arXiv preprint arXiv:2106.06519, 2021.   
[13] Q. Chen, W. Wang, M. Chen, and Q. Zhang, “Discriminative selftraining for punctuation prediction,” arXiv preprint arXiv:2104.10339, 2021.   
[14] Q. Chen, M. Chen, B. Li, and W. Wang, “Controllable time-delay transformer for real-time punctuation prediction and disfluency detection,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8069–8073.   
[15] Y. Jiang, B. Sharma, M. Madhavi, and H. Li, “Knowledge distillation from bert transformer to speech transformer for intent classification,” arXiv preprint arXiv:2108.02598, 2021.   
[16] H. Inaguma, S. Dalmia, B. Yan, and S. Watanabe, “Fast-md: Fast multi-decoder end-to-end speech translation with non-autoregressive hidden intermediates,” in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021, pp. 922–929.   
[17] R. Fan, W. Chu, P. Chang, J. Xiao, and A. Alwan, “An improved single step non-autoregressive transformer for automatic speech recognition,” arXiv preprint arXiv:2106.09885, 2021.   
[18] X. Song, Z. Wu, Y. Huang, C. Weng, D. Su, and H. Meng, “Nonautoregressive transformer asr with ctc-enhanced decoder input,” in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5894–5898.   
[19] J. Nozaki and T. Komatsu, “Relaxing the conditional independence assumption of ctc-based asr by conditioning on intermediate predictions,” arXiv preprint arXiv:2104.02724, 2021.   
[20] H. Futami, H. Inaguma, S. Ueno, M. Mimura, S. Sakai, and T. Kawahara, “Distilling the knowledge of bert for sequence-to-sequence asr,” arXiv preprint arXiv:2008.03822, 2020.   
[21] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, “Mask ctc: Non-autoregressive end-to-end asr with ctc and mask predict,” arXiv preprint arXiv:2005.08700, 2020.   
[22] N. Moritz, G. Wichern, T. Hori, and J. Le Roux, “All-in-one transformer: Unifying speech recognition, audio tagging, and event detection.” in INTERSPEECH, 2020, pp. 3112–3116.   
[23] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, “Toward streaming asr with non-autoregressive insertion-based model,” arXiv preprint arXiv:2012.10128, 2020.   
[24] H. Kheddar, M. Hemis, Y. Himeur, D. Megı´as, and A. Amira, “Deep learning for steganalysis of diverse data types: A review of methods, taxonomy, challenges and future directions,” Neurocomputing, p. 127528, 2024.   
[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.   
[26] H. Kheddar, “Transformers and large language models for efficient intrusion detection systems: A comprehensive survey,” arXiv preprint arXiv:2408.07583, 2024.   
[27] A. C. Mazari, N. Boudoukhani, and A. Djeffal, “Bert-based ensemble learning for multi-aspect hate speech detection,” Cluster Computing, pp. 1–15, 2023.   
[28] S.-H. Chiu and B. Chen, “Innovative bert-based reranking language models for speech recognition,” in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 266–271.   
[29] S.-Y. Weng, T.-H. Lo, and B. Chen, “An effective contextual language modeling framework for speech summarization with augmented features,” in 2020 28th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 316–320.   
[30] M. Henderson, B. Thomson, and J. D. Williams, “The second dialog state tracking challenge,” in Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL), 2014, pp. 263–272.   
[31] S. Ren, S. Liu, Y. Wu, L. Zhou, and F. Wei, “Speech pre-training with acoustic piece,” arXiv preprint arXiv:2204.03240, 2022.   
[32] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, and H. Li, “Lighthubert: Lightweight and configurable speech representation learning with once-for-all hidden-unit bert,” arXiv preprint arXiv:2203.15610, 2022.   
[33] J. L. C. Rugayan, T. K. Svendsen, and G. Salvi, “Semantically meaningful metrics for norwegian asr systems,” 2022.   
[34] K. Fan, J. Wang, B. Li, S. Zhang, B. Chen, N. Ge, and Z. Yan, “Neural zero-inflated quality estimation model for automatic speech recognition system,” arXiv preprint arXiv:1910.01289, 2019.   
[35] F.-J. Chang, J. Liu, M. Radfar, A. Mouchtaris, M. Omologo, A. Rastrow, and S. Kunzmann, “Context-aware transformer transducer for speech recognition,” in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021, pp. 503–510.   
[36] D. Fohr and I. Illina, “Bert-based semantic model for rescoring n-best speech recognition list,” in INTERSPEECH 2021, 2021.   
[37] M. S. Priya, D. K. Renuka, L. A. Kumar, and S. L. Rose, “Multilingual low resource indian language speech recognition and spell correction using indic bert,” S¯adhan¯a, vol. 47, no. 4, p. 227, 2022.   
[38] A. Andrusenko, A. Laptev, and I. Medennikov, “Exploration of endto-end asr for openstt–russian open speech-to-text dataset,” in Speech and Computer: 22nd International Conference, SPECOM 2020, St. Petersburg, Russia, October 7–9, 2020, Proceedings 22. Springer, 2020, pp. 35–44.   
[39] L. Gelin, T. Pellegrini, J. Pinquier, and M. Daniel, “Simulating reading mistakes for child speech transformer-based phone recognition,” in Annual Conference of the International Speech Communication Association (INTERSPEECH), 2021.   
[40] X. Che, C. Wang, H. Yang, and C. Meinel, “Punctuation prediction for unsegmented transcript based on word vector,” in Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 2016, pp. 654–658.   
[41] K. Xiao and Z. Qian, “Automatic voice query service for multi-accented mandarin speech,” in 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE, 2021, pp. 2875–2881.   
[42] B. Essaid, H. Kheddar, N. Batel, M. E. Chowdhury, and A. Lakas, “Artificial intelligence for cochlear implants: Review of strategies, challenges, and perspectives,” IEEE Access, 2024.  