# 5/1/2025, 6:22:14 PM_Next Token Prediction in Multimodal Intelligence  

# 0. Next Token Prediction in Multimodal Intelligence  

# 1. Introduction  

Multimodal Intelligence (MMI) refers to the capability of artificial intelligence systems to process, understand, and integrate information from multiple modalities, such as text, images, audio, and video, analogous to human cognitive abilities [5]. A promising paradigm for achieving comprehensive MMI is Next Token Prediction (NTP), defined as the task of predicting the subsequent token in a sequence given the preceding tokens [11]. While initially successful in unimodal domains like natural language processing (NLP) through language modeling [13,27], recent advancements suggest that NTP can serve as a simple, scalable, and unifying approach for multimodal systems, potentially paving the way toward Artificial General Intelligence (AGI) [1,6,9,11,29,30,32]. This perspective aligns with prominent views suggesting that proficient next-token prediction might be sufficient for achieving AGI [1].​  

The evolution of NTP techniques traces back to early unimodal sequence prediction models, culminating in the development of large language models (LLMs) built upon the Transformer architecture. The Transformer's ability to model long-range dependencies efficiently revolutionized sequence processing [10], making NTP a powerful objective for learning complex data distributions. The transition to multimodal NTP involves extending this autoregressive framework to sequences comprising tokens from diverse modalities. This is typically achieved by converting multimodal information into a unified discrete token space, allowing a single Transformer model to process and generate sequences across modalities [11,13]. This paradigm shift is largely driven by the remarkable success and advancements observed in LLMs [2].  

Traditional approaches to multimodal tasks often relied on specialized models or combination methods, such as diffusion models for generation or integrating separate visual encoders (like CLIP) with LLMs [1,30]. While effective for specific tasks, these methods can lack the versatility and coherence needed for general multimodal understanding and generation. In contrast, the unified NTP framework offers significant advantages by treating different modalities and tasks (both understanding and generation) under a single modeling objective [2,9,13]. This unification simplifies model architecture, potentially reduces computational complexity, and enhances efficiency in handling large multimodal datasets [11]. Recent models like Emu3 and MIO demonstrate the effectiveness of solely relying on NTP for diverse multimodal tasks, challenging the necessity of traditional specialized methods [1,29]. This contrasts with alternative paradigms, such as concept-level modeling, which propose moving beyond token prediction to address limitations in long-context and multimodal processing [3].​  

Despite the promise, applying NTP to multimodal intelligence presents significant challenges and opportunities. These include effectively handling heterogeneous and potentially noisy data from various sources, capturing complex long-range dependencies across modalities, and addressing practical concerns such as data privacy and personalization needs in diverse application scenarios [31]. Furthermore, the fundamental training methodology of NTP, particularly teacher forcing, faces scrutiny regarding its limitations in capturing the underlying reasoning and planning capabilities essential for complex, human-like intelligence, raising questions about its ultimate sufficiency for AGI in intricate multimodal environments [8].​  

This survey provides a comprehensive overview of the state of the art in Next Token Prediction for Multimodal Intelligence.  

![](images/e989d309b426681ed8d0f09141cce1cae8051cdd652b3ce201b79ad9df98f5b8.jpg)  

We structure the discussion by first exploring multimodal tokenization techniques, followed by an analysis of model architectures, unified task representations, and evaluation methodologies [2,9]. We then delve into existing challenges and opportunities in the field and highlight promising future research directions.  

# 2. Background on Next Token Prediction  

Next Token Prediction (NTP) represents a fundamental task in sequence modeling, formally defined as predicting the probability distribution over the next token, given a sequence of preceding tokens [7,11]. Mathematically, this can be expressed as predicting  

$$
P ( t _ { i } \mid t _ { 1 } , t _ { 2 } , . . . , t _ { i - 1 } )
$$  

where $t _ { i }$ is the token at position $\mathbf { \chi } _ { i }$ and $t _ { 1 } , t _ { 2 } , \ldots , t _ { i - 1 }$ ​ is the historical sequence [7]. The training objective typically involves minimizing the difference between the predicted probability distribution and the actual next token using the crossentropy loss function [11].  

The concept's roots can be traced back to foundational work in information theory, such as Shannon's studies [11]. Its practical application in language modeling evolved significantly with the advent of neural networks. Early approaches often utilized Recurrent Neural Networks (RNNs) to process sequences token by token, capturing dependencies in a sequential manner [33]. However, a major breakthrough arrived with the introduction of the Transformer architecture, which leverages attention mechanisms, particularly self-attention, to process tokens in parallel and capture long-range dependencies more effectively than traditional RNNs [21,33,35,36]. The Transformer architecture became the backbone of modern large language models (LLMs), enabling unprecedented scale and performance in NTP tasks [21,22,32,39]. Tokens, the smallest units of text processing, are crucial for computational efficiency and generation quality [7,35].​  

<html><body><table><tr><td>Feature</td><td>Autoregressive Models</td><td>Autoencoding Models</td></tr><tr><td>Architecture</td><td>Decoder-only Transformer</td><td>Encoder-only Transformer</td></tr><tr><td>Attention Type</td><td>Causal (unidirectional)</td><td>Bidirectional</td></tr><tr><td>Processing</td><td>Sequential (predicts next token)</td><td>Simultaneous (processes entire sequence)</td></tr><tr><td>Primary Task</td><td>Generative (e.g., text synthesis)</td><td>Understanding (e.g., classification, NER)</td></tr><tr><td>Examples</td><td>GPT series,Transformer-XL, XLNet</td><td>BERT, ALBERT, RoBERTa, DistilBERT</td></tr><tr><td>Strengths</td><td>Excels at generation,learns global coherence</td><td>Deep contextual understanding of input</td></tr><tr><td>Weaknesses</td><td>Slower inference,potential erroraccumulation</td><td>Typically not used for generation</td></tr></table></body></html>  

Algorithms for NTP primarily fall into two categories based on their architectural approach: autoregressive and autoencoding models [33]. Autoregressive models, often based on the decoder-only Transformer architecture, predict the next token solely conditioned on the preceding sequence [7,21,33]. They utilize causal self-attention to enforce this unidirectional dependency, making them highly suitable for generative tasks like text synthesis, where tokens are produced one after another in an autoregressive manner [10,27,33]. Prominent examples include the GPT series [33]. Autoencoding models, conversely, typically employ the encoder-only Transformer architecture and process the entire input sequence bidirectionally [33,39]. Their training often involves reconstructing corrupted input, most notably through Masked Language Modeling (MLM), where masked tokens are predicted based on context from both sides [33,39]. Models like BERT and its variants (ALBERT, RoBERTa, DistilBERT) are key examples [33,36]. While autoregressive models excel at generation, autoencoding models are primarily used for tasks requiring deep contextual understanding of a fixed input, such as classification, named entity recognition, and question answering [21,33,39].​  

Evaluation of NTP models in unimodal contexts commonly utilizes metrics reflecting different aspects of prediction accuracy and generated text quality. Perplexity measures how well a probability model predicts a sample, serving as an intrinsic evaluation of the language model [11,39]. For generative tasks, extrinsic metrics like BLEU, ROUGE, and CIDEr are used to compare generated text against reference texts, evaluating aspects like n-gram overlap, recall, and image captioning quality, respectively [11,39].  

In unimodal contexts, NTP has demonstrated significant advantages due to its simplicity as a training objective and scalability with increased data and computational power, enabling models to learn complex language statistics and structures [10,11,32]. LLMs trained via NTP can develop deep semantic understanding and grammatical knowledge, moving beyond simple local patterns captured by models like Markov chains [10,16]. However, NTP also presents limitations. During autoregressive inference, the sequential nature can lead to error accumulation, where errors in predicting earlier tokens propagate and negatively affect subsequent predictions [8]. Furthermore, the reliance on teacher-forced training during optimization has been questioned for its sufficiency in developing accurate predictors capable of planning and lookahead, particularly in tasks requiring more than immediate pattern recognition [8]. NTP at the token level may struggle with tasks requiring understanding of long-range context or result in inconsistencies in generated output, suggesting limitations compared to higher-level human cognitive processes [3]. Despite these limitations, NTP remains a powerful paradigm that forms the basis for many modern language models and serves as a foundation for extending these capabilities to multimodal intelligence.  

# 2.1 Autoregressive Models  

Autoregressive models form a fundamental class of generative models in the context of next token prediction, particularly prevalent in large language models [22,27,32,33]. Their core principle lies in predicting the subsequent token based on the sequence of preceding tokens, treating the previous tokens as semantic context [21,33]. The training objective is typically to maximize the likelihood of predicting the next token conditioned on the history of tokens observed so far [21,33].  

These models are commonly architected as decoder-only Transformer models [7,33]. They utilize causal self-attention mechanisms, where attention heads are restricted by a mask to only attend to tokens that precede the current position in the sequence [7,33]. This architectural choice ensures that the prediction for a token at any given position is solely dependent on the preceding sequence, aligning with the autoregressive property [33]. During training, token losses for the next token can be predicted across all positions in a sentence simultaneously, leveraging this causal structure [7]. Autoregressive models are extensively pre-trained on vast amounts of data to capture statistical regularities and linguistic structures [27].​  

Several prominent autoregressive models based on the Transformer architecture have been developed. The original GPT, for instance, was an early autoregressive Transformer model pre-trained on the Book Corpus dataset [33]. GPT-2 represented a scaled-up version, trained on a larger and more diverse dataset, WebText, collected from influential Reddit pages [33]. Transformer-XL introduced a recursive mechanism designed to enhance the model's ability to capture longer-range dependencies than standard Transformer models [33]. Related models like XLNet also leverage autoregressive principles but employ a permuted training strategy where the model predicts tokens in a sentence using varying subsets of preceding tokens [33].​  

The autoregressive framework offers significant advantages, particularly in generative tasks such as text generation [27,33]. By predicting one token at a time based on the generated history, these models are capable of producing highly fluent and contextually relevant sequences [10]. They demonstrate an ability to learn global coherence in generated output, distinguishing them from simpler sequential models like Markov chains [10]. Furthermore, the structure of large language models, often based on autoregressive principles, facilitates seamless integration with techniques like instruction tuning and enables complex reasoning processes such as chain-of-thought [12]. However, a notable disadvantage of this sequential generation process is potentially slower inference speed compared to models that generate tokens in parallel or nonautoregressively [39]. While historically applied predominantly to text, autoregressive models are increasingly being adapted for multimodal intelligence, such as modeling sequences of text and video tokens to enable the generation of long videos from text prompts [30].​  

# 2.2 Autoencoding Models  

Autoencoding models are a class of pretraining methods for large language models that learn rich contextual representations by reconstructing corrupted input data. Unlike autoregressive models—which predict the next token based on preceding ones—autoencoding models process the entire input sequence simultaneously using bidirectional attention. They are typically based on the encoder architecture of the original Transformer model [33,39].  

The core pretraining task for autoencoding models involves corrupting the input tokens and training the model to reconstruct the original input or predict the identity of the corrupted tokens [33]. The most prominent corruption technique is Masked Language Modeling (MLM), where a percentage of input tokens are randomly masked [33,39]. For instance, in BERT, $1 5 \%$ of tokens are selected for masking. Of these, $8 0 \%$ are replaced with a special  token, $1 0 \%$ are replaced with a random token, and $1 0 \%$ remain unchanged [33]. The model is then trained to predict the original identity of the masked tokens. This objective encourages the model to learn deep bidirectional representations, leveraging context from both left and right sides of a token [33]. Beyond MLM, models like BERT also incorporated auxiliary tasks such as Next Sentence Prediction (NSP) to learn relationships between sentences [33].​  

Several notable autoencoding models have been developed, building upon or modifying the original BERT architecture. BERT serves as a foundational model, widely used in various downstream tasks including those requiring robust sentence embeddings, such as Siamese networks for web search [33,36,39]. ALBERT (A Lite BERT) introduced modifications to reduce memory consumption and increase training speed. It notably decoupled the embedding size from the hidden layer size and implemented cross-layer parameter sharing, where all layers share the same parameters [33]. ALBERT also replaced the NSP task with a Sentence Order Prediction (SOP) task, which proved more effective for sentence-level understanding [33]. RoBERTa (A Robustly Optimized BERT Approach) explored the impact of various training hyperparameters and data size. Key differences from BERT include training with dynamic masking (where the masking pattern changes across epochs), removing the NSP objective, training with significantly larger mini-batches, and using byte-level BPE as sub-word units [33]. DistilBERT is a smaller, faster version of BERT created through knowledge distillation, trained to replicate the output probabilities of a larger BERT model [33].​  

The primary strength of autoencoding models lies in their ability to capture deep, bidirectional contextual relationships within a sequence [33]. This makes them highly effective for tasks that require a comprehensive understanding of the entire input, such as sentence classification, token classification (e.g., named entity recognition), question answering, and sequence tagging [33,39]. However, a significant limitation of autoencoding models is that they are typically not used for generative tasks, such as text generation or sequence-to-sequence translation [39].  

# 3. Multimodal Intelligence: Challenges and Opportunities  

Multimodal intelligence (MMI) represents a significant advancement in artificial intelligence, aiming to create systems capable of understanding and processing information from multiple data types, including text, images, audio, and video [2,6,24]. Unlike unimodal AI, which is restricted to a single data source, MMI integrates diverse modalities to achieve improved accuracy and a deeper understanding of real-world contexts [6]. This integration enables AI systems to perceive, reason, and interact more effectively with the complex, multimodal world, leading to enhanced understanding, reasoning, and generation capabilities [6,24]. Modern large language models (LLMs) are increasingly incorporating multimodal capabilities, evolving beyond simple language sequence prediction to understand and generate diverse content types, such as generating images from text or descriptive text from images [10].  

Despite the compelling vision, integrating different modalities presents significant challenges [2,24,29,30]. These difficulties primarily stem from fundamental differences across modalities in data representation, processing techniques, temporal resolutions, and semantic interpretation [24,30]. Key technical hurdles include addressing data heterogeneity, achieving effective feature alignment, bridging the semantic gap between modalities, and establishing a unified representation space [6,24]. For instance, aligning visual features with linguistic descriptions for tasks like image captioning necessitates learning the complex semantic correspondence between images and text [5,15]. Similarly, integrating modalities with varying temporal resolutions, such as high-frame-rate video and slower-rate audio or sensor data, requires sophisticated synchronization and fusion mechanisms [17,24]. The creation of integrative AI systems that seamlessly coordinate diverse component technologies like computer vision, speech recognition, and NLP under latency constraints also highlights the need for new primitives for representing and reasoning about time, synchronization, and data fusion [17]. Even within a single modality category like language, variations such as those found in multilingual text classification [36] present modality-like challenges. Extending models to handle multilingual and multimodal applications also incurs significant computational expense and data requirements [3]. While recent trends explore unifying multimodal understanding and generation within paradigms like next token prediction (NTP), issues like modality diversity and conflict resolution remain unique challenges [2]. Furthermore, the inherent limitations of teacher-forced training in NTP models, particularly their struggle with complex planning and reasoning tasks, could be exacerbated in MMI scenarios that demand intricate crossmodal integration and inference [8].​  

Beyond the technical integration challenges, specific difficulties arise in applying MMI, particularly in distributed settings. Handling data privacy is a significant concern, especially when dealing with complex multimodal user interaction data, necessitating approaches like migrating large model inference to edge devices [30]. Achieving personalization while processing diverse multimodal data, including visual, auditory, and textual information from sources like the Internet of Things, requires specialized methods such as federated learning that balance data privacy with tailored model training [31].  

Overcoming these formidable challenges promises substantial rewards. By effectively integrating and interpreting information across modalities, AI systems can achieve improved contextual understanding, enabling applications in fields ranging from robotics and autonomous driving to sophisticated AI assistants [11]. The goal is to move towards arbitrary-toarbitrary understanding and generation capabilities, supporting complex multimodal interleaved sequences [29,30]. Ultimately, success in MMI will lead to AI systems that are more intelligent, efficient, flexible, and capable of interacting with the world in a manner closer to human perception and cognition [6,11].  

# 4. Multimodal Data Representation and Processing  

Processing and representing diverse data streams are foundational challenges in developing multimodal intelligence systems, particularly for Next Token Prediction (NTP) frameworks. Effectively enabling models to understand, process, and generate sequences comprising elements from modalities such as text, images, audio, and video necessitates specialized techniques for encoding, tokenization, alignment, and fusion [1,2,9,13].  

![](images/ee06d9adab714f5118c4206575216b8d69e9c5c998d35dddaa609fbeec7f0cc6.jpg)  

he primary objective is to transform raw, heterogeneous data into a unified format that preserves salient information while eing amenable to sequence modeling architectures, such as Transformers [13].  

A critical initial step involves representing data from each modality. Techniques are tailored to capture the unique characteristics of different data types. Text is typically encoded using embeddings (e.g., Word2Vec, GloVe, BERT) combined with positional information to retain sequential order [7]. Visual data, including images and video, is often processed using Convolutional Neural Networks (CNNs), Transformers, or pre-trained models like InceptionV3 to extract rich feature representations, capturing visual details and spatial-temporal relationships [15,18]. Similarly, audio and other sequential data employ methods like spectrograms, MFCCs, or specialized networks like 3D CNNs or LSTMs for feature extraction. More advanced approaches aim to map diverse modalities into unified, modality-agnostic embedding spaces designed to facilitate seamless integration [3,31].  

Following representation, multimodal tokenization converts these potentially continuous or complex representations into discrete or continuous token sequences suitable for NTP models [1,2,9,13]. Discrete tokenization involves quantizing the data into a finite set of tokens using techniques like Vector Quantization (VQ), often based on variants like VQ-VAE or VQ-GAN [2,9]. This approach simplifies the input space and aligns naturally with discrete prediction tasks but can suffer from information loss and codebook collapse [9]. Continuous tokenization, conversely, maintains the continuous nature of the data in an embedding space, potentially preserving finer details but posing challenges for direct discrete sequence modeling [2,9]. Training methods for these tokenizers include auto-encoding, denoising auto-encoding, supervised pretraining, and contrastive learning, each optimizing for different representation qualities [2,9].​  

Aligning information across modalities is crucial due to their differing structures, resolutions, and semantics. Fusion strategies are then employed to combine these aligned representations, enabling models to capture complex cross-modal interactions [15]. Common fusion approaches include early fusion, which merges data at the input or early processing stages, late fusion, which combines information at later stages or prediction levels, and attention-based methods (like crossmodal attention or co-attention), which dynamically weight contributions from different modalities [5,15]. While early fusion can capture fine-grained early interactions, it may increase computational complexity. Late fusion offers modularity but might miss crucial early cross-modal cues. Attention mechanisms provide flexibility and the ability to model complex relationships but can be computationally intensive. The Platform for Situated Intelligence highlights the importance of temporal alignment through time-stamping for effective fusion of asynchronous data streams [17].​  

The effectiveness of multimodal NTP models is heavily influenced by the chosen combination of representation, tokenization, and fusion techniques. Discrete tokenization, when combined with a unified sequence model architecture, facilitates joint training across modalities, as demonstrated by models like Emu3 and MIO [1,29,30]. Attention-based fusion mechanisms are particularly effective for tasks requiring deep inter-modal understanding, allowing the model to focus on relevant cross-modal elements [5,15]. However, the optimal approach often depends on the specific modalities involved, the nature of the task, and the desired trade-offs between representation fidelity, computational cost, and model performance. While progress has been made in developing robust individual techniques, comparative analyses detailing the performance implications of different representation, tokenization, and fusion strategies for various multimodal NTP tasks remain areas for further exploration [1,5,15].  

# 4.1 Multimodal Tokenization  

Multimodal tokenization is a foundational component in enabling Next Token Prediction (NTP) models to process and generate diverse data types such as images, videos, and audio. It serves the purpose of breaking down complex, continuous or structured data into manageable, discrete or embedded units (tokens) that can be effectively processed by sequence models, similar to how text is handled in traditional language models [13]. The specific tokenization approach employed is often influenced by the downstream model architecture and the nature of the modalities being integrated [35].  

<html><body><table><tr><td>Approach</td><td>Description</td><td>Key Techniques</td><td>Modalities Applied To</td><td>Strengths</td><td>Weaknesses</td></tr><tr><td>Discrete</td><td>Quantizing latent space into fixed codebook</td><td>VQ, VQ-VAE, VQ-GAN, BBPE, RLT</td><td>Visual, Text, Audio, Speech</td><td>Aligns naturally with discrete prediction</td><td>Information loss, codebook collapse</td></tr><tr><td>Continuous</td><td>Representing data in continuous embedding space</td><td>Various embedding methods</td><td>Audio, Visual (less common)</td><td>Preserves fine-grained details</td><td>Alignment challenges, encoding efficiency</td></tr></table></body></html>  

Tokenization techniques in multimodal contexts are broadly categorized into two main approaches: discrete and continuous [2,9,13].  

Discrete tokenization involves quantizing the input data's latent space into a fixed-size, discrete codebook, analogous to a vocabulary in language models [2,13]. This approach transforms continuous data into sequences of indices from this predefined codebook. Vector Quantization (VQ) techniques, including VQ-VAE and VQ-GAN variants, are central to creating these discrete representations, particularly for visual data like images and video [1,2,9]. For example, the Emu3 model utilizes a discrete tokenization method, with its visual tokenizer based on SBER-MoVQGAN, converting video and images into sequences of discrete tokens [1,11,29,30]. This tokenizer employs a codebook size of 32,768, generating 4096 tokens [1]. Similarly, VideoBERT derives visual frame tokens from vector-quantized video data [2,15], and MIO processes speech, text, images, and videos using discrete tokens [29,30]. For textual data, discrete methods like Byte-level Byte-Pair Encoding (BBPE) with large vocabulary sizes, such as the 102,400 used in the DeepSeek LLM, are standard [39]. A primary strength of discrete tokenization for NTP is its natural alignment with language modeling objectives, facilitating unified sequence processing across modalities. However, challenges include potential information loss due to quantization and the issue of codebook collapse during training, where a limited number of codebook vectors are frequently used [9]. Improvements like FSQ and LFQ have been proposed to address these issues [9].  

In contrast, continuous tokenization represents data in a continuous embedding space, deriving tokens or representations directly from the data's inherent properties without explicit quantization [2,13]. This approach aims to preserve the continuous nature of certain modalities, which can be crucial for capturing fine-grained variations [2,9]. Modalities like audio and certain visual data aspects might benefit from this richer representation [13]. Challenges associated with continuous tokenization include achieving semantic alignment across modalities and ensuring encoding efficiency, as seen in models like CLIP [9]. While potentially offering higher representation fidelity by avoiding quantization loss, continuous methods might face challenges in direct application to discrete NTP frameworks compared to their discrete counterparts.  

The choice between discrete and continuous tokenization depends on the specific modality, the need to capture finegrained information, and the trade-offs between representation fidelity, computational efficiency, and generation quality [1,2,9,13]. Discrete methods simplify the task for NTP by mapping everything to a shared discrete space but risk losing subtle details. Continuous methods retain more information but may require additional steps to integrate into discrete prediction tasks.  

Beyond these general categories, specialized tokenization techniques are being developed for specific modalities. For video data, Run-Length Tokenization (RLT) has been proposed to reduce redundancy by merging consecutive, similar image patches into a single token [25]. Similarity is determined by comparing the L1 distance between the first frame of patch $P _ { 1 }$ and the last frame of patch $P _ { 2 }$ ​ to a threshold $\tau$ [25]. RLT retains the first token of a sequence and encodes its length $l _ { i }$ and spatio-temporal position $( x , y , t )$ into an embedding added to the patch embedding [25].  

It is also worth noting alternative paradigms, such as Meta AI's Large Concept Models (LCMs), which challenge the necessity of traditional tokenization by performing computations directly in a high-dimensional embedding space to represent abstract concepts like sentences or utterances, rather than discrete tokens [3].​  

Training methods for multimodal tokenizers are diverse, driven by distinct objectives. Common methods include autoencoding, denoising auto-encoding, supervised pretraining, and contrastive learning [9,13]. Auto-encoding and denoising auto-encoding focus on reconstructing the input data or a clean version of it from a compressed representation, aiming for high fidelity [13]. Supervised pretraining leverages labeled data to learn representations useful for specific downstream tasks [13]. Contrastive learning focuses on learning embeddings where similar data points are closer together and dissimilar points are farther apart, effective for tasks requiring semantic understanding and alignment [9,13]. For instance, the Emu3 visual tokenizer is trained end-to-end using a combination of L2 loss, LPIPS perceptual loss, GAN loss, and commitment loss, reflecting a composite objective aimed at high-quality reconstruction and effective codebook usage [1]. These varied training methodologies are applied and modified depending on the modality and the desired characteristics of the resulting tokens [9].​  

# 4.2 Multimodal Data Representation (Embeddings)  

Effective data representation is fundamental to Multimodal Next Token Prediction (MMNTP), requiring techniques capable of encoding information from diverse modalities into a format suitable for unified processing. The primary goal is to encode semantically relevant information into a latent space while simultaneously minimizing redundancy, which is crucial for the efficacy of subsequent tasks [2]. Various techniques have been developed for different modalities, each aiming to capture distinct characteristics such as semantic meaning, visual features, or spatial-temporal relationships.​  

For text data, representation typically begins with tokenization, where each token corresponds to an embedding obtained via a look-up table within the language model's embedding layer [7]. Standard methods like Word2Vec contribute to creating word embeddings [36]. As Transformer-based models do not inherently process sequential order, positional encoding is added to token embeddings to incorporate the temporal or sequential arrangement of text [7]. Multilingual text processing often relies on specialized tokenizers, such as QwenTokenizer [1].​  

Visual data representation often involves extracting features using established computer vision architectures. Techniques include employing methods like Faster R-CNN to extract image features, which are subsequently passed through an image embedding layer [5]. Alternatively, pre-trained Convolutional Neural Networks (CNNs), such as InceptionV3, are utilized as encoders, with their last hidden layers serving as dense image embeddings. For instance, InceptionV3 provides embeddings of size 2048 [18]. Some approaches focus on region-of-interest (RoI) based visual features to capture visual cues within specific image areas [15]. Encoding spatial information within visual data is further achieved through methods like visual geometry embeddings, which represent the 4-dimensional position of bounding boxes and are processed using sine/cosine functions and fully connected layers [15]. The concept of discrete visual tokens is also explored, where the embedding layer is expanded to accommodate these non-textual visual units [1].  

For modalities involving sequence and spatial-temporal aspects, such as video or trajectory data, specific encoding strategies are necessary. This includes using learned length encoding matrices to map the length information $l _ { i }$ and the spatial-temporal position $( x , y , t )$ of tokens into a d-dimensional embedding vector. This embedding is then combined with the patch embedding to form the final representation [25]. As noted earlier, visual geometry embeddings also serve to capture positional information for visual elements [15]. Auxiliary data, such as segmented regions from electronic charts used in trajectory prediction, also require encoding for the model's comprehension [26].  

A key challenge in multimodal representation is integrating information from disparate modalities. One strategy involves mapping data from different modalities into a unified representation space, often using linear mapping and modality  

specific embeddings to address data heterogeneity [31]. A more advanced approach involves creating high-dimensional, modality-agnostic embedding spaces. Meta AI's SONAR, for instance, is designed to be language- and modality-agnostic, supporting over 200 languages and multiple modalities, including text and speech, by mapping inputs into its embedding space using concept encoders [3,30].  

The use of pre-trained models and embeddings significantly impacts the performance of MMNTP models. Pre-trained models like InceptionV3 for image encoding [18] or pre-trained multimodal models such as VL-BERT, which incorporates visual and linguistic embeddings [15], leverage knowledge learned from vast datasets, providing robust initial representations. Techniques like CLIP (language-guided contrastive learning) and DINO (self-supervised methods) are particularly noted for their superior representation capabilities achieved through pre-training [2]. These pre-trained embeddings serve as powerful feature extractors that can improve downstream task performance and potentially reduce the need for extensive training on the target multimodal dataset.​  

While the effectiveness of various embedding techniques in capturing semantic, visual, and temporal information is evident from their application in diverse models, the computational complexity and memory requirements of these methods are critical practical considerations. High-dimensional embeddings, such as the 2048-dimensional embeddings from InceptionV3 [18] or the high-dimensional SONAR space [30], can incur substantial memory usage and computational costs during training and inference. Specific analyses comparing the resource demands across different representation techniques are often dependent on implementation details and model architecture, and while important, detailed comparative evaluations on this aspect were not extensively covered in the provided literature digests.  

# 4.3 Multimodal Fusion Techniques  

Multimodal intelligence necessitates the effective integration of information from disparate modalities to facilitate tasks such as next-token prediction in complex sequences. This integration—known as multimodal fusion—can be achieved through various strategies, each presenting distinct trade-offs in terms of computational complexity, information preservation, and the ability to model intricate inter-modal relationships [15].  

<html><body><table><tr><td>Technique</td><td>Description</td><td>Integration Stage</td><td>Example Architectures /Models</td><td>Strengths</td><td>Weaknesses</td></tr><tr><td>Early Fusion</td><td>Combine data at input or early processing layers</td><td>Input or Early Layers</td><td>Single- Stream, B2T2, Emu3</td><td>Captures fine-grained early interactions</td><td>High computation al cost, sensitive to misalignmen</td></tr><tr><td>Late Fusion</td><td>Process modalities separately, combine later</td><td>Late Stages (e.g., predictions)</td><td>Two-Stream, B2T2 (Dual Encoder)</td><td>Modularity, lower early- stage computation</td><td>May miss crucial early cross-modal cues</td></tr><tr><td>Attention- Based</td><td>Dynamically weight information across modalities</td><td>Intermediate Layers</td><td>ViLBERT, LXMERT</td><td>Flexible, captures complex inter-modal relationships</td><td>Computation ally intensive for long sequences</td></tr></table></body></html>  

Key fusion approaches commonly employed include early fusion, late fusion, and attention-based mechanisms [15]. Gated fusion is another strategy, though specific examples from the provided literature are limited.  

# Early Fusion  

Early fusion involves combining data from different modalities at an initial stage—typically at the input layer or within the early processing layers of a model. In this approach, features from various modalities are concatenated or otherwise merge before being fed into a unified model architecture. Single-Stream architectures exemplify this strategy, processing text and visual information jointly within a single model pipeline [5,15]. For example, the B2T2 early fusion design embeds visual features alongside input word markers at the same level [15], allowing the model to capture fine-grained interactions between modalities from the outset. However, early fusion can incur high computational costs due to the increased dimensionality of the fused data and may be sensitive to noise or misalignment between modalities if not properly managed. A notable next-token prediction application is Emu3, which integrates text and visual tokens into a single, document-like sequence for training. It embeds visual metadata—such as resolution, frame rate, and duration—as plain “meta text,” alongside caption text and visual tokens marked by special delimiters, effectively treating all input as a unified token stream [1].​  

# Late Fusion  

In contrast, late fusion processes each modality independently through separate unimodal streams, with fusion occurring only at a later stage—often by combining final representations or predictions. Two-Stream architectures typify this approach, handling text and visual inputs separately before merging their outputs [5,15]. For instance, the “Dual Encoder” variant of B2T2 employs late fusion [15], which allows modality-specific optimization and generally reduces early-stage computational overhead. The trade-off is a potential loss of early cross-modal cues that might be crucial for certain tasks.  

# Attention-Based Fusion  

Attention mechanisms offer a powerful and flexible fusion strategy by dynamically weighting information from each modality when constructing a joint representation. Co-attention transformer modules and cross-modal transformers enable elements from one modality to attend to those in another [5]. ViLBERT, for example, uses a co-attention transformer to fuse text and image embeddings. Attention-based fusion excels at capturing complex, non-linear inter-modal relationships and can adapt its focus based on specific inputs; however, computing attention weights over large multimodal sequences can introduce significant computational overhead.​  

# Additional Fusion Techniques  

Beyond these primary strategies, multimodal systems employ various architectural components—such as graph convolutional networks and additional transformer modules—to align and process data into cohesive representations [6]. The fusion module is pivotal in integrating the strengths of each modality. Diverse applications illustrate its importance: cross-modal mixup via optimal transport for speech-text fusion in translation [37], fusion of voice-activity and lip-movement detector outputs in situated-intelligence platforms [17], and combining AIS data with electronic sea charts for shiptrajectory prediction [26].​  

In summary, the choice of multimodal fusion technique significantly impacts performance and characteristics in next-token prediction tasks. Early fusion enables rich early interactions but at higher computational cost and sensitivity to misalignment. Late fusion offers modality-specific processing with lower initial complexity but may miss crucial early cues. Attention-based fusion provides adaptive, fine-grained integration at the expense of increased computation. While the sources illustrate implementations in architectures like B2T2, ViLBERT, and Emu3, comparative performance data for each fusion strategy in multimodal next-token prediction remains sparse [1,5,15].  

# 5. Multimodal NTP Model Architectures  

Multimodal next token prediction (MMNTP) relies on diverse model architectures that process and synthesize information from multiple modalities to predict the subsequent element in a sequence, which can belong to any of the input or output modalities [1,6,9,13,24,31,33]. These architectures can be broadly categorized based on their approach to integrating different data types and their foundational neural network components.  

<html><body><table><tr><td>Category</td><td>Core Principle</td><td>Integrati on</td><td>Modaliti es</td><td>Backbon e (Often)</td><td>Strength S</td><td>Weaknes ses</td><td>Example</td></tr><tr><td>Composi tional</td><td>Integrate pre-</td><td>Alignme nt/Intera</td><td>Diverse, depends</td><td>Varied unimoda</td><td>Leverage S SOTA</td><td>Informati on</td><td>ViLBERT, LXMERT,</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>unimoda Imodels</td><td>ction Layers</td><td>compon ents</td><td>backbon es</td><td>一 capabiliti es</td><td>ck at interface S</td><td></td></tr><tr><td>Unified</td><td>Process all modaliti es end- to-end</td><td>Single shared framewo rk (Transfor mer)</td><td>Diverse</td><td>Large Transfor mer</td><td>Tighter integrati on, scaling potential</td><td>Requires training from scratch, data hungry</td><td>Emu3, MIO, VL- BERT, S- Transfor mer</td></tr></table></body></html>  

A primary distinction exists between compositional and unified model architectures for MMNTP [9,13]. Compositional models leverage powerful, pre-trained external encoders and decoders for individual modalities, integrating them through alignment layers or interaction modules [5,9,13,20,30]. This approach benefits from state-of-the-art unimodal capabilities but faces potential information bottlenecks at the integration interface [13]. In contrast, unified models process diverse modalities within a single, cohesive framework, typically employing lightweight encoders/decoders to map data to a common space and relying on a central backbone—often a Transformer—for end-to-end processing and next token prediction [1,9,13,30]. Unified models like Emu3 [1,29] aim for tighter integration and scalability but typically require training from scratch on large multimodal datasets, which is computationally intensive and data-demanding [29]. The tradeoff lies between the flexibility and leverage of pre-trained components in compositional models versus the potential for deeper interaction and end-to-end optimization in unified architectures [9,13].  

Beyond this high-level classification, MMNTP models employ specific neural network paradigms, most notably Transformers, but also historically RNNs and hybrid combinations. Transformer-based models have become predominant due to their effectiveness in capturing long-range dependencies through attention mechanisms and their suitability for parallel processing, which is essential for next token prediction in large models [7,33,35]. In multimodal Transformers, modality integration is achieved through techniques like modality-specific tokenization and embedding, often mapping inputs into a shared embedding space, followed by cross-attention or fused input embeddings to enable interaction across modalities [5,15,31]. Models such as ViLBERT and LXMERT use separate Transformer encoders with interaction modules, while VL-BERT integrates modalities directly into a single Transformer stream [5,15]. Variants such as S-Transformer leverage adaptations like Segment Recurrence for specific tasks involving sequential multimodal data [26]. The next token prediction typically involves a linear layer and softmax over the vocabulary space based on the final token embedding output from the Transformer backbone [7]. While powerful, scalability for very long sequences and limitations in tasks requiring complex planning remain challenges [8,26,29].  

RNN-based models, particularly LSTMs and GRUs, have historically been used for sequential prediction tasks in multimodal contexts, such as image captioning, where an RNN decoder generates text based on image features processed by a CNN [18]. Their strength lies in modeling temporal dependencies in sequences. Modality integration often involves conditioning the RNN with features from another modality, such as using CNN features to initialize or input to the LSTM [18]. However, RNNs struggle with capturing very long-range dependencies and are less amenable to parallelization compared to Transformers, limiting their application in large-scale multimodal models [18].​  

Hybrid architectures combine different components like CNNs, RNNs, and Transformers to exploit their complementary strengths [6]. For example, the classic CNN-RNN model for image captioning uses a CNN for spatial image features and an RNN for sequential text generation [18]. Other hybrid approaches may intertwine CNNs and Transformers for tasks like medical image segmentation [28] or use specialized networks such as GNNs to process structured data before feeding it to a Transformer for prediction [23]. Modality integration in hybrid architectures can occur at different stages: early fusion (e.g., joint input to a component), intermediate fusion (e.g., combining outputs from modality-specific encoders), or late fusion (e.g., combining predictions). These architectures offer flexibility, although they introduce additional complexity in design, training, and deployment.​  

In summary, multimodal next token prediction architectures span a spectrum from compositional models that leverage specialized unimodal systems to unified frameworks that integrate modalities end-to-end. Transformer-based models form the backbone of most modern approaches, utilizing various fusion and attention mechanisms to handle multimodal inputs and predict the next token. While RNNs have historically played a role, their limitations with long dependencies and parallel processing make them less common as primary backbones in cutting-edge, large-scale MMNTP models compared to  

Transformers. Hybrid architectures, meanwhile, offer a means to combine the advantages of different network types tailored to specific modality characteristics and task requirements.  

# 5.1 Compositional Models  

Compositional models represent a distinct architectural paradigm in multimodal intelligence, primarily characterized by the integration of powerful, pre-trained external encoders and decoders [9,13]. This approach leverages the highly specialized capabilities developed within unimodal domains, such as state-of-the-art vision models like CLIP or generative models like SD3, by incorporating them as modular components within a larger multimodal framework [9,13].​  

The fundamental architecture typically involves using separate encoders for processing different modalities. For instance, a visual encoder like CLIP can process image data, and its output is then combined with or fed into a language model, such as LLaMA, which handles the text modality and predicts the next token representing attributes or descriptions [20]. Similarly, traditional multimodal approaches have combined visual encoders with diffusion models for generative tasks [11]. Other instantiations, like Cross-Stream models such as ViLBERT and LXMERT, employ distinct encoders for text and image modalities, often leveraging pre-trained components like Faster R-CNN for granular image feature extraction [5].  

A crucial aspect of compositional models is the mechanism used to bridge the gap between these separate unimodal components. This is commonly achieved through the implementation of alignment layers or interaction modules [5,13]. These layers are designed to translate the representations from one modality’s encoder into a format compatible with another modality’s decoder or a central processing unit (like a language model), thereby enabling effective multimodal understanding and generation. For example, alignment layers connect external encoders/decoders like CLIP for multimodal tasks [13]. In Cross-Stream models, fusion modules are explicitly used after separate encoding to facilitate interaction [5].​  

The primary strength of compositional models lies in their ability to directly utilize and benefit from the continuous advancements in state-of-the-art unimodal models. By plugging in powerful, independently trained encoders and decoders, these models can inherit sophisticated understanding and generation capabilities for individual modalities without requiring end-to-end training from scratch on massive multimodal datasets. This modularity can potentially accelerate development and improve performance by standing on the shoulders of highly optimized unimodal systems.  

However, this architectural choice also presents potential weaknesses. A significant challenge is the risk of an information bottleneck between the specialized components. The alignment layer or the interface between the encoder output and the subsequent model (e.g., language model or decoder) might not be sufficiently expressive or high-bandwidth to transfer the full richness of information processed by the unimodal encoder. This can lead to a loss of subtle details or nuances, potentially limiting the overall multimodal performance compared to tightly coupled, end-to-end trained architectures.  

# 5.2 Unified Models  

Unified models represent a significant direction in multimodal intelligence, aiming to process diverse modalities and tasks within a single, cohesive framework, often leveraging the next token prediction objective [9]. This paradigm seeks to achieve end-to-end processing, simplifying the overall system design compared to approaches relying on separate, modalityspecific pipelines.  

The architecture of unified models typically employs lightweight encoders and decoders responsible for processing raw input/output modalities into a common token or latent space [9,13]. Examples of such lightweight components include VQVAE for handling visual information [9]. These components feed into or receive output from a central backbone, most commonly a large Transformer or Transformer decoder, which is tasked with performing the core understanding and generation processes across modalities [9,11,13]. This single backbone processes a multimodal sequence mix, effectively treating tokens from different modalities (and tasks) within the same attention mechanisms and layers [1,29]. Single-stream models like VL-BERT exemplify this by feeding different modal data directly into a single Transformer model [5].  

Several models have been proposed that embody this unified approach. Emu3, for instance, is described as a unified model architecture utilizing lightweight encoders/decoders and a single backbone Transformer for both understanding and generation tasks [11]. It processes all modalities within this single Transformer, explicitly eliminating the need for separate encoders or decoders for different modalities [1]. Emu3 is trained from scratch on a multimodal sequence mix, demonstrating the potential for scaling [29,30]. Another unified model mentioned is MIO, which is based on causal multimodal modeling [29]. Beyond general multimodal intelligence, this architecture extends to specific domain tasks, such as the S-Transformer for ship trajectory prediction, which integrates AIS and electronic sea chart data into a single  

Transformer-based architecture [26]. Similarly, the Generative Verifier (GenRM) employs a unified strategy by jointly training verification and solution generation processes using the next token prediction objective [12]. The Large Concept Models (LCM) explore variations within this paradigm, such as the One-Tower and Two-Tower architectures, which use a single Transformer decoder or separate components within the unified framework to handle tasks like context encoding and denoising [3].​  

The strength of unified models lies in their potential for tighter integration between modalities and tasks. By processing all information within a single model, they can capture more complex cross-modal dependencies and facilitate more efficient end-to-end processing. This unified design simplifies the overall model architecture and can lead to significant scaling potential in both training and inference [1,30]. However, a notable weakness is the typical requirement to train the entire model from scratch on extensive multimodal datasets, which can be computationally intensive and data-demanding [29].  

# 5.3 Transformer-based Models  

![](images/d4b638cf70afbaea8fd8358efd71684298efc4c9ca4fcb39ff9cf215b08c8a70.jpg)  

The Transformer architecture stands as a cornerstone in modern deep learning, profoundly influencing fields beyond its initial success in Natural Language Processing, including computer vision and multimodal learning [33,35]. Its effectiveness in next token prediction, a fundamental task for generative models, stems primarily from its reliance on attention mechanisms that establish global dependencies between input and output elements [11,31].​  

The core structure of the Transformer, typically comprising stacked encoder and decoder blocks, each featuring multiheaded self-attention and feed-forward neural networks, is well-suited for processing sequential data [7,33]. This architecture allows for parallel computation and, crucially, the capture of long-range dependencies through the selfattention mechanism, which weighs the importance of different tokens in the input sequence regardless of their distance [35]. In multimodal settings, adaptations of the Transformer are employed to handle diverse data types such as text, image, video, and speech. This often involves modality-specific encoders to project inputs into a common embedding space, followed by cross-attention mechanisms or fused input embeddings to enable interaction and dependency modeling across modalities [5,15,31]. For instance, models like S-Transformer utilize variations such as Segment Recurrence to capture longterm dependencies in specific multimodal data such as AIS (Automatic Identification System) sequences for maritime trajectory prediction, potentially linking to architectures like Transformer-XL [26]. Other adaptations include specialized Transformer architectures for tasks like speech-to-speech translation (e.g., Directed Acyclic Transformer, Hidden Markov Transformer) and 2D body pose estimation (e.g., PE-former, an encoder-decoder Transformer) [28,37].​  

A diverse array of Transformer variants has been developed specifically for multimodal understanding and generation tasks. These models often build upon established architectures like BERT, adapting them to process and align information from different modalities [15]. For instance, ViLBERT and LXMERT employ separate Transformer encoders for individual modalities and utilize cross-modal interaction modules to fuse features [5]. In contrast, VL-BERT modifies the standard BERT architecture to directly handle simultaneous visual and linguistic inputs, represented as embeddings defined on image regions of interest (RoIs) and text sub-words [5,15]. Other BERT-based multimodal models include VideoBERT, which discretizes visual data into tokens for processing [15], VisualBERT, designed to align text with image regions [15], and UNITER, which extracts features using pre-trained models before feeding them into a Transformer [15]. ActBERT introduces a Tangled Transformer to model relationships between action, object, and text features [5]. These variants showcase different strategies for integrating multimodal information, ranging from early fusion via input modifications to later fusion through dedicated interaction layers. Foundational large models relevant to next token prediction in multimodal contexts, such as GPT, BERT, LLaMA, Gemma, Emu3, and Loong, are predominantly based on the Transformer architecture or its derivatives [1,12,30,33,39]. Emu3, for example, uses a single Transformer trained end-to-end on multimodal sequences [29,30].​  

Scalability for processing long multimodal sequences remains a significant challenge. While Transformer architectures are capable of capturing long-range dependencies, the quadratic complexity of standard self-attention with respect to  

sequence length can be prohibitive for very long inputs. Techniques such as Segment Recurrence used in S-Transformer [26] or lightweight self-attention layers, as seen in the Fast Point Transformer for efficiency [28], are explored to mitigate this issue. Papers also discuss techniques aimed at improving the scalability of Transformer models for long multimodal sequences, although specific details on these methods are not uniformly provided across digests [29]. Further architectural innovations, such as the modified attention mechanisms in Transformer-based decoders for tasks like object recognition as next token prediction, which utilize non-causal masks for labels while preserving causal dependencies for tokens, also contribute to adapting the architecture for specific tasks and potentially managing sequence structure [20]. Despite their power, studies indicate that even Transformer architectures may encounter limitations in complex scenarios, such as simple planning tasks, when trained with methods like teacher forcing, suggesting that merely scaling up models is not a panacea for all challenges in complex multimodal next token prediction [8]. Research also explores the fundamental properties of these models, such as the "equal contribution" law, which highlights the consistent role of each layer in refining token embeddings for next token prediction across Transformer-based models [22].  

# 5.4 RNN-based Models  

Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) variants, have historically played a significant role in modeling sequential data and temporal dependencies, making them suitable for certain multimodal tasks involving sequential prediction. Their inherent recursive structure aligns well with the generation of sequences token by token [18].​  

For instance, in image captioning, LSTMs have been widely employed as decoders to generate textual descriptions token by token based on features extracted from an image [18]. In this sequence-to-sequence architecture, the LSTM processes tokens sequentially, taking the previously generated token (or ground truth token during training) and the internal state to predict the next token [18].​  

Similarly, hierarchical LSTM architectures have been explored for tasks like sign language translation, which involves processing sequences from different modalities [24]. Beyond typical language generation, RNN variants like ConvLSTM, STLSTM, and models such as PredRNN have been developed to handle spatio-temporal sequences, relevant in tasks like next frame prediction, demonstrating their capacity to capture dependencies across both spatial and temporal dimensions [14].  

Specifically, the ST-LSTM architecture incorporates spatio-temporal memory operations within the LSTM cell, utilizing a shared output gate to fuse distinct memory components, thereby enhancing the modeling of spatio-temporal structures [14]. While the sub-section description indicates relevance to multimodal ship trajectory prediction [26], the specific details of RNN application in that context are not available from the provided digests.  

Regarding the fusion of information from different modalities within the RNN framework, the provided digests offer limited detailed technical descriptions. In the context of image captioning using a CNN-RNN model, the image features are typically used to condition the RNN decoder, often by initializing the LSTM's hidden and cell states or by being incorporated into the input at each time step. However, the precise mechanisms for dynamically integrating multimodal inputs within the recursive computation steps of standard LSTMs or GRUs for next token prediction are not elaborated upon in the provided materials. Some advanced RNN cells, like ST-LSTM, are designed for fusing different types of memory (e.g., spatial and temporal) related to a single data stream (e.g., video frames) [14], but this differs from fusing information from disparate modalities like text and vision within the core RNN recurrence for multimodal generation.​  

<html><body><table><tr><td>Feature</td><td>RNN-based Models</td><td>Transformer-based Models</td></tr><tr><td>Architecture</td><td>Sequential (LSTM, GRU)</td><td>Parallel (Self-Attention)</td></tr><tr><td>Temporal Handling</td><td>Inherently sequential</td><td>Explicitly modeled via Attention/Pos.</td></tr><tr><td>Long-Range Dependencies</td><td>Difficult due to sequential processing</td><td>Effective via Self-Attention</td></tr><tr><td>Parallelism</td><td>Limited during training/inference</td><td>High</td></tr><tr><td>Primary Use Case (MMNTP)</td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Sequential generation (e.g., captioning)</td><td>Dominant for generation & understanding</td></tr><tr><td>Strengths</td><td>Good for modeling short</td><td>Excellent for long</td></tr><tr><td>Weaknesses dependencies</td><td> Struggles with long</td><td>Computationally intensive for very long seqs</td></tr></table></body></html>  

Comparing RNN-based models to Transformer-based models for multimodal next token prediction reveals distinct strength and weaknesses. RNNs inherently process sequences one element at a time, which naturally aligns with token-by-token generation as demonstrated in image captioning [18]. This sequential processing makes them intuitive for handling temporal dependencies. However, this very characteristic leads to significant weaknesses compared to Transformer architectures. RNNs face challenges with capturing very long-range dependencies due to the potential for vanishing or exploding gradients through many time steps, despite the gating mechanisms in LSTMs and GRUs. Furthermore, their sequential nature fundamentally limits parallelism during training and inference, as the computation for each time step depends on the output of the previous step. In contrast, Transformer models, leveraging self-attention mechanisms, can process sequences non-sequentially, allowing for much greater parallelism and enabling effective modeling of dependencies regardless of the distance between tokens. While not detailed in the digests, Transformer-based models are generally considered superior for tasks requiring extensive parallelization and handling very long sequences, which is increasingly relevant in complex multimodal scenarios.  

# 5.5 Hybrid Architectures  

Hybrid architectures represent a significant direction in multimodal intelligence, combining different neural network components such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers to leverage their respective strengths and mitigate individual limitations in processing diverse modalities. The rationale behind such combinations typically lies in optimizing the extraction, processing, and integration of modality-specific features before performing a unified task.  

One prevalent hybrid approach for multimodal tasks, particularly involving vision and language, is the integration of CNNs and RNNs. For instance, in image captioning, a standard architecture utilizes a CNN encoder to extract salient visual features from an image, followed by an RNN decoder (such as an LSTM) to generate a descriptive text sequence based on the visual representation [18]. The CNN excels at capturing spatial hierarchies and local patterns in image data, while the RNN is adept at modeling sequential dependencies necessary for generating coherent text. This design effectively processes the image modality via the CNN and the text modality via the RNN, integrating them by feeding the CNN’s output as input or initial state to the RNN.​  

Beyond the classic CNN-RNN model, more sophisticated hybrids incorporate Transformers and other specialized networks. Combinations of CNNs and Transformers have been explored to capitalize on the CNN’s inductive bias for local feature extraction and the Transformer's ability to capture global dependencies through attention mechanisms. While not specifically in a multimodal NTP context in the provided digest, a paper explores combining CNNs and Transformers for semi-supervised medical image segmentation [28], illustrating the synergy of these architectures for intricate spatial analysis. This principle can be extended to multimodal tasks where CNNs process visual inputs, and Transformers handle attention-based fusion with other modalities or generate sequence outputs based on combined features.  

Furthermore, hybrid architectures are designed to accommodate modalities or data structures that may not be standard sequences or grids. For example, processing event-based data from neuromorphic cameras, which arrives asynchronously and represents spatiotemporal correlations more akin to a graph, can benefit from specialized preprocessing. A hybrid architecture for neuromorphic camera denoising utilizes a Graph Neural Network (GNN) with an EventConv messagepassing framework to model these spatiotemporal correlations, driving a subsequent Transformer network [23]. This highlights how GNNs can effectively capture the unique structure of certain data types before passing a processed representation to a Transformer, which can then operate on this structured information.​  

Hybrid models can also involve combining different variants or specialized instances of a single architecture type, such as Transformers, designed to handle different modalities or feature types. An example is ActBERT, which employs distinct atransformer, r-transformer, and w-transformer modules specifically tailored to model the relationships between action  

features, object features, and text features, respectively [5]. This modular approach allows for specialized processing of distinct data streams or feature types within a unified framework, integrating information through cross-attention mechanisms or feature concatenation.  

Designing, training, and deploying these hybrid models involve several trade-offs. Design complexity increases significantly compared to monolithic architectures, requiring careful consideration of how different components interact and pass information. Training can be challenging due to potential differences in convergence rates, optimization strategies for different parts of the network, and the need for large, often multimodal, datasets. Deploying hybrid models can incur higher computational costs and latency compared to simpler models, particularly if they involve multiple large components like Transformers. However, the ability of hybrid architectures to leverage the specific strengths of different neural network types for modality-specific processing and complex cross-modal interactions often justifies these trade-offs for achieving state-of-the-art performance in multimodal tasks.  

# 6. Training Strategies and Optimization Techniques  

Effective training strategies and optimization techniques are paramount for developing robust and generalizable Multimodal Next Token Prediction (MMNTP) models. This section provides a comprehensive overview of the diverse approaches employed, encompassing fundamental training task types, sequential training stages, objective functions, regularization methods, and the inference process.​  

Training tasks in MMNTP are primarily categorized based on the nature of the predicted token: Discrete Token Prediction (DTP) and Continuous Token Prediction (CTP) [9,13]. This fundamental distinction dictates the model's output head structure and the specific training objectives [9]. DTP, a prevalent approach, involves predicting the next token from a predefined vocabulary or a set of class labels [1,13]. Examples include predicting masked language tokens conditioned on visual information, classifying masked image regions using text, or tasks verifying cross-modal relationships framed as discrete outcomes [5,15]. DTP can also reframe tasks like object recognition into sequence prediction [20]. A variation, multitoken prediction (MTP), aims to predict multiple subsequent tokens simultaneously to enhance sample efficiency and bridge the gap between teacher forcing during training and autoregressive inference [4,16]. In contrast, CTP focuses on predicting continuous vector representations, relevant for generating continuous data modalities, although it is less extensively detailed in the provided literature than DTP [9,13]. The choice between DTP and CTP is contingent upon the specific multimodal generation or understanding goal.  

MMNTP model training commonly follows a multi-stage pipeline to progressively acquire capabilities [9,13]. The initial pretraining stage focuses on learning foundational representations across modalities and aligning these with language space, typically using large-scale multimodal datasets and self-supervised objectives [5,9,13]. Following pre-training, Supervised Fine-Tuning (SFT) adapts the model to specific downstream tasks and instruction following by training on high-quality labeled datasets [7,9,13,21]. SFT leverages task-specific input–output pairs, computing loss primarily on the desired output sequence [7]. Models like DeepSeek LLM and MIO have employed two-stage or four-stage SFT processes, respectively, incorporating diverse tasks and mitigating issues like repetitive generation [29,30,39]. While SFT is effective, challenges like teacher forcing limitations, where models might exploit answer cues rather than learning robust reasoning, exist [8]. The final stage, preference alignment, refines model behavior to align with human preferences and safety, often using methods like Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO) [1,9,13,39]. This staged approach, representing a form of transfer learning [21], allows for gradual skill acquisition. Other training strategies like multi-task learning have also been explored [36].​  

The optimization process is guided by loss functions tailored to the prediction task. Cross-entropy loss is the dominant choice for discrete token prediction and generation, encouraging the model to predict accurate probability distributions over the vocabulary [7,11,18,21]. Variations include masked loss for specific prediction targets (e.g., in MLM or SFT) [5,7] and weighted loss to balance contributions from different modalities or token types, as seen in Emu3 which weights visual token loss by 0.5 [1]. For multi-token prediction, a modified cross-entropy sums expected loss over multiple future tokens  

$$
L _ { M T P } = - \sum _ { i = 1 } ^ { n } \mathbb { E } _ { ( x , y ) \sim D } \left( \ell ( x , y _ { i } ) \right)
$$  

where $x$ is the context, $y _ { i }$ ​ is the $\mathbf { \chi } _ { i }$ -th future token, and $\ell$ is the loss function [4]. For continuous predictions, regression losses such as $L _ { 1 }$ or $L _ { 2 }$ are used, though $L _ { 2 }$ ​ can lead to blurred outputs in generative tasks [14]. Advanced losses like DPO are integrated during fine-tuning for preference alignment [1], and techniques like loss re-weighting address challenges  

such as imbalance in long sequence training [29,30]. Optimization typically involves gradient descent methods like Adam [18].​  

Regularization methods are essential to prevent overfitting and improve the generalization of MMNTP models [35]. Dropout is widely used, randomly zeroing neuron outputs during training to improve stability and generalization, and also exhibiting implicit regularization effects [1,34]. While weight decay and batch normalization are standard deep learning practices for controlling complexity and improving training dynamics, their specific application details vary across MMNTP models and were not consistently highlighted in the surveyed literature [5]. Efficiency techniques, such as memory-efficient implementations for forward and backward passes, are also critical optimization considerations. Data handling techniques like "example packing" for variable-length sequences can optimize batch processing during training [25]. Insights into the "equal contribution" law can potentially guide the optimization of pre-training tasks [22].  

The final inference stage generates multimodal outputs through an autoregressive process, predicting tokens sequentially conditioned on previous inputs and generations [1,7,11]. A critical aspect is the choice of sampling strategy to select the next token from the model's probability distribution, balancing determinism and diversity. Common strategies include greedy decoding, beam search, temperature sampling, Top- $\cdot \boldsymbol { \mathsf { k } } ,$ and Top-p sampling, each impacting the quality and coherence of the generated sequence [7]. Techniques like video token re-encoding are used to mitigate error accumulation in multimodal generation [29,30]. Inference efficiency is improved via methods like speculative decoding or memory optimization [4,35]. Prompt engineering, applied during the preparation phase, also significantly influences the output generation during inference [2,9,13]. Challenges include optimizing inference speed and controlling long-range coherence in generated multimodal sequences. Future research directions involve developing more sophisticated multi-task and curriculum learning strategies, exploring hybrid discrete–continuous prediction objectives, designing novel loss functions for complex multimodal alignment, enhancing regularization for diverse and potentially noisy multimodal data, and further optimizing inference for real-time multimodal interaction.​  

# 6.1 Training Task Types  

Within the realm of Multimodal Next Token Prediction (MMNTP), training tasks are fundamentally distinguished by the nature of the predicted token, primarily falling into the categories of Discrete Token Prediction (DTP) and Continuous Token Prediction (CTP) [9,13]. This distinction profoundly influences the training objective and the structure of the model's output head [9].​  

Discrete Token Prediction (DTP) constitutes a dominant paradigm, where the model is trained to predict the next discrete token from a predefined vocabulary or a specific class label [1,13]. This approach is prevalent in many multimodal pretraining tasks, often involving predicting masked elements in one modality conditioned on others or aligning information across modalities [5,15]. Examples of DTP-based tasks detailed in the literature include Masked Language Modeling (MLM), where masked text tokens are predicted using visual cues [5,15]; Masked Region Classification, predicting the class of masked image regions using linguistic cues [5,15]; Masked Frame Prediction in video contexts [5]; and tasks verifying relationships between modalities, such as Image-Text Matching or Video-Subtitle Matching, which can be framed as predicting a discrete outcome (e.g., match/no match) [5,15]. DTP is also employed to reframe tasks like image classification into a sequence prediction problem, predicting the text token corresponding to the object class [20]. This type of task typically employs a classification head on top of the model's final hidden state, predicting probabilities over the discrete vocabulary or class set. The training objective commonly involves minimizing cross-entropy loss between the predicted distribution and the ground truth token [4]. Variations exist, such as predicting multiple future tokens simultaneously, still within the discrete domain [4].  

In contrast, Continuous Token Prediction (CTP) involves representing multimodal information as continuous vectors and training the model to predict these continuous representations [9,13]. While digests offer less detail on specific applications of CTP compared to DTP, the core idea implies predicting numerical vectors rather than discrete indices. This would necessitate regression-style output heads, predicting vector values directly or via transformations, and training objectives centered around minimizing the distance (e.g., L2 loss) between predicted and target continuous vectors.  

The suitability of each task type depends on the specific multimodal goal. DTP is well-suited for generating discrete sequences like text descriptions, captions, or discrete actions, and for understanding tasks framed as classification or discrete prediction. Its strength lies in mapping multimodal input to symbolic output. However, it has been implicitly suggested that relying solely on DTP might face limitations in tasks requiring complex planning or generating highly nuanced, non-symbolic outputs [8]. CTP, on the other hand, holds potential for tasks involving the generation or manipulation of continuous data modalities (e.g., generating raw audio waveforms, continuous control signals, or complex visual features) or for learning richer, less constrained representations that might be beneficial for reasoning or tasks with continuous output spaces. While DTP has been the more widely explored approach in initial multimodal next token prediction models like Emu3 [1], the exploration of CTP or hybrid approaches may become increasingly relevant for achieving more sophisticated multimodal generation and understanding capabilities, particularly for tasks that extend beyond the prediction of discrete symbols.​  

# 6.2 Training Stages  

Training Multimodal Next Token Prediction (MMNTP) models typically involves a sequence of stages designed to progressively enhance their capabilities—from fundamental multimodal understanding to alignment with specific task requirements and human preferences [9,13]. The common stages include pre-training, supervised fine-tuning (SFT), and preference alignment, often implemented through methods like Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO) [1,39]. Some models may employ variations or additional stages; for instance, MIO utilizes a four-stage process encompassing alignment pre-training, interleaved pre-training, speech enhancement pretraining, and supervised fine-tuning [29].​  

The pre-training stage serves as the foundation, primarily aimed at learning robust representations across different modalities and aligning these representation spaces with the language space [9,13]. This stage focuses on enabling the model to understand and generate content involving multiple modalities by training on large-scale multimodal datasets, often composed of multimodal data–text pairs [5,9]. Self-supervised tasks are frequently employed during pre-training to capture inherent correlations and structures within the multimodal data [5]. For example, models like VL-BERT are pretrained on combined visual-language datasets and pure language datasets [5]. The objective is to establish a broad understanding and foundational generation capability across modalities [13].​  

Following pre-training, Supervised Fine-Tuning (SFT) is conducted to adapt the model to specific downstream tasks and improve its ability to understand and execute more complex instructions and queries [9,13]. While pre-training focuses on general representation learning using vast, often self-supervised data, SFT utilizes smaller, high-quality labeled datasets specific to target tasks [7,21]. Data construction for SFT typically involves creating samples consisting of prompt–answer pairs or instruction–context–response formats [7,21]. These labeled examples explicitly provide the desired output for a given input prompt, guiding the model to learn specific task-oriented behaviors [7]. The training process for SFT is essentially also next token prediction, similar in mechanism to pre-training, but with a key difference in objective and loss calculation. The SFT objective is to predict the correct sequence of tokens for the “answer” or “response” part of the sample, rather than the entire input sequence as often seen in pre-training [7]. To achieve this focused learning, a mask vector is typically generated to exclude the prompt or instruction portion from the loss calculation, concentrating the gradient updates on the desired output sequence [7]. SFT aligns the model towards the target label provided in the dataset [21] and integrates seamlessly with instruction tuning methodologies [12].  

The final stage discussed is preference alignment, which aims to refine the model’s behavior to better align with human preferences, safety guidelines, and desired output characteristics that are difficult to capture through supervised labeling alone [9,13]. This stage typically involves training on human- or machine-generated preference data, indicating which model outputs are preferred over others for given prompts [21]. Reinforcement learning approaches, such as RLHF, or direct optimization methods, like DPO, are commonly employed to train the model to generate outputs that score higher according to a learned reward model or to directly optimize a preference objective [1,21,39]. This process ensures that the final model behavior is refined to be more helpful, honest, and harmless, reflecting human values and expectations [13]. The combination of these stages allows MMNTP models to transition from understanding complex multimodal inputs to generating task-specific, human-aligned outputs via sequential token prediction.​  

# 6.3 Loss Functions  

The selection and design of loss functions are fundamental to training multimodal models based on the next token prediction (NTP) paradigm, as they guide the optimization process towards desired outcomes such as accurate sequence generation or robust representation learning [9]. A variety of loss functions are employed depending on the specific task and model architecture.​  

Cross-entropy loss is the most prevalent choice for NTP tasks, particularly those involving generation, due to its effectiveness in optimizing models to predict categorical outcomes—that is, the probability distribution over the vocabulary for the next token [7,11,21]. This loss encourages the model to assign a high probability to the ground truth next token. For a standard NTP setup predicting token yᵢ given previous context, the objective is to maximize the probability of the real next token [7]. In the context of sequences, the cross-entropy is typically computed between the predicted logits and the ground truth tokens, often averaged over the sequence, as seen in image captioning models using LSTM decoders [18].​  

For multi-token prediction, a variation of cross-entropy loss is utilized. One formulation presented for multi-token prediction (Lₘₜₚ) is defined as:  

# $\bigcirc$ 无效公式  

where x is the context sequence, yᵢ is the i-th future token, and $\ell ( x , y \boxtimes )$ denotes the loss incurred for predicting yᵢ based on x. Here, D represents the training corpus [4]. While the specific form of $\ell ( x , y \boxtimes )$ is not fully elaborated in the original formulation, the equation indicates that the expected loss over multiple future tokens (from i $= 1$ to n) is summed.  

Variations of cross-entropy loss are also common. Masked language modeling and masked RoI classification tasks implicitly rely on cross-entropy applied only to specific masked positions [5]. Similarly, in supervised fine-tuning (SFT), a mask is often applied to compute the loss solely on the answer portion of the output sequence [7]. Weighted cross-entropy can be used to balance the contribution of different modalities or token types. For instance, Emu3 applies a weight of 0.5 to the loss associated with visual tokens to prevent them from dominating the learning process during standard NTP training [1].  

Beyond categorical losses like cross-entropy and the related perplexity loss [21], regression losses such as L₁ or $L _ { 2 }$ distance are relevant for tasks involving continuous predictions. These are often used for predicting features or future frames in video contexts, measuring the distance between generated and real continuous values [5,14]. However, a notable disadvantage of using $L _ { 1 } O r L _ { 2 }$ distance alone for generative tasks like next frame prediction is their tendency to produce blurred outputs [14].  

To address challenges like loss imbalance, particularly during training with long sequences such as videos, techniques like loss re-weighting schemes have been adopted, as demonstrated in the “Loong” model [29,30]. Furthermore, fine-tuning stages may incorporate additional or alternative loss functions. For example, Emu3 utilizes Direct Preference Optimization (DPO) loss combined with the next token prediction cross-entropy loss for fine-tuning, optimizing for human preferences alongside predictive accuracy [1]. Research also explores novel loss functions beyond traditional Maximum Likelihood Estimation (MLE) for text generation, which could be applicable to optimizing NTP in multimodal settings [37]. These diverse loss functions and optimization strategies highlight the ongoing effort to tailor training objectives to the complex nature of multimodal data and the varied demands of MMNTP tasks.  

# 6.4 Regularization Methods  

Effective regularization methods are crucial in Multimodal Next Token Prediction (MMNTP) models to mitigate overfitting and enhance generalization capabilities on diverse and unseen multimodal datasets [35]. These techniques improve model robustness and training stability. Common regularization strategies include dropout, weight decay, and batch normalization  

Dropout is a widely adopted technique that involves randomly setting a fraction of neurons to zero during training, preventing complex co-adaptations on the training data and thereby improving generalization. In the context of MMNTP, dropout has been employed to specifically address training stability issues. For instance, a dropout rate of 0.1 was implemented in one model to improve its stability during the training process [1]. Beyond its explicit role in preventing overfitting, research also indicates that dropout possesses implicit regularization effects [34]. This implicit regularization can facilitate weight condensation within the network, contributing to a more structured and potentially more efficient learned representation [34]. Thus, dropout serves a dual purpose: enhancing stability and generalization while subtly influencing the internal structure of the model [1,34].  

While other regularization techniques such as weight decay and batch normalization are standard practices in deep learning to control model complexity and improve training dynamics, their specific application details were not explicitly provided in all relevant reviewed works. For example, one overview of multimodal pre-training models did not specifically detail the use of methods like dropout, weight decay, or batch normalization [5]. This suggests that while these methods are likely employed, their precise configuration and reported impact may vary across different MMNTP architectures and training setups, or were not the primary focus of discussion in some surveyed literature. Further research detailing the specific  

implementation and empirical impact of various regularization methods on MMNTP model performance and stability would provide deeper insights into best practices within this domain.  

# 6.5 Prompt Engineering  

Prompt engineering, a methodology initially established in large language model (LLM) research, has been effectively integrated into multimodal next token prediction (MMNTP) models [2,9]. This approach leverages the inherent capabilities of LLMs to guide multimodal understanding, reasoning, and generation through carefully constructed prompts [9]. Functioning similarly to providing clear instructions to enhance efficiency and quality, prompt engineering enables researchers to steer complex multimodal models towards desired outputs and behaviors [35].  

Within the MMNTP paradigm, prominent prompt engineering techniques include multimodal in-context learning (MICL) and multimodal chain-of-thought (MCoT) prompting [2,9]. Multimodal in-context learning involves providing the model with several multimodal examples directly within the prompt context. These examples demonstrate the input-output format or task execution, allowing the model to infer the desired behavior without explicit fine-tuning, thereby improving performance by facilitating pattern recognition based on presented instances [9].  

In contrast, multimodal chain-of-thought prompting focuses on encouraging the model to generate intermediate multimodal reasoning steps before producing the final output [9]. This technique typically involves crafting prompts that explicitly request or implicitly encourage the model to articulate its reasoning process, often integrating information across modalities. By making the model's reasoning process explicit, MCoT enhances the model's reasoning abilities and the transparency of its decision-making, potentially leading to more accurate and reliable outputs on complex multimodal tasks compared to direct generation [9,35]. The key difference lies in their mechanism of guidance: MICL guides by showing completed examples, while MCoT guides by eliciting and structuring the reasoning process itself.  

# 6.6 Supervised Fine-Tuning (SFT)  

Supervised Fine-Tuning (SFT) represents a crucial post-training phase in the development of large multimodal models trained via next token prediction, primarily aimed at aligning the pre-trained model with specific downstream tasks or enhancing its ability to follow instructions accurately [7,21]. Unlike the broad, foundational objective of the pre-training stage, which focuses on learning general data distributions through large-scale next-token prediction across various modalities, SFT has a focused, task-oriented training objective [7]. This targeted training adapts the model's extensive knowledge acquired during pre-training to specialized applications [21].  

The SFT process involves aligning a pre-trained model on a specific dataset comprising labeled examples relevant to the target tasks [21]. Data construction for SFT entails creating task-specific multimodal data pairs. For instance, this could involve concatenating a prompt or instruction with the desired answer or response, often incorporating start and end symbols to delineate the segments [7]. For multimodal tasks, these pairs would integrate different modalities, such as image-text pairs for visual instruction following [1] or data involving text, visual, and speech components [29]. A common technique involves creating a mask vector where the answer part is assigned a value of 1, and other parts (like the prompt or padding) are assigned 0. This mask is crucial for subsequent loss calculation [7].​  

Model training during SFT reuses the architecture of the pre-trained model. After constructing the input data as described, tokens are converted to embeddings and passed through the transformer layers, identical to the forward pass during pretraining [7]. The model produces output logits for each token position, representing the probability distribution over the vocabulary for the next token. Loss calculation is then performed, specifically focusing only on the target or answer portion of the sequence, utilizing the previously generated mask vector. This means that the loss contributions from the prompt or instruction part are effectively ignored by multiplying them by zero [7]. The objective function minimizes the discrepancy between the model's predicted token distribution and the actual target tokens in the labeled examples, guiding the model to generate the correct task-specific outputs.  

SFT has been applied in various contexts to improve model performance and alignment. For instance, DeepSeek LLM utilized SFT on 1.5 million instructions spanning general language, math, and code tasks to enhance alignment [39]. They employed a two-stage SFT strategy to mitigate issues like repetitive generation [39]. Similarly, the MIO model incorporates SFT as a distinct stage, fine-tuning on diverse text, visual, and speech tasks [29]. In the context of multimodal understanding, models like EMU3 undergo post-training phases that include instruction tuning using question-answer pairs sampled to improve visual instruction following capabilities [1]. The effectiveness of SFT can also be influenced by the  

preceding pre-training strategy, with findings suggesting that pre-training methods like multi-token prediction can enhance performance in subsequent fine-tuning stages by fostering better understanding and problem-solving abilities [4].  

Despite its effectiveness in task adaptation, SFT, particularly when relying heavily on teacher forcing (where the model is always conditioned on the ground truth prefix), has limitations. It may sometimes allow the model to exploit cues present in the provided answer during training rather than genuinely learning the underlying reasoning process from the input, potentially hindering its generalization to novel inputs [8]. Addressing such limitations remains an active area of research in improving fine-tuning methodologies.  

# 6.7 Inference Stage  

![](images/b7813b795ed4ae9621b5cf85f4990027abda99be6de420c78055220423b534fe.jpg)  

The inference stage in next token prediction models involves generating the output sequence based on the input and the model's learned probability distribution. This process is fundamentally autoregressive, meaning the model predicts one token at a time, conditioned on the preceding sequence of input and previously generated tokens [7]. Specifically, at each step, the model outputs a probability distribution over the entire vocabulary for the next token. The predicted token is then appended to the current sequence, and this augmented sequence serves as the input for predicting the subsequent token [7]. This iterative process continues until a termination condition is met, such as generating an end-of-sequence token or reaching a maximum length [7]. This autoregressive approach is central to various multimodal applications, such as extending 5-second video clips infinitely as demonstrated by Emu3 [1,11].  

A critical aspect of the inference stage is the selection of sampling strategies, which determine how the next token is chosen from the predicted probability distribution [7]. These strategies play a crucial role in balancing the determinism and diversity of the generated multimodal output sequences and significantly impact their quality and coherence [7]. Common sampling methods include greedy decoding, beam search, temperature sampling, Top-k sampling, and Top-p (or nuclear) sampling [7]. Greedy decoding deterministically selects the token with the highest probability at each step, leading to highly predictable but potentially repetitive or unnatural outputs. Beam search explores multiple high-probability sequences simultaneously, typically yielding more coherent results than greedy decoding by considering a wider set of possibilities. Temperature sampling adjusts the softmax distribution's sharpness, effectively controlling the randomness of generation; a lower temperature leads to more deterministic, focused sampling, while a higher temperature results in a more uniform distribution and thus more diverse but potentially less coherent outputs [7]. Top-k sampling restricts the sampling pool to the k most likely tokens, while Top-p sampling (nuclear sampling) selects tokens from the smallest set whose cumulative probability exceeds a threshold p [7]. These latter methods introduce controlled randomness, favoring high-probability tokens while allowing for some variation.​  

The choice and application of sampling strategies are particularly important in multimodal generation. For instance, specific sampling strategies, alongside video token re-encoding, have been explored in models like "Loong" to mitigate error accumulation during video generation inference, enhancing the coherence and fidelity of the output [29,30]. Furthermore, the inference stage can incorporate specialized techniques beyond standard autoregressive sampling. For example, in Vision-Language Models applied to tasks like object recognition, novel methods such as one-pass sampling have been introduced. This technique generates text tokens for labels in parallel and ranks them based on probabilities, leveraging the parallel processing capabilities inherent in architectures like the Transformer to accelerate inference for specific tasks [20].​  

Beyond sampling, efficiency and acceleration techniques are vital for practical inference. Models designed for next token prediction can utilize additional output heads or mechanisms for self-speculative decoding, including blockwise parallel decoding or Medusa-like tree attention, to significantly speed up the inference process [4]. Memory efficiency is also a concern, addressed by techniques like offloading and mixed reasoning during inference to reduce memory load and enable reasoning with lower resource usage [35]. In specific application contexts, such as generative verification, inference might involve methods like majority voting over multiple predictions to enhance reliability [12]. While prompt engineering primarily influences the model's input, its application during the preparation phase before inference also significantly impacts the subsequent generation performance on multimodal tasks [13].​  

# 7. Advanced and Related Concepts  

Expanding upon the foundational concept of next token prediction (NTP), research in multimodal intelligence has explored advanced techniques and alternative paradigms to enhance model efficiency, broaden capabilities, and unify processing across different data modalities. This section delves into several key areas that evolve or diverge from standard sequential token prediction.  

One significant advancement is Multi-Token Prediction (MTP), a technique designed to improve both the sample efficiency during training and the inference speed of sequence models by predicting multiple future tokens simultaneously [4,19]. Unlike standard NTP, which predicts one token at a time, MTP leverages architectural elements such as shared backbones and multiple output heads to predict a sequence of tokens in parallel at each step. Empirical studies demonstrate that MTP leads to notable performance improvements and speedups on tasks like code generation (e.g., on HumanEval and MBPP benchmarks) and natural language generation [4,16]. Ablation studies are crucial for determining the optimal number of tokens to predict, revealing dependencies on data characteristics and identifying trade-offs between prediction window size and performance outcomes [4].  

Complementing token-based approaches, Large Concept Models (LCMs) represent an alternative paradigm that operates at the concept level within a high-dimensional embedding space, often utilizing techniques like SONAR [3]. This approach aims for language- and modality-agnostic processing, offering potential benefits for integrating information across diverse data types in a unified manner [3]. LCMs hold significant implications for the development of more versatile multilingual and multimodal AI systems, potentially enabling capabilities like zero-shot generalization across modalities [3,6].  

Furthermore, the NTP framework can be applied creatively to multimodal tasks. Object recognition, for instance, can be effectively reframed as a next token prediction problem within Vision-Language Models (VLMs) [20]. In this setup, VLMs process visual input and predict text tokens corresponding to image attributes or labels. This framing, often utilizing architectures combining visual encoders and language model backbones, offers advantages for open-domain image tagging by leveraging the extensive vocabulary and knowledge base of the language model [20].  

These advanced concepts and related applications highlight the evolving landscape of models moving beyond strict sequential single-token prediction towards more efficient, unified, and versatile approaches for handling complex data, particularly in the realm of multimodal intelligence.  

# 7.1 Multi-Token Prediction (MTP)  

Multi-Token Prediction (MTP) is a mechanism designed to enhance the efficiency and performance of language models by predicting multiple future tokens simultaneously, rather than adhering to the traditional single-token prediction paradigm [4,19]. The fundamental principle involves predicting a sequence of $\mathsf { \backslash } ( \mathsf { n } \backslash )$ subsequent tokens in parallel at each step, aiming to improve sample efficiency during training [4,16]. This approach increases training signal density, which potentially leads to improved data efficiency and enables the model to better pre-plan its internal representations [39]. Empirical evidence suggests that training models to predict multiple future tokens can yield improved performance, particularly in tasks where standard teacher-forced training exhibits limitations [8].  

The implementation of MTP typically involves several key steps. The input sequence is initially processed through a feature mapping layer to generate a rich representation [19]. This representation is then fed into a sequence prediction component which employs multiple independent output heads. Specifically, the model utilizes $\left\backslash \left( \mathsf { n } \right\backslash \right)$ separate output heads, each dedicated to predicting a different token within the future sequence of length $\left\backslash \left( \mathsf { n } \right\backslash \right)$ [4,19]. Thus, at each position in the training corpus, the model is required to predict the next $\left\backslash \left( \mathsf { n } \right\backslash \right)$ tokens [16].  

The training objective for MTP is defined to optimize the prediction of all $\mathsf { \backslash } ( \mathsf { K } \backslash )$ tokens in the target sequence at each step. The mathematical formulation typically builds upon the standard next-token prediction probability. The probability of the next token $\backslash ( \mathsf { y \_ } \{ \mathsf { t } + \mathsf { 1 } \} \backslash )$ given the preceding sequence $\backslash ( \mathsf { x \_ 1 } , \mathsf { \backslash d o t s } , \mathsf { \times \_ t \backslash } )$ is given by:​   
\​   
For subsequent tokens in the multi-token prediction window, a recursive generation probability can be considered: \​   
Here, $\backslash ( \mathsf { f \_ f l t h e t a } ) \backslash \rangle$ represents the model parameterized by \( \theta \). The overall loss function \( \mathcal{L} \) is typically formulated as the sum of negative log-likelihoods over the sequence length $\backslash ( \intercal \backslash )$ and the prediction window size $\backslash ( \mathsf { K } \backslash )$ :​  

This loss function encourages the model to accurately predict the entire sequence of \( K \) future tokens at each step [19].  

During inference, the capability to predict multiple tokens simultaneously offers significant advantages, primarily in terms of increased speed and efficiency. By predicting several tokens in parallel, the model can potentially accelerate the generation process compared to predicting one token at a time. This capability can be leveraged by inference techniques such as speculative decoding, which utilizes a draft model (potentially the MTP model itself predicting multiple tokens) to generate a sequence of candidate tokens that are then verified by a more powerful model [4]. Although the specifics of MTP integration with speculative decoding vary, the core multi-token prediction ability provides the necessary look-ahead capacity to enable such accelerated inference methods. Furthermore, MTP's training benefits, such as improved data efficiency and better pre-planning [39], contribute to the overall performance and efficacy of models utilizing this mechanism on evaluation benchmarks [39].​  

# 7.2 Performance Improvements with MTP  

Multi-Token Prediction (MTP) has demonstrated notable empirical benefits in sequence generation tasks, leading to both accelerated inference and enhanced accuracy compared to traditional single-token prediction methods [19]. This holds true for various generation domains, including code and natural language generation [4].  

Quantitative evaluations on standard benchmarks highlight these improvements. For instance, models trained with MTP have shown significant gains in problem-solving ability on programming benchmarks such as HumanEval and MBPP [4,16]. A 13 billion-parameter model specifically exhibited a $1 2 \%$ increase in performance on the HumanEval benchmark and a 17 $\%$ increase on the MBPP benchmark when trained with MTP [4,16]. These results underscore MTP’s effectiveness in improving the quality of generated code [4].​  

Beyond accuracy, a key advantage of MTP lies in its ability to accelerate inference speed. Experiments have shown that training models with MTP can yield up to a $3 \times$ increase in inference speed [4,16]. This acceleration is observed even in computationally demanding scenarios like large-batch processing, making MTP a valuable technique for improving the efficiency of large language models [16]. The mechanism of predicting multiple tokens concurrently addresses certain limitations inherent in traditional teacher forcing during training, which may contribute to the observed accuracy improvements [8].  

Analysis of factors influencing MTP’s effectiveness indicates that its advantages tend to be more pronounced with increasing model scale [16]. Studies involving models ranging from 300 million to 13 billion parameters have shown that larger models benefit more significantly from the application of MTP [16]. While the impact of training epochs is also a relevant consideration, the empirical data strongly supports the positive correlation between model size and the performance gains achieved through MTP [4,16].​  

# 7.3 Ablation Studies and Optimal Token Prediction Count  

Ablation studies are pretty crucial for figuring out the optimal configuration of multi-token prediction (MTP) models— especially when it comes to choosing how many future tokens, $\scriptstyle n$ , you predict in one go [4].  

Research shows that picking the right value of $n$ can make a big difference for both performance and efficiency. In fact, ablation experiments found that predicting four future tokens ( $n = 4$ ) at once delivers the best results on benchmarks like HumanEval and MBPP [4].  

Interestingly, the best window size $n$ isn’t one-size-fits-all—it really depends on the nature of your data. Code versus natural language text, for example, can call for different values of $n$ to hit peak performance [4].  

While these digests highlight the main takeaways about optimal token counts and data dependency, they stop short of detailing the exact ablation methodology or providing a quantitative breakdown of the trade-offs between inference speedup and any drop in generation quality or accuracy as you vary $\scriptstyle n$ .​  

# 7.4 Large Concept Models (LCMs)  

Large Concept Models (LCMs) represent a departure from traditional AI models that rely on discrete token representation, such as those used extensively in natural language processing. The core idea behind LCMs is to model information within a high-dimensional concept space rather than processing sequences of tokens [3]. This approach utilizes techniques like  

SONAR to create these high-dimensional embedding spaces, enabling a language- and modality-agnostic representation of concepts [3,30]. By operating at the concept level, LCMs aim to capture richer semantic relationships and information across various data types in a unified manner [3].​  

This concept-level modeling offers several potential benefits compared to conventional token-level processing. Operating in a continuous, high-dimensional space is posited to enhance efficiency and scalability, allowing for more flexible handling of diverse information [3]. A key advantage is the inherent language- and modality-agnostic nature of the representation. Since information is encoded as concepts in a shared space, the model is not constrained by the specific syntax or structure of individual modalities like text, images, or audio [3,30]. This facilitates seamless integration and processing of multimodal data, contrasting with methods that might require specific tokenization or alignment strategies for each modality [3].  

The implications of LCMs for achieving broader multimodal AI capabilities are significant. By providing a unified conceptua grounding, LCMs are designed to process and generate content that transcends the boundaries of single modalities [3,30]. This architecture, which may incorporate hierarchical structures and diffusion-based generation, supports zero-shot generalization, allowing the models to handle unseen combinations of concepts and modalities [3]. While detailed performance metrics are not exhaustively provided in the analyzed materials, LCMs have been explored for tasks such as summarization, indicating their applicability to complex information synthesis requiring cross-modal understanding or generation [3]. The scalability and inherent flexibility of concept-level processing via high-dimensional embeddings like SONAR suggest a promising direction for developing more integrated and versatile multimodal AI systems [3,30].  

# 7.5 Object Recognition as Next Token Prediction  

A significant approach within multimodal intelligence involves framing object recognition as a next token prediction task [20]. In this paradigm, the system analyzes visual input and subsequently predicts text tokens that represent image attributes, labels, or other relevant textual descriptions of the objects present [20]. This method leverages the power of large language models (LLMs) by aligning visual information with linguistic representations. The typical architecture employed in such systems involves the integration of a visual encoder with a language model backbone [20]. For instance, research has explored combining visual encoders like CLIP with language models based on Llama parameters to facilitate the prediction of image attributes as a sequence of text tokens [20].  

A key advantage of framing object recognition in this manner is its inherent ability to enable open-domain image tagging. By utilizing the vast vocabulary and world knowledge embedded within the language model, the system is not limited to predicting a predefined set of classes, unlike traditional closed-set object recognition systems. Instead, it can generate descriptive tags or labels for a wide variety of objects, including novel or less common ones, by leveraging its understanding of both visual and linguistic concepts [20]. The process involves feeding the visual features extracted by the encoder into the language model, which then autoregressively predicts the sequence of tokens corresponding to the desired output (e.g., object labels, attributes) [20]. While this approach offers considerable flexibility and open-domain capabilities, its practical deployment necessitates the development and evaluation of efficiency strategies to manage the computational demands associated with large language models and sequential token generation [20].  

# 8. Applications of Multimodal Next Token Prediction  

Multimodal Next Token Prediction (MMNTP) models represent a significant advancement in AI, enabling a wide spectrum of applications by processing and generating content across diverse modalities.  

<html><body><table><tr><td>Category</td><td>Examples</td></tr><tr><td>Multimodal Generation</td><td>Image/Video from Text, Image Captioning, Video Captioning, Interleaved Video/Text Gen,Speech-to-Speech</td></tr><tr><td>Multimodal Understanding</td><td>Visual Question Answering (VQA), Video Understanding, Recipe Generation from Photos, Speech Translation</td></tr><tr><td>Specific Domains</td><td>Autonomous Navigation (Trajectory Pred.), Robotics, Healthcare, Education,</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Autonomous Driving, Industry</td></tr></table></body></html>  

These applications can be broadly categorized based on their primary function: multimodal generation, multimodal understanding, or their implementation within specific domains, often requiring real-time processing and integration [6]. The MMNTP paradigm offers a unified sequence-to-sequence framework that simplifies model architecture and training for various tasks, although the primary focus in the provided literature is on the breadth of applications and associated challenges rather than architectural details.  

In the realm of multimodal generation, MMNTP models are employed to create new content by predicting subsequent elements in a sequence that can span different modalities. This includes generating images from text descriptions, as demonstrated by models like DALL-E 3 [6], and generating videos from text prompts, exemplified by Runway Gen-2 and Loong, which can generate long videos [6,29]. A core task is image captioning, which involves producing descriptive text for visual inputs, sometimes incorporating specific image attributes or text present within the image, as in TextCap where methods like "Anchor-Captioner" generate detailed descriptions for different parts [18,20,38]. Related generative tasks include emotional video captioning [24], cross-lingual image captioning [24], and video captioning [5]. Advanced models can also generate interleaved sequences of video and text or perform multilingual summarization and summary expansion [3,29]. Speech-to-speech translation, converting spoken language in one modality to spoken language in another, also falls under generation [37].​  

Multimodal understanding tasks leverage MMNTP models to interpret and derive insights from combined multimodal inputs. This encompasses tasks like Visual Question Answering (VQA), where models answer questions based on image content [1,5,24], and broader video understanding tasks [5,24]. Examples include generating recipes from photos [6], speech translation [37], video anomaly detection using frameworks like MIST [38], RGBT crowd counting combining RGB and thermal data [38], visual chain-of-thought reasoning [29], and entity recognition [15]. Models like LLaVA-Video specifically focus on video understanding benchmarks [30]. Retrieval tasks, such as finding images or videos based on text queries, also rely on multimodal understanding [5,15].​  

Beyond this categorization, MMNTP and related multimodal approaches are applied across various specific domains. In autonomous navigation, trajectory prediction is crucial, utilizing multimodal data like AIS and electronic charts for ships with models like S-Transformer or predicting vehicle movements using LSTM-based models [14,26]. Autonomous driving more broadly involves processing multi-sensor data for perception and decision-making [11,35]. Robotics benefits significantly, particularly in embodied intelligence, where multimodal understanding improves interaction and environmental navigation [6,11]. Specific computer vision tasks relevant to domains like tracking include object detection, object recognition/tagging, and pose estimation, which leverage multimodal inputs or are enhanced by transformer-based approaches [20,28,38]. Image restoration tasks like denoising, inpainting, and super-resolution also utilize related transformer-based techniques [23,38].​  

MMNTP holds significant potential in healthcare for enhanced diagnostics and treatment prediction using diverse patient data, including medical image segmentation [6,11,28]. In education, it can facilitate personalized learning resource creation [10,11]. Multimodal AI also finds applications in industry for process optimization [6], customer service by analyzing various communication cues [6], and supporting complex tasks like sign language translation and remote physiological measurement [24].  

Implementing these applications, especially those requiring real-time processing in dynamic environments, necessitates robust infrastructure. Frameworks like the open-source Platform for Situated Intelligence are designed to manage streaming multimodal data efficiently, supporting the development of situated, real-time AI systems [17]. Such platforms provide essential runtime capabilities and tools for handling diverse data streams, synchronization, and low-latency processing, which are critical for deploying MMNTP models in interactive scenarios like embodied conversational agents or smart environments [17].​  

Despite the versatility, challenges persist. Ensuring coherent and relevant outputs in generation, accurately interpreting complex inputs in understanding, and handling specific data characteristics (e.g., text within images, limited annotations for video anomaly detection) remain active research areas [24,38]. Limitations in training paradigms, such as teacher-forced training, may also affect the ability of models to perform tasks requiring planning and reasoning in real-world scenarios, impacting applications like robotics and autonomous driving [8]. Nevertheless, the emergence of advanced MMNTP models like Emu3, which demonstrate strong performance across both generation and perception tasks, often surpassing task  

specific models, highlights the potential of this approach towards achieving generalized multimodal intelligence [1,29,30]. Future directions involve overcoming these challenges through improved model architectures, training strategies, and specialized infrastructure to fully realize the potential of MMNTP across an ever-expanding range of multimodal applications.  

# 8.1 Multimodal Generation and Understanding  

Multimodal Next Token Prediction (MMNTP) models are extensively applied to fundamental multimodal tasks, encompassing both the creation of content across different modalities (generation) and the interpretation of multimodal inputs (understanding) [38]. This framework allows diverse tasks to be modeled within a unified sequence-to-sequence paradigm, where the model predicts the next token in a multimodal sequence based on preceding tokens and inputs. While the simplification of model architecture and training through this unified framing is a key advantage, the provided digests primarily highlight the range of applications and associated challenges rather than detailing the architectural simplifications themselves.  

In multimodal generation, MMNTP models are employed for tasks such as generating images from text, exemplified by models like DALL-E 3 [6], and creating videos from textual prompts, demonstrated by Runway Gen-2 [6] and Loong, which focuses on generating long videos [29]. Image captioning, a core generation task, involves producing descriptive text for images [18], including generating specific image attributes [20]. More specialized forms include generating captions for images containing text (TextCap) using methods like "Anchor-Captioner" which produces multiple detailed descriptions for different image parts [38], generating emotional video captions with Emotion-Prior Awareness Networks, and cross-lingual image captioning utilizing Visual-Linguistic-Stylistic Triple Rewards [24]. Video captioning is another application of generating text from video input [5]. Furthermore, models like MIO can generate interleaved sequences of video and text [29], and Large Concept Models (LCMs) have been applied to multilingual summarization and summary expansion, demonstrating capabilities in generating coherent text across languages [3]. The paradigm also extends to speech-to-speech translation, converting and generating speech in another language [37].  

For multimodal understanding, MMNTP models interpret multimodal inputs to derive meaningful insights. Key tasks include Visual Question Answering (VQA), where models answer questions based on image content [1,5,24], and video understanding [5,24]. This encompasses diverse applications like generating recipes from dish photos [6] and connecting visual and textual data for broader understanding [6]. Specific understanding tasks addressed include speech translation [37], video anomaly detection (WS-VAD) using frameworks like MIST [38], RGBT crowd counting which uses RGB and thermal images to understand scenes under challenging conditions [38], and visual chain-of-thought reasoning as demonstrated by MIO [29]. While primarily focused on network interpretation, studies on image super-resolution networks using Local Attribution Maps also indirectly relate to understanding generated visual content [38]. Models like LLaVA-Video have shown strong performance on video understanding benchmarks [30].​  

Despite the versatility afforded by the MMNTP framework, both multimodal generation and understanding present specific challenges. For generation, ensuring coherent and relevant outputs is critical [24]. In understanding tasks, accurately interpreting complex multimodal inputs remains a significant hurdle [24]. Specific challenges arise from the nature of the input, such as describing complex scenes containing both visual elements and text in TextCap [38] or performing tasks like video anomaly detection with limited video-level annotations [38]. MMNTP models attempt to solve these by employing various techniques tailored to the specific task and modalities, such as cross-modal collaborative representation learning for RGBT counting [38] or leveraging synthetic datasets for instruction tuning as in LLaVA-Video [30]. Advanced models like Emu3 have demonstrated remarkable capabilities, excelling in both generation and perception tasks and often outperforming task-specific models, underscoring the potential of the MMNTP approach to achieve generalized multimodal intelligence [1,29,30].​  

# 8.2 Specific Application Examples  

Multimodal next token prediction (MMNTP) models are increasingly applied across a diverse range of domains, leveraging their ability to process and generate information from multiple modalities. These applications highlight the flexibility and growing capabilities of MMNTP systems in tasks that require understanding and generating content that spans visual, textual, auditory, and other data types.  

One prominent application area is Image Captioning, where models generate descriptive text for images. Early approaches often employed a CNN encoder and an RNN decoder architecture to translate visual features into sequential text [18]. More advanced tasks, such as Text-based Image Captioning (TextCap), require models to read and reason about text present within images, necessitating methods like the Anchor-Captioner to generate detailed descriptions capturing various image elements [38]. Multimodal pre-trained models are widely applied to image captioning [5], and recent models like Emu3 have demonstrated capabilities in this area [1]. Relatedly, models are being developed for emotional video captioning, adding a layer of affective understanding to the generated descriptions [24].  

Trajectory Prediction is another domain benefiting from multimodal approaches. Specifically, models like S-Transformer have been applied to predicting ship trajectories by integrating Automatic Identification System (AIS) data with electronic sea charts, demonstrating the use of varied data sources for forecasting dynamic movements [26].  

In the realm of language and speech, MMNTP models contribute to Speech Translation. These models are designed to convert spoken language directly into text in another language, addressing challenges such as the lack of readily available transcripts and the modality gap between audio and text. Applications extend to simultaneous machine translation and related tasks like automatic speech recognition [36,37].  

While visual object tracking was noted as a potential application area, related tasks in computer vision are explicitly discussed, providing insights into the underlying capabilities pertinent to tracking. These include Object Detection and Object Recognition or tagging, where models process visual input to identify and label objects [20,28]. Furthermore, applications like object pose estimation, particularly category-level 6D pose estimation from RGB-D images, are relevant as they contribute to understanding object state and location, crucial for tracking systems [28,38]. An example includes using an Azure Kinect device to detect and identify objects a person is pointing to, involving steps like depth mapping, point cloud intersection, and object detection [17].​  

Image Restoration tasks, such as denoising and inpainting, also leverage transformer-based approaches within a multimodal context [23]. Image super-resolution, which aims to enhance image quality by generating high-resolution outputs from low-resolution inputs, is another related area of research [38].  

MMNTP capabilities significantly enhance Robotics, particularly in embodied intelligence scenarios. By improving multimodal understanding, models like Emu3 enable robots to better interact with humans and navigate their environment [11]. This involves interpreting complex sensory input for improved perception and facilitating more natural human–robot collaboration.  

In the sectors of Healthcare and Education, multimodal AI holds considerable promise. Applications include processing diverse patient data for enhanced diagnostics and treatment prediction in healthcare [6,11], as well as creating personalized educational resources [11]. Medical image segmentation is a specific task within healthcare benefiting from advanced vision models [28].  

Autonomous Driving is another complex application domain where processing multi-sensor data is critical for perception and decision-making [11]. Multimodal systems can enhance situational awareness, for instance, by monitoring drivers for fatigue and providing recommendations [6]. Automatic license plate recognition, often involving image processing and optical character recognition, represents a specific multimodal application within the automotive context [28].  

Beyond the explicitly listed areas, multimodal models underpin various other applications. These include Visual Question Answering (VQA) and Visual Commonsense Reasoning (VCR), where models answer questions or reason about images based on visual and textual input [1,5,24]. Retrieval tasks like image and video retrieval based on text queries or vice versa are also common applications [5]. Video understanding tasks such as action segmentation, action step localization, and video anomaly detection utilize temporal and visual information [5,38]. Furthermore, generative applications are expanding, encompassing video generation from text prompts [1,29,30], extending videos by predicting future frames and actions [1], interleaved video–text generation, visual chain-of-thought reasoning, visual guide generation, and instructional image editing [29]. Applications like sign language translation and remote physiological measurement also demonstrate the breadth of multimodal processing [24]. In industrial settings, multimodal AI is employed to optimize manufacturing processes and enhance product quality [6]. Customer service benefits from analyzing text, voice tone, and facial expressions to gauge satisfaction and provide support [6]. Specific tasks like crowd counting using combined RGB and thermal data further illustrate cross-modal integration [38].​  

# 8.3 Situated Intelligence  

Developing multimodal intelligence applications, particularly those requiring real-time processing of streaming data and adherence to strict latency constraints in dynamic environments, necessitates robust and specialized infrastructure. Frameworks designed to handle these complex requirements are crucial for the successful deployment of advanced models like Multimodal Next Token Prediction (MMNTP) in real-world situated intelligence scenarios. The Platform for Situated Intelligence is presented as an open‐source example of such a framework, offering a modern infrastructure specifically tailored for managing multimodal streams of data [17].  

The platform provides a comprehensive environment that includes a runtime, a rich set of development tools, and an open ecosystem of components, collectively facilitating the development cycle and promoting rapid prototyping and reuse [17]. Its core architecture is built around a streaming infrastructure optimized for the specific demands of multimodal, integrative AI applications [17]. The runtime component plays a critical role in this architecture by carrying messages on the data streams and orchestrating the execution of various components. This orchestration is achieved through the scheduled delivery of messages, which enables efficient processing of incoming real-time data [17]. Furthermore, the runtime leverages pipeline parallelism, an architectural feature that allows different stages of processing to occur concurrently, thereby enhancing throughput and potentially reducing latency. The design also aims to make efficient use of CPU resources within a multi-core execution environment, which is essential for handling the computational demands of processing multiple high-bandwidth data streams simultaneously [17].​  

These architectural features and capabilities directly address the challenges inherent in situated intelligence applications, such as handling diverse data modalities (e.g., vision, audio, text) that arrive as continuous streams, maintaining synchronization between these streams, and processing them with low latency to enable timely actions or predictions. While the specific mechanisms for multimodal data handling, synchronization, and fusion within the platform's components would determine its full capabilities, its fundamental design around optimized multimodal streaming and efficient runtime orchestration provides a strong foundation. Such platforms hold significant potential to facilitate the development and deployment of MMNTP models in real-world scenarios. By providing the necessary infrastructure for handling real-time, multimodal input streams efficiently, they allow researchers and developers to focus on the model architecture and training, abstracting away much of the complexity associated with the data pipeline and execution environment required for situated intelligence [17]. The support for rapid prototyping and reuse within its ecosystem furthe lowers the barrier to entry and accelerates the iteration cycle for developing and deploying MMNTP-powered applications in interactive, dynamic environments.​  

# 9. Challenges and Future Directions  

The field of Multimodal Next Token Prediction (MMNTP), while demonstrating significant promise towards developing more integrated and capable AI systems, faces a spectrum of formidable challenges and inherent limitations [2,8,9,13,16,32]. These hurdles span technical, data-centric, and ethical dimensions, collectively demanding substantial research effort to realize the full potential of MMNTP.  

A primary technical challenge lies in managing the computational efficiency and scalability required to process highdimensional multimodal data and train increasingly large models. The sheer scale of multimodal data and the complexity of modern architectures, particularly the quadratic scaling of Transformers with sequence length, lead to significant computational costs for both training and inference [13]. This necessitates ongoing work in model compression, efficient architectures, and distributed computing [25,35]. Closely related is the difficulty in handling long sequences and capturing long-range dependencies across modalities, which is essential for maintaining coherence and understanding context over extended inputs [13]. Techniques involving architectural modifications and specialized training strategies are being explored to address this limitation. Furthermore, achieving accurate multimodality alignment and integration remains challenging due to the intrinsic heterogeneity of data types, making it difficult to capture complex, fine-grained interactions and synchronize information temporally [6,13,17].  

Beyond technical architecture, data scarcity and quality pose significant barriers. Training robust and generalizable MMNTP models requires vast, diverse, and high-quality multimodal datasets, which are often limited in availability across specific domains [6]. Issues such as data completeness, integrity, density, and inherent biases across modalities can significantly impact training effectiveness and perpetuate biases in model outputs [6]. Addressing this requires innovation in data collection, curation, and the development of techniques like synthetic data generation and multi-stage data processing [29,30].​  

Current MMNTP models also exhibit limitations in complex reasoning and planning tasks. The standard next token prediction objective, particularly when paired with methods like teacher forcing, can restrict a model's capacity for lookahead and exploration, crucial for multi-step problem-solving and generating logically consistent outputs [8]. This highlights the need for integrating external knowledge and improving the underlying reasoning mechanisms [35]. Ensuring robustness and generalization is another critical area, as models must maintain performance under noisy or incomplete inputs and generalize effectively to unseen data distributions [29]. Techniques like multimodal data augmentation and domain adaptation are being investigated to enhance these properties.  

Furthermore, critical concerns surrounding model honesty and reliability necessitate rigorous attention. The potential for MMNTP models to generate biased, harmful, or hallucinated content underscores the need for robust detection and filtering mechanisms, as well as established usage norms [10,29]. The inherent opacity of large neural networks complicates understanding and diagnosing these issues [6]. This is closely intertwined with the broader ethical and societal implications of deploying powerful multimodal AI systems, including data privacy, potential misuse for generating fake content, fairness issues arising from biases, and impacts on employment and information integrity [6,32].  

<html><body><table><tr><td>Category</td><td>Areas</td></tr><tr><td>Model Design&Arch.</td><td>Efficient& scalable architectures,Novel paradigms (e.g., concept models),Improved attention/fusion</td></tr><tr><td>Data&Tokenization</td><td>Handling data scarcity/quality, Advanced tokenization, Representing complex relationships</td></tr><tr><td>Capabilities</td><td>Enhancing reasoning/planning, Improving robustness/generalization, Model honesty/reliability</td></tr><tr><td>Broader Al Goals</td><td>Unified world models, Pathways to AGl, Continuous Al evolution</td></tr></table></body></html>  

Addressing these multifaceted challenges delineates several key future research areas. Developing novel, more efficient, and scalable architectures better suited for multimodal data is paramount, alongside exploring improved tokenization techniques that reduce redundancy and enhance adaptability [5,25]. Overcoming the limitations of standard training paradigms like teacher forcing by exploring alternative training methodologies such as reinforcement learning and incorporating external verifiers or reward models is crucial for enhancing planning and reasoning capabilities [8,12,39]. Future work must also focus on enhancing methods for capturing complex cross-modal relationships and long-range dependencies more effectively [5]. Ultimately, these efforts contribute towards the ambitious long-term goal of developing unified world models and exploring the potential of MMNTP as a foundational paradigm for Artificial General Intelligence (AGI), while concurrently prioritizing rigorous ethical and safety research to ensure beneficial societal outcomes [10,32].  

# 9.1 Computational Efficiency and Scalability  

The development and deployment of large multimodal models for next token prediction (MMNTP) are significantly challenged by computational demands. Processing high-dimensional multimodal data inherently requires substantial computational resources and memory. Furthermore, the complex architectures commonly employed—particularly Transformers—exhibit quadratic complexity with respect to sequence length, leading to high computational costs for both training and inference [13]. Beyond model processing, the sheer volume and variety of data in multimodal AI introduce challenges related to data quality, storage requirements, and the handling of redundancy [6]. These factors necessitate the exploration and implementation of techniques aimed at improving computational efficiency and scalability.  

Addressing the computational burden requires a multi-faceted approach encompassing optimizations at the model, algorithmic, and system levels. Model size reduction is a key strategy, enabling deployment on devices with limited resources. Knowledge distillation is one such technique used to compress large models, making them suitable for mobile devices [35]. Similarly, the design of lightweight network architectures, such as the CDNet with approximately 1.8M  

parameters for person re-identification, demonstrates that comparable performance can be achieved with significantly fewer parameters compared to state-of-the-art lightweight models [38]. Concept-level modeling offers another route by reducing the effective sequence length compared to fine-grained token-level processing, directly mitigating the quadratic complexity issue of Transformers and allowing more efficient handling of long multimodal contexts [3].  

Beyond model size, algorithmic and architectural innovations enhance efficiency. Specialized architectures like the voxel hashing-based approach in the Fast Point Transformer significantly reduce inference time [28]. Efficiency can also be improved by optimizing the processing of input data; for instance, the Run-Length Tokenization (RLT) method enhances efficiency by reducing the number of tokens that need to be processed, demonstrating approximately $3 0 \%$ reduction in training time and a $3 0 \mathrm { - } 6 0 \%$ reduction in inference computation and latency on specific vision tasks with minimal accuracy loss (less than $0 . 1 \%$ for training and less than $0 . 5 \%$ for inference) [25]. Efficient architecture search methods, which compare architectures rather than estimating absolute performance, also contribute to reducing the computational cost of finding suitable efficient models [38].​  

For training and deploying increasingly large models, system-level optimizations and distributed computing are indispensable. Large models like DeepSeek-V3, trained on substantial computing clusters (e.g., 2048 NVIDIA H800 GPUs), rely on sophisticated distributed training frameworks. Efficient frameworks like HAI-LLM integrate techniques such as data parallelism, tensor parallelism, and pipeline parallelism (e.g., the DualPipe algorithm) to maximize hardware utilization and minimize communication overhead. Customized communication kernels, such as cross-node All-to-All, further optimize communication and computation overlap. The adoption of mixed-precision training, including formats like FP8, also contributes to reducing memory bandwidth and computational requirements during training.​  

Efficiency is also critical for inference and enabling accessibility, particularly on resource-constrained edge devices. Techniques like offloading and mixed reasoning are employed to reduce memory load during inference [35]. Runtime systems can leverage pipeline parallelism for efficient CPU utilization in multi-core environments [17]. For large models (e.g., 70B-scale LLMs), tensor-parallel inference systems combined with memory management strategies like sliding window schedulers enable deployment on low-resource edge devices, addressing significant computational and memory limitations [29,30]. Furthermore, specific memory-efficient implementations are being developed to tackle high GPU memory usage challenges in advanced prediction tasks like multi-token prediction [4].  

The pursuit of computational efficiency inherently involves trade-offs. Reducing model size through compression or lightweight designs may sometimes lead to minor reductions in performance compared to larger, more complex models, although efforts focus on minimizing this gap [25,38]. Achieving higher performance often requires larger models and more complex architectures, escalating computational costs and limiting accessibility. Conversely, prioritizing computational efficiency and reduced cost, such as enabling edge device deployment through methods like tensor parallelism and optimized schedulers [29], enhances accessibility but might constrain the maximum achievable model size and performance on that specific hardware. The ongoing research explores the optimal balance point, leveraging insights into model behavior—such as layer contributions for efficient scaling [22]—and adopting the perspective that algorithmic improvements should primarily facilitate scalability [32]. These efforts collectively aim to make powerful MMNTP models more practical and widely deployable.​  

# 9.2 Robustness and Generalization  

Robustness and generalization are critical properties for Multimodal Next Token Prediction (MMNTP) models to function effectively in real-world, unpredictable multimodal environments. Robustness refers to the model's ability to maintain performance when exposed to noisy, incomplete, or adversarial inputs, while generalization concerns its capacity to perform well on unseen data or distributions different from the training set.  

A key issue impacting robustness is the ambiguity inherent in unimodal inputs when presented without their corresponding multimodal context. For example, interpreting the precise meaning of text often requires supplementary information from modalities like speech inflections or facial cues, which, if missing or distorted, can lead to misinterpretations by the AI [6]. This underscores the necessity for MMNTP models to effectively integrate and weigh information from various modalities to resolve ambiguities and achieve robust understanding. Furthermore, models trained using techniques such as teacher forcing, common in sequence generation tasks including next token prediction, have demonstrated brittleness and a reduced ability to generalize to unseen data, particularly in tasks that require complex planning or exhibit significant divergence from the training distribution [8]. The process of fine-tuning large models, often necessary for adapting them to specific multimodal tasks, introduces challenges such as overfitting on the potentially limited fine-tuning data and catastrophic forgetting of broader knowledge acquired during pre-training, both of which degrade generalization performance [35]. Addressing domain shift, where the distribution of data encountered during deployment differs significantly from the training data, represents a significant challenge for generalization [38].​  

Various techniques are explored to enhance the robustness and generalization capabilities of MMNTP models. Multimodal data augmentation is a strategy aimed at increasing the variability and coverage of the training data by applying transformations to multimodal inputs while preserving their semantic consistency [35]. For instance, online box-cage based 3D deformation mechanisms have been proposed as a data augmentation method to improve the generalization ability of networks for category-level 6D object pose estimation, a task involving visual and 3D data [38]. Adversarial training, which involves exposing the model to adversarial examples designed to challenge its current capabilities, and the implementation of effective regularization methods are techniques used to build resilience against input perturbations and prevent overfitting [35]. To mitigate the impact of domain shift, domain adaptation techniques are employed. Cross-domain adaptive clustering, for example, has been investigated for semi-supervised domain adaptation to align features across different data domains [38]. Beyond specific training techniques, the inherent design and pre-training of models play a crucial role. Large Concept Models (LCMs), leveraging extensive multilingual and multimodal datasets like SONAR during training, have demonstrated strong zero-shot generalization performance on unseen languages and modalities [3]. While evaluations of models like DeepSeek LLM on standard benchmarks indicate generalization across tasks and languages [39], the specific challenges and methods for achieving robust next token prediction across diverse and potentially noisy multimodal inputs require dedicated focus.  

In summary, improving robustness and generalization in MMNTP models requires addressing challenges related to multimodal ambiguity, training method limitations, overfitting, and domain shift. Techniques such as multimodal data augmentation, adversarial training, regularization, and domain adaptation methods contribute to enhancing performance in real-world settings. Furthermore, developing architectures and pre-training strategies that inherently promote zero-shot generalization across modalities holds significant promise. Despite these advances, consistently ensuring robustness against complex adversarial multimodal inputs and achieving seamless generalization across vastly different and unpredictable multimodal distributions remain areas of ongoing research.​  

# 9.3 Data Scarcity and Quality  

A significant challenge in effectively training multimodal next token prediction (MMNTP) models is the necessity for vast, high-quality, and diverse multimodal datasets [5,6]. Limited availability of data, particularly in specific domains such as 3D shape modeling and reconstruction [38], medical imaging [28], and high-quality raw video data for large multimodal models (LMMs) [29,30], hinders model development and generalization. Beyond mere volume, data quality—encompassing completeness, integrity, and information density—profoundly impacts training efficiency and model performance [6,39]. Datasets with simple or short captions, such as some iterations of Conceptual Captions, may restrict the model's capacity to capture complex relationships between modalities [5]. Furthermore, data quality is critical for the effectiveness of downstream applications, as seen in the reliance of Retrieval Augmented Generation (RAG) on the quality of its knowledge base for result relevance [35]. Data quality issues, including inconsistencies and inherent biases across modalities, pose substantial risks during model training, potentially perpetuating or even amplifying these biases in the model's outputs.  

To address these data limitations, researchers have explored various solutions. Generating synthetic data has emerged as a viable strategy to mitigate scarcity, particularly demonstrated in the creation of datasets for tasks like video instruction tuning [29] and addressing the lack of high-quality raw video data with resources like LLaVA-Video-178K [30]. Data curation and refinement techniques are also crucial; strategies such as multi-stage processing involving de-duplication, filtering, and re-mixing can enhance data diversity and information density, as implemented in the training of large language models [39]. Improving the content quality of existing data, such as rewriting image captions, has also been shown to enhance pretraining performance for multimodal foundation models [29].  

In addition to data generation and curation, methods focusing on enriching feature representations or improving model generalization under scarcity have been developed. Examples include utilizing position embeddings and Multi-Graph Reasoning (MGR) to handle insufficient samples in medical imaging data [28], or optimizing learned priors and latent codes after training to improve generalization in 3D learning tasks with limited data [38]. Other potential avenues for leveraging limited data include transfer learning from large unimodal or multimodal datasets, multimodal data augmentation, and methods for leveraging weakly supervised or unlabeled data. Addressing data scarcity and ensuring high data quality  

remain central challenges requiring ongoing innovation in data collection, generation, curation, and model design for the advancement of MMNTP.  

# 9.4 Handling Long Sequences and Dependencies  

Effectively processing and capturing long-range dependencies constitutes a critical challenge in developing multimodal next token prediction (MMNTP) models, particularly when dealing with extensive sequences of multimodal data [13]. These dependencies can span across significant temporal distances, integrate information across different modalities, or connect disparate semantic concepts within the sequence. Addressing this challenge is essential for models to maintain coherence, understand context, and make accurate predictions over extended inputs.  

Researchers are exploring various techniques to mitigate the complexities arising from long multimodal sequences, such as the high computational cost and memory requirements associated with standard attention mechanisms, and the difficulty in preserving relevant information over time [16]. One approach involves specialized training strategies aimed at extending the model's effective context length. For instance, the DeepSeek-V3 model utilized a two-stage context length extension training process to bolster its capacity for handling longer contexts [39].  

Other methods focus on architectural modifications or specialized mechanisms tailored to sequential and long-term dependencies. For sequential multimodal data streams, such as Automated Identification System (AIS) data, Segment Recurrence has been introduced as a technique specifically designed to capture long-term dependencies [26]. In the domain of video processing, which inherently involves long temporal sequences, models like HERO are designed to capture temporal information, implicitly addressing the challenge of handling video sequences over time [5]. For generating long videos with autoregressive language models, a progressive training strategy combined with specific inference methods has been proposed in the "Loong" paper to reduce error accumulation across frames [29,30].​  

Structural innovations are also being explored. Large Concept Models (LCMs) adopt a hierarchical structure, which has been observed to improve coherence in generated long-form content and enable localized edits without negatively impacting the broader context [3]. This aligns with the potential benefits of employing hierarchical model structures for managing dependencies across different granularities in long sequences [16]. Additional techniques discussed in the field include employing efficient attention mechanisms (such as sparse, global, or hierarchical attention), combining attention with recurrence, and exploring concepts similar to Multi-Token Prediction (MTP) for processing larger data chunks more efficiently [16].​  

The effectiveness of these techniques is evaluated based on their ability to enable MMNTP models to generate coherent, contextually relevant, and accurate outputs when conditioned on extended multimodal inputs. Developments highlighting the critical importance of effective long context processing and the utilization of long-range dependencies, such as the focus seen in models like Kimi, underscore the ongoing need for advancements in this area [32]. Successful techniques demonstrate improved performance on tasks requiring deep understanding and generation based on long multimodal histories, such as complex question answering, long video generation, and prediction in sequential multimodal streams.  

# 9.5 Multimodality Alignment and Integration  

A persistent and significant challenge in the field of multimodal intelligence lies in achieving robust and accurate alignment and integration of information across disparate modalities [13]. This difficulty stems from the inherent differences in the temporal, spatial, and semantic characteristics of various data types, making it challenging to properly align meaningful data that represents the same real-world events or concepts [6,13]. The ability to learn the correspondence between modalities is considered crucial, often emphasized in pre-training tasks designed to encourage models to capture these relationships [5].​  

Current alignment and fusion techniques face limitations in effectively capturing complex, fine-grained interactions and dependencies between modalities [13]. While progress has been made through approaches such as scene graph prediction, which aims to enable models like ERNIE-ViL to capture fine-grained alignment between images and text [5], or cross-modal collaborative representation learning frameworks that utilize mechanisms like the Information Aggregation-Distribution Module (IADM) to capture complementary information [38], these methods often struggle with the full spectrum of crossmodal complexity. Handling temporal synchronization is a specific hurdle; platforms designed for situated intelligence, for instance, address this by making time a primary construct to facilitate data fusion while being aware of latency [17].  

Furthermore, exploring different data mixtures, such as combining synthetic captions and AltTexts, has been shown to influence alignment and performance, highlighting the empirical nature of finding effective alignment strategies [29].  

The limitations of current techniques underscore the need for more sophisticated alignment and integration methods. The development of unified world models capable of processing different modalities is highlighted as an important direction, pointing towards the necessity for improved techniques in this area [32]. Modality-agnostic approaches, such as those processing content at a purely semantic level, offer a promising alternative paradigm, potentially allowing for seamless transitions across languages and modalities by abstracting away lower-level representation differences [3]. Future research directions should focus on developing robust techniques that can handle the intrinsic heterogeneity of multimodal data, capture subtle and complex inter-modal dependencies more effectively, and move towards more unified, representationlevel solutions that can generalize across diverse multimodal tasks and scenarios.​  

# 9.6 Reasoning and Planning Limitations  

Multimodal models grounded in the next token prediction (NTP) paradigm face notable limitations when confronted with tasks demanding complex multimodal reasoning, strategic planning, and the seamless integration of external or world knowledge [8,34]. The very nature of the NTP framework, particularly when coupled with training methodologies like teacher forcing, can restrict a model's inherent lookahead capability and its capacity to explore alternative sequence paths, which are crucial for effective planning and complex problem-solving [8]. This limitation can manifest in various ways, including difficulty in capturing nuanced semantic meanings, generating predictions that appear illogical or inconsistent within a broader context, and struggling with tasks requiring chained logical deductions or multi-step processes. For instance, models might exhibit weakness in handling problems that necessitate deep logical chains, such as the steps involved in mathematical proofs, when compared to methods potentially better equipped for structured reasoning or knowledge retrieval [35]. While Retrieval-Augmented Generation (RAG) approaches aim to integrate external knowledge, their effectiveness in complex logical deductions highlights the underlying challenge of reasoning within the generation paradigm [35]. Current models are broadly acknowledged to possess inherent limitations in sophisticated reasoning and planning abilities [32]. Achieving a level of multimodal intelligence akin to human cognitive abilities requires overcoming these limitations, necessitating models capable of robust, long-horizon planning, intricate causal reasoning, and flexible integration of diverse knowledge sources, capabilities that represent a significant advancement over the current state [32].  

# 9.7 Model Honesty and Reliability  

Ensuring the honesty, reliability, and factuality of outputs generated by Multimodal Next Token Prediction (MMNTP) models presents critical challenges. These models face the potential for generating biased, harmful, or hallucinated content, necessitating robust research and development efforts [29].  

A fundamental challenge lies in the inherent opacity of complex neural networks, making it difficult to fully comprehend the internal evaluation processes and decision-making mechanisms employed by AI models [6]. This lack of transparency complicates efforts to diagnose the root causes of unreliable or non-factual outputs.  

Addressing these challenges draws significantly from research in Large Language Models (LLMs), which form a core component of many MMNTP architectures. A survey specifically on LLM honesty clarifies definitions, reviews evaluation methods, and explores strategies for improvement [29]. This underscores the importance of dedicated ethical and safety research in AI development. Technical means to detect and filter false or harmful content are deemed essential, alongside the formulation of clear usage norms to guide responsible application of these powerful models [10,29].  

Evaluation methodologies are crucial for assessing model reliability and detecting undesirable outputs. For instance, LLaVACritic is a tool designed specifically to evaluate multimodal models, providing scores for various tasks and aiming to enhance model alignment [30]. Furthermore, safety evaluation systems, such as the safety content classification system used for DeepSeek LLM models, are employed to assess the potential for generating harmful content [10,39]. These evaluation frameworks are vital for identifying issues related to bias and harmful outputs across modalities and in the generated multimodal content.​  

Mitigating risks involves developing techniques for detecting and filtering problematic content and establishing guidelines for model deployment and usage [10,29]. Ultimately, the development of MMNTP models that are transparent about their limitations and capable of accurately reflecting known information is paramount for building user trust and ensuring responsible AI deployment.​  

# 9.8 Overcoming Limitations (Alternative Training / Verifiers)  

While Next Token Prediction (NTP) has proven highly effective for sequence generation, standard teacher-forced training exhibits inherent limitations—particularly in complex tasks requiring exploration or planning [8]. A primary challenge is error accumulation, where mistakes in early token predictions propagate and amplify throughout the generated sequence. This dependency on a fixed, step-by-step target sequence provided by the teacher hinders the model's ability to deviate and explore alternative, potentially more optimal, solution paths [8]. Consequently, models trained solely with teacher forcing may struggle with tasks demanding multi-step reasoning or novel generation.​  

To mitigate these issues, researchers are exploring alternative training paradigms that move beyond the strict reliance on teacher-forced supervision [8,39]. Teacherless training approaches, for instance, aim to reduce the model's dependence on a ground truth sequence at each step, encouraging greater autonomy in generation. Reinforcement Learning (RL) represents another promising avenue, reframing the generation process as a sequential decision-making task [8,39]. Unlike teacher forcing, which provides feedback at the token level, RL typically employs a reward signal that evaluates the quality of an entire sequence—or a significant portion thereof. This allows models to learn strategies for generating outputs that maximize a given reward, potentially leading to improved long-range coherence and the capacity for exploring diverse solution spaces [8,39]. The DeepSeek-R1 model, for example, leverages Group Relative Policy Optimization (GRPO) within a reinforcement learning framework to enhance its reasoning capabilities [39].​  

Complementary to alternative training methodologies is the incorporation of external verifiers or reward models [8]. These mechanisms provide richer feedback signals than simple next-token accuracy, offering evaluations based on criteria such as logical consistency, factual correctness, or overall output quality. Generative verifiers (GenRM) have been proposed as an alternative to traditional discriminative classifiers for evaluating model outputs, particularly in the context of improving reasoning performance [12]. While ironically trained using a next token prediction objective themselves, these verifiers are designed to assess the plausibility or correctness of generated sequences, providing a more holistic evaluation signal that can be used to guide model training or selection [12]. By incorporating feedback from such verifiers, multimodal models can potentially improve the quality, coherence, and logical consistency of their generated outputs, addressing limitations inherent in training solely on next-token accuracy.​  

# 9.9 Ethical and Societal Implications  

The development and deployment of multimodal next token prediction (MMNTP) systems introduce significant ethical and societal considerations that necessitate careful examination [6]. Key concerns include the potential for biases inherent in multimodal data and models, which can lead to unfair or discriminatory outputs across various applications [6]. Furthermore, processing and generating multimodal content raises complex privacy issues [6]. A particularly salient risk is the potential misuse of powerful generative capabilities, enabling the creation of highly realistic fake content with potentially harmful consequences [6]. This concern is echoed in discussions surrounding increasingly capable large language models (LLMs), where the risk of generating misleading information and being misused is explicitly highlighted [10]. The ethical dimension also extends to specific tasks, such as hate speech detection, where the design and application of multimodal systems must account for potential biases and fairness issues, as indicated by datasets like HS-BAN [36].  

Beyond specific technical risks, the broader societal impact of MMNTP technologies warrants analysis, including potential effects on employment and the integrity of information [6]. The generative capacity that facilitates realistic fake content directly challenges information integrity, demanding robust countermeasures [6,10]. Addressing these implications requires a commitment to responsible AI development [6]. This includes advocating for long-term thinking and prioritizing the creation of valuable products over short-term competitive gains [32], suggesting an approach that considers user needs and societal impact. Furthermore, efficiency in resource utilization, such as precise questioning to avoid wasteful token generation, implicitly touches upon ethical considerations related to resource allocation and cost [35]. To mitigate risks and ensure beneficial deployment, there is a clear need for strengthened ethical and safety research [10]. This research should focus on developing techniques to detect and filter false or harmful content and establishing clear usage norms for the responsible application of these powerful models [10]. Potential ethical guidelines and regulatory frameworks are required to steer the development and deployment of MMNTP towards safe and beneficial outcomes [6]. It is notable that while these concerns are critical, some overviews of multimodal pre-training models, such as those focusing on image-text and videotext methods, may not explicitly discuss these ethical and societal implications [5].​  

# 9.10 Future Research Areas  

The future research landscape in Multimodal Next Token Prediction (MMNTP) presents numerous avenues to address current limitations and advance the field towards more capable and generalizable AI systems [2,9,13,32]. Key directions span fundamental model design, data handling, learning processes, and broader AI goals.  

A primary focus is on developing more efficient, scalable, and novel model architectures and training methodologies [5,8,30]. Research is needed to explore alternative architectures beyond prevailing paradigms [8], potentially adhering to principles such as the "equal contribution" law to enhance performance [22]. Concepts like Large Concept Models (LCMs) represent a potential path towards more scalable and adaptable architectures for AI-driven communication [3]. Concurrently, developing new training strategies is crucial to overcome limitations, such as those associated with teacher forcing [8]. This also includes exploring novel training paradigms like continuous AI evolution that does not require constant human supervision [32]. Designing more efficient architectures capable of handling larger, higher-quality multimodal datasets is also a priority [5].  

Improving tokenization techniques represents another critical area for enhancing robustness and adaptability across diverse and complex modalities [25]. Future work could focus on refining criteria for identifying redundant tokens within sequences, potentially incorporating more sophisticated measures of similarity or importance [25]. Optimizing length encoding methods to more accurately represent compressed multimodal information is also essential [25]. Exploring the application of efficient tokenization strategies developed for one modality, such as video (e.g., RLT), to others like audio or text could yield significant benefits across the multimodal landscape [25].​  

Significant effort is required to improve methods for capturing and modeling complex cross-modal relationships, longrange dependencies, and enhancing multimodal reasoning and planning capabilities [5,30]. This involves developing more sophisticated pre-training tasks designed to better capture the intricate relationships between data from different modalities, such as the relationship between nouns and objects in images [5]. Enhancing cross-modal reasoning is a specific challenge [30], which includes improving knowledge reasoning [10] and incorporating explicit planning mechanisms into models [8]. The development of unified world models represents an ambitious long-term goal in this domain [32]. Furthermore, exploring smarter interaction interfaces and prompting strategies offers a pathway to better leverage existing models' analytical and generative capabilities for complex multimodal tasks such as content analysis, creative writing, education, entertainment, or assisting scientific research through multi-turn dialogues [10].  

Finally, a pivotal future research direction is investigating the potential of MMNTP as a foundational paradigm towards achieving Artificial General Intelligence (AGI) [30]. This pursuit involves exploring fundamental concepts like unified world models and pathways towards continuous, self-improving AI systems without human input [32]. It also necessitates exploring non-consensus ideas and paradigms that venture beyond the current state-of-the-art, such as models like GPT-4, to push the boundaries of what is possible [32].  

# 10. Conclusion  

This survey has systematically reviewed the landscape of multimodal intelligence through the lens of Next Token Prediction (NTP), highlighting its potential as a unified and powerful paradigm [2,9,11,30]. By integrating understanding and generation tasks across diverse modalities, NTP offers a promising path towards achieving more accurate determinations, insightful conclusions, and precise predictions in complex real-world scenarios [2,6,11]. Significant progress has been demonstrated by models such as Emu3, MIO, and Loong, which leverage NTP to advance multimodal capabilities [29,30]. Notably, Emu3 illustrates the potential of NTP to surpass task-specific models and enable challenging generation tasks like video creation without relying on diffusion or combination methods, suggesting a move towards general multimodal intelligence [1,11]. Furthermore, multimodal approaches, exemplified by models like S-Transformer for ship trajectory prediction, have shown superior performance over unimodal benchmarks in specific applications [26].  

Despite these advancements, the field faces substantial challenges that require continued research and innovation [2,6,9]. Key difficulties persist in data handling, including issues of data quality, alignment, and scarcity [6,29,30]. Challenges also lie in optimizing model architectures and training paradigms for multimodal data [5,9], addressing computational efficiency [29,30], and enhancing model robustness, reliability, and honesty [29,30]. Moreover, questions have been raised regarding the inherent limitations of NTP, particularly when applied to tasks demanding complex reasoning and planning, suggesting that NTP and teacher-forced training might not be solely sufficient for achieving human-like intelligence [8]. This perspective aligns with arguments that reducing large language models, which underpin many multimodal systems, to mere "next token predictors" oversimplifies their evolving capabilities in complex content analysis and reasoning [10].  

Looking ahead, future research directions are largely centered on overcoming these identified challenges. This involves exploring more sophisticated pre-training tasks and improving model architectures [5,9]. Additionally, research into alternative training paradigms and model approaches beyond traditional token-based methods, such as the development of Large Concept Models (LCMs) utilizing high-dimensional concept embeddings, represents a promising avenue to address existing limitations and enhance capabilities like zero-shot generalization [3,8]. Advancements in areas like personalized federated learning for multimodal data also indicate potential for developing privacy-aware and tailored applications [31]. The continuous and rapid evolution of this field necessitates persistent exploration and innovation to fully realize the potential of multimodal intelligence.  

# References  

[1] Emu3：仅需下一个标记预测即可实现通用多模态智能 https://blog.csdn.net/Together_CZ/article/details/143966869   
[2] 多模态智能的下一个词预测：全面综述 https://blog.csdn.net/Together_CZ/article/details/145303030​   
[3] Meta AI's Large Concept Models: A Leap Beyond Toke https://www.marktechpost.com/2024/12/15/meta-ai-proposes  
large-concept-models-lcms-a-semantic-leap-beyond-token-based-language-modeling/   
[4] Meta新突破：多Token训练加速LLM推理，性能显著提升 https://www.thepaper.cn/newsDetail_forward_27603618​   
[5] 多模态预训练模型：图像-文本与视频-文本方法概览 http://baijiahao.baidu.com/s?   
id=1698265461857104313&wfr=spider&for=pc​   
[6] Multimodal AI: A Comprehensive Guide https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI   
[7] 大模型预训练：Next Token Prediction 详解 https://blog.csdn.net/kalilinuxsafe/article/details/140180245   
[8] Next Token Prediction 的陷阱：教师强制训练的局限性 https://cloud.tencent.com/developer/article/2400682   
[9] 多模态智能的未来：基于Next Token Prediction的统一范式探索 https://blog.csdn.net/AIBigModel/article/details/145303038   
[10] 超越Token预测：重新定义大型语言模型 https://baijiahao.baidu.com/s?id $\ c =$ 1827706581230743832&wf $\scriptstyle \mathbf { \bar { \rho } } = \mathbf { : }$ spider&for=pc   
[11] 下一个Token预测：AI技术革新与多模态世界模型 https://www.zgcsswdx.cn/info/11390.html   
[12] Generative Verifiers: Next-Token Prediction for En https://hub.baai.ac.cn/paper/09691e91-f96b-4359-a4eb-1230c1ff6ec4   
[13] 多模态 AGI：基于下一 Token 预测的技术架构综述 https://www.shangyexinzhi.com/article/24199803.html​   
[14] 深度学习下一帧预测：基准回顾 https://blog.csdn.net/qq_30516823/article/details/107599482   
[15] 跨模态预训练BERT模型研究综述 https://blog.csdn.net/qq_39388410/article/details/105167691   
[16] Meta研究：多Token预测提升大模型推理效率 https://developer.aliyun.com/article/1526502​   
[17] Platform for Situated Intelligence: An Open-Source https://www.microsoft.com/en-us/research/blog/platform-for  
situated-intelligence-an-open-source-framework-for-multimodal-integrative-ai/​   
[18] Image Captioning with CNN Encoder and RNN Decoder https://blog.csdn.net/s09094031/article/details/80536978​   
[19] DeepSeek R1核心技术：MTP多Token预测机制详解 https://blog.csdn.net/universsky2015/article/details/145530165​   
[20] Object Recognition as Next Token Prediction: VLM论文 https://download.csdn.net/blog/column/12478057/135465039   
[21] Fine-Tuning LLMs: A Deep Dive into Domain Adaptati https://towardsdatascience.com/stepping-out-of-the-comfort  
zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224​   
[22] LLM下一Token预测定律：层级贡献均等性研究 https://hub.baai.ac.cn/paper/db8095ff-0100-460e-ba4c-c89c5990dfd4​   
[23] 计算机视觉与模式识别学术速递[12.20] - Transformer & 检测 & 分类识别   
https://cloud.tencent.com/developer/article/1924109   
[24] 郭丹教授：机器视觉、机器学习与多模态理解研究 http://faculty.hfut.edu.cn/gd/zh_CN/zhym/196818/list/index.htm​   
[25] 视频大模型提速：删除冗余token，训练时间减少 $3 0 \%$ https://baijiahao.baidu.com/s?   
id=1815972513069088374&wfr=spider&for=pc​   
[26] S-Transformer：基于多模态数据的船舶轨迹预测 https://www.c-s-a.org.cn/csa/article/abstract/Iq098​   
[27] GPT：Next Token Prediction 核心机制探秘 https://blog.csdn.net/2405_88636357/article/details/145607944​   
[28] 计算机视觉与模式识别学术速递[12.10]：Transformer与目标检测进展   
https://cloud.tencent.com/developer/article/1917008   
[29] 每周AI论文速递 (240930-241004): 多模态模型与LLM诚实性等 https://juejin.cn/post/7422142844985311251   
[30] AI论文速递(240930-241004)：多模态、诚实性、长度控制及边缘设备优化 http://leafw.cn/2024/10/06/每周ai论文速递 （240930-241004）/​   
[31] 基于Transformer的多模态个性化联邦学习方法 https://www.juestc.uestc.edu.cn/cn/article/doi/10.12178/1001-   
0548.2024050​   
[32] 曼巴投资：从杨植麟谈第一性原理看投资与AI https://xueqiu.com/2140389661/284284560   
[33] Transformer模型概览（四）：自回归、自编码、序列到序列及多模态模型   
https://blog.csdn.net/weixin_42167712/article/details/110817653​   
[34] Deep Learning Research: Language Models, Frequency https://ins.sjtu.edu.cn/people/xuzhiqin/pub.html   
[35] AI达人速成：十大关键词解读，掌握智能时代入场券 https://baijiahao.baidu.com/s?   
id=1830517140448514493&wfr=spider&for=pc​   
[36] NLP学术速递：Transformer、BERT与多任务学习等最新研究 https://cloud.tencent.com/developer/article/1916490   
[37] 2023 年度论文发表 https://iip.ict.ac.cn/lwfb/2023/   
[38] CSIG-广东省CVPR 2021论文预交流在线学术报告会 https://www.pazhoulab.com/2021/04/1704/   
[39] Deepseek模型：R1推理、V3混合专家及LLM开源扩展三大论文解读 https://juejin.cn/post/7468947269426790463  