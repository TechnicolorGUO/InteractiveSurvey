# A Survey on All-in-One Image Restoration: Taxonomy, Evaluation and Future Trends  

Junjun JiangB, Senior Member, IEEE, Zengyuan Zuo, Gang Wu, Kui Jiang, and Xianming Liu, Member, IEEE  

Abstract—Image restoration (IR) to the process of improving visual quality of images while removing degradation, such as noise, blur, weather effects, and so on. Traditional IR methods typically target specific types of degradation, which limits their effectiveness in real-world scenarios with complex distortions. In response to this challenge, the all-in-one image restoration (AiOIR) paradigm has emerged, offering a unified framework that adeptly addresses multiple degradation types. These innovative models enhance both convenience and versatility by adaptively learning degradation-specific features while simultaneously leveraging shared knowledge across diverse corruptions. In this review, we delve into the AiOIR methodologies, emphasizing their architecture innovations and learning paradigm and offering a systematic review of prevalent approaches. We systematically categorize prevalent approaches and critically assess the challenges these models encounter, proposing future research directions to advance this dynamic field. Our paper begins with an introduction to the foundational concepts of AiOIR models, followed by a categorization of cutting-edge designs based on factors such as prior knowledge and generalization capability. Next, we highlight key advancements in AiOIR, aiming to inspire further inquiry and innovation within the community. To facilitate a robust evaluation of existing methods, we collate and summarize commonly used datasets, implementation details, and evaluation metrics. Additionally, we present an objective comparison of open-sourced methods, providing valuable insights for researchers and practitioners alike. This paper stands as the first comprehensive and insightful review of AiOIR. A related repository is available at https://github.com/Harbinzzy/ All-in-One-Image-Restoration-Survey.  

Index Terms—All-in-One Model, Image Restoration, Computer Vision, Deep Learning  

# I. INTRODUCTION  

MAGE processing is an integral part of low-level vision tasks, and digital image processing has evolved significantly over the past few decades, transitioning from traditional methods to advanced deep learning techniques. Initially, image processing relied heavily on algorithms that performed tasks such as filtering, edge detection, image synthesis and image segmentation. These methods, while effective, were limited by their inability to handle complex and varied image degradation scenarios. With the rise of deep learning, image processing has achieved remarkable results, especially driven by convolutional neural networks (CNNs) [1], Transformer [2] and StableDiffusion [3]. Among the image restoration field, single-task IR have achieved notable breakthroughs, which focuses on correcting a specific type of image degradation(e.g., denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]). Although they have set state-of-theart performance on restoring diverse degraded images, existing single-task IR methods: 1) lack the flexibility to adapt to new types of image degradations without extensive retraining; 2) typically require separate models for each type of degradation, which can be resource-intensive and impractical in real-world applications where images often suffer from multiple types of degradations simultaneously; 3) employ fixed network structures, which restrict the diversity of corruption-specific knowledge. To alleviate the aforementioned issues, researchers have proposed all-in-one image restoration (AiOIR), devoting significant effort to introduce various key improvements(e.g., prompt-based learning approach, Mixture-of-Experts (MoE) structure and multimodal model) beyond single-task IR.  

The AiOIR approach aims to develop models capable of handling multiple degradations tasks simultaneously within a unified framework. AiOIR integrates a variety of image enhancement and restoration techniques into a single model, offering greater performance and generalizability. These models are more efficient, as they eliminate the need for multiple specialized models, and they are more robust, as they can adapt to different types of degradations presented in an image. It is worth noting that diverse and unforeseen degradations may arise in the real-world scenarios jointly, hence the need for AiOIR models further developing. In some works, the corruption type label is available while inferencing, namely non-blind IR. In contrast, if the input images are without priors, which is called blind IR. If we focus on whether the training set and the test set are the same, that is, whether it’s zero-shot, we can divide the tasks into open-set scenario IR and close-set scenario IR. Despite the variety of AiOIR models, their ability to generate high-quality images is being actively explored. Recently, pioneering researchers have been investigating ways to optimize the architecture of these models to balance the trade-offs between computational complexity and restoration quality. We have listed the representative works on all-in-one models based on the timeline in Fig. 2.  

With the rapid development of AiOIR, numerous researchers collected a series of datasets tailored for various IR tasks, e.g., BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for low-light image enhancement, etc. Leveraging these datasets, the majority of recent works focused on improving the representation capability of IR networks for complicated degradation through well-designed methods based on prompt learning, contrast learning, multimodel representations, etc. Although these works achieve superior progress in the objective quality(e.g., PSNR, SSIM [36], NIQE [37] and FID [38]), the restored images still suffer from unsatisfied texture generation, hindering the application of IR methods in real-world scenarios. Collectively, these methodologies represent a substantial progression in the pursuit of sophisticated, accurate, and versatile AiOIR solutions.  

![](images/b680f1920696fedde363bffee032fe3110ea64bc6f40a89b4196a506312a782c.jpg)  
Fig. 1. Hierarchically-structured taxonomy of this survey.  

AiOIR methods have emerged as a positive step forward, although the details and applications of these approaches still need to be worked out. Keeping up with the latest research is vital for CV researchers. While there are comprehensive surveys for single-task IR methods, such as image superresolution [39]–[42], deraining [43], [44], dehazing [45], denoising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is currently no review that specifically addresses the AiOIR field. This gap limits its development. Consequently, we aim to provide the first comprehensive overview of AiOIR methods in IR tasks, shedding light on both its representative approaches and diverse improvements. Fig. 1 shows the taxonomy of this survey in a hierarchicallystructured way. In the following sections, we will cover various aspects of recent advances in AiOIR:  

1) Preliminaries (Sec. II) $\because$ We present the concept definition of AiOIR and related concepts comparison. Meanwhile, We point out different task settings and typical scenarios.   
2) Methodology (Sec. III) : We provide a detailed analysis of several representative AiOIR networks, aiming to illustrate the prevailing methods and elucidate the different categories of methods. By analyzing the methods used to achieve state-of-the-art, we summarized network architectures, basic methods, learning strategy as well as some key improvements.   
3) Experiments (Sec. IV) : To facilitate a reasonable and exhaustive comparison, we clarify the commonly-used  

datasets, and experimental settings in different AiOIR tasks. Furthermore, the comprehensive comparisons between the benchmarks across different tasks are provided. 4) Challenges and Future Directions (Sec. V) : There are still some challenges to extending them to the practical application. To further improve the development of AiOIR, we summarize the primary challenges and propose the potential directions and trends for solving them.  

# II. PRELIMINARIES  

In this section, we first define the concept of All-in-One image restoration and compare it with related concepts. Subsequently, we examine task settings in AiOIR from various perspectives. Finally, we provide a comprehensive review of diverse scenarios within AiOIR and the corresponding approaches.  

# A. Concept Definition  

Single degradation image restoration (SDIR) focuses on recovering clean images from observations that have been corrupted by a specific type of degradation, such as noise, blur, or rain. These methods are typically task-specific, meaning that each model is designed for a particular degradation type. While these models excel in handling known degradation types, they often struggle when faced with unseen degradations or new levels of corruption. Consequently, multiple degradation image restoration (MDIR) addresses the challenge of processing images affected by various degradation types. The primary strategy involves training a single model using datasets encompassing diverse degradation types. Although the model encounters multiple tasks during training, it lacks specialized design elements to effectively manage the coexistence of different degradations. As a result, MDIR models frequently exhibit mediocre performance across tasks due to their inability to provide a true all-in-one solution specifically. To tackle this issue, researchers propose AiOIR, which aims to recover clean images under various degraded conditions explicitly tailored to address multiple degradations based on multi-head and multi-tail structures, prior, or pretrained models within an unified framework. These models present practical advantages such as reduced storage requirements and simplified deployment; however, the principal challenge lies in developing robust architectures capable of effectively addressing diverse degradations with a singular set of parameters while achieving high-quality restorations across varying conditions. In addition, these approaches often necessitate prior knowledge regarding the type or extent of degradation involved limiting their applicability in real-world scenarios where such information is unknown or variable. In subsequent sections, we will elucidate key concepts related to degradation awareness and generalization capabilities pertinent to all-inone models.  

![](images/0bd7d73409770161ef91a99087ba470361a1d34335955da1282af74e5419a2a2.jpg)  
Fig. 2. Representative works in AiOIR based on the timeline. The bar chart shows number of papers in AiOIR from 2019 to October 2024.  

# B. Task Settings  

1) Blind/Non-Blind: We can preliminarily categorize AiOIR methods into two main types: blind and non-blind, based on whether the type or level of degradation information is known or not. Non-blind image restoration assumes that the degradation process is either known or can be explicitly modeled. For instance, when the blurring kernel or noise distribution is available, the restoration task primarily focuses on recovering the original image based on this known information. As a result, non-blind restoration is typically considered less challenging than blind restoration, as it involves fewer unknowns. Some works [55]–[62] require prior knowledge of the degradation in the corrupted image to feed it to the appropriate restoration head or guide the design of the image restoration network. It is worthy to note that a more challenging scenario is the opposite case [63]–[78], where the degradation affecting the input image is unknown—this defines blind AiOIR, which has shown promising potential for real-world photographs.  

In blind AiOIR, the degradation type (e.g., blurring, noise, or compression) is unknown and should be estimated concurrently with the recovery of the original image. The difficulty in blind restoration arises from this dual estimation task, which requires advanced techniques to effectively model both the degradation process and the image restoration simultaneously. As a result, blind restoration often demands more sophisticated algorithms or more complex models to estimate the reconstructed image accurately. To achieve blind AiOIR, AirNet [63] learns degradation representations from corrupted images using a contrastive learning strategy. These learned representations are then employed to restore the clean image. Subsequently, IDR [66] models various degradations based on underlying physical principles and accomplishes AiOIR in two stages. More recently, several prompt-learning-based approaches have emerged [61], [65], [79], [79]–[84]. For instance, PromptIR [65] introduces a series of learnable prompts to encode discriminative information about different degradation types, involving a substantial number of parameters.  

2) Open-set/Closed-set: In the field of AiOIR, a novel distinction exists between close-set and open-set image restoration, which addresses the consistency and generalization between training and testing scenarios. Close-set image restoration assumes a predefined set of degradations encountered during the training phase, restricting the model’s application to scenarios where the degradations in test images are identical to those in the training data. This approach, while effective for known degradations (i.e., close-set scenario), lacks the flexibility to handle unexpected or unseen types of degradation, which are more common in real-world applications.  

In contrast, open-set image restoration (e.g., Gou et al. [85]) addresses the challenge of restoring images affected by unforeseen degradations that are absent from the training data. Meanwhile, unlike zero-shot IR, which focuses on the recovery of a single degraded image without relying on preexisting degradation-specific training. Although free from the constraints of training on predefined degradations, zero-shot IR methods often require some form of prior knowledge or assumptions about the test degradations in advance to guide the restoration process. The core difficulty in open-set scenarios lies in the distribution shifts between training and test data. This requires models to generalize beyond their training experience and adapt to previously unseen degradation types. Test-time adaptation (TTA) (e.g., [86]–[88]) has emerged as a key technique in addressing this issue, allowing models to dynamically adjust their parameters during the testing phase to better align with the characteristics of the degraded input images. TTA effectively addresses distribution shifts between test and training data by adjusting the pre-trained model using test samples. By enabling the model to restore images with unknown degradations, open-set image restoration aims to provide a more robust and versatile solution, making it a crucial step towards real-world applicability in diverse image restoration tasks.  

3) Zero-shot: Zero-shot image restoration involves restoring images with distortions not encountered during training, requiring the model to classify unknown classes into specific categories, unlike Open-set methods [89]. This approach demands a robust and adaptable model, relying on learned priors and general features to effectively handle unseen distortions. For instance, Zero-shot methods (e.g., [90]–[93]) leverage pre-trained diffusion models as generative priors, seamlessly integrating degraded images as conditions into the sampling process. In the case of AiOIR, the pre-trained MPerceiver [71] demonstrates strong zero-shot and few-shot capabilities across six unseen tasks. Additionally, TAO [85] employs Test-Time Adaptation for AiOIR tasks, achieving results comparable to or better than traditional supervised methods.  

![](images/89b3b776854f1755100e13a62f9ad38ff2c89c523d54396be6dad3663908a9b8.jpg)  
Fig. 3. Ten categories of the image restoration method prototype (the last nine are AiOIR methods) (a) independent encoders and decoders. (b) multiple heads and shared decoders. (c) single branch end-to-end model. (d) shared encoders and multiple decoders. (e) a reusable pretrained middle-level backbone. (f) Mixture-of-Experts architectures. (g) pretrained large vision-language models. (h) visual prompt to guide image restoration. (i) textual prompt or instructions to guide image restoration. (j) a question answering paradigm.  

# C. Typical Scenarios  

We categorize the task of AiOIR scenario into several scenarios based on the type of images being processed: natural images, adverse weather conditions, medical images, and document images.  

Natural images. The degradation types commonly encountered in natural images include Gaussian noise, real noise, defocus blur, motion blur, low-light conditions, JPEG compression artifacts, mosaic effects, underwater distortions, and issues arising from under-display cameras, among others. Various AiOIR approaches have been developed to address these degradations, including techniques outlined in studies such as [55], [61], [63], [65], [66], [70], [72], [73], [94]–[99]. These approaches primarily focus on resolving typical problems found in both natural and man-made scenes, enhancing the quality and usability of images affected by these common degradations.  

Adverse weather conditions. In more extreme cases, the images requiring restoration may be severely impacted by various adverse weather conditions, such as snowflakes or dense haze effects. These conditions lead to ill-posed inverse problems that are crucial for applications in autonomous navigation and surveillance systems. The AiOIR field has seen the emergence of multiple solutions aimed at addressing these challenges, including works like [56], [64], [67]–[69], [75], [100]–[105]. These solutions are designed to effectively restore visibility and clarity in images affected by harsh environmental conditions.  

Medical images. The domain of medical imaging within AiOIR encompasses various types, including clinical Computed Tomography (CT), Magnetic Resonance Imaging (MRI), and Positron Emission Tomography (PET). Notable approaches in this area include AMIR [106] and ProCT [107]. AMIR employs a task-adaptive routing strategy that achieves state-of-the-art performance across three key medical imaging restoration tasks: MRI super-resolution, CT denoising, and PET synthesis. On the other hand, ProCT introduces innovative view-aware prompting techniques along with artifactaware contextual learning, enabling universal incomplete-view CT reconstruction while demonstrating seamless adaptability to out-of-domain hybrid CT scenarios.  

Document images. Currently, there is only one notable article focused on All-in-One document image restoration, which is DocRes [108]. This work addresses a variety of tasks, including dewarping, deshadowing, appearance enhancement, deblurring, and binarization. DocRes employs a straightforward yet highly effective visual prompting method known as DTSPrompt, which effectively distinguishes between different tasks and accommodates various resolutions. This approach not only streamlines the restoration process for document images but also enhances the overall quality and readability of the restored documents.  

# III. METHODOLOGY  

All-in-One image restoration (AiOIR) methods have gained significant attention for their ability to address multiple types of degradations within a unified model. These methods are particularly valuable in real-world scenarios where images may be simultaneously affected by various artifacts, such as noise, blur, and adverse weather conditions like fog and rain. In this section, we provide a comprehensive analysis of AiOIR methods from multiple perspectives. Firstly, we undertake a review of both novel and traditional network architectures and deliberate on the related approaches for their implementation (classifying these methods into several typical network designs). Subsequently, we further investigate diverse learning strategies, encompassing training methodologies and perspectives, to augment the accuracy of image reconstruction and enhance the overall restoration quality. Finally, there exist other techniques that further optimize AiOIR models, including Prompt Learning, Mixture-of-Experts (MoE), Multimodal Model, and so on.  

# A. Network Design for AiOIR  

Existing all-in-one image restoration (AiOIR) methods, as a form of Multi-Task Learning (MTL), employ various architectural designs to handle inputs and outputs across multiple tasks, enabling efficient information sharing among them. Despite significant differences in their architectures, AiOIR methods can be categorized into ten representative frameworks, as illustrated in Fig. 3 and described below.  

(a) Task-Specific Encoders and Decoders: This straightforward approach assigns each type of degradation to a specifically designed encoder-decoder pair. For example, one encoder-decoder might handle low-light enhancement, while another addresses image denoising. This setup requires prior knowledge of the degradation type to select the appropriate components. However, in real-world scenarios, images often suffer from multiple or unknown degradations, making this approach less practical.  

To overcome this limitation, models (b), (c), (d), and beyond aim to handle multiple degradation types within a unified framework without relying on prior knowledge, offering greater flexibility and efficiency.  

(b) Shared Decoder with Multiple Heads: Models in this category share a common decoder but have multiple heads trained individually for different degradation types. For instance, the All-in-One model by Li et al. [56] processes various types of adverse weather images using shared weights, requiring separate training for each degradation type.  

(c) Unified Encoder-Decoder Architecture: These models use a single encoder-decoder architecture without separate heads or tails, designed to remove one specific type of noise at a time. They serve as the foundation for prompt-based methods like TransWeather [64], AirNet [63], and TANet [109].  

(d) Shared Backbone with Multiple Decoders: Universal models adopt mixed inputs without any task-specific indicators, using a shared backbone for feature extraction and multiple task-specific decoders. For example, BIDeN [101] follows this approach. However, this reintroduces the complexity of multiple decoders and requires extensive supervision from degradation labels.  

(e) Pretrained Mid-Level Backbone: Some models introduce a reusable pretrained transformer backbone with taskspecific heads and tails. IPT [57] leverages pretraining to address general noise removal, significantly simplifying the pipeline by utilizing prior knowledge.  

(f) Mixture-of-Experts (MoE) Architecture: In MoEbased models, inputs are routed to different experts via a gating mechanism. MEASNet [110], for instance, considers both pixel-level and global features—including low-frequency and high-frequency components—to select the appropriate expert for image restoration.  

(g) Pretrained Model Prior: Models like Perceive-IR [111] and DA-CLIP [76] utilize frozen vision-language models, such as CLIP [112], DINO [113], and DINO-v2 [114]. These models predict high-quality feature embeddings to enhance the restoration process by leveraging semantic alignment between vision and language.  

(h) Visual Prompting: These models employ a single encoder-decoder architecture and inject learnable visual prompts at multiple decoding stages to implicitly predict degradation conditions. Examples include works by Potlapalli et al. [65], Li et al. [72], and Fan et al. [115]. The prompts guide the decoder in adaptively recovering various degraded images, serving as lightweight, plug-and-play modules with minimal additional parameters.  

(i) Textual and Multimodal Prompting: Extending the concept of prompting, models like Yan et al. [94], Conde et al. [62], and Tian et al. [116] incorporate textual or multimodal prompts. These models allow for natural language instructions or combine visual and textual cues to steer the restoration process, enhancing adaptability to unknown degradations.  

(j) Question-Answering Paradigm: Finally, models such as promptGIP [80] and AutoDIR [70] employ a questionanswering framework. They empower users to customize image restoration according to their preferences by interpreting user inputs and adjusting the restoration process accordingly.  

AiOIR models have evolved from task-specific architectures requiring prior knowledge of degradation types to more flexible and unified frameworks capable of handling multiple degradations without explicit information. A significant trend in recent research is the incorporation of prompt-based techniques—visual, textual, or multimodal—that guide the restoration process and enhance adaptability. By leveraging prompts, models can effectively handle unknown degradations and offer user-controllable restoration, making prompt technology a promising direction in AiOIR.  

![](images/97ded199f50e477df8052e67170ddfe690db548ddf24792ea6a8553d9016991a.jpg)  
Fig. 4. Transfer learning scenarios in AiOIR.  

# B. Learning Strategies of AiOIR  

Beside network design (as discussed in Sec. III-A), robust learning strategies are crucial for achieving satisfactory results in AiOIR. In this section, we discuss several promising learning strategies in the field. We begin with continual learning, exploring how it can prevent catastrophic forgetting. Next, we focus on contrastive learning and its application in complex degradation scenarios, emphasizing its role in enhancing the discriminative power of image features. Then, we highlight the potential of Multi-Task Learning (MTL) to optimize performance across various degradation tasks, pointing out the importance of addressing task relationships and conflicts. Finally, we introduce the concept of machine unlearning, exploring its potential for privacy protection.  

1) Continual Learning: AiOIR methods can be categorized into two learning styles from the perspective of transfer learning: MTL and sequential learning. MTL involves learning different tasks simultaneously, while sequential learning involves learning different tasks in sequence. An illustration of transfer learning scenarios [117], [118] is shown in Fig. 4. In human cognition, forgetting is a gradual process; except in rare cases, people do not suddenly lose their memory. However, in computational models, catastrophic forgetting [119] often occurs, where after learning new knowledge, the model almost completely forgets previously learned content. In the field of AiOIR, since a single network is expected to restore images with multiple degradations, the model must learn knowledge related to various degradations, making catastrophic forgetting more likely. To enable the model to incrementally accumulate knowledge and avoid catastrophic forgetting, researchers have proposed novel learning strategies, such as review learning [120] and sequential learning [121]. These methods are inspired by continual learning [122], [123] and replace mixed training (mixing datasets with different degradations) with sequential training. It is worth noting that the learning order of multiple tasks is crucial to the quality of image restoration. In MiOIR [121], the authors studied the effect of training order on the results and pointed out the impact of multi-task orders. In SimpleIR [120], the authors delve into the entropy difference distribution for various IR tasks. They propose determining the training dataset order based on abnormally high loss values and the intrinsic difficulty of tasks, measured by the entropy difference between original and degraded images.  

![](images/1b3851577a58865a2bf5ffec6dd5dd27b7d3d0117e07116e4e5f16087b0ed773.jpg)  
Fig. 5. Schematic illustration of Contrastive Learning in AiOIR.  

2) Contrastive Learning: One of the significant challenges in image restoration is effectively handling unseen tasks and degradation types. The vast variability in possible degradations can severely impede a model’s generalization capabilities, making it less effective when confronted with new, unseen data. To address this issue, researchers have drawn inspiration from contrastive learning techniques that have proven successful in both high-level and low-level tasks [124]–[126]. Contrastive learning approaches typically serve as an additional form of regularization to improve the generalization of single-task restoration models [126]–[130]. By incorporating contrastive regularization, these methods aim to enhance model performance across a wide array of image restoration applications. In contrastive learning, the definitions of positive and negative samples can be flexibly adjusted (as illustrated in Fig.5), allowing researchers to tailor the learning process to better suit specific tasks and datasets. This flexibility ultimately enhances the model’s adaptability and performance in diverse image restoration scenarios [111]. Moreover, contrastive learning-based loss functions have been proposed to obtain discriminative degradation representations in the latent space [63], [66], [72], further improving the model’s ability to distinguish between different types of degradations and to generalize to unseen ones.  

3) Multi-Task Learning: MTL is a learning paradigm that leverages shared representations and knowledge across tasks, allowing models to learn more efficiently and effectively. By jointly learning from multiple objectives, MTL can improve generalization, reduce overfitting, and achieve better performance on individual tasks. It is widely used in various domains [131]–[133], where related tasks can benefit from shared information. However, in the context of AiOIR, the optimization process often receives less attention, leading to an oversight of the complex relationships and potential conflicts between multiple degradations in mixed training scenarios. Unlike unified models (e.g., [63]–[65], [121]) that employ mixed training for multiple-degradation restoration, some novel studies approach AiOIR from the perspective of MTL to address the inconsistencies and conflicts among different degradations, treating each degradation as an independent task. By focusing on the optimization process and the interactions between tasks, these methods aim to mitigate conflicts and enhance overall performance. As illustrated in Fig. 6, we can broadly classify AiOIR methods in MTL into two types: task grouping and task balancing.  

Task Grouping. A notable example is GRIDS [134], which enhances MTL by strategically dividing tasks into optimal groups based on their correlations. Tasks that are highly related are grouped together, allowing for more effective training. GRIDS introduces an adaptive model selection mechanism that automatically identifies the most suitable task group during testing. This approach leverages the benefits of group training, ultimately improving overall performance by ensuring that related tasks are processed in a complementary manner.  

![](images/cc1f3dc6a13aea51a3228a93d214d86f2c433a643a67b66587903d4cb507ab0a.jpg)  
Fig. 6. Schematic comparison of multi-task learning methods in AiOIR with traditional all-in-one methods.  

Task Balancing. Conversely, the work by Art [135] proposes a straightforward yet effective loss function that incorporates task-specific reweighting. This method dynamically balances the contributions of individual tasks, fostering harmony among diverse tasks. By adjusting the weight of each task based on its specific characteristics and performance, this approach aims to mitigate conflicts and enhance the overall effectiveness of the restoration process.  

4) Machine Unlearning: Privacy protection is an increasingly critical issue in the field of artificial intelligence (AI), particularly as models become more integrated into sensitive applications. One promising solution to address this challenge is machine unlearning [136]. This innovative approach aims to effectively erase the influence of private data from trained models, allowing them to function as if that sensitive information was never utilized during the training process.  

Existing methodologies for unlearning can be broadly categorized into two types: exact unlearning [137], which seeks to completely eliminate the impact of specific data points, and approximate unlearning [138], which aims to diminish the influence of such data to a certain degree. While the concept of unlearning has been extensively explored in various contexts, including classification tasks and federated learning scenarios, its application in the realm of end-to-end image restoration remains largely uncharted.  

To fill this gap, Su et al. [139] propose an innovative framework that applies machine unlearning techniques to AiOIR models. In their work, they define a scenario in which certain types of degradation data—such as haze or rain—are treated as private information that must be effectively ”forgotten” by the model. Importantly, this process is designed to preserve the model’s performance on other types of degradations, ensuring that the overall functionality remains intact. To achieve this goal, the authors introduce a technique called Instance-wise Unlearning, which leverages adversarial examples in conjunction with gradient ascent methods. This approach not only enhances the model’s ability to forget specific data but also maintains its robustness across various image restoration tasks.  

# C. Key Improvements of AiOIR  

In addition to the network design in Sec. III-A and learning strategies in Sec. III-B, there are other key techniques aiming at improving AiOIR models. In this section, we preliminarily divided the key improvements of AiOIR Model into the following three areas, i.e., Prompt Learning, Mixture-ofExperts (MoE), and Multimodal Model, which correspond to Sec. III-C1, Sec. III-C2, Sec. III-C3, respectively. We also illustrate some other improvements, including deep unfolding methods, masked image modeling, etc. For the sake of clarity, we have listed the representative works on all-in-one models based on the three types in Fig. 7.  

1) Prompt Learning: Prompt learning, initially successful in Natural Language Processing (NLP) [140]–[142], aims to tap into the knowledge that the language model itself possesses by providing instructions or relevant information. Inspired by the ability of effectively modeling task-specific context, prompts have been used for finetuning to vision tasks [143], [144]. To be specific, unlike single-task image restoration, learnable prompts enable better parameter-efficient adaptation of models facing with multiple degradations. Recently, various prompts have been explored in AiOIR, as an adaptive lightweight module to encode degradation representation in the restoration network. The core idea is to enable pre-trained models to better understand and perform downstream tasks by constructing visual, textual or multimodal prompts, which are as follows.  

Visual Prompting. Visual prompting has been extensively explored in various studies [61], [80], [145], [146], addressing both high-level and low-level vision problems, which has sparked significant interest across different visual domains. In the context of AiOIR, prompt learning-based techniques have been implemented, allowing a generic model to automatically select prompts for specific tasks, resulting in excellent performance. Among these, vision-based prompts are the most widely used. For instance, AirNet [63] utilizes degradation encoder representations to guide network recovery, while Transweather [64] applies queries to direct the recovery process. An increasing number of researchers are applying the concept of prompt learning to develop AiOIR models.  

PromptIR [65] is one of the most representative works, integrating a prompt block into the U-Net architecture [147] to enhance AiOIR. This prompt block takes prompt components and the output from the previous transformer block as inputs, with its output fed into the next transformer block. The prompts serve as an adaptive, lightweight plug-and-play module as shown in Fig. 8, encoding degradation context across multiple scales in the restoration network. In contrast, ProRes [61] introduces a target visual prompt added to an input image, resulting in a “prompted image”. This prompted image is then flattened into patches, with the weights of ProRes frozen, while learnable prompts are randomly initialized for new tasks or datasets. Building on these ideas, PromptGIP [80] proposes a training method similar to masked autoencoding [148], where certain portions of question and answer images are randomly masked. This prompts the model to reconstruct these patches from the unmasked areas, and during inference, input-output pairs are assembled as task prompts to facilitate image restoration. PIP [72] introduces a novel Prompt-In-Prompt learning framework for universal image restoration, which employs two innovative prompts: a degradation-aware prompt and a basic restoration prompt. PIP relies solely on the input image to recover a clean image, without requiring any prior knowledge of the degradation present in the image.  

![](images/dcba0f58e31ea624a3c54c53d30aebb07639606eb0f0111a08aee0dcd3e2ef61.jpg)  
Fig. 7. The overview of AiOIR methods. This figure categorizes AiOIR into three types based on their key improvements, namely the Prompt-based models (indicated with a blue background), MoE-based models (indicated with an orange background), Multimodal models (indicated with a green background).  

![](images/f80c862cd061c5974b4708dd2396634af0fd88953e9bc621e7c3cde6e5658c54.jpg)  
Fig. 8. Illustration of the prompt block in AiOIR.  

Textual Prompting. As elaborated above, many studies leverage learnable prompts to guide restoration. Nevertheless, these models primarily concentrated on learning visual prompts based on distributional differences in the training data. Due to the presence of a semantic gap [149], accurately identifying degradation types remained a challenge, resulting in only moderate restoration performance. In this context, several innovative studies [62], [70], [74], [94], [116], [150] have introduced textual prompts, advancing the field of AiOIR. For example, TextPromptIR [94] utilizes a task-specific finetuned BERT [151] to accurately understand textual instructions and generating semantic prompts for concerned all-in-one tasks. Unlike traditional methods, text prompts also allow users to describe specific degradations in their images using natural language, which the model then interprets to apply appropriate restoration techniques. This approach enhances the adaptability and usability of AiOIR, making them more accessible to non-expert users and enabling the models to generalize better across different types of degradation scenarios. As GPT-3.5 [152] becomes increasingly popular, Clarity ChatGPT [150], combined with advanced visual models, offers users a simple and efficient way to perform complex image manipulation and enhancement through natural language interaction. Furthermore, AutoDIR [70] can handle images with unknown degradations in unseen task via an intuitive text prompt, empowering users to refine restoration results in alignment with their visual preferences.  

Multimodal Prompting. Recently, vision-language models (VLMs) have demonstrated the great potential of applying pretrained VLMs to improve downstream tasks with generic visual and text representations. A traditional VLM typically includes a text encoder and an image encoder, aiming to learn aligned multimodal features from noisy image-text pairs using contrastive learning. This greatly facilitates multimodal prompts in textual for holistic representation and visual for multiscale detail representation. MPerceiver [71], a groundbreaking multimodal prompt learning method, is designed to harness the generative priors of Stable Diffusion [153], enhancing the adaptiveness, generalizability, and fidelity of AiOIR. Both prompts are dynamically refined based on predictions from the CLIP image encoder, allowing for adaptive handling of various unknown degradations. More recently, DACLIP [112] leveraged pre-trained large-scale vision models to excel in AiOIR with an image controller that detects degradation and adapts the fixed CLIP image encoder to generate high-quality content embeddings from corrupted inputs.  

2) Mixture-of-Experts: The Mixture of Experts (MoEs) concept was first introduced in the groundbreaking 1991 paper “Adaptive Mixture of Local Experts” [154]. Like ensemble methods, MoEs utilize a supervised learning approach within a framework of distinct networks. Each network, referred to as an expert, is trained to specialize in a particular subset of the data, focusing on specific regions of the input space. A gating network is responsible for selecting the most suitable expert for a given input, determining the weight assigned to each expert. Both the experts and the gating network are optimized simultaneously during training. Due to the integration of multiple expert models in the MoE architecture, each expert model can be built for different data distribution and construction patterns, thus significantly improving the professional ability of large models in various subdivisions, making MoE significantly better in handling complex tasks. With the expansion of the model scale, MoE also faces the problems of training instability and overfitting, how to ensure the generalization and robustness of the model, how to balance model performance and resource consumption, etc., waiting for the continuous optimization and improvement of large model developers.  

Researchers in AiOIR have observed that IR model parameters tend to be degradation-specific. For example, parameters related to one type of degradation are typically inactive when dealing with other types of degradation, and zeroing out these unrelated parameters has few impact on image restoration quality illustrated in LDR [74]. This observation aligns with the concept of conditional computation in Mixture of Experts (MoEs), where sparsity plays a key role. Applying MoEs to the field of AiOIR could lead to several improvements, including faster pretraining compared to dense models and quicker inference with the same number of parameters. Moreover, MoEs facilitate the synergistic handling of both lowlevel upstream tasks, such as removing weather noise, and high-level downstream tasks, such as object detection and segmentation. The following section will explore various approaches that combine MoEs with AiOIR as shown in Fig. 9, offering diverse strategies to leverage the MoE framework for improving recovered images.  

![](images/c25f2431be775246924b784a7f3b717d9a5097049da16b24445ac6486aa0da5c.jpg)  
Fig. 9. Schematic illustration of the mixture-of-experts in AiOIR.  

Yang et al. [74] proposed Language-driven all-in-one adverse weather removal. This method generates degradation priors based on textual descriptions of weather conditions, which are then used to guide the adaptive selection of restoration experts through a MoE structure. It enables the model to handle diverse and mixed weather conditions without the need for specific weather-type labels, streamlining the image restoration process. WM-MoE [103] introduces a weatheraware routing mechanism (WEAR) to direct image tokens to specialized experts and employs multi-scale experts (MSE) to handle diverse weather conditions effectively. This approach leverages MoE to achieve state-of-the-art performance in adverse weather removal, enhancing both image restoration and downstream tasks like segmentation. By using a Feature Modulated Expert (FME) block and an Uncertainty-aware Router (UaR), the method achieves superior performance in image restoration while significantly reducing model parameters and inference time. The MoFME [155] framework outperforms previous approaches in both image restoration and downstream tasks, demonstrating its effectiveness and efficiency. In contrast, MEASNet [110] presents a novel multi-expert adaptive selection mechanism, which leverages both local and global image features to select the most appropriate expert models for different image restoration tasks. By balancing task-specific demands and promoting resource sharing among tasks, the approach demonstrates superior performance across multiple image degradation scenarios compared to existing methods, making it a valuable contribution to the field of image restoration.  

3) Multimodal Model: Multi-modal tasks have become increasingly pivotal in the domain of computer vision, enabling the enrichment of visual understanding through the integration of diverse information sources. The fundamental objective of multi-modal tasks is to learn valuable potential feature representations from a multitude of modalities, such as textual captions and visual images, RGB images with supplementary components like depth or thermal images, and various forms of medical imaging data. Nevertheless, multimodal models in image restoration harness data from multiple sources to improve the fidelity and robustness of restored images. These models integrate complementary modalities to address limitations inherent in single-modality approaches, particularly in scenarios involving complex degradation, such as severe noise, blur, or low-light conditions. By leveraging different types of information, multimodal models are able to enhance the structural details, texture, and overall quality of the restored image.  

However, multimodal models also introduce challenges, including the increased computational complexity of handling diverse data streams and the requirement for well-aligned multimodal datasets. Additionally, the process of effectively fusing different types of data, which may have varying resolutions and characteristics, remains a significant technical hurdle. Here, we summarize a variety of approaches to multimodal AiOIR methods (e.g., Clarity-ChatGPT [150], AutoDIR [70], Instruct-IPT [116], InstructIR [62]). These involve the continuous guidance of image restoration via human language instructions, as well as the use of multimodal prompts for AiOIR, as previously mentioned. Clarity-ChatGPT is the first system that bridges adaptive image processing with interactive user feedback, which innovatively integrates large language and visual models. AutoDIR automatically detects and restores images with multiple unknown degradations by the SemanticAgnostic Blind Image Quality Assessment (SA-BIQA). InstructIR [62] trains models using common image datasets and prompts generated using GPT-4, note that this generalizes to human-written instructions .  

4) Other Methods: In addition to highlighting three key improvements, we also review other methods for AiOIR. Some models benefit from iterative algorithms in the deep unfolding framework, while large-scale vision models (VLMs) like CLIP and BLIP show great promise in enhancing image restoration tasks by leveraging multimodal features. Integrating network design with pretrained Mask Image Modeling (MIM) also holds significant potential. These approaches harness semantic alignment and prior knowledge to enable robust and flexible image restoration, achieving impressive results within the AiOIR domain.  

TABLE I SUMMARY OF USED DATASETS IN AIOIR TASKS.   


<html><body><table><tr><td>Task</td><td>Dataset</td><td>Year</td><td>Training</td><td>Testing</td><td>ShortDescription</td><td>Type</td></tr><tr><td rowspan="3">Image Deblurring</td><td>GoPro[33]</td><td>2017</td><td>2103</td><td>1111</td><td>Blurred imagesat 1280x720</td><td>real</td></tr><tr><td>HIDE [34]</td><td>2019</td><td>6397</td><td>2025</td><td>Blurry and sharp image pairs</td><td>real</td></tr><tr><td>RealBlur[158]</td><td>2020</td><td>3758</td><td>980</td><td>182 different scenes</td><td>real</td></tr><tr><td rowspan="6">Image Denoising</td><td>Krbdan100 1601</td><td>2013</td><td>--</td><td>20</td><td>Losistrecrimesute</td><td>real</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>real</td></tr><tr><td>WSD4016121</td><td>2016</td><td>4744</td><td>--</td><td>Im400 collectedfromInternt</td><td></td></tr><tr><td>CBSD68 [26]</td><td>2001</td><td></td><td>68</td><td>Images with different noisy levels</td><td>real</td></tr><tr><td>McMaster[163]</td><td>2011</td><td>：</td><td>18</td><td>Crop size=500x500</td><td>real</td></tr><tr><td rowspan="8">Image Super-resolution Shadow Removal</td><td>DIV2K[164]</td><td>2017</td><td>800</td><td>100</td><td>2K resolutions</td><td>real</td></tr><tr><td>Set416165</td><td>2012</td><td>2650</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>-544</td><td></td><td>国</td></tr><tr><td>BSD100 [26]</td><td>2001</td><td></td><td>100</td><td>Objects, Natural images</td><td>real</td></tr><tr><td>Manga109 [27]</td><td>2015</td><td>-</td><td>109</td><td>109 manga volumes</td><td>synthetic</td></tr><tr><td>Urban100 [28]</td><td>2015</td><td></td><td>100</td><td>100 urban scenes</td><td></td></tr><tr><td></td><td></td><td>1330</td><td></td><td></td><td>real</td></tr><tr><td rowspan="6">Image Desnowing</td><td>STD [168]</td><td>2017</td><td></td><td></td><td>La15 sceaeswithshtomask imoeval</td><td>synthetic</td></tr><tr><td>Snow100k [170]</td><td>2017</td><td>50000</td><td>50000</td><td>Including1369 realistic snowy images</td><td></td></tr><tr><td>CSD[15]</td><td>2021</td><td>2000</td><td>1</td><td>A large-scale dataset called Comprehensive Snow Dataset</td><td>real and synthetic</td></tr><tr><td>REVIDE [171]</td><td>2021</td><td>1698</td><td>284</td><td></td><td>synthetic</td></tr><tr><td>RealSnow+ [67]</td><td>2023</td><td>1650</td><td>240</td><td>Real-world Video Dehazing</td><td>real</td></tr><tr><td></td><td>2023</td><td>50</td><td></td><td>Containing various resolutions</td><td>real</td></tr><tr><td rowspan="6">Image Deraining</td><td>KITTI-snow [172]</td><td>2023</td><td></td><td></td><td>Two intensity levels: severe and extremely severe</td><td>synthetic</td></tr><tr><td>WeatherStream [173]</td><td>2018</td><td>176100</td><td>11400</td><td>Three weather conditions,i.e.,rain,snow,and fog</td><td>real</td></tr><tr><td>RainDrop[29]</td><td>2019</td><td>1119</td><td></td><td>Various scenes and raindrops</td><td>real</td></tr><tr><td>Outdoor-Rain [30]</td><td>2019</td><td>9000 295000</td><td>1500</td><td>Degraded by both fog and rain streak</td><td>synthetic</td></tr><tr><td>SPA [31]</td><td>2017</td><td>12600</td><td>1000</td><td>Various natural rain scenes</td><td>real</td></tr><tr><td>Rain1400 [174]</td><td>2017</td><td>200</td><td>1400</td><td>Diverse rainfall directions and levels</td><td>synthetic</td></tr><tr><td rowspan="4">Image Dehazing</td><td>Rain100L/H [32]</td><td>2019</td><td>33</td><td>100</td><td>Five rain streaks</td><td>synthetic</td></tr><tr><td>Dense-Haze[175]</td><td>2019</td><td>313950</td><td></td><td>Out-door hazy scenes</td><td>real</td></tr><tr><td>RESIDE-OTS[176]</td><td>2019</td><td>110000</td><td>--</td><td>Outdoor training set</td><td>real</td></tr><tr><td>RESIDE-ITS[176] RESIDE-HSTS [176]</td><td>2019</td><td>1</td><td>20</td><td>Indoor training set</td><td>synthetic</td></tr><tr><td>Low-light Image Enhancement</td><td>LOL[35]</td><td>2018</td><td>485</td><td>15</td><td>Hybrid subjective testing set Low-and normal-light images at 400x600</td><td>synthetic and real real</td></tr></table></body></html>  

Deep Unfolding Methods. Zhang et al. [156] were the first to introduce the use of the deep unfolding framework, combining CNNs with model-based approaches for specific image restoration (IR) tasks. Considering the degradation model, the target clean image can be estimated by minimizing the energy function. Using the Half-Quadratic Splitting (HQS) algorithm [157], the equation can be decomposed into two separate subproblems, each addressing the data term and the prior term individually. The optimization is carried out by iteratively solving these subproblems in an alternating fashion. The data term subproblem reduces to a simple least squares optimization, while the prior term subproblem is addressed using a trainable CNN model. We can also use the Expectation Maximization Algorithm to model the image restoration network. DRM-IR [73] enhances the flexibility of All-In-One scenarios by introducing a reference-based, taskadaptive modeling paradigm. An advanced yet efficient AiOIR framework is developed, integrating two coupled subtasks: task-adaptive degradation modeling and model-based image restoration techniques intuitively.  

Large-scale Vision Models. Recent works have demonstrated the potential of pretrained vision-language models (VLMs) to enhance downstream tasks using generic visual and text representations [177]–[179]. A classic VLM typically consists of a text encoder and an image encoder, learning aligned multimodal features from noisy image-text pairs through contrastive learning [177]. BLIP [179] improves this by eliminating noisy web data with synthetic captions. LVMs have shown robust feature representation and zero-shot transfer capabilities across various tasks. Models like CLIP [112] have demonstrated effective semantic alignment between vision and language, aiding numerous downstream tasks. Self-supervised models like DINO [113] and DINO-v2 [114] have proved effective across multiple tasks, eliminating the need for labeled data. VLMs have also gained significant traction in the domain of AiOIR. Perceive-IR [111] utilizes the semantic prior knowledge and structural information mined by a DINO-v2- based guidance module to enhance the restoration process. DA-CLIP [76] trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings.  

Mask Image Modeling. Mask Image Modeling (MIM) (e.g., [148], [180]) is a technique in computer vision that involves training models to predict masked portions of images based on their surrounding context inspired by Mask Language Modeling [151], [181]. This approach leverages the self-supervised learning paradigm, where the model learns to reconstruct missing parts of an image, improving its understanding of visual features and representations. MIM has been shown to be effective in various tasks, including image classification, object detection, and segmentation. The MAE [148] framework effectively employs MIM to predict hidden tokens, showcasing impressive performance and generalization capabilities across a range of downstream tasks. Meanwhile,  

TABLE II SUMMARY OF IMPLEMENTATION DETAILS OF SOME POPULAR AIOIR METHODS.   


<html><body><table><tr><td>Settings</td><td>Num.</td><td>DetailDegradation</td><td>Datasets</td><td>Methods</td></tr><tr><td>multiple degrada- tion [71]</td><td>16</td><td>Gaussian Noise,Real Noise,Rain,Fog，Snow,Raindrop, Motion Blur,Defocus Blur,Lowlight,Mixed Degrada- tions,Under-display camera(POLED), Under-display cam-</td><td>BSD68，WED，Urban100，Kodak24，McMaster，DIV2K, Flickr2K,SIDD,Rain1400,LHP,RESIDE,，NH-HAZE, DenseHaze,Snow1Ook,RealSnow,Raindrop,RainDS,Go-</td><td>MPerceiver [71]</td></tr><tr><td>multiple degrada- tion [76]</td><td>10</td><td>era(TOLED),Underwater,JPEG,Mosaicking,oire Noise,Rain,Fog,Snow, Raindrop,Shadow removal,Blur, Low-light,Ipinting,JEG</td><td>Pro,RealBlur,DPDD,LOL DIV2K,Flicr2K,Rain0,EE,now10k,inro SRD,GoPro,LOL,CelebaHQ,C68</td><td>DA-CLIP [76]</td></tr><tr><td>multiple degrada- tion [121]</td><td>7</td><td>SR,Blur,Noise,JPEG,Rain,Haze,Low-Light</td><td>DIV2K,Flickr2K WED,</td><td>MiOIR [121] AirNet [63],PromptIR [65],PIP[72],Textpromp-</td></tr><tr><td>rain-haze- noise [63]</td><td>3</td><td>Rain,Haze,Noise-g15,Noise-g25,Noise-g50</td><td>BSD400, Urban100, Rain100L， RESIDE-OTS, RESIDE-SOTS</td><td>tIR[94],NDR [96], InstructIR_[62],AdaIR[78], U-WADN[186],DyNet[185],DaAIR[187], AnyIR[188],MEASNet[110], HAIR [189], Perceive-IR [111]</td></tr><tr><td>rain-haze-noise- blur-dark [66]</td><td>5</td><td>Rain,Haze,Noise-o25,Blur,Low-Light</td><td>BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro,LOL</td><td>IDR_[66],AdaIR[78],InstructIR_[62], DaAIR[187]，AnyIR [188], MEASNet [110], HAIR [189],Perceive-IR [111] PIP[72],</td></tr><tr><td>rain-noise-blur- dark [61]</td><td>4</td><td>Rain,Noise,Blur,Low-light</td><td>SIDD,merged deraining,merged debluring,LOL</td><td>ProRes [61]</td></tr><tr><td>rain-haze- snow [67]</td><td>3</td><td>Rain,Haze,Snow</td><td>SPA+,REVIDE,RealSnow</td><td>WGWS-Net [67],TKMANet [100],Art [135]</td></tr><tr><td>rain-haze- snow [64]</td><td>3</td><td>Raindrop,Rain+Fog,Snow</td><td>All-weather (Outdoor-Rain,Snow10ok,Raindrop)</td><td>Transweather[64],WeatherDif[69],AWRCP[68] AirNet [56]</td></tr><tr><td>rain-haze- snow [173]</td><td>3</td><td>Rain, Haze, Snow</td><td>WeatherStream (176,100 training images and11, 400 testing images)</td><td>LDR [74]</td></tr></table></body></html>  

SimMIM [180] introduced a generalized masked image modeling approach based on the Swin-ViT [182] architecture. In image restoration field, Painter [183] also leverages MIM pretraining. Qin et al. [184] introduces RAM into AiOIR, a framework designed to extract intrinsic image information from corrupted images through the use of MIM pre-training, along with a fine-tuning algorithm that facilitates the transition from masked images to fully restored ones. DyNet [185] are also trained in parallel branches to reconstruct the clean image from masked degraded inputs. For the AiOIR field, combining the exploration of the network design with pretrained MIM is of great potential.  

# IV. EXPERIMENTS  

To facilitate a comprehensive and efficient comparison of various AiOIR methods, we begin by summarizing the key datasets, experimental setups, and evaluation metrics commonly used across different tasks. Following this, we conduct a detailed comparison of existing benchmarks across several representative image restoration tasks, such as low-light enhancement, dehazing, deblurring, image super-resolution, deraining, and desnowing. This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.  

# A. Datasets and Implementation Details  

Datasets. A wide range of datasets are available for AiOIR, varying significantly in terms of image quantity, quality, resolution, and diversity. Some datasets offer paired input and target images, while others provide only ground-truth images. In the latter case, LR images are generally generated by manually. For example, BSD [162] is another classical dataset for image denoising and super-resolution. BSD100 is a classical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food, etc. Notably, the real-world distortions are usually blind/unknown, where the distributions are different from the simple synthetic distortions. The degradation mainly falls into the following 4 categories and 15 types: Noise: Gaussian, Shot, Impulse Blur: Defocus, Glass, Motion, Zoom Weather: Snow, Frost, Fog, Bright Digital: Contrast, Elastic, Pixel, JPEG Tab. I summarizes the datasets used for different AiOIR tasks, including SR, image inpainting, deblurring, denoising, shadow removal, image desnowing, image draining and image dehazing. It is composed of the released year, the number of training samples and testing samples, and short description.  

Implementation Details. We also summarize the implementation details and datasets of some AiOIR methods in Tab. II. We describe the configurations in the training process and testing process, including types, task numbers, detail degradations, datasets. We classify the types based on different experiment settings. We summarize the same experimental setup as in the original paper, where the widely adopted data augmentation techniques primarily consist of rotation and flipping operations.  

Evaluation Metrics. The performance of AiOIR methods is typically evaluated using three aspects of metrics: Distortion metrics(e.g., PSNR, SSIM [36]) refer to the relationship between the restored image and the original image. A higher evaluation index indicates a greater similarity between the reconstructed image and the reference image. Perceptual metrics(e.g., FID [190], LPIPS [191]) evaluate how much the image appears like a natural image, independent of its similarity to any reference image. No-reference metrics(e.g., NIQE [192], BRISQUE [193]) are commonly based on estimating deviations from natural image statistics. In addition, there are several objective and subjective metrics that play a crucial role in measuring and comparing the performances of different AiOIR algorithms, including IL-NIQE [194], NIMA [195], CLIP-IQA [196], LOE [197], Consistency [198], PI [199], and MUSIQ [200]. In this section, we provide a detailed overview of the commonly-used metrics in AiOIR.  

TABLE III PERFORMANCE COMPARISONS OF AIOIR MODELS ON THREE CHALLENGING DATASETS.   


<html><body><table><tr><td></td><td>Method</td><td>Venue</td><td>Params.</td><td>SDrs [176]</td><td> RaieroL [382]</td><td>DenisigonBSD68dtase</td><td></td><td></td><td>Average</td><td>Approach</td></tr><tr><td rowspan="4">ug</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>21.20.52</td><td></td><td></td></tr><tr><td>LPN 1201202J</td><td>CVAr19</td><td>3M</td><td>20.840.828</td><td>24.80.784</td><td>26 470.778</td><td>24,70.748</td><td></td><td>23640.738</td><td>Specfie</td></tr><tr><td>DehazeFormer [203]</td><td>TIP'23</td><td>25M</td><td>29.58/0.970</td><td>35.37/0.969</td><td>33.01/0.914</td><td>30.14/0.858</td><td>27.37/0.779</td><td>31.09/0.898</td><td>Specific</td></tr><tr><td>DRSformer [204]</td><td>CVPR'23</td><td>33M</td><td>29.02/0.968</td><td>35.89/0.970</td><td>33.28/0.921</td><td>30.55/0.862</td><td>27.58/0.786</td><td>31.26/0.902</td><td>Specific</td></tr><tr><td rowspan="5"></td><td>MPRNet [205]</td><td>CVPR'21</td><td>16M</td><td>28.00/0.958</td><td>33.86/0.958</td><td>33.27/0.920</td><td>30.76/0.871</td><td>27.29/0.761</td><td>30.63/0.894</td><td>General</td></tr><tr><td>Restormer [147]</td><td>CVPR'22</td><td>26M</td><td>27.78/0.958</td><td>33.78/0.958</td><td>33.72/0.865</td><td>30.67/0.865</td><td>27.63/0.792</td><td>30.75/0.901</td><td>General</td></tr><tr><td>NAFNet [206]</td><td>ECCV'22</td><td>17M</td><td>24.11/0.960</td><td>33.64/0.956</td><td>33.18/0.918</td><td>30.47/0.865</td><td>27.12/0.754</td><td>29.67/0.844</td><td>General</td></tr><tr><td>FSNet[207]</td><td>TPAMI'23</td><td>13M</td><td>29.14/0.969</td><td>35.61/0.969</td><td>33.81/0.874</td><td>30.84/0.872</td><td>27.69/0.762</td><td>31.42/0.906</td><td>General</td></tr><tr><td>MambaIR [208]</td><td>ECCV'24</td><td>27M</td><td>29.57/0.970</td><td>35.42/0.969</td><td>33.88/0.931</td><td>30.95/0.874</td><td>27.74/0.793</td><td>31.51/0.907</td><td>General</td></tr><tr><td rowspan="20"></td><td>DL [55]</td><td></td><td>2M</td><td>26.92/0.931</td><td>32.62/0.931</td><td>33.05/0.914</td><td>30.41/0.861</td><td>26.90/0.740</td><td>29.98/0.875</td><td>parameterized image operator</td></tr><tr><td>TKMANet [100]</td><td>TPAMI19 CVPR'22</td><td>29M</td><td>30.41/0.973</td><td>34.94/0.972</td><td>33.02/0.924</td><td>30.31/0.820</td><td>23.80/0.556</td><td>30.50/0.849</td><td></td></tr><tr><td>AirNet [63]</td><td>CVPR'22</td><td>9M</td><td>27.94/0.962</td><td>34.90/0.967</td><td>33.92/0.933</td><td>31.26/0.888</td><td>28.00/0.797</td><td>31.20/0.910</td><td>two-stage knowledge learning</td></tr><tr><td>PIPrestormer [72]</td><td>arXiv'23</td><td>27M</td><td>32.09/0.981</td><td>38.29/0.984</td><td>34.24/0.936</td><td>31.60/0.893</td><td>28.35/0.806</td><td>32.91/0.920</td><td>contrastive-based&degradation-guided prompt-in-prompt learning</td></tr><tr><td>IDR [66]</td><td>CVPR’23</td><td>15M</td><td>29.87/0.970</td><td>36.03/0.971</td><td>33.89/0.931</td><td>31.32/0.884</td><td>28.04/0.798</td><td>31.83/0.911</td><td>ingredient-oriented learning</td></tr><tr><td>PromptIR [65]</td><td>NeurIPS'23</td><td>33M</td><td>30.58/0.974</td><td>36.37/0.972</td><td>33.98/0.933</td><td>31.31/0.888</td><td>28.06/0.799</td><td>32.06/0.913</td><td>prompt for AiOIR</td></tr><tr><td>TextPromptIR [94]</td><td>arXiv'23</td><td></td><td>31.65/0.978</td><td>38.41/0.982</td><td>34.17/0.936</td><td>31.52/0.893</td><td>28.26/0.805</td><td>32.80/0.919</td><td>textual prompt for AiOIR</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>MCVv23</td><td>34M</td><td>30.370.978</td><td>37.15/0972</td><td>3.1370937</td><td>31.370.887</td><td>2.110.80</td><td>32.90.912</td><td>transformerawitgriduatpromp</td></tr><tr><td>NDR [96]</td><td>arXiv'23</td><td>28M</td><td>28.64/0.962</td><td>35.42/0.969</td><td>34.01/0.932</td><td>31.36/0.887</td><td>28.10/0.798</td><td>31.51/0.910</td><td>neural degradationrepresentation</td></tr><tr><td>DA-CLIP[76]</td><td>ICLR'24</td><td>125M</td><td>29.46/0.963</td><td>36.28/0.968</td><td>30.02/0.821</td><td>24.86/0.585</td><td>22.29/0.476</td><td>--</td><td>degradation-aware vision-language model</td></tr><tr><td>AnyIR [188]</td><td>arXiv'24</td><td>6M</td><td>31.38/0.979</td><td>37.90/0.981</td><td>33.95/0.933</td><td>31.29/0.889</td><td>28.03/0.797</td><td>32.51/0.916</td><td>local-global gated intertwining</td></tr><tr><td>DaAIR [187]</td><td>arXiv'24</td><td>6M</td><td>32.30/0.981</td><td>37.10/0.978</td><td>33.92/0.930</td><td>31.26/0.884</td><td>28.00/0.792</td><td>32.51/0.913</td><td></td></tr><tr><td>ArtAirNet[135]</td><td>ACM MM'24</td><td>9M</td><td>30.56/0.977</td><td>37.74/0.981</td><td>34.02/0.934</td><td>31.37/0.890</td><td>28.12/0.802</td><td>32.36/0.917</td><td>efficient degradation-aware</td></tr><tr><td>ArtpromptIR [135]</td><td>ACMMM'24</td><td>33M</td><td>30.83/0.979</td><td>37.94/0.982</td><td>34.06/0.934</td><td>31.42/0.891</td><td>28.14/0.801</td><td>32.49/0.917</td><td>viamulti-task collaboration viamulti-task collaboration</td></tr><tr><td>AdaIR [78]</td><td>arXiv'24</td><td>29M</td><td>31.06/0.980</td><td>38.64/0.983</td><td>34.12/0.935</td><td>31.45/0.892</td><td>28.19/0.802</td><td>32.69/0.918</td><td>frequency mining and modulation</td></tr><tr><td>CAPTNet[95]</td><td>TCSVT'24</td><td></td><td>29.28/-</td><td>37.86/-</td><td>-</td><td>30.75/-</td><td>-</td><td>-</td><td>prompt-based&ingredient-Oriented</td></tr><tr><td>InstructIR-3D [62]</td><td>ECCV'24</td><td>16M</td><td>30.22/0.959</td><td>37.98/0.978</td><td>34.15/0.933</td><td>31.52/0.890</td><td>28.30/0.804</td><td>32.43/0.913</td><td>natural language prompts</td></tr><tr><td>InstructIR-5D [62]</td><td>ECCV'24</td><td>16M</td><td>27.10/0.956</td><td>36.84/0.973</td><td>34.00/0.931</td><td>31.40/0.887</td><td>28.15/0.798</td><td>31.50/0.909</td><td>natural language prompts</td></tr><tr><td>MEASNet [110]</td><td>arXiv'24</td><td>31M</td><td>31.61/0.981</td><td>39.00/0.985</td><td>34.12/0.935</td><td>31.46/0.892</td><td>28.19/0.803</td><td>32.85/0.919</td><td>multi-expert adaptive selection</td></tr><tr><td>Instruct-IPT[116]</td><td>arXiv'24</td><td></td><td>39.95/-</td><td>37.88/-</td><td>34.40/-</td><td>31.79/-</td><td>28.61/-</td><td>-/-</td><td>IPT via weight modulation</td></tr><tr><td>U-WADN[186]</td><td>arXiv'24</td><td>6M</td><td>29.21/0.971</td><td>35.36/0.968</td><td>33.73/0.931</td><td>31.14/0.886</td><td>27.92/0.793</td><td>31.47/0.910</td><td>unified-width adaptive network</td></tr><tr><td>Shi et al. [209]</td><td>arXiv'24</td><td>-</td><td>29.20/0.972</td><td>37.50/0.980</td><td>34.59/0.941</td><td>31.83/0.900</td><td>28.46/0.814</td><td>32.32/0.921</td><td>frequency-aware transformers</td></tr><tr><td>Perceive-IR [111]</td><td>arXiv'24</td><td>42M</td><td>30.87/0.975</td><td>38.29/0.980</td><td>34.13/0.934</td><td>31.53/0.890</td><td>28.31/0.804</td><td>32.63/0.917</td><td>quality-aware degradation</td></tr><tr><td>UniProcessor [210]</td><td>arXiv'24</td><td>1</td><td>31.66/0.979</td><td>38.17/0.982</td><td>34.08/0.935</td><td>31.42/0.891</td><td>28.17/0.803</td><td>32.70/0.918</td><td>support multimodal control</td></tr><tr><td>DyNet[185]</td><td>arXiv'24</td><td>16M</td><td>31.98/0.981</td><td>38.71/0.983</td><td>34.11/0.936</td><td>31.44/0.892</td><td>28.18/0.803</td><td>32.88/0.920</td><td>dynamic pre-training</td></tr><tr><td>Hair [189]</td><td></td><td>arXiv'24</td><td>29M</td><td>30.98/0.979</td><td>38.59/0.983</td><td>34.16/0.935</td><td>31.51/0.892</td><td>28.24/0.803</td><td>32.70/0.919</td><td>hypernetworks-based</td></tr></table></body></html>  

# B. Experiments and Analysis  

To demonstrate the superiority of different AiOIR models, we provide an objective quality comparison in Tab. III, Tab. IV, Tab.V. The evaluation metrics are composed of PSNR, SSIM [36]. Specifically, we summarize the experimental results of different methods under three common experimental settings in the field of AiOIR, i.e., Setting1 for three distinct tasks in Tab. III: dehazing, deraining, denoising (the noise level $\sigma = 1 5$ , $\sigma = 2 5$ , $\sigma = 5 0 ^ { \circ } .$ ), Setting2 for five distinct tasks in Tab. IV: dehazing, deraining, denoising (the noise level $\sigma = 2 5$ ), deblurring, and low-light enhancement, Setting3 for synthetic deweathering (the All-weather datasets [64]) in Tab. V: snow, rain+fog, raindrop, Setting4 for real-world deweathering (the WeatherStream dataset [173]) in Tab. VI: haze, rain, snow. To compare the computation cost and network complexity, we also measure the parameters for partial methods. The results align closely with the original paper, and for settings not tested in the original, we choose results with higher evaluation metrics.  

For Setting1 in Tab. III, PIP [72] and TextPromptIR [94] achieve the best average performance, indicating strong generalization across different degradation types. PIP achieves PSNR/SSIM of 32.91/0.920, while TextPromptIR closely follows with $3 2 . 8 0 / 0 . 9 1 9$ , which reflects the effectiveness of prompt-based strategies and multimodal prompt. For dehazing, Instruct-IPT [116] achieves the highest PSNR of 39.95, significantly outperforming others, followed by MEASNet [110] and DyNet [185]. However, MEASNet and DyNet stand out with the highest PSNR values of 39.00 and 38.71 for deraining, respectively. Many recent models adopt complex mechanisms such as frequency-aware transformations [209], multi-expert selection [110], and hypernetwork-based architectures [189], suggesting a growing emphasis on specialized designs to tackle varying degradation patterns.  

For Setting2 in Tab. IV, PIPRestormer also achieves the best average performance across tasks, particularly excelling in dehazing and deraining. MEASNet and DaAIR [187] also show strong results, with high average scores, indicating their robustness across multiple degradation types. Prompt-based models like PIPRestormer and PromptIR [65] are among the top performers, suggesting that prompt learning strategies and AiOIR are becoming dominant approaches for tackling multiple degradations simultaneously. Degradation-awareness seems to be a key factor, with models like DaAIR and Perceive-IR [111] showing that tailoring the restoration process based on the specific type of degradation improves overall performance. The balance between model size (params) and performance varies significantly. For example, smaller models like TAPE [59] perform reasonably well in certain tasks, indicating efficient design, while larger models like Gridformer [104] leverage more complex architectures for more consistent results across tasks.  

For Setting3 in Tab. V, the AWRCP model [68] achieves the best results across all three weather conditions, demonstrating codebook priors’ strong capacity to handle complex weather degradations. Models like WeatherDiff [69] and TK  

TABLE IV PERFORMANCE COMPARISONS OF AIOIR MODELS ON FIVE CHALLENGING DATASETS. DENOISING RESULTS ARE REPORTED FOR THE NOISE LEVEL $\sigma = 2 5$ .   


<html><body><table><tr><td></td><td>Method</td><td>Venue</td><td>Params.</td><td>SOrs [176]</td><td>Raiero0 [B2]</td><td>Ben6s I261</td><td>Depr ts</td><td>Low. l3s1t</td><td>Average</td><td>Approach</td></tr><tr><td rowspan="6"></td><td>ADFNet [202]</td><td>AAAI'23</td><td>8M</td><td>24.18/0.928</td><td>32.97/0.943</td><td>31.15/0.882</td><td>25.79/0.781</td><td>21.15/0.823</td><td>27.05/0.871</td><td>Specific</td></tr><tr><td>DehazeFormer [203]</td><td>TIP'23</td><td>25M</td><td>25.31/0.937</td><td>33.68/0.954</td><td>30.89/0.880</td><td>25.93/0.785</td><td>21.31/0.819</td><td>27.42/0.875</td><td>Specific</td></tr><tr><td>DRSformer [204]</td><td>CVPR'23</td><td>34M</td><td>24.66/0.931</td><td>33.45/0.953</td><td>30.97/0.881</td><td>25.56/0.780</td><td>21.77/0.821</td><td>27.28/0.873</td><td>Specific</td></tr><tr><td>HI-Diff [211]</td><td>NeurIPS'23</td><td>24M</td><td>25.09/0.935</td><td>33.26/0.951</td><td>30.61/0.878</td><td>26.48/0.800</td><td>22.01/0.870</td><td>27.49/0.887</td><td>Specific</td></tr><tr><td>Retinexformer [212]</td><td>ICCV'23</td><td>2M</td><td>24.81/0.933</td><td>32.68/0.940</td><td>30.84/0.880</td><td>25.09/0.779</td><td>22.76/0.863</td><td>27.24/0.873</td><td>Specific</td></tr><tr><td>SwinIR[182]</td><td>ICCVW'21</td><td>1M</td><td>21.50/0.891</td><td>30.78/0.923</td><td>30.59/0.868</td><td>24.52/0.773</td><td>17.81/0.723</td><td>25.04/0.835</td><td>General</td></tr><tr><td rowspan="7"></td><td>MIRNet-v2 [213]</td><td>TPAMI'22</td><td>6M</td><td>24.03/0.927</td><td>33.89/0.954</td><td>30.97/0.881</td><td>26.30/0.799</td><td>21.52/0.815</td><td>27.34/0.875</td><td>General</td></tr><tr><td>DGUNet [214]</td><td>CVPR'22</td><td>17M</td><td>24.78/0.940</td><td>36.62/0.971</td><td>31.10/0.883</td><td>27.25/0.837</td><td>21.87/0.823</td><td>28.32/0.891</td><td>General</td></tr><tr><td>Restormer [147]</td><td>CVPR'22</td><td>26M</td><td>24.09/0.927</td><td>34.81/0.960</td><td>31.49/0.884</td><td>27.22/0.829</td><td>20.41/0.806</td><td>27.60/0.881</td><td>General</td></tr><tr><td>NAFNet [206]</td><td>ECCV'22</td><td>17M</td><td>25.23/0.939</td><td>35.56/0.967</td><td>31.02/0.883</td><td>26.53/0.808</td><td>20.49/0.809</td><td>27.76/0.881</td><td>General</td></tr><tr><td>FSNet [207]</td><td>TPAMI'23</td><td>13M</td><td>25.53/0.943</td><td>36.07/0.968</td><td>31.33/0.883</td><td>28.32/0.869</td><td>22.29/0.829</td><td>28.71/0.898</td><td>General</td></tr><tr><td>MambaIR [208]</td><td>ECCV'24</td><td>27M</td><td>25.81/0.944</td><td>36.55/0.971</td><td>31.41/0.884</td><td>28.61/0.875</td><td>22.49/0.832</td><td>28.97/0.901</td><td>General</td></tr><tr><td>DL [55]</td><td>TPAMI19</td><td>2M</td><td>20.54/0.826</td><td>21.96/0.762</td><td>23.09/0.745</td><td>19.86/0.672</td><td>19.83/0.712</td><td>21.05/0.743</td><td>parameterized image operator</td></tr><tr><td rowspan="14"></td><td>Transweather [64]</td><td>CVPR'22</td><td>38M</td><td>21.32/0.885</td><td>29.43/0.905</td><td>29.00/0.841</td><td>25.12/0.757</td><td>21.21/0.792</td><td>25.22/0.836</td><td>weather type queries</td></tr><tr><td>TAPE [59]</td><td>ECCV'22</td><td>1M</td><td>22.16/0.861</td><td>29.67/0.904</td><td>30.18/0.855</td><td>24.47/0.763</td><td>18.97/0.621</td><td>25.09/0.801</td><td>task-agnostic prior</td></tr><tr><td>AirNet [63]</td><td>CVPR'22</td><td>9M</td><td>21.04/0.884</td><td>32.98/0.951</td><td>30.91/0.882</td><td>24.35/0.781</td><td>18.18/0.735</td><td>25.49/0.846</td><td>contrastive-based&degradation-guided</td></tr><tr><td>IDR [66]</td><td>CVPR'23</td><td>15M</td><td>25.24/0.943</td><td>35.63/0.965</td><td>31.60/0.887</td><td>27.87/0.846</td><td>21.34/0.826</td><td>28.34/0.893</td><td>ingredient-oriented learning</td></tr><tr><td>PIPNAFNet[72]</td><td>arXiv23</td><td></td><td>31.75/0.978</td><td>37.67/0.980</td><td>31.25/0.878</td><td>28.08/0.853</td><td>23.37/0.854</td><td>30.66/0.899</td><td>prompt-in-prompt learning</td></tr><tr><td>PIPRestormer [72]</td><td>arXiv'23</td><td>27M</td><td>32.11/0.979</td><td>38.09/0.983</td><td>30.94/0.877</td><td>28.61/0.861</td><td>24.06/0.859</td><td>30.81/0.901</td><td>prompt-in-prompt learning</td></tr><tr><td></td><td>NexivPS923</td><td></td><td>26.54/0949</td><td>36.370970</td><td>31370.886</td><td></td><td></td><td></td><td></td></tr><tr><td>DASLNIRN6s12151</td><td></td><td>33M</td><td></td><td></td><td></td><td>28.7/088</td><td>22.680.832</td><td>29.15/0904</td><td>decomproptforAirgsie</td></tr><tr><td>DASLDGINet [215]</td><td>arXiv'23</td><td>17M</td><td>25.33/0.943</td><td>36.96/0.972</td><td>31.23/0.885</td><td>27.23/0.836</td><td>21.78/0.824</td><td>28.51/0.892</td><td>decomposition ascribed synergistic</td></tr><tr><td>DASLMPRNet[215]</td><td>arXiv'23</td><td>5M</td><td>23.64/0.924</td><td>34.93/0.961</td><td>30.99/0.883</td><td>26.04/0.788</td><td>20.06/0.805</td><td>27.13/0.872</td><td>decomposition ascribed synergistic</td></tr><tr><td>Gridformer [104]</td><td>IJCV'23</td><td>34M</td><td>26.79/0.951</td><td>36.61/0.971</td><td>31.45/0.885</td><td>29.22/0.884</td><td>22.59/0.831</td><td>29.33/0.904</td><td>transformerwith grid structure</td></tr><tr><td>DaAIR[187]</td><td>arXiv'24</td><td>6M</td><td>31.97/0.980</td><td>36.28/0.975</td><td>31.07/0.878</td><td>29.51/0.890</td><td>22.38/0.825</td><td>30.24/0.910</td><td>efficient degradation-aware</td></tr><tr><td>AnyIR[188]</td><td>arXivi24</td><td>6M</td><td>29.84/0.977</td><td>36.91/0.977</td><td>31.15/0.882</td><td>26.86/0.822</td><td>23.50/0.845</td><td>29.65/0.901</td><td>local-global gated intertwining</td></tr><tr><td>InstructIR [62]</td><td>ECCV'24</td><td>16M</td><td>36.84/0.973</td><td>27.10/0.956</td><td>31.40/0.887</td><td>29.40/0.886</td><td>23.00/0.836</td><td>29.55/0.907</td><td>natural language prompts</td></tr><tr><td>MEASNet[110]</td><td>arXiv'24</td><td>31M</td><td>31.05/0.980</td><td>38.32/0.982</td><td>31.40/0.888</td><td>29.41/0.890</td><td>23.00/0.845</td><td>30.64/0.917</td><td></td></tr><tr><td>Perceive-IR[111]</td><td>arXiv24</td><td>42M</td><td>28.19/0.964</td><td>37.25/0.977</td><td>31.44/0.887</td><td>29.46/0.886</td><td>22.88/0.833</td><td>29.84/0.909</td><td>multi-expert adaptive selection</td></tr><tr><td>Hair [189]</td><td></td><td>arXiv'24</td><td>29M</td><td>30.62/0.978</td><td>38.11/0.981</td><td>31.49/0.891</td><td>28.52/0.874</td><td>23.12/0.847</td><td>30.37/0.914</td><td>quality-awaredegradation hypernetworks-based</td></tr></table></body></html>  

MANet [100] also perform well, particularly in handling snow and foggy conditions. Finally, older methods like All-inOne [56] perform significantly lower compared to more recent models like AWRCP and Transweather [64], with an average PSNR of only $2 8 . 0 5 ~ \mathrm { d B }$ , showing that recent advancements.  

For Setting4 in Tab. VI, despite Transweather’s larger parameter count, struggles to deliver competitive performance. In contrast, though more parameter-efficient, AirNet still falls short in overall restoration quality. TKMANet and WGWSNet show a notable improvement in both metrics, balancing effectiveness and efficiency, with WGWS-Net standing out as a lightweight model that achieves higher restoration quality. The best performance is observed with the model by Yang et al. [74], which leverages language-driven techniques to achieve superior results across all weather conditions. Overall, these results demonstrate a clear progression in multi-weather restoration, suggesting that the field is advancing towards more sophisticated and robust techniques across diverse conditions.  

# V. CHALLENGES AND FUTURE DIRECTIONS  

# A. Challenges  

AiOIR models encounter several challenges that limit their effectiveness in real-world applications. Task conflicts arise from differing objectives in denoising, deblurring, and dehazing, complicating simultaneous optimization and leading to inconsistent performance. Additionally, these models struggle with Out of Distribution (OOD) degradations, as realworld images often exhibit a mix of degradation types that don’t align with training data. The computational demands of current models hinder deployment on resource-constrained devices, necessitating a balance between restoration quality and efficiency. Furthermore, the reliance on large-scale, highquality labeled datasets poses challenges due to the resourceintensive nature of data acquisition, resulting in generalization issues. Lastly, most models focus on RGB images, while handling high-dimensional data introduces further complexities. Addressing these challenges is essential for improving AiOIR models’ practicality and performance.  

TABLE V PERFORMANCE COMPARISONS OF AIOIR MODELS ON ALL-WEATHER DATASETS [64]. BELOW THE DIVIDING LINE IN THE TABLE IS AIOIR METHODS.   


<html><body><table><tr><td>Method</td><td>Snow</td><td>Rain+Fog</td><td>Raindrop</td><td>Average</td><td>Params</td></tr><tr><td>SwinIR [56]</td><td>28.18/0.880</td><td>23.23/0.869</td><td>30.82/0.904</td><td>27.41/0.884</td><td>12M</td></tr><tr><td>MPRNet [64]</td><td>28.66/0.869</td><td>30.25/0.914</td><td>30.99/0.916</td><td>29.30/0.900</td><td>4M</td></tr><tr><td>Restormer [147]</td><td>29.37/0.881</td><td>29.22/0.907</td><td>31.21/0.919</td><td>29.93/0.902</td><td>18M</td></tr><tr><td>All-in-One [56]</td><td>28.33/0.882</td><td>24.71/0.898</td><td>31.12/0.927</td><td>28.05/0.902</td><td>44M</td></tr><tr><td>Transweather [64]</td><td>29.31/0.888</td><td>28.83/0.900</td><td>30.17/0.916</td><td>29.44/0.901</td><td>38M</td></tr><tr><td>AirNet [63]</td><td>27.92/0.858</td><td>23.12/0.837</td><td>28.23/0.892</td><td>26.42/0.862</td><td>9M</td></tr><tr><td>WGWS-Net [67]</td><td>28.91/0.856</td><td>29.28/0.922</td><td>32.01/0.925</td><td>30.07/0.901</td><td>6M</td></tr><tr><td>WeatherDiff [69]</td><td>30.09/0.904</td><td>29.64/0.931</td><td>30.71/0.931</td><td>30.15/0.922</td><td>83M</td></tr><tr><td>TKMANet [100]</td><td>30.24/0.902</td><td>29.92/0.917</td><td>30.99/0.927</td><td>30.38/0.915</td><td>29M</td></tr><tr><td>UtilityIR[75]</td><td>29.47/0.879</td><td>31.16/0.927</td><td>32.01/0.925</td><td>30.88/0.910</td><td>26M</td></tr><tr><td>AWRCP[68]</td><td>31.92/0.934</td><td>31.39/0.933</td><td>31.93/0.931</td><td>31.75/0.933</td><td></td></tr></table></body></html>  

TABLE VI PERFORMANCE COMPARISONS OF AIOIR MODELS ON WEATHERSTREAM DATASETS [173]. BELOW THE DIVIDING LINE IN THE TABLE IS AIOIR METHODS.   


<html><body><table><tr><td>Method</td><td>Haze</td><td>Rain</td><td>Snow</td><td>Average</td><td>Params</td></tr><tr><td>NAFNet [206]</td><td>22.20/0.803</td><td>23.01/0.803</td><td>22.11/0.826</td><td>22.44/0.811</td><td>17M</td></tr><tr><td>GRL [216]</td><td>22.88/0.802</td><td>23.75/0.805</td><td>22.59/0.829</td><td>23.07/0.812</td><td>3M</td></tr><tr><td>Restormer [147]</td><td>22.90/0.803</td><td>23.67/0.804</td><td>22.51/0.828</td><td>22.86/0.812</td><td>18M</td></tr><tr><td>MPRNet [205]</td><td>21.73/0.763</td><td>21.50/0.791</td><td>20.74/0.801</td><td>21.32/0.785</td><td>20M</td></tr><tr><td>Transweather [64]</td><td>22.55/0.774</td><td>22.21/0.772</td><td>21.79/0.792</td><td>22.18/0.779</td><td>38M</td></tr><tr><td>AirNet [63]</td><td>21.56/0.770</td><td>22.52/0.797</td><td>21.44/0.812</td><td>21.84/0.793</td><td>9M</td></tr><tr><td>TKMANet [100]</td><td>22.38/0.805</td><td>23.22/0.795</td><td>22.25/0.827</td><td>22.62/0.809</td><td>29M</td></tr><tr><td>WGWS-Net [67]</td><td>22.78/0.800</td><td>23.80/0.807</td><td>22.72/0.831</td><td>23.10/0.813</td><td>6M</td></tr><tr><td>LDR [74]</td><td>23.11/0.809</td><td>24.42/0.818</td><td>23.12/0.838</td><td>23.55/0.822</td><td>-</td></tr></table></body></html>  

Task Conflicts. In AiOIR, task conflicts arise due to differing objectives across various tasks like denoising, deblurring, and dehazing. These conflicts occur because tasks may require opposite optimizations—denoising reduces high-frequency noise, while deblurring enhances high-frequency details. Additionally, data characteristics for different tasks vary, leading to inconsistent performance when training on multiple tasks. Model capacity limitations also hinder the network’s ability to simultaneously handle diverse tasks effectively. To address these challenges, strategies such as multi-branch architectures, balanced multi-task loss functions, phased restoration, and adaptive learning can help mitigate conflicts and improve overall performance in all-in-one restoration models.  

Handling OOD Degradations. AiOIR models face significant difficulty in handling highly diverse and unforeseen image degradations, which can be treated as OOD degradations. In real-world scenarios, images may suffer from a combination of different degradations, such as blur, noise, low resolution, and compression artifacts. At the same time, the degree of each degradation type is also varied, and it is likely to be inconsistent with the sample distribution at training time while testing.  

Model Complexity and Efficiency. Despite recent advancements in AiOIR, these models are often computationally expensive and complex. Their large size and high computational demands make them impractical for deployment on resourceconstrained devices such as mobile phones or embedded systems. Striking a balance between performance and efficiency remains a significant issue, requiring models to maintain high restoration quality without becoming too cumbersome.  

Limited High-Quality Data. Many AiOIR models depend on large-scale, high-quality labeled datasets for supervised training, but acquiring these datasets is resource-intensive. Realworld degraded data is often scarce, and the unpredictability of degradations makes it difficult for models to perform well in practical applications. Real-world degradations are typically more complex than those represented in synthetic training datasets, leading to generalization issues. Additionally, domain shifts can hinder performance when models are applied to different image types. To address these challenges, solutions like data augmentation, domain adaptation, self-supervised learning, and transfer learning are being explored, but limitations still exist.  

High Dimensional Data. Current IR models primarily focus on 2D images, but handling 3D data and video sequences presents additional challenges. For video restoration, not only does each frame need high-quality restoration, but temporal consistency across frames must also be preserved. This adds complexity and demands more sophisticated methods that can integrate spatial and temporal information simultaneously.  

# B. Future Research Directions  

In future research, the development of AiOIR methods will focus on several key directions. First, establishing a robust multi-task learning theory will be essential to effectively address task conflicts and optimize informationsharing mechanisms, thereby enhancing system performance. Second, reducing reliance on large-scale labeled datasets by exploring semi-supervised and unsupervised learning methods will improve the model’s adaptability in data-scarce situations. Additionally, designing efficient models suitable for edge computing will make AiOIR more feasible for practical applications. Furthermore, investigating more complex realworld degradation scenarios will drive improvements in model performance across various environments. Finally, integrating large multimodal pretrained models and generative priors will enhance the restoration capabilities by leveraging rich multimodal data. These research directions will lay the foundation for the practicality and flexibility of AiOIR models, enabling them to tackle a broader range of real-world challenges.  

Incorporating Robust Multi-Task Learning Theory. The development of a robust multi-task learning theory for AiOIR is still in its early stages, offering significant research opportunities [66], [217]. Key challenges include modeling task conflicts, dynamically assigning task weights, and optimizing the information-sharing mechanisms between tasks to maximize performance without interference [218], [219]. Additionally, understanding and establishing the optimal task sequence or phased restoration process can help improve results, as well as incorporating adaptive task prioritization based on degradation severity. Moreover, a multi-task loss function design that balances conflicting objectives is needed. Advancing MTL theory in these areas will be crucial for creating efficient and generalized AiOIR systems capable of handling a wide variety of real-world degradation scenarios.  

Semi-Supervised and Unsupervised Learning Methods. Reducing the dependency on large-scale labeled datasets is essential for the scalability and applicability of AiOIR models. Future research should focus on developing semi-supervised and unsupervised learning approaches that can learn effective representations from unlabeled or partially labeled data [220], [221]. Techniques such as self-supervised learning, contrastive learning, and unsupervised domain adaptation can be leveraged to improve model performance in scenarios where labeled data is scarce or unavailable. By advancing these methods, AiOIR models can become more adaptable to diverse and unforeseen degradations encountered in real-world conditions.  

Platform-Aware Model Design and Efficient Approaches. A critical direction is the design of edge models that enhance the applicability of AiOIR in real-world scenarios. This involves creating models that are not only accurate but also efficient in terms of computational resources, making them suitable for deployment on various platforms, including mobile devices and embedded systems [222], [223]. Techniques such as model compression, pruning, quantization, and efficient neural architecture search can be employed to develop lightweight models without significant loss in performance. By focusing on efficient all-in-one approaches, researchers can ensure that AiOIR models are practical for everyday use, bridging the gap between research and real-world application.  

Addressing More Practical and Complex Degradations. There is a need to focus on more practical tasks and datasets that reflect the complexities of real-world image degradations [224], [225]. Future research should explore composite and complex degradation scenarios, such as image restoration in nighttime conditions, deraining and dehazing in the dark, and images affected by multiple overlapping degradations rather than isolated, mixed single degradation tasks. Developing and utilizing datasets that capture these challenging conditions will enable models to learn from and be tested on data that closely resembles real-world challenges. This focus will drive the development of AiOIR models that are more robust and effective in practical applications.  

Incorporating Large Multimodal Pretrained Models and Generative Priors. Another promising avenue is exploiting large multimodal pretrained models, particularly those incorporating generative models, to enhance AiOIR tasks [226]– [229]. Models like CLIP [112] and recent advancements in generative models (e.g., Stable Diffusion [153]) have demonstrated remarkable abilities to capture complex data distributions across multiple modalities. By mining the rich representations and priors from these universal models, AiOIR can benefit from enhanced understanding of image content and context, leading to better restoration in low-level tasks. Integrating these models can help in handling a broader range of degradations and in generating more realistic and high-quality restored images, especially in cases where the degradation is severe or complex.  

Leveraging Multi-Modal Information. Most current AiOIR models primarily rely on single-modal image information, such as RGB images, limiting their effectiveness in addressing complex restoration tasks. Future research can focus on integrating multi-modal information—like depth maps, optical flow, and infrared images—into AiOIR frameworks [230]–[232]. This integration would provide models with rich contextual and structural insights, enhancing their ability to accurately recover images with varied degradations [233]. For example, infrared data can reveal details hidden in RGB images, enabling more comprehensive restoration. By leveraging multi-modal data, all-in-one models can achieve higher robustness and versatility across various applications, from medical imaging to low-light photography.  

Establishment of Standardized Evaluation Protocols and Benchmarks. In contrast to single image restoration tasks, such as super-resolution, image deraining, and image dehaze, where there are well-established benchmark training and testing datasets, the AiOIR task lacks a standardized dataset. To facilitate fair comparison and assessment of AiOIR models, it is important to establish standardized evaluation protocols and comprehensive benchmarks. Creating diverse and representative benchmarks that encompass a wide range of realworld scenarios, including high-resolution image restoration, medical image enhancement, old photo restoration, and adverse weather conditions (e.g., sandstorms, nighttime fog), will allow for a more thorough evaluation of model performance. Standardized benchmarks will help identify the strengths and limitations of different models, fostering progress and encouraging the development of more generalizable AiOIR methods.  

Extension to Other Data. In addition to RGB images, extending AiOIR methods to other data, such as video [234], [235], 3D data [235], [236], dynamic event data [237], [238], and hyperspectral data [239], presents significant opportunities for future research. Video restoration requires not only enhancing individual frames but also preserving temporal consistency across frames. Similarly, 3D data restoration involves handling spatial information in multiple dimensions and dealing with depth information. Developing AiOIR models for hyperspectral data requires handling the high dimensionality of this data while maintaining spectral consistency across bands. Developing techniques that effectively integrate spatial, temporal, spectral, dynamic, and 3D information will be vital for applications such as video enhancement, 3D rendering, spectral analysis, and augmented reality . Addressing these challenges will expand the capabilities of AiOIR models, making them more versatile and applicable to a broader range of tasks.  

# VI. CONCLUSION  

In this paper, we present a comprehensive review of recent advancements in all-in-one image restoration (AiOIR), a rapidly emerging domain that integrates multiple types of image degradations into a single framework. Through an indepth exploration of state-of-the-art models, we emphasize their robust capabilities, diverse architectures, and the rigorous experimentations. By contrasting these models with traditional single-task approaches, we underscore the limitations of the latter in addressing real-world complexities, while highlighting the significant gains in efficiency, adaptability, and scalability offered by AiOIR models.  

Our comprehensive taxonomy of existing works offers a multi-dimensional perspective, covering structural innovations, key methodologies such as prompt learning, Mixture-ofExperts (MoE), and the incorporation of multimodal models. We further present an analysis of critical datasets, providing researchers and practitioners a holistic toolkit to better assess the current landscape of AiOIR. Despite the considerable strides made in recent years, challenges persist. Current models still struggle with handling complex and compounded degradations, lack computational efficiency, and do not generalize well in real-world scenarios. We believe that future research will focus on several key areas: the development of more lightweight and efficient architectures, advancements in semi-supervised learning, and expanding the scope of AiOIR models to accommodate multimodal inputs and video data. Additionally, as the field progresses, innovations in crossmodal learning, real-time processing, and interpretability will likely become central to pushing the boundaries of what AiOIR systems can achieve.  

In conclusion, AiOIR represents a promising, unified approach to tackling diverse degradation challenges in a more holistic manner. As the field continues to evolve, it holds immense potential for broader applications in real-world contexts, from media enhancement to autonomous systems. We hope that this review not only maps the current state-of-the-art but also inspires further innovations and breakthroughs in the pursuit of more sophisticated, efficient, and versatile AiOIR models.  

# REFERENCES  

with deep convolutional neural networks,” Communications of the ACM, vol. 60, no. 6, pp. 84–90, 2017.   
[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS, vol. 30, 2017. [3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “Highresolution image synthesis with latent diffusion models,” in CVPR, 2022, pp. 10 684–10 695. [4] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Color image denoising via sparse 3d collaborative filtering with grouping constraint in luminance-chrominance space,” in ICIP, vol. 1. Ieee, 2007, pp. I–313.   
[5] J. Liu, J. Lin, X. Li, W. Zhou, S. Liu, and Z. Chen, “LIRA: Lifelong image restoration from unknown blended distortions,” in ECCV. Springer, 2020, pp. 616–632. [6] C. Tian, Y. Xu, and W. Zuo, “Image denoising using deep cnn with batch renormalization,” Neural Networks, vol. 121, pp. 461–473, 2020. [7] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond a Gaussian Denoiser: Residual learning of deep cnn for image denoising,” IEEE Trans. Image Process., vol. 26, no. 7, pp. 3142–3155, 2017. [8] K. Zhang, W. Zuo, S. Gu, and L. Zhang, “Learning deep cnn denoiser prior for image restoration,” in CVPR, 2017, pp. 3929–3938. [9] K. Zhang, W. Zuo, and L. Zhang, “FFDNet: Toward a fast and flexible solution for cnn-based image denoising,” IEEE Trans. Image Process., vol. 27, no. 9, pp. 4608–4622, 2018.   
[10] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “DehazeNet: An end-to-end system for single image haze removal,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5187–5198, 2016.   
[11] Y. Dong, Y. Liu, H. Zhang, S. Chen, and Y. Qiao, “FD-GAN: Generative adversarial networks with fusion-discriminator for single image dehazing,” in AAAI, vol. 34, no. 07, 2020, pp. 10 729–10 736.   
[12] X. Fan, Y. Wang, X. Tang, R. Gao, and Z. Luo, “Two-layer gaussian process regression with example selection for image dehazing,” IEEE Trans. Circuits Syst. Video Technol., vol. 27, no. 12, pp. 2505–2517, 2016.   
[13] K. He, J. Sun, and X. Tang, “Single image haze removal using dark channel prior,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12, pp. 2341–2353, 2010.   
[14] B. Li, Y. Gou, S. Gu, J. Z. Liu, J. T. Zhou, and X. Peng, “You Only Look Yourself: Unsupervised and untrained single image dehazing neural network,” Int. J. Comput. Vision, vol. 129, pp. 1754–1767, 2021.   
[15] W.-T. Chen, H.-Y. Fang, C.-L. Hsieh, C.-C. Tsai, I. Chen, J.-J. Ding, S.-Y. Kuo et al., “All Snow Removed: Single image desnowing algorithm using hierarchical dual-tree complex wavelet representation and contradict channel loss,” in CVPR, 2021, pp. 4196–4205.   
[16] S. Chen, T. Ye, C. Xue, H. Chen, Y. Liu, E. Chen, and L. Zhu, “Uncertainty-driven dynamic degradation perceiving and background modeling for efficient single image desnowing,” in ACM MM, 2023, pp. 4269–4280.   
[17] Y. Wang, C. Ma, and J. Liu, “Smartassign: Learning a smart knowledge assignment strategy for deraining and desnowing,” in CVPR, 2023, pp. 3677–3686.   
[18] D. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng, “Progressive Image Deraining Networks: A better and simpler baseline,” in CVPR, 2019, pp. 3937–3946.   
[19] W. Yang, R. T. Tan, S. Wang, Y. Fang, and J. Liu, “Single Image Deraining: From model-based to data-driven and beyond,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 11, pp. 4059–4077, 2020.   
[20] X. Chen, H. Li, M. Li, and J. Pan, “Learning a sparse transformer network for effective image deraining,” in CVPR, 2023, pp. 5896–5905.   
[21] X. Tao, H. Gao, X. Shen, J. Wang, and J. Jia, “Scale-recurrent network for deep image deblurring,” in CVPR, 2018, pp. 8174–8182.   
[22] L. Yuan, J. Sun, L. Quan, and H.-Y. Shum, “Image deblurring with blurred/noisy image pairs,” in ACM SIGGRAPH, 2007, pp. 1–es.   
[23] X. Guo, Y. Li, and H. Ling, “LIME: Low-light image enhancement via illumination map estimation,” IEEE Trans. Image Process., vol. 26, no. 2, pp. 982–993, 2016.   
[24] L. Ma, T. Ma, R. Liu, X. Fan, and Z. Luo, “Toward fast, flexible, and robust low-light image enhancement,” in CVPR, 2022, pp. 5637–5646.   
[25] Y. Wang, W. Xie, and H. Liu, “Low-light image enhancement based on deep learning: a survey,” Opt. Eng., vol. 61, no. 4, pp. 040 901–040 901, 2022   
[26] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics,” in ICCV, vol. 2. Ieee, 2001, pp. 416–423.   
[27] Y. Matsui, K. Ito, Y. Aramaki, A. Fujimoto, T. Ogawa, T. Yamasaki, and K. Aizawa, “Sketch-based manga retrieval using manga109 dataset,” Multimed. Tools Appl., vol. 76, pp. 21 811–21 838, 2017.   
[28] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution from transformed self-exemplars,” in CVPR, 2015, pp. 5197–5206.   
[29] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative adversarial network for raindrop removal from a single image,” in CVPR, 2018, pp. 2482–2491.   
[30] R. Li, L.-F. Cheong, and R. T. Tan, “Heavy Rain Image Restoration: Integrating physics model and conditional adversarial learning,” in CVPR, 2019, pp. 1633–1642.   
[31] T. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. Lau, “Spatial attentive single-image deraining with a high quality real rain dataset,” in CVPR, 2019, pp. 12 270–12 279.   
[32] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain detection and removal from a single image,” in CVPR, 2017, pp. 1357–1366.   
[33] S. Nah, T. Hyun Kim, and K. Mu Lee, “Deep multi-scale convolutional neural network for dynamic scene deblurring,” in CVPR, 2017, pp. 3883–3891.   
[34] Z. Shen, W. Wang, X. Lu, J. Shen, H. Ling, T. Xu, and L. Shao, “Human-aware motion deblurring,” in CVPR, 2019, pp. 5572–5581.   
[35] C. Wei, W. Wang, W. Yang, and J. Liu, “Deep retinex decomposition for low-light enhancement,” arXiv preprint arXiv:1808.04560, 2018.   
[36] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image Quality Assessment: from error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, pp. 600–612, 2004.   
[37] L. Zhang, L. Zhang, and A. C. Bovik, “A feature-enriched completely blind image quality evaluator,” IEEE Trans. Image Process., vol. 24, no. 8, pp. 2579–2591, 2015.   
[38] H. R. Sheikh, A. C. Bovik, and G. De Veciana, “An information fidelity criterion for image quality assessment using natural scene statistics,” IEEE Trans. Image Process., vol. 14, no. 12, pp. 2117–2128, 2005.   
[39] Z. Wang, J. Chen, and S. C. H. Hoi, “Deep learning for image superresolution: A survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 10, pp. 3365–3387, 2021.   
[40] S. Anwar, S. Khan, and N. Barnes, “A deep journey into superresolution: A survey,” ACM Compt. Surv., vol. 53, no. 3, pp. 1–34, 2020.   
[41] H. Chen, X. He, L. Qing, Y. Wu, C. Ren, R. E. Sheriff, and C. Zhu, “Real-world single image super-resolution: A brief review,” Inf. Fus., vol. 79, pp. 124–145, 2022.   
[42] A. Liu, Y. Liu, J. Gu, Y. Qiao, and C. Dong, “Blind image superresolution: A survey and beyond,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 5, pp. 5461–5480, 2022.   
[43] H. Wang, Y. Wu, M. Li, Q. Zhao, and D. Meng, “Survey on rain removal from videos or a single image,” Sci. China Inform. Sci., vol. 65, no. 1, pp. 1–23, 2022.   
[44] Z. Su, Y. Zhang, J. Shi, and X.-P. Zhang, “A survey of single image rain removal based on deep learning,” ACM Compt. Surv., vol. 56, no. 4, pp. 1–35, 2023.   
[45] S. Krishna B V, B. Rajalakshmi, U. Dhammini, M. Monika, C. Nethra, and K. Ashok, “Image de-hazing techniques for vision based applications - a survey,” in ICONAT, 2023, pp. 1–5.   
[46] C. Tian, L. Fei, W. Zheng, Y. Xu, W. Zuo, and C.-W. Lin, “Deep learning on image denoising: An overview,” Neural Networks, vol. 131, pp. 251–275, 2020.   
[47] B. Goyal, A. Dogra, S. Agrawal, B. S. Sohi, and A. Sharma, “Image denoising review: From classical to state-of-the-art approaches,” Inf. Fus., vol. 55, pp. 220–244, 2020.   
[48] M. Elad, B. Kawar, and G. Vaksman, “Image denoising: The deep learning revolution and beyond—a survey paper,” SIAM J. Imaging Sci., vol. 16, no. 3, pp. 1594–1654, 2023.   
[49] A. Mahalakshmi and B. Shanthini, “A survey on image deblurring,” in ICCCI, 2016, pp. 1–5.   
[50] A. Ranjan and R. M, “Deep learning based image deblurring: A comparative survey,” in ICAC3N, 2022, pp. 996–1002.   
[51] J. Liu, D. Xu, W. Yang, M. Fan, and H. Huang, “Benchmarking lowlight image enhancement and beyond,” Int. J. Comput. Vision, vol. 129, pp. 1153–1184, 2021.   
[52] C. Li, C. Guo, L. Han, J. Jiang, M.-M. Cheng, J. Gu, and C. C. Loy, “Low-light image and video enhancement using deep learning: A survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 12, pp. 9396–9416, 2021.   
[53] Wang, w-light image enhancement with normalizing flow,” in AAAI, vol. 36, no. 3, 2022, pp. 2604–2612.   
[54] X. Li, Y. Ren, X. Jin, C. Lan, X. K. Wang, W. Zeng, X. Wang, and Z. Chen, “Diffusion models for image restoration and enhancement - a comprehensive survey,” CoRR, vol. abs/2308.09388, 2023.   
[55] Q. Fan, D. Chen, L. Yuan, G. Hua, N. Yu, and B. Chen, “A general decoupled learning framework for parameterized image operators,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 1, pp. 33–47, 2019.   
[56] R. Li, R. T. Tan, and L. F. Cheong, “All in one bad weather removal using architectural search,” in CVPR, 2020, pp. 3172–3182.   
[57] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao, “Pre-trained image processing transformer,” in CVPR, 2021, pp. 12 299–12 310.   
[58] W. Li, X. Lu, S. Qian, J. Lu, X. Zhang, and J. Jia, “On efficient transformer-based image pre-training for low-level vision,” arXiv preprint arXiv:2112.10175, 2021.   
[59] L. Liu, L. Xie, X. Zhang, S. Yuan, X. Chen, W. Zhou, H. Li, and Q. Tian, “TAPE: Task-agnostic prior embedding for image restoration,” in ECCV. Springer, 2022, pp. 447–464.   
[60] B. Fei, Z. Lyu, L. Pan, J. Zhang, W. Yang, T. Luo, B. Zhang, and B. Dai, “Generative diffusion prior for unified image restoration and enhancement,” in CVPR, 2023, pp. 9935–9946.   
[61] J. Ma, T. Cheng, G. Wang, Q. Zhang, X. Wang, and L. Zhang, “ProRes: Exploring degradation-aware visual prompt for universal image restoration,” arXiv preprint arXiv:2306.13653, 2023.   
[62] M. V. Conde, G. Geigle, and R. Timofte, “High-quality image restoration following human instructions,” arXiv preprint arXiv:2401.16468, 2024.   
[63] B. Li, X. Liu, P. Hu, Z. Wu, J. Lv, and X. Peng, “All-in-one image restoration for unknown corruption,” in CVPR, 2022, pp. 17 452– 17 462.   
[64] J. M. J. Valanarasu, R. Yasarla, and V. M. Patel, “TransWeather: Transformer-based restoration of images degraded by adverse weather conditions,” in CVPR, 2022, pp. 2353–2363.   
[65] V. Potlapalli, S. W. Zamir, S. H. Khan, and F. Shahbaz Khan, “PromptIR: Prompting for all-in-one image restoration,” in NeurIPS, vol. 36, 2024.   
[66] J. Zhang, J. Huang, M. Yao, Z. Yang, H. Yu, M. Zhou, and F. Zhao, “Ingredient-oriented multi-degradation learning for image restoration,” in CVPR, 2023, pp. 5825–5835.   
[67] Y. Zhu, T. Wang, X. Fu, X. Yang, X. Guo, J. Dai, Y. Qiao, and X. Hu, “Learning weather-general and weather-specific features for image restoration under multiple adverse weather conditions,” in CVPR, 2023, pp. 21 747–21 758.   
[68] T. Ye, S. Chen, J. Bai, J. Shi, C. Xue, J. Jiang, J. Yin, E. Chen, and Y. Liu, “Adverse weather removal with codebook priors,” in CVPR, 2023, pp. 12 653–12 664.   
[69] O. ¨Ozdenizci and R. Legenstein, “Restoring vision in adverse weather conditions with patch-based denoising diffusion models,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 8, pp. 10 346–10 357, 2023.   
[70] Y. Jiang, Z. Zhang, T. Xue, and J. Gu, “AutoDIR: Automatic all-in-one image restoration with latent diffusion,” arXiv preprint arXiv:2310.10123, 2023.   
[71] Y. Ai, H. Huang, X. Zhou, J. Wang, and R. He, “Multimodal prompt perceiver: Empower adaptiveness generalizability and fidelity for allin-one image restoration,” in CVPR, 2024, pp. 25 432–25 444.   
[72] Z. Li, Y. Lei, C. Ma, J. Zhang, and H. Shan, “Prompt-in-prompt learning for universal image restoration,” arXiv, 2023.   
[73] Y. Cheng, M. Shao, Y. Wan, C. Wang, and W. Zuo, “DRM-IR: Taskadaptive deep unfolding network for all-in-one image restoration,” arXiv, 2023.   
[74] H. Yang, L. Pan, Y. Yang, and W. Liang, “Language-driven all-in-one adverse weather removal,” in CVPR, 2024, pp. 24 902–24 912.   
[75] Y.-W. Chen and S.-C. Pei, “Always clear days: Degradation type and severity aware all-in-one adverse weather removal,” arXiv preprint arXiv:2310.18293, 2023.   
[76] Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sj¨olund, and T. B. Sch¨on, “Controlling vision-language models for universal image restoration,” arXiv preprint arXiv:2310.01018, 2023.   
[77] D. Zheng, X.-M. Wu, S. Yang, J. Zhang, J.-F. Hu, and W.-S. Zheng, “Selective hourglass mapping for universal image restoration based on diffusion model,” in CVPR, 2024, pp. 25 445–25 455.   
[78] Y. Cui, S. W. Zamir, S. Khan, A. Knoll, M. Shah, and F. S. Khan, “AdaIR: Adaptive all-in-one image restoration via frequency mining and modulation,” arXiv preprint arXiv:2403.14614, 2024.   
[79] W. Liu, X. Shen, C.-M. Pun, and X. Cun, “Explicit visual prompting for low-level structure segmentations,” in CVPR, 2023, pp. 19 434–19 445.   
[80] Y. Liu, X. Chen, X. Ma, X. Wang, J. Zhou, Y. Qiao, and C. Dong, “Unifying image processing as visual prompting question answering,” arXiv 2310.10513, 2023.   
[81] Q. Huang, X. Dong, D. Chen, W. Zhang, F. Wang, G. Hua, and N. Yu, “Diversity-aware meta visual prompting,” in CVPR, 2023, pp. 10 878– 10 887.   
[82] X. Wang, W. Wang, Y. Cao, C. Shen, and T. Huang, “Images Speak in Images: A generalist painter for in-context visual learning,” in CVPR, 2023, pp. 6830–6839.   
[83] Y. Shen, C. Fu, P. Chen, M. Zhang, K. Li, X. Sun, Y. Wu, S. Lin, and R. Ji, “Aligning and prompting everything all at once for universal visual perception,” in CVPR, 2024, pp. 13 193–13 203.   
[84] Z. Wang, Z. Zhang, C.-Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su, V. Perot, J. Dy, and T. Pfister, “Learning to prompt for continual learning,” in CVPR, 2022, pp. 139–149.   
[85] Y. Gou, H. Zhao, B. Li, X. Xiao, and X. Peng, “Test-time degradation adaptation for open-set image restoration,” in ICML, 2023.   
[86] I. Shin, Y.-H. Tsai, B. Zhuang, S. Schulter, B. Liu, S. Garg, I.-S. Kweon, and K.-J. Yoon, “MM-TTA: Multi-modal test-time adaptation for 3d semantic segmentation,” in CVPR, 2022, pp. 16 907–16 916.   
[87] Q. Wang, O. Fink, L. V. Gool, and D. Dai, “Continual test-time domain adaptation,” in CVPR, 2022, pp. 7191–7201.   
[88] S. Niu, J. Wu, Y. Zhang, Z. Wen, Y. Chen, P. Zhao, and M. Tan, “Towards stable test-time adaptation in dynamic wild world,” CoRR, vol. abs/2302.12400, 2023.   
[89] J. Wu, X. Li, H. Yuan, H. Ding, Y. Yang, X. Li, J. Zhang, Y. Tong, X. Jiang, B. Ghanem, D. Tao, and S. Xu, “Towards open vocabulary learning: A survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, pp. 5092–5113, 2023.   
[90] H. Chung, B. Sim, D. Ryu, and J. C. Ye, “Improving diffusion models for inverse problems using manifold constraints,” CoRR, vol. abs/2206.00941, 2022.   
[91] H. Chung, J. Kim, M. T. McCann, M. L. Klasky, and J. C. Ye, “Diffusion posterior sampling for general noisy inverse problems,” CoRR, vol. abs/2209.14687, 2022.   
[92] B. Fei, Z. Lyu, L. Pan, J. Zhang, W. Yang, T. jian Luo, B. Zhang, and B. Dai, “Generative diffusion prior for unified image restoration and enhancement,” in CVPR, 2023, pp. 9935–9946.   
[93] B. Kawar, M. Elad, S. Ermon, and J. Song, “Denoising diffusion restoration models,” CoRR, vol. abs/2201.11793, 2022.   
[94] Q. Yan, A. Jiang, K. Chen, L. Peng, Q. Yi, and C. Zhang, “Textual prompt guided image restoration,” arXiv preprint arXiv:2312.06162, 2023.   
[95] H. Gao, J. Yang, Y. Zhang, N. Wang, J. Yang, and D. Dang, “Promptbased ingredient-oriented all-in-one image restoration,” IEEE Trans. Circuits Syst. Video Technol., pp. 1–1, 2024.   
[96] M. Yao, R. Xu, Y. Guan, J. Huang, and Z. Xiong, “Neural degradation representation learning for all-in-one image restoration,” IEEE Trans. Image Process., vol. Pp, 2023.   
[97] C. Zhang, Y. Zhu, Q. Yan, J. Sun, and Y. Zhang, “All-in-one multidegradation image restoration network via hierarchical degradation representation,” in ACM MM, 2023.   
[98] X. Wang, H. Chen, H. Gou, J. He, Z. Wang, X. He, L. Qing, and R. E. Sheriff, “RestorNet: An efficient network for multiple degradation image restoration,” Knowl. Based Syst., vol. 282, p. 111116, 2023.   
[99] J. Lin, Z. Zhang, Y. Wei, D. Ren, D. Jiang, and W. Zuo, “Improving image restoration through removing degradations in textual representations,” in CVPR, 2023, pp. 2866–2878.   
[100] W.-T. Chen, Z.-K. Huang, C.-C. Tsai, H.-H. Yang, J.-J. Ding, and S.- Y. Kuo, “Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model,” in CVPR, 2022, pp. 17 632–17 641.   
[101] J. Han, W. Li, P. Fang, C. Sun, J. Hong, M. A. Armin, L. Petersson, and H. Li, “Blind image decomposition,” in ECCV. Springer, 2022, pp. 218–237.   
[102] T. Gao, Y. Wen, K. Zhang, J. Zhang, T. Chen, L. Liu, and W. Luo, “Frequency-oriented efficient transformer for all-in-one weather-degraded image restoration,” IEEE Trans. Circuits. Syst. Video. Technol., vol. 34, pp. 1886–1899, 2024.   
[103] Y. Luo, R. Zhao, X. Wei, J. Chen, Y. Lu, S. Xie, T. Wang, R. Xiong, M. Lu, and S. Zhang, “WM-MoE: Weather-aware multi-scale mixtureof-expe s for blind advers val,” 2024.   
[104] T. Wang, K. Zhang, Z. Shao, W. Luo, B. Stenger, T. Lu, T.-K. Kim, W. Liu, and H. Li, “GridFormer: Residual dense transformer with grid structure for image restoration in adverse weather conditions,” IJCV, 2024.   
[105] R. W. Liu, Y. Lu, Y. Guo, W. Ren, F. Zhu, and Y. Lv, “AiOENet: All-in-one low-visibility enhancement to improve visual perception for intelligent marine vehicles under severe weather conditions,” IEEE Trans. Intell. Veh., vol. 9, no. 2, pp. 3811–3826, 2024.   
[106] Z. Yang, H. Chen, Z. Qian, Y. Yi, H. Zhang, D. Zhao, B. Wei, and Y. Xu, “All-in-one medical image restoration via task-adaptive routing,” in MICCAI. Springer, 2024, pp. 67–77.   
[107] M. Chenglong, L. Zilong, H. Junjun, Z. Junping, Z. Yi, and S. Hongming, “Prompted contextual transformer for incomplete-view ct reconstruction,” arXiv preprint arXiv:2312.07846, 2023.   
[108] J. Zhang, D. Peng, C. Liu, P. Zhang, and L. Jin, “DocRes: A generalist model toward unifying document image restoration tasks,” in CVPR, 2024, pp. 15 654–15 664.   
[109] H.-H. Wang, F.-J. Tsai, Y.-Y. Lin, and C.-W. Lin, “TANet: Triplet attention network for all-in-one adverse weather image restoration,” arXiv preprint arXiv:2410.08177, 2024.   
[110] X. Yu, S. Zhou, H. Li, and L. Zhu, “Multi-Expert Adaptive Selection: Task-balancing for all-in-one image restoration,” CoRR, vol. abs/2407.19139, 2024.   
[111] X. Zhang, J. Ma, G. Wang, Q. Zhang, H. Zhang, and L. Zhang, “Perceive-IR: Learning to perceive degradation better for all-in-one image restoration,” CoRR, vol. abs/2408.15994, 2024.   
[112] Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sjolund, and T. B. Schon, “Controlling vision-language models for multi-task image restoration,” in ICLR, 2023.   
[113] M. Caron, H. Touvron, I. Misra, H. Je´gou, J. Mairal, P. Bojanowski, and A. Joulin, “Emerging properties in self-supervised vision transformers,” in ICCV, 2021, pp. 9630–9640.   
[114] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P. Huang, S. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Je´gou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski, “Dinov2: Learning robust visual features without supervision,” Trans. Mach. Learn. Res., 2024.   
[115] D. Fan, J. Zhang, and L. Chang, “ConStyle v2: A strong prompter for all-in-one image restoration,” arXiv preprint arXiv:2406.18242, 2024.   
[116] Y. Tian, J. Han, H. Chen, Y. Xi, G. Zhang, J. Hu, C. Xu, and Y. Wang, “Instruct-IPT: All-in-one image processing transformer via weight modulation,” arXiv preprint arXiv:2407.00676, 2024.   
[117] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf, “Transfer learning in natural language processing,” in NAACL, 2019, pp. 15–18.   
[118] H. H. Mao, “A survey on self-supervised pre-training for sequential transfer learning in neural networks,” 2020.   
[119] R. M. French, “Catastrophic forgetting in connectionist networks,” Trends in Cognitive Sciences, vol. 3, no. 4, pp. 128–135, 1999.   
[120] X. Su, Z. Zheng, and C. Wu, “Review Learning: Advancing all-in-one ultra-high-definition image restoration training method,” arXiv preprint arXiv:2408.06709, 2024.   
[121] X. Kong, C. Dong, and L. Zhang, “Towards effective multiple-in-one image restoration: A sequential and prompt learning strategy,” arXiv preprint arXiv:2401.03379, 2024.   
[122] Z. Chen and B. Liu, Lifelong machine learning. Springer Nature, 2022.   
[123] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuytelaars, “A continual learning survey: Defying forgetting in classification tasks,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 7, pp. 3366–3385, 2021.   
[124] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” in ICML. Pmlr, 2020, pp. 1597–1607.   
[125] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual representation learning,” in CVPR, 2020, pp. 9729–9738.   
[126] G. Wu, J. Jiang, K. Jiang, and X. Liu, “Learning from history: Taskagnostic model contrastive learning for image restoration,” in AAAI, vol. 38, no. 6, Mar. 2024, pp. 5976–5984.   
[127] H. Wu, Y. Qu, S. Lin, J. Zhou, R. Qiao, Z. Zhang, Y. Xie, and L. Ma, “Contrastive learning for compact single image dehazing,” in CVPR. Computer Vision Foundation / IEEE, 2021, pp. 10 551–10 560.   
[128] G. Wu, J. Jiang, and X. Liu, “A practical contrastive learning framework for single-image super-resolution,” IEEE Trans. Neural Netw. Learn. Syst., pp. 1–12, 2023.   
[129] Y. Zheng, J. Zhan, S. He, J. Dong, and Y. Du, “Curricular contrastive regularization for physics-aware single image dehazing,” in CVPR, 2023, pp. 5785–5794.   
[130] W. Ran, P. Ma, Z. He, H. Ren, and H. Lu, “Harnessing joint rain-/detailaware representations to eliminate intricate rains,” in ICLR, 2024.   
[131] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn, “Gradient surgery for multi-task learning,” CoRR, vol. abs/2001.06782, 2020.   
[132] B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu, “Conflict-averse gradient descent for multi-task learning,” in Neural Information Processing Systems, 2021.   
[133] C. Fifty, E. Amid, Z. Zhao, T. Yu, R. Anil, and C. Finn, “Efficiently identifying task groupings for multi-task learning,” CoRR, vol. abs/2109.04617, 2021.   
[134] S. Cao, Y. Liu, W. Zhang, Y. Qiao, and C. Dong, “GRIDS: Grouped multiple-degradation restoration with image degradation similarity,” CoRR, vol. abs/2407.12273, 2024.   
[135] G. Wu, J. Jiang, K. Jiang, and X. Liu, “Harmony in Diversity: Improving all-in-one image restoration via multi-task collaboration,” in ACM MM, 2024.   
[136] Y. Cao and J. Yang, “Towards making systems forget with machine unlearning,” ISSP, pp. 463–480, 2015.   
[137] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, “Machine unlearning,” ISSP, pp. 141–159, 2019.   
[138] Q. P. Nguyen, B. K. H. Low, and P. Jaillet, “Variational bayesian unlearning,” CoRR, vol. abs/2010.12883, 2020.   
[139] X. Su and Z. Zheng, “Accurate forgetting for all-in-one image restoration model,” arXiv preprint arXiv:2409.00685, 2024.   
[140] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot learners,” in NeurIPS, vol. 33, 2020, pp. 1877– 1901.   
[141] T. Gao, A. Fisch, and D. Chen, “Making pre-trained language models better few-shot learners,” arXiv, 2020.   
[142] X. Liu, K. Ji, Y. Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang, “P-Tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks,” arXiv, 2021.   
[143] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, “Visual prompt tuning,” in ECCV. Springer, 2022, pp. 709–727.   
[144] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan, “MaPLe: Multi-modal prompt learning,” in CVPR, 2023, pp. 19 113– 19 122.   
[145] A. Bar, Y. Gandelsman, T. Darrell, A. Globerson, and A. Efros, “Visual prompting via image inpainting,” in NeurIPS, vol. 35, 2022, pp. 25 005–25 017.   
[146] T. Chen, S. Saxena, L. Li, T.-Y. Lin, D. J. Fleet, and G. E. Hinton, “A unified sequence interface for vision tasks,” in NeurIPS, vol. 35, 2022, pp. 31 333–31 346.   
[147] S. W. Zamir, A. Arora, S. H. Khan, M. Hayat, F. S. Khan, and M.- H. Yang, “Restormer: Efficient transformer for high-resolution image restoration,” in CVPR, 2021, pp. 5718–5729.   
[148] K. He, X. Chen, S. Xie, Y. Li, P. Doll’ar, and R. B. Girshick, “Masked autoencoders are scalable vision learners,” in CVPR, 2021, pp. 15 979– 15 988.   
[149] Y. Pang, Y. Li, J. Shen, and L. Shao, “Towards bridging semantic gap to improve semantic segmentation,” in ICCV, 2019, pp. 4229–4238.   
[150] Y. Wei, Z. Zhang, J. Ren, X. Xu, R. Hong, Y. Yang, S. Yan, and M. Wang, “Clarity ChatGPT: An interactive and adaptive processing system for image restoration and enhancement,” arXiv preprint arXiv:2311.11695, 2023.   
[151] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pretraining of deep bidirectional transformers for language understanding,” in NAACL, 2019.   
[152] T. B. Brown, B. Mann, N. Ryder, and Others, “Language models are few-shot learners,” CoRR, vol. abs/2005.14165, 2020.   
[153] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “Highresolution image synthesis with latent diffusion models,” 2021.   
[154] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive mixtures of local experts,” Neural Computation, vol. 3, no. 1, pp. 79– 87, 1991.   
[155] R. Zhang, Y. Luo, J. Liu, H. Yang, Z. Dong, D. A. Gudovskiy, T. Okuno, Y. Nakata, K. Keutzer, Y. Du, and S. Zhang, “Efficient deweather mixture-of-experts with uncertainty-aware feature-wise linear modulation,” CoRR, vol. abs/2312.16610, 2023.   
[156] K. Zhang, W. Zuo, S. Gu, and L. Zhang, “Learning deep cnn denoiser prior for image restoration,” in CVPR, 2017, pp. 2808–2817.   
[157] D. Geman and C. Yang, “Nonlinear image recovery with half-quadratic regularization,” IEEE Trans. Image Process., vol. 4 7, pp. 932–46, 1995.   
[158] J. Rim, H. Lee, J. Won, and S. Cho, “Real-world blur dataset for learning and benchmarking deblurring algorithms,” in ECCV. Springer, 2020, pp. 184–201.   
[159] R. Franzen, “Kodak lossless true color image suite,” Source: https://r0k.us/graphics/kodak/, 1999.   
[160] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution from transformed self-exemplars,” in CVPR, 2015, pp. 5197–5206.   
[161] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and L. Zhang, “Waterloo Exploration Database: New challenges for image quality assessment models,” IEEE Trans. Image Process., vol. 26, no. 2, pp. 1004–1016, 2016.   
[162] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and hierarchical image segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5, pp. 898–916, 2010.   
[163] L. Zhang, X. Wu, A. Buades, and X. Li, “Color demosaicking by local directional interpolation and nonlocal adaptive thresholding,” J Electron Imaging, vol. 20, no. 2, pp. 023 016–023 016, 2011.   
[164] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image uper-resolution: Dataset and study,” in CVPR, Jul. 2017.   
[165] Y. Wang, L. Wang, J. Yang, W. An, and Y. Guo, “Flickr1024: A largescale dataset for stereo image super-resolution,” in ICCVW, 2019.   
[166] M. Bevilacqua, A. Roumy, C. M. Guillemot, and M.-L. Alberi-Morel, “Low-complexity single-image super-resolution based on nonnegative neighbor embedding,” in BMVC, 2012.   
[167] R. Zeyde, M. Elad, and M. Protter, “On single image scale-up using sparse-representations,” in ICCS. Springer, 2012, pp. 711–730.   
[168] J. Wang, X. Li, and J. Yang, “Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal,” in CVPR, 2018, pp. 1788–1797.   
[169] L. Qu, J. Tian, S. He, Y. Tang, and R. W. Lau, “DeshadowNet: A multicontext embedding deep network for shadow removal,” in CVPR, 2017, pp. 4067–4075.   
[170] Y.-F. Liu, D.-W. Jaw, S.-C. Huang, and J.-N. Hwang, “DesnowNet: Context-aware deep network for snow removal,” IEEE Trans. Image Process., vol. 27, no. 6, pp. 3064–3073, 2018.   
[171] X. Zhang, H. Dong, J. Pan, C. Zhu, Y. Tai, C. Wang, J. Li, F. Huang, and F. Wang, “Learning to restore hazy video: A new real-world dataset and a new method,” in CVPR, 2021, pp. 9239–9248.   
[172] J. Wang, C. Lin, L. Nie, S. Huang, Y. Zhao, X. Pan, and R. Ai, “WeatherDepth: Curriculum contrastive learning for self-supervised depth estimation under adverse weather conditions,” arXiv preprint arXiv:2310.05556, 2023.   
[173] H. Zhang, Y. Ba, E. Yang, V. Mehra, B. Gella, A. Suzuki, A. Pfahnl, C. C. Chandrappa, A. Wong, and A. Kadambi, “WeatherStream: Light transport automation of single image deweathering,” in CVPR, 2023, pp. 13 499–13 509.   
[174] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley, “Removing rain from single images via a deep detail network,” in CVPR, 2017, pp. 3855–3863.   
[175] C. O. Ancuti, C. Ancuti, M. Sbert, and R. Timofte, “Dense-Haze: A benchmark for image dehazing with dense-haze and haze-free images,” in ICIP. Ieee, 2019, pp. 1014–1018.   
[176] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang, “Benchmarking single-image dehazing and beyond,” IEEE Trans. Image Process., vol. 28, no. 1, pp. 492–505, 2019.   
[177] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” in ICML, 2021.   
[178] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y.- H. Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language representation learning with noisy text supervision,” in ICML, 2021.   
[179] J. Li, D. Li, C. Xiong, and S. C. H. Hoi, “BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation,” in ICML, 2022.   
[180] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, “SimMIM: a simple framework for masked image modeling,” in CVPR, 2021, pp. 9643–9653.   
[181] A. Radford and K. Narasimhan, “Improving language understanding by generative pre-training,” OpenAI Blog, 2018.   
[182] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin Transformer: Hierarchical vision transformer using shifted windows,” in ICCV, 2021, pp. 9992–10 002.   
[183] X. Wang, W. Wang, Y. Cao, C. Shen, and T. Huang, “Images Speak in Images: A generalist painter for in-context visual learning,” in CVPR, 2022, pp. 6830–6839.   
[184] C.-J. Qin, R.-Q. Wu, Z. Liu, X. Lin, C.-L. Guo, H. H. Park, and C. Li, “Restore Anything with Masks: Leveraging mask image modeling for blind all-in-one image restoration,” 2024.   
[185] A. Dudhane, O. Thawakar, S. W. Zamir, S. Khan, F. S. Khan, and M.-H. Yang, “Dynamic Pre-training: Towards efficient and scalable all-in-one image restoration,” CoRR, vol. abs/2404.02154, 2024.   
[186] Y. Xu, N. Gao, Z. Shan, F. Chao, and R. Ji, “Unified-width adaptive dynamic network for all-in-one image restoration,” CoRR, vol. abs/2401.13221, 2024.   
[187] E. Zamfir, Z. Wu, N. Mehta, D. D. Paudel, Y. Zhang, and R. Timofte, “Efficient degradation-aware any image restoration,” CoRR, vol. abs/2405.15475, 2024.   
[188] B. Ren, E. Zamfir, Y. Li, Z. Wu, D. P. Paudel, R. Timofte, N. Sebe, and L. V. Gool, “Any image restoration with efficient automatic degradation adaptation,” CoRR, vol. abs/2407.13372, 2024.   
[189] J. Cao, Y. Cao, L. Pang, D. Meng, and X. Cao, “HAIR: Hypernetworksbased all-in-one image restoration,” CoRR, vol. abs/2408.08091, 2024.   
[190] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “GANs trained by a two time-scale update rule converge to a local nash equilibrium,” in NeurIPS, vol. 30, 2017.   
[191] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in CVPR, 2018, pp. 586–595.   
[192] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely blind” image quality analyzer,” IEEE Signal Proc. Let., vol. 20, no. 3, pp. 209–212, 2012.   
[193] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image quality assessment in the spatial domain,” IEEE Trans. Image Process., vol. 21, pp. 4695–4708, 2012.   
[194] L. Zhang, L. Zhang, and A. C. Bovik, “A feature-enriched completely blind image quality evaluator,” IEEE Trans. Image Process., vol. 24, pp. 2579–2591, 2015.   
[195] H. Talebi and P. Milanfar, “NIMA: Neural image assessment,” IEEE Trans. Image Process., vol. 27, no. 8, pp. 3998–4011, 2018.   
[196] J. Wang, K. C. K. Chan, and C. C. Loy, “Exploring clip for assessing the look and feel of images,” in AAAI, 2022.   
[197] S. Wang, J. Zheng, H.-M. Hu, and B. Li, “Naturalness preserved enhancement algorithm for non-uniform illumination images,” IEEE Trans. Image Process., vol. 22, no. 9, pp. 3538–3548, 2013.   
[198] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi, “Image super-resolution via iterative refinement,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 4, pp. 4713–4726, 2023.   
[199] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely blind” image quality analyzer,” IEEE Signal Processing Letters, vol. 20, no. 3, pp. 209–212, 2013.   
[200] J. Ke, Q. Wang, Y. Wang, P. Milanfar, and F. Yang, “MUSIQ: Multiscale image quality transformer,” in ICCV, 2021, pp. 5128–5137.   
[201] H. Gao, X. Tao, X. Shen, and J. Jia, “Dynamic scene deblurring with parameter selective sharing and nested skip connections,” in CVPR, 2019, pp. 3843–3851.   
[202] H. Shen, Z. Zhao, and W. Zhang, “Adaptive dynamic filtering network for image denoising,” CoRR, vol. abs/2211.12051, 2022.   
[203] Y. Song, Z. He, H. Qian, and X. Du, “Vision transformers for single image dehazing,” IEEE Trans. Image Process., vol. 32, pp. 1927–1941, 2022.   
[204] X. Chen, H. Li, M. Li, and J. shan Pan, “Learning a sparse transformer network for effective image deraining,” in CVPR, 2023, pp. 5896–5905.   
[205] S. W. Zamir, A. Arora, S. H. Khan, M. Hayat, F. S. Khan, M.-H. Yang, and L. Shao, “Multi-stage progressive image restoration,” in CVPR, 2021, pp. 14 816–14 826.   
[206] L. Chen, X. Chu, X. Zhang, and J. Sun, “Simple baselines for image restoration,” in ECCV, 2022.   
[207] Y. Cui, W. Ren, X. Cao, and A. Knoll, “Image restoration via frequency selection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, pp. 1093– 1108, 2023.   
[208] H. Guo, J. Li, T. Dai, Z. Ouyang, X. Ren, and S.-T. Xia, “MambaIR: A simple baseline for image restoration with state-space model,” in ECCV, 2024.   
[209] Z. Shi, T. Su, P. Liu, Y. Wu, L. Zhang, and M. Wang, “Learning frequency-aware dynamic transformers for all-in-one image restoration,” CoRR, vol. abs/2407.01636, 2024.   
[210] H. Duan, X. Min, S. Wu, W. Shen, and G. Zhai, “UniProcessor: A text-induced unified low-level image processor,” CoRR, vol.   
[211] Z. Chen, Y. Zhang, L. Ding, X. Bin, J. Gu, L. Kong, and X. Yuan, “Hierarchical integration diffusion model for realistic image deblurring,” in NeurIPS, 2023.   
[212] Y. Cai, H. Bian, J. Lin, H. Wang, R. Timofte, and Y. Zhang, “Retinexformer: One-stage retinex-based transformer for low-light image enhancement,” in ICCV, 2023.   
[213] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang, and L. Shao, “Learning enriched features for fast image restoration and enhancement,” IEEE Trans. Pattern Anal. Mach. Intell., 2022.   
[214] C. Mou, Q. Wang, and J. Zhang, “Deep generalized unfolding networks for image restoration,” in CVPR, 2022.   
[215] J. Zhang, J. Huang, M. Zhou, C. Li, and F. Zhao, “Decomposition ascribed synergistic learning for unified image restoration,” CoRR, vol. abs/2308.00759, 2023.   
[216] Y. Li, Y. Fan, X. Xiang, D. Demandolx, R. Ranjan, R. Timofte, and L. V. Gool, “Efficient and explicit modelling of image hierarchies for image restoration,” in CVPR, 2023, pp. 18 278–18 289.   
[217] W. Zhang, X. Li, G. Shi, X. Chen, Y. Qiao, X. Zhang, X.-M. Wu, and C. Dong, “Real-world image super-resolution as multi-task learning,” in NeurIPS, vol. 36, 2024.   
[218] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn, “Gradient surgery for multi-task learning,” in NeurIPS, vol. 33, 2020, pp. 5824–5836.   
[219] B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu, “Conflict-averse gradient descent for multi-task learning,” in NeurIPS, vol. 34, 2021, pp. 18 878– 18 890.   
[220] Y. Poirier-Ginter and J.-F. Lalonde, “Robust unsupervised stylegan image restoration,” in CVPR, 2023, pp. 22 292–22 301.   
[221] L. Xie, C. Zheng, W. Xue, L. Jiang, C. Liu, S. Wu, and H. S. Wong, “Learning degradation-unaware representation with prior-based latent transformations for blind face restoration,” in CVPR, 2024, pp. 9120– 9129.   
[222] P. N. Michelini, Y. Lu, and X. Jiang, “edge-SR: super-resolution for the masses,” in WACV, 2022, pp. 1078–1087.   
[223] X. Zhang, H. Zeng, and L. Zhang, “Edge-oriented convolution block for real-time super resolution on mobile devices,” in ACM MM, 2021, pp. 4034–4043.   
[224] Y. Li, K. Zhang, J. Liang, J. Cao, C. Liu, R. Gong, Y. Zhang, H. Tang, Y. Liu, D. Demandolx et al., “LSDIR: A large scale dataset for image restoration,” in CVPR, 2023, pp. 1775–1787.   
[225] F. Yu, J. Gu, Z. Li, J. Hu, X. Kong, X. Wang, J. He, Y. Qiao, and C. Dong, “Scaling up to excellence: Practicing model scaling for photorealistic image restoration in the wild,” in CVPR, 2024, pp. 25 669– 25 680.   
[226] H. Wu, Z. Zhang et al., “Q-Instruct: Improving low-level visual abilities for multi-modality foundation models,” in CVPR, 2024, pp. 25 490– 25 500.   
[227] X. Jin, Y. Shi, B. Xia, and W. Yang, “LLMRA: Multi-modal large language model based restoration assistant,” arXiv, 2024.   
[228] X. Xu, S. Kong, T. Hu, Z. Liu, and H. Bao, “Boosting image restoration via priors from pre-trained models,” in CVPR, 2024, pp. 2900–2909.   
[229] B. Zheng, J. Gu, S. Li, and C. Dong, “LM4LV: A frozen large language model for low-level vision tasks,” arXiv preprint arXiv:2405.15734, 2024.   
[230] X. Deng and P. L. Dragotti, “Deep convolutional neural network for multi-modal image restoration and fusion,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 10, pp. 3333–3348, 2020.   
[231] Y. Zhang, C. Peng, Q. Wang, D. Song, K. Li, and S. K. Zhou, “Unified multi-modal image synthesis for missing modality imputation,” IEEE Trans. Med. Imaging, 2024.   
[232] Z. Zhao, H. Bai, J. Zhang, Y. Zhang, K. Zhang, S. Xu, D. Chen, R. Timofte, and L. Van Gool, “Equivariant multi-modality image fusion,” in CVPR, 2024, pp. 25 912–25 921.   
[233] J. Ma, Y. Ma, and C. Li, “Infrared and visible image fusion methods and applications: A survey,” INFORM FUSION, vol. 45, pp. 153–178, 2019.   
[234] J. Liang, J. Cao, Y. Fan, K. Zhang, R. Ranjan, Y. Li, R. Timofte, and L. Van Gool, “VRT: A video restoration transformer,” IEEE Trans. Image Process., 2024.   
[235] K. Zhou, W. Li, Y. Wang, T. Hu, N. Jiang, X. Han, and J. Lu, “NeRFLix: High-quality neural view synthesis by learning a degradationdriven inter-viewpoint mixer,” in CVPR, 2023, pp. 12 363–12 374.   
[236] D. Li, K. Ma, J. Wang, and G. Li, “Hierarchical prior-based super resolution for point cloud geometry compression,” IEEE Trans. Image Process., 2024.   
[237] L. Wang, T.-K. Kim, and K.-J. Yoon, “Eventsr: From asynchronous events to image reconstruction, restoration, and super-resolution via end-to-end adversarial learning,” in CVPR, 2020, pp. 8315–8325.   
[238] Q. Liang, X. Zheng, K. Huang, Y. Zhang, J. Chen, and Y. Tian, “Event-diffusion: Event-based image reconstruction and restoration with diffusion models,” in ACM MM, 2023, pp. 3837–3846.   
[239] L. Pang, X. Rui, L. Cui, H. Wang, D. Meng, and X. Cao, “HIR-Diff: Unsupervised hyperspectral image restoration via improved diffusion models,” in CVPR, 2024, pp. 3005–3014.  