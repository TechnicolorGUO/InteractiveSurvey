# 5/1/2025, 6:02:24 PM_All-in-One Image Restoration  

# 0. All-in-One Image Restoration  

# 1. Introduction  

Image restoration is a fundamental task in image processing and computer vision, aiming to recover a high-quality, uncorrupted image from its degraded observation [5,6,16,18,19,20,28,30]. Images captured in real-world scenarios frequently suffer from various forms of degradation, including noise, blur (motion, defocus, atmospheric turbulence) [16,28,30], rain, haze [16,21,25], low light [4,16], and geometrical distortions, caused by factors such as camera sensor limitations, atmospheric conditions, relative motion, or optical system imperfections [21,28,30]. The presence of these imperfections decreases image accuracy and detail, significantly impacting subsequent image processing tasks and limiting the effectiveness of applications across diverse fields, including autonomous driving, medical imaging, surveillance, remote sensing, computational photography, and digital media restoration [6,19,25,27,28]. It is important to distinguish image restoration from image enhancement; while enhancement subjectively improves visual quality, restoration is a more objective process that attempts to reverse known or estimated degradation processes to recover the original scene [4,28].  

Historically, image restoration primarily relied on traditional signal processing techniques based on mathematical and probabilistic models to solve inverse problems, often assuming the degradation model and noise characteristics were known apriori[18,30]. These methods typically involved iterative algorithms based on maximum likelihood or Bayesian approaches [18]. Techniques like blind image deconvolution attempted to estimate the degradation attributes directly from the degraded image before restoration [30]. However, traditional methods were often tailored to specific degradation types or required explicit knowledge of the degradation process [17,30].  

![](images/d1dae1834a2c62d693c42d2f66d625bd41d09869793652290c740d239956c5aa.jpg)  

The field has undergone a significant transformation with the advent of machine learning and deep learning [18,19,20]. Deep learning models, particularly Convolutional Neural Networks (CNNs), demonstrated remarkable capabilities in learning generalizable image priors from large datasets, leading to substantial performance improvements across various restoration tasks like denoising, deblurring, and super-resolution [12,18,19]. Base architectures like ResNet became fundamental building blocks for deep learning-based restoration methods [18]. More recently, Transformer models, leveraging self-attention mechanisms to capture long-range dependencies, have emerged as powerful alternatives, often surpassing CNNs by mitigating their limitations such as limited receptive fields and input-content unawareness [3,12].  

Despite the successes of deep learning, existing single-task restoration methods, whether traditional or deep learningbased, primarily focus on addressing specific degradation types or levels [1,17,25]. In realistic scenarios, however, images often suffer from multiple simultaneous degradations or unknown degradation types and levels that can vary dynamically [1,6,10,21,25]. Relying on separate models for each degradation type necessitates identifying the degradation beforehand and incurs significant computational costs, storage overhead, and a lack of flexibility [1,16,25].​  

This challenge has motivated the development of "All-in-One Image Restoration" solutions [5,6,10,16,21]. All-in-one image restoration aims to create a single, unified model capable of handling diverse degradation types and levels without requiring prior knowledge or manual specification of the degradation [1,25]. This paradigm offers significant advantages, including improved flexibility, reduced computational costs compared to managing multiple task-specific models, and enhanced efficiency, especially in complex environments where degradations are unknown or mixed [1,6,10,16,25]. Representative works like AirNet demonstrate the potential of this approach by learning degradation representations from observed images to guide the restoration process [6,25]. Other directions explore utilizing human instructions or prompts to guide universal restoration models, enhancing interactivity and flexibility [5,16].​  

Despite the promise, developing effective all-in-one solutions presents several challenges and research gaps. A critical challenge lies in balancing restoration quality across diverse degradation types with computational efficiency and generalization ability to unseen degradations [1,5,16]. Existing methods based on CNNs often suffer from limited receptive fields and content-unawareness, struggling to capture global context necessary for effective restoration [3,12,19]. While Transformers address these limitations by capturing long-range dependencies, they face challenges with computational complexity for high-resolution images and can introduce boundary artifacts when processing images in patches [3,12]. Furthermore, effectively distinguishing authentic image content from various types of degradation without prior knowledge remains a key hurdle [17]. Recent research explores new avenues, such as leveraging frequency-domain characteristics that are distinct for different degradations [9,10] or employing dynamic network architectures [1], to improve all-in-one capabilities. The field is also advancing through innovative architectural designs, loss functions, training strategies, and the creation of large-scale datasets tailored for diverse degradations [1,18,20]. The increasing prominence of AI-driven methods, including generative models like diffusion models [11], holds significant promise for addressing the complexities of all-inone restoration.​  

This survey provides a comprehensive overview of the advancements in all-in-one image restoration. We will detail the evolution from traditional single-task methods to modern deep learning approaches, emphasizing the shift towards unified models that handle multiple degradations simultaneously. The survey will categorize and analyze prominent all-in-one architectures and methodologies, discuss benchmark datasets and evaluation metrics, highlight current challenges and research gaps, and outline future directions in this rapidly evolving field. Our contribution lies in synthesizing the state-ofthe-art in all-in-one image restoration, providing a structured analysis of existing techniques, and identifying key areas for future investigation.​  

# 2. Background: Image Degradation Models and Traditional Techniques  

Image restoration is fundamentally the process of recovering an original, high-quality image from a degraded observation. This degradation can manifest in various forms, commonly modeled as a transformation of the original image f(x,y) into a degraded image $\displaystyle \mathbf { g } ( \mathsf { x } , \mathsf { y } )$ . A widely adopted model for this process involves a degradation function (often a linear, positioninvariant process represented by a convolution kernel or Point Spread Function, PSF) and an additive noise term [26,28]. Mathematically, this relationship in the spatial domain is expressed as:​  

$$
g ( x , y ) = f ( x , y ) * h ( x , y ) + \eta ( x , y )
$$  

where $\mathfrak { h } ( \mathsf { x } , \mathsf { y } )$ is the spatial representation of the degradation function (PSF) and \eta(x,y) represents the additive noise [26,28]. The equivalent representation in the frequency domain is:  

$$
G ( u , v ) = H ( u , v ) F ( u , v ) + N ( u , v )
$$  

where $\mathsf { F } ( \mathsf { u } , \mathsf { v } ) , \mathsf { H } ( \mathsf { u } , \mathsf { v } ) , \mathsf { G } ( \mathsf { u } , \mathsf { v } ) .$ , and $\mathsf { N } ( \mathsf { u } , \mathsf { v } )$ are the Fourier Transforms of $\mathsf { f } ( \mathsf { x } , \mathsf { y } ) , \mathsf { h } ( \mathsf { x } , \mathsf { y } ) , \mathsf { g } ( \mathsf { x } , \mathsf { y } )$ , and $\scriptstyle \mathtt { \backslash e t a ( x , y ) }$ , respectively [26]. While multiplicative noise exists, it can often be addressed by transforming it into additive noise via logarithmic operations, simplifying the focus to additive noise characteristics [26]. The objective of image restoration is to obtain an estimate \hat{f} $( \boldsymbol { \mathsf { x } } , \boldsymbol { \mathsf { y } } )$ of the original image $\mathsf { f } ( \mathsf { x } , \mathsf { y } )$ , given the degraded image $\displaystyle \boldsymbol { \mathrm { g } } ( \boldsymbol { \mathrm { x } } , \boldsymbol { \mathrm { y } } )$ and some understanding or estimation of the degradation function $\mathfrak { h } ( \mathsf { x } , \mathsf { y } )$ and the noise \eta $( \mathsf { x } , \mathsf { y } )$ [26].  

Image degradation models are typically classified into categories such as noise and blur [5]. Noise represents undesirable random variations in pixel values, characterized by its statistical properties, particularly its Probability Density Function (PDF), mean, and variance [26]. Different noise types, including Gaussian, Rayleigh, Erlang, Exponential, Uniform, and Impulse noise, exhibit distinct PDFs and statistical parameters, arising from various sources like electronic interference, sensor limitations, or transmission errors [26]. Understanding the specific noise model is crucial as it dictates the appropriate restoration approach.​  

Blur, on the other hand, represents a loss of image detail due to factors like camera motion or optical imperfections [5]. It is modeled by the Point Spread Function (PSF), which describes the spatial extent of the degradation caused by a point source [28,30]. Various physical causes, such as linear motion, out-of-focus optics, and atmospheric turbulence, result in distinct blur types, each characterized by a unique PSF shape and mathematical formulation [30]. For instance, uniform linear motion creates a streak-like PSF, uniform out-of-focus blur results in a disk-shaped PSF, and atmospheric turbulence often yields a Gaussian PSF [30]. The parameters within these mathematical models (e.g., motion length/direction, circle of confusion radius, turbulence spread) directly influence the severity and appearance of the blur [30]. Degradations relevant to All-in-One (AIO) restoration scenarios extend beyond noise and blur to include effects like rain, dehazing, and low-light conditions [5]. The presence of multiple degradation types simultaneously in a single image presents significant complexity, as different degradations may require distinct processing methods [5].​  

Historically, image restoration has been approached through various traditional techniques [13,28]. These methods can be broadly categorized into filtering-based, transformation-based, statistical, and other image processing techniques [13]. Filtering-based methods operate directly on pixel values in the spatial domain or modify frequency components in the frequency domain [13]. Frequency domain filters like the Inverse Filter aim to reverse the blur operation but are highly sensitive to noise [30]. More robust approaches include Least-Squares Filters like the Wiener filter, which minimizes meansquared error by balancing deblurring and noise suppression [30].​  

Transformation-based methods leverage mathematical transforms like the Fast Fourier Transform (FFT), Wavelet Transform, or Surfacelet Transform to process the image in a domain where degradations might be more easily isolated or suppressed [9,13]. FFT-based methods are efficient for global degradations but can introduce artifacts, while multi-resolution transforms like wavelets offer better spatial localization and detail preservation [13].  

Statistical methods utilize inherent statistical properties of images or degradations. A notable example is the Dark Channel Prior (DCP), which has proven effective in image dehazing by exploiting the observation that dark pixels exist in most hazefree image patches [18].  

While traditional methods have made significant contributions, they often focus on specific degradation types and lack unified processing capabilities for multiple degradations [5]. Their limitations include sensitivity to noise, inability to handle complex or spatially varying degradations, and often the requirement of prior knowledge about the degradation process (e.g., the blur kernel) [17]. The challenge is further compounded in blindimage deconvolution, where the blur kernel is unknown and must be estimated directly from the degraded image through a process called blur identification before restoration can proceed [30]. This estimation step adds significant complexity compared to non-blind scenarios. Understanding these degradation models and the strengths and limitations of traditional techniques provides essential context for appreciating the need for and development of more advanced, unified image restoration approaches.  

# 2.1 Noise Models  

Noise is an undesirable degradation that corrupts image data, and its characteristics are fundamentally described by statistical models, specifically their probability density functions (PDFs) [26]. The spatial descriptor of noise is the statistical behavior of the intensity values of the noise component, which is characterized by its PDF [26].  

<html><body><table><tr><td>Noise Type</td><td>Probability Density Function (PDF)</td><td>Characteristics</td><td>Source /Example</td></tr><tr><td>Gaussian</td><td>exp 2g2 (2-2)2、</td><td>diymutin, continuous values</td><td>clemponts, thermal noise</td></tr><tr><td>Rayleigh</td><td>Asymmetric, depends on a,b</td><td>Asymmetric distribution, continuous values</td><td>Range imaging (ultrasound, radar)</td></tr><tr><td>Erlang/Gamma</td><td>abzb-1 for z ≥0 (b-1) exp(-az)</td><td>Skewed distribution, continuous values</td><td>Various statistical processes</td></tr><tr><td>Exponential</td><td>ae-az for z≥0 (Erlang with b= 1)</td><td>Simple skewed distribution, continuous values</td><td>Specific imaging contexts</td></tr><tr><td></td><td></td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td>Uniform Impulse</td><td>1 for a≤z≤b b-a</td><td>Constant density over a range, continuous values</td><td>Quantization error</td></tr><tr><td></td><td>Discrete values a, b with probabilities Pa,Pb</td><td>Sparse,replaces original values with impulses</td><td>Faulty sensors, transmission errors</td></tr></table></body></html>  

Various noise models exist, each with distinct statistical properties, influencing how noise appears in images and how it is subsequently addressed by restoration algorithms [26].  

One of the most common and mathematically tractable noise models is Gaussian noise, also known as normal noise [26]. It is often used to model noise arising from electronic components and sensor thermal noise. Its PDF is given by:  

$$
p ( z ) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { ( z - \bar { z } ) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)
$$  

where $z$ is the intensity value, $\bar { z }$ is the mean, and $\sigma ^ { 2 }$ is the variance [26]. The mean $\bar { z }$ determines the central tendency of the noise values, while the variance $\sigma ^ { 2 }$ dictates the spread of the distribution. A higher variance results in noise values that are more widely scattered around the mean, leading to a more visually disruptive noise pattern. Due to its prevalence and mathematical tractability, many image restoration algorithms, such as the Wiener filter, are designed under the assumption of additive Gaussian noise.  

Rayleigh noise exhibits an asymmetric PDF, defined as:  

$$
p ( z ) = { \left\{ \begin{array} { l l } { { \frac { 2 } { b } } \left( z - a \right) \exp \left( - { \frac { ( z - a ) ^ { 2 } } { b } } \right) } & { { \mathrm { f o r ~ } } z \geq a } \\ { 0 } & { { \mathrm { f o r ~ } } z < a } \end{array} \right. }
$$  

Its mean and variance are given by  

$$
\bar { z } = a + \sqrt { \frac { \pi b } { 4 } } , \quad \sigma ^ { 2 } = \frac { b ( 4 - \pi ) } { 4 }
$$  

respectively [26]. This model can be relevant in range imaging, such as in ultrasound or radar imaging. The parameter $a$ determines the minimum value of the noise, and $b$ influences the shape and spread of the distribution. Restoration techniques tailored for asymmetric noise distributions may be required, differing from those optimized for symmetric Gaussian noise.​  

Erlang noise, also known as Gamma noise, has a PDF given by:  

$$
p ( z ) = { \left\{ \begin{array} { l l } { { \frac { a ^ { b } z ^ { b - 1 } } { ( b - 1 ) ! } } \exp ( - a z ) } & { { \mathrm { f o r ~ } } z \geq 0 } \\ { 0 } & { { \mathrm { f o r ~ } } z < 0 } \end{array} \right. }
$$  

for $z \geq 0$ [26]. Its mean is  

$$
\bar { z } = \frac { b } { a }
$$  

and its variance is  

$$
\sigma ^ { 2 } = { \frac { b } { a ^ { 2 } } }
$$  

[26]. This model arises in various statistical processes and can sometimes approximate noise in specific imaging contexts, potentially including multiplicative noise effects after logarithmic transformation. The parameters $a$ and $b$ control the shape and scale of the distribution.  

A special case of Erlang noise where $b = 1$ is Exponential noise, with the PDF:  

$$
p ( z ) = { \left\{ \begin{array} { l l } { a e ^ { - a z } } & { { \mathrm { f o r ~ } } z \geq 0 } \\ { 0 } & { { \mathrm { f o r ~ } } z < 0 } \end{array} \right. }
$$  

where $a > 0$ [26]. Its mean is  

$$
\bar { z } = \frac { 1 } { a }
$$  

and variance is  

$$
\sigma ^ { 2 } = \frac { 1 } { a ^ { 2 } }
$$  

[26]. This model is simpler due to a single parameter $a$ determining both the mean and variance. Like Erlang and Rayleigh noise, its asymmetric nature distinguishes it from Gaussian noise.  

Uniform noise has a constant PDF over a specified range:  

$$
p ( z ) = { \left\{ \begin{array} { l l } { { \frac { 1 } { b - a } } } & { { \mathrm { f o r ~ } } a \leq z \leq b } \\ { 0 } & { { \mathrm { o t h e r w i s e } } } \end{array} \right. }
$$  

Its mean is  

$$
{ \bar { z } } = { \frac { a + b } { 2 } }
$$  

and variance is  

$$
\sigma ^ { 2 } = \frac { ( b - a ) ^ { 2 } } { 1 2 }
$$  

[26]. This model is often used to represent quantization error. The parameters ​a and $b$ define the minimum and maximum values of the noise. Uniform noise, like Gaussian noise, is additive but its distinct distribution requires different considerations for optimal filtering compared to peaked distributions.  

In contrast to the additive noise models above, Impulse noise, such as Salt-and-Pepper noise, is characterized by discrete values appearing randomly in the image [26]. Its PDF is given by:  

$$
p ( z ) = { \left\{ \begin{array} { l l } { P _ { a } } & { { \mathrm { f o r ~ } } z = a } \\ { P _ { b } } & { { \mathrm { f o r ~ } } z = b } \\ { 0 } & { { \mathrm { o t h e r w i s e } } } \end{array} \right. }
$$  

Here, $a$ and $b$ are specific intensity values (e.g., 0 for pepper, 255 for salt in 8-bit images), and $P _ { a }$ ​ and $P _ { b }$ ​ are the probabilities of occurrence for each value, with $P _ { a } + P _ { b } \leq 1$ . This type of noise typically arises from faulty sensors, memory cell defects, or errors in data transmission. Unlike additive noise which affects most pixels to varying degrees, impulse noise affects only a subset of pixels, replacing their original values with impulse values. The parameters $( a , b , P _ { a } , P _ { b } )$ directly control the intensity values and density of the corrupted pixels. Image restoration algorithms designed for additive noise are often ineffective against impulse noise; instead, non-linear filters like the median filter, which are robust to outliers, are commonly employed.​  

In summary, the choice of noise model is crucial for understanding image degradation and selecting appropriate restoration techniques. Gaussian and Uniform noise are typically additive with continuous distributions, often arising from electronic processes or quantization. Rayleigh, Erlang, and Exponential noise are also continuous but possess asymmetric distributions, found in specific imaging modalities. Impulse noise, conversely, is a non-additive, discrete model representing localized severe corruption. The parameters of each model directly influence the visual characteristics of the noise and dictate the underlying statistical assumptions necessary for effective image restoration [26]. Different noise models necessitate distinct algorithmic approaches, ranging from linear filters for additive noise to robust non-linear methods for impulse noise, highlighting the importance of accurate noise characterization in image processing.  

# 2.2 Blur Models  

Image blur represents a common form of degradation, resulting from imperfections in the image formation process. This degradation is typically modeled as a convolution between the latent sharp image and a Point Spread Function (PSF), denoted as $d ( x , y )$ . The PSF encapsulates the spatial extent of the blur effect caused by a point source. Physically, the PSF is nonnegative and real-valued, modeling the passive nature of optical systems that neither absorb nor generate energy [30]. In the ideal scenario of a perfectly sharp image with no blur, the PSF is represented by a unit pulse [30].  

<html><body><table><tr><td>Blur Type</td><td>Cause</td><td>PSF Shape (Idealized)</td><td>Key Parameters</td></tr><tr><td>No Blur</td><td>Ideal imaging system</td><td>Unit Pulse</td><td>N/A</td></tr><tr><td>Linear Motion</td><td></td><td>Line Segment</td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Relative movement during exposure</td><td></td><td>Motion length, direction</td></tr><tr><td>Uniform Out-of- Focus</td><td>Subject outside focal plane, circular aperture</td><td>Disk</td><td>Circle of Confusion (COC) radius(R)</td></tr><tr><td>Atmospheric Turbulence</td><td>Refractive index variations in atmosphere</td><td>Gaussian</td><td>Gaussian spread parameter(σG）</td></tr></table></body></html>  

Various physical phenomena contribute to different types of blur, each characterized by a distinct PSF. One prevalent cause is Linear Motion Blur, arising from relative movement between the camera and the scene during exposure [30]. This motion can manifest as translation, rotation, changes in scale, or combinations thereof [30]. Among these, uniform motion in a straight line is the most common, while other complex motions can often be approximated as piecewise uniform linear motion [30]. The appearance of motion blur is dictated by parameters such as the direction and length of motion; longer motion streaks result in more severe blurring.  

Another significant blur type is Uniform Out-of-Focus Blur, which occurs when imaging a three-dimensional scene onto a two-dimensional plane where not all parts are in focus [30]. For a circular aperture, a point source outside the focal plane is rendered as a small disk, known as the circle of confusion (COC) [30]. When the degree of defocusing is substantial compared to the wavelengths of light, a geometrical optics approximation can be used, resulting in a uniform intensity distribution within the COC [30]. This gives rise to the uniform out-of-focus blur. The diameter (and thus radius $R$ ) of the COC is a critical parameter, determined by factors including the lens diameter $( D )$ , image distance $\left( v _ { 0 } \right)$ , focal length $( f )$ , and object distance $( u )$ [30]. A larger COC diameter $R$ corresponds to greater defocus blur. The space-continuous PSF for this blur model is given by:​  

$$
d ( x _ { 1 } , x _ { 2 } ) = \left\{ \begin{array} { l l } { \frac { 1 } { \pi R ^ { 2 } } } & { \mathrm { i f ~ } x _ { 1 } ^ { 2 } + x _ { 2 } ^ { 2 } \leq R ^ { 2 } , } \\ { 0 } & { \mathrm { o t h e r w i s e } . } \end{array} \right.
$$  

While the continuous form is well-defined, deriving a precise discrete version $d ( n _ { 1 } , n _ { 2 } )$ for digital image processing is not straightforward [30]. A common approximation utilizes a discrete PSF that is constant within a circular region:  

$$
d ( n _ { 1 } , n _ { 2 } ) = { \left\{ \begin{array} { l l } { C } & { { \mathrm { i f ~ } } n _ { 1 } ^ { 2 } + n _ { 2 } ^ { 2 } \leq R ^ { 2 } , } \\ { 0 } & { { \mathrm { o t h e r w i s e } } . } \end{array} \right. }
$$  

where $C$ is a normalization constant ensuring the PSF sums to 1 [30].  

Atmospheric Turbulence Blur is caused by variations in the refractive index of the atmosphere, leading to a spatially varying distortion of light propagation. This type of blur is often modeled using a Gaussian PSF [30]:  

$$
d ( x _ { 1 } , x _ { 2 } ) = C \cdot \exp \left( - \frac { x _ { 1 } ^ { 2 } + x _ { 2 } ^ { 2 } } { 2 \sigma _ { G } ^ { 2 } } \right)
$$  

Here, the parameter $\sigma _ { G }$ ​ determines the spread of the Gaussian function and, consequently, the extent of the blur [30]. A larger $\sigma _ { G }$ ​ signifies stronger turbulence and a wider, more severe blur. $C$ is again a normalization constant [30].  

Comparing these models, the NoBlur PSF is a sharp spike, while Linear Motion Blur creates a streak (often approximated as a line segment PSF for uniform motion). Uniform Out-of-Focus blur results in a disk-shaped PSF, and Atmospheric Turbulence blur is characterized by a radially symmetric Gaussian PSF [30]. The parameters (motion length/direction, COC radius $R$ influenced by lens/distance parameters, turbulence spread $\sigma _ { G }$ ​ ) directly control the size, shape, and intensity distribution of the PSF, dictating the visual appearance and severity of the blur in the degraded image.  

Restoring images degraded by these different blur models presents distinct challenges. Blind deconvolution, where the blur kernel (PSF) is unknown, is inherently more difficult than non-blind deconvolution. Estimating the PSF for complex motions, spatially varying blur (common in out-of-focus and turbulence), or the challenging discrete approximation of the uniform out-of-focus PSF adds significant complexity [30]. Robust restoration algorithms must be able to handle various PSF shapes and sizes, often requiring accurate blur estimation or powerful models capable of implicitly learning the blur process [30].  

# 2.3 Filtering-based Methods  

Filtering-based techniques represent a classical approach to image restoration, aiming to recover an uncorrupted image from its degraded observation by applying various filters [13,28]. These methods primarily operate either in the spatial domain or the frequency domain, with extensions including decision-based and hybrid approaches [13]. The efficacy of restoration, particularly in handling noise, is often measured by metrics such as the signal-to-noise ratio (SNR) improvement [30].​  

Spatial domain filtering involves directly manipulating the pixel values of an image, typically through convolution with a kernel or mask [13]. This class of filters is particularly effective when the primary degradation is additive noise [26]. A common objective in spatial filtering for noise reduction is to estimate the original image \( \hat{f} $\langle \mathsf { x } , \mathsf { y } \rangle \backslash )$ such that the squared difference $\backslash ( | \backslash \mathsf { h a t } \{ \mathsf { f } \} ( \mathsf { x } , \mathsf { y } )  \mathsf { - g } \} ( \mathsf { x } , \mathsf { y } ) | { \mathsf { \Omega } } 2 \setminus )$ between the estimate and the observed degraded image $\backslash ( \operatorname { g } ( \mathsf { x } , \mathsf { y } ) \backslash )$ is minimized in a manner consistent with the known noise distribution [26]. Examples include linear filters like the Gaussian filter, often used for smoothing and noise suppression [13]. Advantages include computational simplicity and direct interpretability of filter operations. However, basic spatial filters may blur edges while reducing noise, and their effectiveness in deblurring scenarios is limited compared to frequency domain methods.  

Frequency domain filtering operates by transforming the image into the frequency domain, applying a filter, and then transforming the result back to the spatial domain [13]. This approach is well-suited for handling degradations characterized by specific frequency responses, such as blur introduced by convolution with a point spread function (PSF). The Inverse Filter is a fundamental frequency domain technique that attempts to recover the original image by directly inverting the degradation operator $\backslash ( \mathsf { D } ( \mathsf { u } , \mathsf { v } ) \backslash )$ in the frequency domain [30]. In the theoretical absence of noise, the inverse filter can perfectly restore the original image [30]. However, its major drawback lies in its extreme sensitivity to noise. If the degradation function $\backslash ( \mathsf { D } ( \mathsf { u } , \mathsf { v } ) \backslash )$ is zero or very small at certain frequencies, direct inversion amplifies any noise present at those frequencies, leading to severe artifacts in the restored image [30]. This issue is particularly pronounced for common degradations like linear motion blur and out-of-focus blur, where $\backslash ( \mathsf { D } ( \mathsf { u } , \mathsf { v } ) \backslash )$ can have zeros [30].  

To overcome the noise sensitivity of the inverse filter, Least-Squares Filters were developed [30]. The Wiener filter is a notable example, formulated as a linear, spatially invariant filter that minimizes the mean-squared error (MSE) between the ideal and the restored image. Its frequency domain representation is given by:​  

$$
H ( u , v ) = \frac { D ^ { * } ( u , v ) } { | D ( u , v ) | ^ { 2 } + \frac { S _ { w } ( u , v ) } { S _ { f } ( u , v ) } } = \frac { D ^ { * } ( u , v ) } { | D ( u , v ) | ^ { 2 } + \gamma }
$$  

where $\backslash ( \mathsf { D } ^ { \wedge \star } ( \mathsf { u } , \mathsf { v } ) \backslash )$ is the complex conjugate of the degradation function, $\backslash ( \mathsf { S } _ { - } \mathsf { f } ( \mathsf { u } , \mathsf { v } ) \backslash )$ and $\backslash ( \mathsf { S } _ { - } \mathsf { w } ( \mathsf { u } , \mathsf { v } ) \backslash )$ are the power spectra of the ideal image and noise, respectively, and \( \gamma \) is a constant approximation for the ratio of noise to signal power spectra when the true spectra are unknown [30]. The Wiener filter effectively balances deblurring and noise suppression by attenuating frequencies where noise is dominant relative to the signal. In the noiseless case $( \backslash ( \mathsf { S } _ { - } \mathsf { w } ( \mathsf { u } , \mathsf { v } ) = 0$ $\backslash \backslash$ ), the Wiener filter simplifies to the inverse filter [30]. The Constrained Least-Squares Filter is another approach that regularizes the restoration by seeking a solution that is acceptably “smooth” while ensuring that the difference between the blurred estimate of the restored image and the observed degraded image aligns with the characteristics of the noise [30]. Unlike the Wiener filter which directly minimizes MSE, the constrained approach focuses on satisfying a constraint related to the noise energy, offering an alternative way to handle noise during deblurring [30].  

Beyond traditional spatial and frequency domain methods, Decision Filters and Hybrid Filters represent more sophisticated filtering strategies [13]. Decision filters often employ data-dependent processing, sometimes incorporating techniques like clustering (e.g., k-means) or dimensionality reduction (e.g., PCA) to adapt the filtering process based on local image characteristics [13]. Hybrid filters combine the strengths of multiple different filtering techniques to achieve enhanced restoration performance that might not be possible with a single filter type [13]. These methods offer increased flexibility and potential for better performance on complex or mixed degradations but can be more complex in design and analysis. Overall, filtering-based methods provide a foundation for image restoration, with frequency domain approaches like Wiener filtering offering robust solutions for deblurring with noise, while spatial and adaptive filters are valuable for noise reduction and localized processing. The effectiveness of each technique depends significantly on the specific type and characteristics of the image degradation [26,30].​  

# 2.4 Transformation-based Methods  

Transformation‐based methods constitute a fundamental class of techniques in image restoration, operating by converting the image from the spatial domain into a transformed domain where degradation effects may be more amenable to analysis and removal. Subsequent processing in the transformed domain is followed by an inverse transformation to reconstruct the restored image in the spatial domain [9,13]. Key examples include the Fast Fourier Transform (FFT), Wavelet Transform, and Surfacelet Transform [13].  

The Fast Fourier Transform (FFT) is a cornerstone method that transforms an image into its frequency components, providing insights into the spatial frequency distribution of the image content and degradation [9,28]. The transformation from the spatial domain $\mathsf { I } ( \mathsf { x } , \mathsf { y } )$ to the frequency domain $\mathsf { F } ( \mathsf { u } , \mathsf { v } )$ is defined by the 2D Discrete Fourier Transform, efficiently computed using the FFT algorithm:​  

$$
F ( u , v ) = \frac { 1 } { \sqrt { H W } } \sum _ { x = 0 } ^ { H - 1 } \sum _ { y = 0 } ^ { W - 1 } I ( x , y ) e ^ { - j 2 \pi \left( \frac { u x } { H } + \frac { v y } { W } \right) }
$$  

where $( \boldsymbol { \mathsf { x } } , \boldsymbol { \mathsf { y } } )$ are spatial coordinates, $( \mathsf { u } , \mathsf { v } )$ are frequency coordinates, and H, W are the image dimensions [9]. Image restoration in the frequency domain typically involves applying a filter to $\mathsf { F } ( \mathsf { u } , \mathsf { v } )$ to suppress degradation components, followed by the inverse Fast Fourier Transform (IFFT) to return to the spatial domain [9,13]. FFT‐based filtering is particularly effective for addressing periodic noise, which manifests as distinct peaks in the frequency spectrum [26]. A primary advantage of FFT is its computational efficiency for large images. However, FFT is a global transform, meaning that operations in the frequency domain affect the entire image. This can lead to artifacts, such as the Gibbs phenomenon near edges, and may not be optimal for spatially varying degradations.​  

In contrast to the global nature of FFT, the Wavelet Transform offers a multi-scale analysis capability [13]. It decomposes an image into different frequency subbands at various resolution levels, providing both frequency and spatial localization information. This multi-scale representation makes the Wavelet Transform particularly suitable for tasks such as denoising, where noise often distributes differently across scales, and for feature extraction [13]. By analyzing signals across different scales, wavelets can effectively capture both smooth regions and sharp transitions (like edges) in the image. The localized nature of wavelets helps in handling spatially localized degradation and preserving image details better than global transforms like FFT, reducing the occurrence of artifacts like ringing.​  

The Surfacelet Transform is another multi-resolution analysis technique specifically designed to capture local image features [13]. Like wavelets, surfacelets provide a multi-scale representation, but they are designed to handle surface-like structures in images, potentially offering advantages in representing and restoring images with intricate textures and contours. Although its mathematical basis or specific applications beyond local feature capture are not detailed here, the description as a multi-resolution transform focusing on local features suggests that it aims to improve upon the limitations of simpler transforms by providing better directional and geometrical localization. This can lead to superior performance in preserving fine details and handling complex degradation patterns compared to both FFT and standard wavelets.​  

Comparing these transformation‐based methods, FFT is computationally efficient and well‐suited for global degradations like periodic noise, but its global nature can limit its effectiveness for localized noise or blurring and may introduce artifacts around edges. The Wavelet Transform offers a balance between frequency and spatial localization through multi-scale analysis, making it more adaptable for denoising and better at preserving details than FFT by processing information locally across scales. The Surfacelet Transform, focusing on local features within a multi-resolution framework, is designed for even better preservation of intricate image structures, potentially excelling in scenarios where detailed textures and contours are critical, though it requires more complex computation than FFT. Ultimately, the choice of transform depends heavily on the nature of the image degradation and the desired trade-off between computational cost and restoration quality, particularly regarding the preservation of image details and management of artifacts.​  

# 2.5 Statistical Methods  

Statistical methods represent a foundational approach in image restoration, leveraging statistical properties inherent in natural images or specific image degradations [13]. A notable example within this category is the Dark Channel Prior (DCP), widely applied, particularly in image dehazing [18]. The underlying statistical assumption of DCP is rooted in the observation that in most outdoor haze-free images, for any patch, at least one color channel (Red, Green, or Blue) contains pixels with very low intensity values, often close to zero. This property is termed the "dark channel" [18].​  

The application of DCP for image restoration, specifically dehazing, involves utilizing this dark channel information to estimate the atmospheric light and the transmission map. The dark channel of an image is typically computed as the minimum intensity across the color channels for each pixel, followed by a minimum filter within a local patch. Based on the dark channel prior, the atmospheric light can be estimated from the brightest pixels in the dark channel. Subsequently, the transmission map, which represents the portion of light that reaches the camera without scattering, is derived. With estimates for the atmospheric light and transmission map, the haze-free image can be recovered using the atmospheric scattering model [13].  

The effectiveness of the Dark Channel Prior has been highlighted in its primary application area, demonstrating notable success in removing haze from images [18]. This is attributed to its ability to provide a robust prior for estimating the scene transmission, which is crucial for inverting the haze formation model. While effective in many scenarios, the core principle relies heavily on the statistical property of the dark channel being sparse and low-valued in haze-free regions.​  

# 2.6 Blind Image Deconvolution  

Blind image deconvolution addresses the challenging problem of restoring images degraded by an unknown blur kernel and noise. A crucial prerequisite for successful restoration in such scenarios is the accurate estimation of the properties of the imperfect imaging system directly from the observed degraded image itself. This process, known as blur identification, is undertaken prior to initiating the actual image restoration procedure [30]. The goal of blur identification is specifically to estimate these attributes of the degradation, laying the foundation for subsequent deconvolution steps aimed at recovering the original image [30].  

# 3. Evolution to Data-Driven Methods: Machine Learning and Deep Learning  

The landscape of image restoration has undergone a significant transformation, moving from traditional model-based and handcrafted methods towards data-driven approaches, primarily driven by advancements in machine learning and deep learning [5,14]. Early data-driven efforts explored various machine learning techniques to model complex degradation processes and inherent image structures [13].  

Within the realm of traditional machine learning, methods like neural networks, Support Vector Machines (SVMs), and genetic algorithms were adapted for image restoration tasks. Neural networks were employed for extracting relevant image features, applicable to problems such as deblurring and denoising [13]. SVMs were utilized to construct classifiers relevant to restoration, including image denoising [13]. Genetic algorithms offered optimization capabilities, aiding in the search for optimal parameters within restoration models [13]. Optimization techniques, such as Total Variation (TV) regularization, were crucial for incorporating prior knowledge, enforcing piecewise smoothness and sparsity in image gradients, particularly in denoising applications, often solved using sparse optimization [13]. These methods relied on learned models but often required hand-designed features or simpler architectures compared to subsequent approaches [5].  

The field witnessed a paradigm shift with the advent of deep learning, which offered significant advantages in capturing intricate image features and dependencies directly from data [13,18,19]. Deep learning models, particularly deep neural networks, possess enhanced capacity to learn complex, hierarchical features and implicit image priors from large datasets, often surpassing the capabilities of traditional machine learning models [18,19]. This allows them to learn end-to-end mappings from degraded to clean images, demonstrating stronger expressive power and the ability to handle complex nonlinear relationships [5,18].​  

Key deep learning architectures have emerged as central to modern image restoration. Convolutional Neural Networks (CNNs) became foundational due to their efficacy in extracting hierarchical, spatially invariant features through convolutional and pooling layers [13,18]. CNNs are widely used to predict pixel values based on local neighborhoods and can be structured within encoder-decoder frameworks like UNet to process information at multiple scales and preserve detail through skip connections [7]. CNNs have been applied successfully in various tasks, including low-light image restoration following Retinex theory principles [4] and old photo restoration [29]. However, a limitation of standard CNNs is their restricted receptive field, which challenges their ability to model long-range dependencies crucial for certain degradations [12].​  

Addressing this limitation, Transformer models, leveraging self-attention mechanisms, gained prominence for their ability to capture global context and long-range dependencies across the image [12,13,14]. Architectures like SFHformer integrate Transformers, sometimes within encoder-decoder structures and incorporating techniques like FFT for comprehensive feature extraction [9]. While powerful, applying standard Transformers to high-resolution images poses significant computational challenges due to the quadratic complexity of self-attention [2,12,14]. This has spurred research into more efficient Transformer variants [2,12].  

Encoder-Decoder models represent a prevalent structural framework in deep learning for image restoration, utilized by both CNN and Transformer-based methods [13]. This structure compresses the degraded input into a latent representation via an encoder and reconstructs the image through a decoder, allowing for multi-scale processing and progressive refinement [7,9,13].​  

Generative Adversarial Networks (GANs) have proven particularly effective for tasks demanding high perceptual quality and realistic texture synthesis, such as underwater image enhancement and photo restoration [18,27,29]. GANs employ an adversarial training process between a generator and a discriminator to produce outputs that are indistinguishable from real images [13,27]. However, GAN training can be unstable and computationally expensive [18].​  

While deep learning methods offer significant advantages in expressive power and feature learning, they typically necessitate large annotated datasets and substantial computational resources for training and inference [5]. Furthermore, handling multiple degradation types within a single model remains a challenge, although "All-in-One" models are being explored to address this, sometimes employing techniques like contrastive learning to capture degradation information without explicit parameters [5,6]. The ongoing evolution explores hybrid architectures, unrolled networks, and other techniques to further enhance performance and versatility [1,13].​  

# 3.1 Machine Learning-Based Methods  

Machine learning techniques have been actively explored for addressing various image restoration challenges, offering datariven approaches to model complex degradation processes and underlying image structures [13].  

Within this paradigm, several distinct machine learning methodologies have been applied. Neural networks, particularly those incorporating convolutional layers, are utilized for extracting pertinent image features, which are then leveraged for tasks such as image deblurring and denoising [13].​  

These networks can also be employed within a deep learning framework to learn effective image priors or kernels, serving as implicit regularization terms for ill-posed restoration problems [18].  

Such learned priors are integrated into optimization procedures, often utilizing techniques like Alternating Direction Metho of Multipliers (ADMM) or Half Quadratic Splitting (HQS), to iteratively refine the restored image [18].  

Beyond neural networks, other machine learning models contribute to the field. Support Vector Machines (SVMs), for instance, have been adapted to construct classifiers relevant to image restoration tasks, including image denoising [13].  

Genetic algorithms, which simulate evolutionary processes, provide a means to optimize parameters within image restoration models by searching for configurations that yield improved restoration results [13].  

Optimization techniques play a crucial role in solving the objective functions formulated in many machine learning-based restoration methods. Total Variation (TV) regularization, for example, is a widely used technique to enforce piecewise smoothness and sparsity in the image gradient, particularly effective for image denoising problems. Its application often involves sparse optimization techniques to find the optimal solution [13].  

As previously noted, optimization frameworks like ADMM and HQS are instrumental in integrating complex learned priors from deep learning models into the restoration process [18].  

While traditional machine learning methods like SVM and genetic algorithms offer valuable tools for specific aspects of image restoration, the advent of deep learning techniques—particularly deep neural networks—represents a significant evolution. Deep learning models possess the capacity to learn highly complex and hierarchical features directly from data and to implicitly capture sophisticated image priors [18], which often surpass the capabilities of simpler, more traditional machine learning models that might rely on hand-crafted features or simpler model architectures. This enhanced ability to model intricate image degradations and structures contributes to the superior performance observed with deep learning approaches in many modern image restoration benchmarks.​  

# 3.2 Deep Learning-Based Methods Overview  

<html><body><table><tr><td>Architecture Type</td><td>Core Mechanism</td><td>Key Strengths</td><td>Key Weaknesses</td><td>Common Role in Restoration</td></tr><tr><td>CNNs</td><td></td><td></td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td>Transformers</td><td>layers,local feature extraction</td><td>at local patterns& textures</td><td>receptive field, struggles with global context</td><td>feature extraction, reconstruction</td></tr><tr><td></td><td>global context</td><td>range dependencies,</td><td>y expensive for high-res</td><td>structure, complex</td></tr><tr><td>Encoder- Decoder</td><td>Downsampling Multi-scale (Encoder) + Upsampling (Decoder)</td><td>processing, hierarchical features</td><td>Potential detail loss, semantic gap (basic forms)</td><td>Structural framework for CNN/Transform er models</td></tr><tr><td>GANs</td><td>Generator + Discriminator (Adversarial Training)</td><td>Generates perceptual realism, texture synthesis</td><td>Unstable training, complex,mode collapse risk</td><td>Generating realistic details & textures</td></tr></table></body></html>  

The field of image restoration has witnessed a significant paradigm shift with the advent of deep learning, moving beyond traditional signal processing techniques [13]. Deep learning architectures, particularly Convolutional Neural Networks (CNNs), Transformers, Encoder-Decoder models, and Generative Adversarial Networks (GANs), have demonstrated superior capabilities in handling complex image degradations [18,20]. These methods learn intricate mappings from degraded images to clean outputs by training on large datasets, overcoming the limitations of explicit model-based approaches.  

Convolutional Neural Networks (CNNs) are foundational deep learning models widely used in image processing due to their effectiveness in extracting hierarchical features through convolutional and pooling layers [13]. For image restoration, CNNs are typically adapted to predict clean pixel values based on local neighborhoods, leveraging their ability to learn spatially invariant features. Architectures like UNet, which combine an encoder for feature extraction and a decoder for image reconstruction, are commonly employed within CNN frameworks [7]. The encoder progressively reduces spatial dimensions while increasing channel depth, capturing abstract features, while the decoder upsamples these features to reconstruct the image, often incorporating skip connections to preserve fine details [7]. Examples include CNN-based photo restorers trained to identify and restore damaged regions [29]. While effective at capturing local dependencies and textural information, a key limitation of standard CNNs is their limited receptive field, making it challenging to model long-range dependencies and global context essential for addressing degradations that affect the entire image [12].  

Transformer models, originally developed for sequence processing, have emerged as powerful alternatives in computer vision, particularly due to their self-attention mechanisms which enable capturing long-range dependencies across the entire image [13]. This global perspective is highly advantageous for image restoration, where the context from distant pixels can be crucial for accurate reconstruction [12]. Architectures like Vision Transformer (ViT) and Swin Transformer have been adapted for image restoration tasks [31]. Transformer-based image restoration models often adopt an encoderdecoder structure, similar to CNN-based methods, but replace or augment convolutional layers with attention mechanisms. For instance, the SFHformer employs a five-stage hierarchical encoder-decoder structure incorporating Local-Global Perception Mixers and processes features in the frequency domain using FFT to extract comprehensive features [9]. Despite their advantages in global context modeling, direct application of standard Transformers to high-resolution images faces significant computational complexity due to the quadratic complexity of self-attention with respect to image size [12,14]. This has motivated research into more efficient Transformer variants like Restormer [12] and ART [2] designed to handle high-resolution inputs more effectively.​  

Encoder-Decoder models serve as a common framework underlying many deep learning approaches to image restoration, regardless of whether CNNs or Transformers are the primary building blocks [13]. The fundamental idea involves an encoder compressing the input degraded image into a latent feature representation and a decoder expanding this representation back to the spatial dimensions of the desired clean image [13]. This structure allows for processing information at different scales, extracting invariant features, and then progressively reconstructing the image. CNN-based encoder-decoders, like the UNet-like structure used in MPRNet, often incorporate mechanisms such as channel attention blocks and original  

resolution subnetworks to improve feature representation and preserve spatial details during upsampling [7]. Transformerbased models like SFHformer also utilize a hierarchical encoder-decoder framework to process multi-scale features [9]. The effectiveness lies in the hierarchical processing and the ability to progressively refine the image during decoding.  

Generative Adversarial Networks (GANs) have become increasingly prominent in image restoration, particularly for tasks requiring realistic texture synthesis and reconstruction [18]. GANs consist of a generator network and a discriminator network engaged in an adversarial training process [13,27]. The generator takes the degraded image (or a latent representation) and attempts to produce a restored image that is indistinguishable from real clean images. The discriminator is trained to differentiate between the generator's output and actual clean images [27]. This adversarial process compels the generator to produce highly realistic outputs. GAN-based methods have shown state-of-the-art performance in various restoration tasks, including underwater image enhancement [27] and photo restoration [29], often surpassing traditional CNN approaches in terms of perceptual quality [18]. The strength of GANs lies in their powerful capacity to learn complex data distributions and generate plausible image content. However, GAN training is notoriously unstable and sensitive to hyperparameters, often requiring larger and deeper networks, which contributes to higher computational costs and training difficulties [18].​  

Comparing these architectures, CNNs excel at capturing local features and have established efficient implementations, but struggle with long-range dependencies. Transformers, on the other hand, are powerful at modeling global context but face significant computational challenges with high-resolution data, although efficient variants are emerging [12]. EncoderDecoder structures provide a flexible framework for multi-scale processing, often integrating elements from both CNNs and Transformers. GANs offer exceptional realism but come with training complexities and instability [18]. The field is continuously evolving, with recent advancements exploring hybrid architectures, end-to-end unrolled networks combining deep learning with prior knowledge, hierarchical networks, and techniques like contrastive learning and visual prompts to address the challenges of all-in-one restoration [1,13]. Despite their successes, deep learning methods generally require large training datasets and significant computational resources for both training and inference, which remain key limitations.​  

# 4. Deep Learning Architectures for Image Restoration  

Deep learning has revolutionized the field of image restoration, with various architectural paradigms demonstrating significant success in addressing diverse degradation types. This section provides a comprehensive overview of the prominent deep learning architectures employed for image restoration, focusing on Convolutional Neural Networks (CNNs), Generative Adversarial Networks (GANs), and Transformer-based models. We analyze their fundamental mechanisms, architectural variations, advantages, and limitations, and discuss how they are leveraged to recover high-quality images from degraded inputs. The discussion aims to provide a theoretical framework for understanding the current landscape of deep learning-based image restoration, guided by insights from relevant literature [13,18,20].​  

Convolutional Neural Networks (CNNs) form a foundational pillar in deep learning for image restoration, capitalizing on their ability to effectively capture local features and spatial hierarchies through convolution operations [18]. Different CNN architectures are suited for distinct restoration tasks and scales. Encoder-decoder structures, often inspired by the U-Net design, are widely used for their capacity to capture multi-scale context by progressively down-sampling and then upsampling features [7,19]. While effective for broader context, down-sampling can compromise fine spatial details critical for high-quality restoration [19]. Conversely, networks maintaining high resolution throughout excel at preserving spatial precision but may struggle with a limited receptive field, restricting their ability to integrate global information [19]. Increasing network depth or using multi-kernel convolutions can enlarge the receptive field, enabling the learning of richer hierarchical features [9,18]. Hybrid and multi-stage architectures, such as MPRNet [7,22] and DTSD [4], combine these strategies, using multi-stage processing or parallel branches with mechanisms like channel attention [22] or selective kernel fusion [19] to balance context modeling and detail preservation. While specific mentions of residual connections in the digests are limited, architectural designs processing features across multiple scales implicitly benefit from mechanisms that enhance information flow, often facilitated by skip or residual connections to aid in training deeper networks [19]. The performance of CNNs is typically evaluated using metrics like PSNR, SSIM, and visual quality [13].  

Generative Adversarial Networks (GANs) offer a distinct approach by framing image restoration as a generation task, aiming to synthesize realistic details often missing in traditional methods or solely CNN-based outputs [18,29]. A GAN comprises a generator network that produces the restored image and a discriminator network trained to distinguish between real and generated images [27]. This adversarial process encourages the generator to produce outputs that are perceptually convincing. GANs excel at generating visually appealing results and plausible textures [29], often surpassing methods optimized solely for pixel-wise metrics. Architectures like MUGAN employ U-shaped generators and dual discriminators operating at different scales to improve realism [27]. However, training GANs presents significant challenges, including model complexity, training instability, and the risk of mode collapse, where the generator produces limited variations [18]. Comparing GAN-based methods with CNNs often involves evaluating visual quality, sharpness, and the tendency to generate artifacts [29].​  

Transformer-based architectures represent a more recent but rapidly advancing paradigm, particularly noted for their efficacy in modeling long-range dependencies and capturing global context through self-attention mechanisms [13,14]. Unlike CNNs with their inherent local focus, self-attention allows interaction between distant image regions, which is beneficial for understanding global structures and complex degradations. A primary challenge with standard Transformers is the quadratic computational complexity of self-attention with respect to spatial resolution, making processing highresolution images computationally expensive and memory-intensive. They also typically require large datasets for training [13]. To mitigate these issues, efficient Transformer variants have been developed. Many adopt multi-scale hierarchical structures within an encoder-decoder framework, akin to CNNs, to handle both local details and global context efficiently [1,9,12,20]. Innovations in attention mechanisms are also key; examples include Restormer's channel-wise Multi-Dconv Head Transposed Attention (MDTA) [12] and SwinIR's window-based self-attention with shifted windows [3,31]. These methods significantly reduce computational cost while retaining the ability to model dependencies across different scales. Other approaches, like HINT's Hierarchical Multi-head Attention (HMHA) [14] and ART's combination of dense and sparse attention [2], further enhance efficiency and effective receptive field. Performance comparison with CNNs and GANs typically considers PSNR, SSIM, and visual quality, often highlighting Transformers' strength in resolving global structures [20].​  

In summary, deep learning architectures for image restoration offer complementary strengths. CNNs excel at local feature extraction and efficiency for many tasks, but face challenges in capturing global context. GANs are powerful for generating perceptually realistic details but are notoriously difficult to train. Transformers are adept at modeling long-range dependencies but incur high computational costs for high-resolution images. Current research often explores hybrid approaches and architectural innovations within each paradigm to overcome their inherent limitations and achieve improved performance across a wider range of image degradation scenarios, moving towards more effective all-in-one restoration solutions.  

# 4.1 CNN-based Architectures  

Convolutional Neural Networks (CNNs) have been widely employed in image restoration tasks due to their intrinsic abilities to capture local features and leverage weight sharing, architectural sparsity, training stability, and hierarchical feature extraction [18]. These advantages have significantly contributed to achieving state-of-the-art performance across various restoration problems [18]. CNNs analyze images and identify damaged areas by using learned patterns to predict and generate restored content [29].​  

Different architectural choices significantly impact the effectiveness of CNNs in image restoration. Commonly used structures include encoder-decoder models and networks that process features at a single, often high, resolution [19]. Encoder-decoder architectures, such as those inspired by U-Net structures, involve down-sampling the input into a lowresolution representation before mapping it back to the original resolution [7,19]. Although effective for capturing multiscale context information [7], this process can lead to the loss of fine spatial details that are crucial for high-quality restoration [19]. Conversely, networks that process high-resolution features throughout avoid down-sampling and are better at preserving spatial details [19]. However, these single-scale networks typically have a limited receptive field, which restricts their ability to encode broader contextual information [19].​  

The size of the receptive field plays a critical role in the performance of CNNs, as it determines the extent of the input image influencing a specific output feature. Increasing network depth is a common method for enlarging the receptive field, thereby enabling the model to learn more meaningful hierarchical features [18]. A larger receptive field allows the network to aggregate information from a wider area, which is beneficial for understanding the global image context [18]. Nonetheless, excessively deep networks can face issues related to training stability and may require substantial computational resources [18].  

To mitigate the trade-offs between capturing context and preserving spatial details, hybrid and multi-stage architectures have been developed. For instance, the MPRNet architecture employs a multi-stage progressive approach [7,22]. Early stages utilize encoder-decoder subnetworks to learn multi-scale context, while the final stage—the Original Resolution Subnetwork (ORSNet)—operates at high resolution to preserve fine details [7,22]. This strategy combines the benefits of different resolution processing paths. Similarly, the DTSD network leverages CNNs within a structure that explores multilevel features, proposing a U-attention block to enhance traditional U-Net designs for capturing features at different scales [4].​  

Although the provided summaries do not explicitly detail the impact of residual or skip connections, many architectural designs that process features at multiple scales—such as multi-stage MPRNet or U-Net–like structures—implicitly benefit from mechanisms that facilitate information flow between layers, often achieved through skip connections or similar residual formulations.  

Furthermore, specific convolutional block designs are crucial for enhancing feature extraction. Architectures like SFHformer use convolutional layers for initial low-level feature extraction [9]. The Multi-Kernel Convolution Feed-Forward Network (MCFN) within SFHformer incorporates convolutions with various kernel sizes to capture local information at different scales [9]. Additionally, attention mechanisms—such as Channel Attention Blocks (CABs)—are integrated into CNN architectures like MPRNet to refine feature representations by adaptively weighting the importance of different channels [7,22].  

In summary, CNN-based architectures excel at capturing local image features through convolution operations and are foundational in image restoration [18]. Their ability to build hierarchical representations is a key strength [18]. However, balancing the need for a large receptive field to encode context with the preservation of fine spatial details remains a significant challenge [19]. Various architectural designs—including multi-stage processing, the use of varying kernel sizes, and attention mechanisms—are continuously being explored to address these limitations and enhance the performance of CNNs in restoring complex image degradations [4,7,9].​  

# 4.2 GAN-based Architectures  

Generative Adversarial Networks (GANs) have emerged as a prominent approach in the field of image restoration, recognized for their capability to generate realistic and visually appealing results [18]. This rise is attributed to the models' inherent compatibility and capacity, which can mitigate the necessity for designing highly specific network architectures for diverse restoration tasks [18]. GANs operate by training a generator network to produce synthetic images that are indistinguishable from real images by a discriminator network [27]. The generator aims to fool the discriminator, while the discriminator learns to differentiate between real and generated images.​  

The architecture of GANs for image restoration typically involves a generator responsible for mapping the degraded input image to a restored output image, and a discriminator that evaluates the realism of the generated output [27]. For instance, the MUGAN architecture designed for underwater image enhancement employs a generator with a U-shaped structure, incorporating both mixed convolution and self-attention blocks to process features at multiple scales [27]. This U-shaped design, common in many image-to-image translation tasks including restoration, allows for capturing context and detail effectively. The discriminator in MUGAN is a dual discriminator that assesses the generated images at both global semantic and local detail levels based on patch-level analysis [27]. This dual approach helps to ensure that the generated images are realistic not only in overall structure but also in fine details, which is crucial for high-quality restoration.​  

A significant advantage of GAN-based architectures lies in their ability to produce perceptually superior results compared to methods relying solely on pixel-wise error metrics like mean squared error. By leveraging the adversarial training process, GANs can learn to generate plausible image content, effectively filling in missing or damaged regions, removing noise, enhancing sharpness, and adjusting colors and tones to create high-quality, visually appealing restorations [29]. This is particularly beneficial in tasks requiring the synthesis of realistic textures and structures.​  

However, the adoption of GANs also introduces certain disadvantages and challenges. The complexity of GAN models often necessitates larger and deeper network architectures [18]. Furthermore, training GANs is known to be considerably more complex and demanding compared to training conventional deep learning models based on standard loss functions [18]. Challenges such as mode collapse, where the generator produces limited variations of output, and training instability, leading to oscillating or non-converging performance, are significant concerns in GAN training. While the provided digests do not delve into specific techniques for mitigating general instability or mode collapse, they do highlight architectural choices aimed at improving output realism and performance. For example, the dual discriminator in MUGAN, by providing feedback on both global and local realism, guides the generator towards producing more convincing results across different scales [27]. This architectural design can be seen as a technique to enhance the quality and realism of the generated output, which is a key performance metric for stable and effective GAN training in image restoration contexts. Despite the training complexities, the capacity of GANs to generate realistic and high-quality restored images positions them as a powerful tool in the image restoration landscape.  

# 4.3 Transformer-based Architectures  

Transformer-based architectures have emerged as powerful models in image restoration, primarily due to their ability to effectively capture long-range dependencies within images through self-attention mechanisms. Unlike convolutional neural networks (CNNs) which inherently possess limited receptive fields and require many layers to aggregate global information, self-attention allows each pixel or feature token to interact with every other pixel or token in the image, enabling direct modeling of relationships across significant spatial distances. This capability is particularly advantageous for complex image restoration tasks where degradation patterns or required structural details span large areas.​  

However, the application of standard Transformer architectures to high-resolution image restoration faces significant computational challenges. The complexity of the self-attention mechanism is typically quadratic with respect to the number of tokens, which corresponds to the spatial resolution of the image. Processing high-resolution images directly with global self-attention is computationally prohibitive and requires substantial memory resources. Furthermore, Transformer models often necessitate large datasets for training to generalize effectively [13].  

To address these limitations while retaining the benefits of long-range dependency modeling, various efficient Transformer architectures have been proposed. A common strategy involves incorporating multi-scale hierarchical structures, similar to those found in CNNs, to process features at different resolutions. For instance, the Restormer model employs an encoderdecoder framework with a multi-scale hierarchical design [12,20]. This allows the model to capture both local details at higher resolutions and global context at lower resolutions, enhancing performance [12]. Similarly, DyNet utilizes Transformer modules within its encoder and decoder levels for multi-scale feature extraction [1]. SFHformer adopts a fivestage hierarchical encoder-decoder structure and a dual-domain hybrid approach, incorporating both spatial and frequency branches in its Local-Global Perception Mixer (LGPM) for multi-scale receptive field modeling and efficient processing [9].  

Innovations in the self-attention mechanism itself are crucial for improving efficiency. Restormer introduces the Multi-Dconv Head Transposed Attention (MDTA) module, which performs query-key feature interactions across channels rather than spatial dimensions [12]. This approach significantly reduces computational cost compared to standard spatial self-attention, especially for high-resolution images, while still effectively aggregating local and non-local information [12]. Restormer also includes a Gated-Dconv Feed-Forward Network (GDFN) for controlled feature transformations [12]. The HINT architecture proposes a Hierarchical Multi-head Attention (HMHA) mechanism to alleviate redundancy in standard Multi-head Attention by dividing the channel space into subspaces of different sizes and re-ordering operations to encourage heads to learn distinct contextual features [14]. It further enhances interaction between attention heads via the Query-Key Cache Updating (QKCU) module [14]. The Attention Retractable Transformer (ART) addresses the limited receptive field issue of some Transformer variants by incorporating both dense and sparse attention modules, allowing interaction between tokens from sparse areas for a wider effective receptive field [2].​  

Another notable approach to enhance efficiency is seen in SwinIR, which leverages the Swin Transformer architecture [3]. SwinIR utilizes local window self-attention, limiting the attention calculation within non-overlapping windows [3]. To enable interaction between windows, shifted window partitioning is employed in consecutive layers. This window-based approach drastically reduces the computational complexity from quadratic to linear with respect to image size, making it highly efficient for high-resolution images [3]. SwinIR demonstrates superior parameter efficiency and performance across various image restoration tasks compared to many state-of-the-art methods, achieving competitive or better results with significantly fewer parameters [31]. While global attention methods like those in Restormer (MDTA) aim to capture interactions across the entire image without windowing, SwinIR's local window approach provides a strong balance between efficiency and the ability to model dependencies through layer stacking and window shifting, making it a compelling alternative.​  

In summary, Transformer-based architectures excel at capturing long-range dependencies essential for tackling complex degradations in image restoration. While standard implementations face computational hurdles with high-resolution data, various architectural innovations, including multi-scale hierarchies [1,9,12,20] and efficient attention mechanisms like  

channel-wise attention [12] and window-based attention [3], effectively mitigate these challenges, paving the way for their widespread application in high-performance all-in-one image restoration models.  

# 5. All-in-One Image Restoration: Concepts and Methods  

All-in-one image restoration signifies a paradigm shift towards developing a single computational model capable of effectively addressing multiple image degradation types and levels using a unified framework [6]. Unlike traditional methods that are typically tailored to specific degradations such as noise or blur, these methods aim to handle a wide spectrum of distortions, including unknown or mixed degradations, without requiring explicit knowledge of their precise nature or parameters [6]. The core idea involves designing models that can implicitly learn degradation characteristics or explicitly process them through dedicated representations, enabling a flexible restoration process adaptable to diverse inputs [6]. Achieving this requires innovative architectural designs, sophisticated training methodologies, and appropriate loss functions tailored to the multifaceted challenges of unified restoration [5,7,16].​  

The technical landscape of all-in-one image restoration is characterized by diverse network architectures designed to effectively capture and process image features under varying degradation conditions. Prominent architectural types include encoder–decoder networks, often augmented with robust skip connections to preserve multi-scale features and fine details [12]. Attention mechanisms, such as self–attention and its efficient variants like Multi-Dconv Head Transposed Attention (MDTA) [12] or Hierarchical Multi-head Attention (HMHA) [14], play a crucial role in enabling models to selectively focus on informative regions and capture long–range dependencies, which is essential for complex degradations [24]. Hybrid architectures combining Convolutional Neural Networks (CNNs) and Transformers leverage the strengths of both paradigms —local feature extraction by CNNs and global context modeling by Transformers—to enhance comprehensive feature learning [9,27]. Furthermore, key components such as multi–stage processing, feature fusion, and multi–scale designs are commonly integrated to facilitate progressive refinement, enrich feature representations, and handle information across different spatial granularities [7,12]. Frequency domain analysis is also utilized in some architectures, such as AdaIR [10,21], through techniques like frequency mining and modulation to process degradation–specific characteristics, though potential limitations related to spatial information loss and computational overhead exist. To explicitly address the understanding of unknown degradations, contrastive learning is employed in networks like AirNet [6,25] to learn a structured latent space for degradation representations. This involves constructing positive pairs from patches within the same image and negative pairs from different images, using a contrastive loss such as the InfoNCE loss:  

$$
L _ { c l } = - \log \left( \frac { \exp \left( \frac { q \cdot k ^ { + } } { \tau } \right) } { \sum _ { i = 0 } ^ { N } \exp \left( \frac { q \cdot k _ { i } ^ { - } } { \tau } \right) } \right)
$$  

where $\tau$ is the temperature parameter [6]. Comparisons between different architectures often involve analyzing their performance metrics alongside computational efficiency (parameter size, GFLOPs) and generalization capabilities, revealing inherent trade–offs between model complexity and restoration accuracy [1].  

Effective training strategies and the selection of appropriate loss functions are fundamental to optimizing all–in–one image restoration models. Supervised learning, relying on extensive datasets of degraded–clean image pairs, is a common approach, sometimes involving multi–task learning to handle specific degradation types [16]. However, the difficulty in acquiring diverse real–world paired data necessitates the exploration of unsupervised learning methods and transfer learning from pre–trained models on related tasks or large datasets [18]. Advanced training techniques include multi–stage training, where components like degradation estimators are pre–trained before end–to–end fine–tuning [6,24], dynamic pre–training involving training multiple model variants concurrently [1], and progressive learning which trains models on increasingly larger image patches to improve performance on high–resolution images while managing computational costs [12]. Loss functions are designed to guide the model towards high–quality restoration. Pixel–wise losses like L1 loss ensure fidelity to the ground truth pixels [5,6], while perceptual loss and adversarial loss from GANs are crucial for generating visually realistic outputs that better align with human perception [24,27]. Many state–of–the–art methods utilize multi– objective loss functions, combining these terms and sometimes adding degradation–specific losses like the contrastive loss for degradation representation learning [6,25]. The total loss function is often a weighted sum of these components, e.g.,  

$$
L = L _ { R e c } + \lambda L _ { c l }
$$  

[6,25]. Other specialized losses, such as Charbonnier loss for robust pixel–wise comparisons [7] or dual–domain losses for incorporating frequency information [9], are also employed. The selection and weighting of these loss components  

represent a trade–off between quantitative accuracy (e.g., PSNR/SSIM) and perceptual quality (e.g., LPIPS), with ongoing research aiming to bridge the gap between objective metrics and subjective human assessment.  

Prompt–based methods introduce an innovative approach by leveraging external information, typically natural language instructions or structured prompts, to guide the image restoration process [5,16]. This allows for enhanced flexibility and user control, enabling restoration based on semantic descriptions or desired outcomes beyond merely reversing physical degradations [5,20]. Examples include using human–written or Large Language Model (LLM)–generated instructions [5] or fusing degradation–aware and basic restoration prompts [16]. A key challenge in this paradigm is addressing potential contamination of textual representations by image degradation [23]. Techniques such as dedicated Textual Restoration modules are proposed to purify textual features, ensuring they primarily convey clean semantic content [23]. Text guidance is incorporated by encoding both images and text into a shared feature space, often facilitated by models like CLIP [23], followed by dynamic aggregation mechanisms to fuse these multimodal features and guide the restoration network [23].​  

In conclusion, all–in–one image restoration is an active and evolving field driven by the need for unified models capable of handling diverse and unknown degradations. Progress is propelled by advancements in network architectures that capture multi–scale context and leverage mechanisms like attention and frequency analysis, sophisticated training strategies that address data limitations and improve efficiency, and composite loss functions that balance fidelity and perceptual quality. The integration of prompt–based methods further expands the scope by incorporating external semantic guidance. Key challenges include achieving greater robustness to truly unknown and complex mixed degradations, improving computational efficiency for high–resolution applications, and better aligning model outputs with subjective human visual preferences. Future research directions are likely to explore more advanced degradation understanding and modeling, novel hybrid architectures, more efficient attention and multi–scale processing techniques, and improved methods for leveraging external guidance and learning from less–curated data.​  

# 5.1 Network Architectures for All-in-One Restoration  

<html><body><table><tr><td>Architectural Concept</td><td>Description</td><td>Role in All-in-One</td><td>Examples/Related</td></tr><tr><td>Encoder-Decoder</td><td>Downsampling path (encoder) and</td><td>Captures multi-scale context and</td><td>U-Net, Hierarchical structures, Skip Connections</td></tr><tr><td></td><td>important features/regions. Self-attention</td><td>processes relevant features, models long-range context</td><td>Self-Attention, MDTA, HMHA</td></tr><tr><td></td><td>representations by</td><td>types/levels to condition</td><td></td></tr><tr><td>Processing</td><td>features to frequency domain (FFT) for processing,</td><td>characteristics distinct in frequency</td><td></td></tr><tr><td>Architectures</td><td>paradigms, e.g.,</td><td>of multiple</td><td>hybrids, Mixed</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>CNNs and Transformers.</td><td>architectures (local features + global context).</td><td>blocks</td></tr></table></body></html>  

Network architectures are fundamental to the success of all-in-one image restoration, requiring designs capable of handling multiple degradation types and levels simultaneously [6]. These architectures must effectively capture and process image features to reconstruct high-quality images from diverse degraded inputs. Common architectural paradigms explored in this domain include encoder–decoder structures, multi-stage networks, and increasingly, architectures leveraging attention mechanisms, frequency domain processing, or hybrid designs [7].  

Encoder–decoder architectures form a foundational structure in all-in-one image restoration, adept at extracting multi-scale features and reconstructing the output [12,24]. These networks typically downsample inputs via an encoder to capture hierarchical features and context, and then upsample via a decoder to reconstruct the image [12]. Effective encoder designs are crucial for capturing degradation-aware information [6,25], employing hierarchical stages [1,9,14] or specialized components like the CLIP image encoder [23]. Decoders perform the inverse, using upsampling techniques like pixel shuffling or bilinear upsampling to restore resolution [7,9] and progressively refining the output, often guided by features from earlier stages or degradation information [1,6,22,23,25]. Skip connections are vital components, bridging encoder and decoder layers to preserve fine-grained details and high-frequency information often lost during downsampling, thereby mitigating detail loss and alleviating the semantic gap [9,19,24,27]. The bottleneck layer processes the most compressed features, aiming to capture abstract global context [14]. While effective, standard encoder–decoders can struggle with detail loss and semantic gaps [19,27], leading to explorations of enhanced skip connections, advanced upsampling, degradationaware integration [1,25] or integration with other architectures like Transformers [4,9,12,14].  

Attention mechanisms are increasingly integral, allowing networks to selectively focus on relevant features and regions, crucial for handling complex degradations [1,23,24]. Self-attention captures long-range dependencies, complementing the local processing of convolutions [27]. Various attention types are used, including channel attention [9,22,27]. The fundamental self-attention is defined as:  

$$
A t t e n t i o n ( Q , K , V ) = \mathrm { S o f t M a x } \Big ( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \Big ) V
$$  

where $\mathsf { Q } , \mathsf { K } ,$ and $\mathsf { V }$ are query, key, and value matrices [27]. Attention assists in managing diverse degradations by prioritizing informative features; for instance, Supervised Attention Modules (SAMs) in multi-stage networks refine features progressively based on intermediate outputs [7,22]. Transformer-based architectures introduce specialized attention mechanisms to improve efficiency and effectiveness, such as Multi-Dconv Head Transposed Attention (MDTA) in Restormer which operates across channels for linear complexity and incorporates local context [12], combined sparse and dense attention for balancing context and computation [2], and Hierarchical Multi-head Attention (HMHA) in HINT to reduce redundancy and capture diverse features [14]. Other variants include U-attention and self-attention for dynamic kernel selection [4,19].  

Contrastive learning provides a method for explicitly learning degradation representations, structuring a latent space where similar degradations are grouped [6]. In AirNet, the Contrastive-Based Degradation Encoder (CBDE) uses contrastive learning to distinguish different degradation types by maximizing consistency within same-degradation pairs and minimizing it across different ones [6,25]. Positive pairs are often patches from the same image, while negatives are from different images [6,25]. The InfoNCE loss is commonly used:  

$$
L _ { c l } = - \log \left( \frac { \exp \left( \frac { q \cdot k ^ { + } } { \tau } \right) } { \sum _ { i = 0 } ^ { N } \exp \left( \frac { q \cdot k _ { i } ^ { - } } { \tau } \right) } \right)
$$  

where $\mathsf { \backslash } ( \mathsf { \backslash t a u \backslash } )$ is the temperature parameter [6]. This approach enhances the network's ability to condition restoration on detected degradations but is sensitive to sample quality and diversity.  

Processing in the frequency domain leverages signal characteristics across frequency bands, beneficial for degradations like noise or blur. AdaIR utilizes frequency mining and modulation, guided by decoupled spectra, to process low- and highfrequency components specifically [10,21]. SFHFormer employs 2D FFT/IFFT with Frequency-conditioned Position Encoding and Frequency Dynamic Convolution for tailored frequency feature extraction [9]. Such frequency-aware processing  

enhances restoration quality by allowing processing tailored to different degradation types, though potential limitations include spatial relationship loss and computational cost.  

Hybrid architectures combine paradigms like CNNs and Transformers to leverage their complementary strengths: CNNs for local feature extraction and Transformers for global context [9,27]. Strategies include mixed blocks with both convolution and self-attention [27] or dedicating different network types to distinct stages, such as CNNs for low-level features and Transformers for multi-scale context [9,23]. These hybrids aim to balance comprehensive feature learning with computational considerations, particularly addressing the cost of Transformers for high resolutions.  

Beyond core architectural types, several key components contribute significantly to all-in-one performance. Multi-stage processing decomposes restoration into progressive steps, enabling iterative refinement and handling features across scales [1,7,9,12]. Examples include MPRNet's three stages [7,22] or DGRN's degradation-guided groups [6]. Feature fusion integrates information from various layers, resolutions, or branches to enrich representations and aid detail recovery [7]. Techniques range from skip connections [9,12] and cross-stage feature fusion [7,22] to dedicated modules for combining features [4,19] or fusing prompts [16]. Residual connections, notably in Residual Swin Transformer Blocks (RSTB), are vital for deep feature learning and gradient flow [31]. Multi-scale designs are essential for capturing both local details and global context [12], implemented through hierarchical structures [7,12,22], multi-kernel convolutions [9], or preserving highresolution features [19]. Additional components like supervised attention modules (SAM) [7], query–key cache updating (QKCU) [14], prompt modules [1], gated blocks [27], deformable convolution modules (DGM) [6], and hypernetworks for dynamic weight generation [24] further enhance architectural capabilities.​  

Comparing architectures involves analyzing performance, computational complexity, and adaptability. Encoder–decoders with CNNs are generally efficient but may struggle with global context and detail preservation. Transformers excel at longrange dependencies but face quadratic complexity challenges, addressed by efficient attention or hybrid designs [2,12]. Multi-stage and multi-scale designs improve adaptability by allowing progressive processing and multi-granular feature capture. Frequency domain methods are effective for certain degradations but may require integration with spatial processing. Contrastive learning aids in degradation adaptation by learning a structured latent space [6,25]. Trade-offs between model complexity (parameter size, GFLOPs) and restoration accuracy are a constant consideration [1], with research striving for efficient yet powerful architectures. The role of each component, from basic skip connections to sophisticated attention or prompt mechanisms, is to contribute to state-of-the-art performance by enabling robust feature representation, degradation understanding, and accurate image reconstruction [3,12]. Current trends show a strong emphasis on hybrid approaches, efficient attention, and methods for explicitly modeling or adapting to diverse degradation information to achieve robust all-in-one image restoration. Challenges include further improving computational efficiency, ensuring robustness to unknownand mixed degradations, and better integrating degradation information throughout the network. Future directions may involve more sophisticated hybrid strategies, exploring novel attention mechanisms, and developing more effective methods for learning and leveraging degradation representations.​  

# 5.1.1 Encoder-Decoder based Architectures  

Encoder-decoder networks constitute a fundamental architectural paradigm widely adopted in all-in-one image restoration due to their efficacy in capturing multi-scale features and synthesizing restored outputs [12,24]. These structures typically consist of a downsampling path (encoder) that progressively extracts hierarchical features and an upsampling path (decoder) that reconstructs the image from these learned representations. This design inherently allows models to capture both local fine-grained details in early encoder layers and global contextual information in deeper layers and the bottleneck, which is crucial for addressing diverse degradation types [12].  

The design of the encoder is critical for effectively extracting features and, importantly, encoding degradation-aware information. Various approaches exist, including hierarchical encoders with multiple stages where spatial resolution is gradually reduced [1,9,14]. For instance, SFHformer employs a five-stage hierarchical encoder [9], while HINT utilizes a 4- level encoder [14]. Some networks like MPRNet incorporate encoder-decoder subnetworks in their early stages specifically to learn multi-scale context features [7,22]. Specialized encoders, such as a CLIP image encoder, have also been leveraged to map image features into alternative feature spaces, like textual feature space, for guidance [23]. In other models, components like the CBDE are conceptualized as encoders responsible for capturing initial representations [6]. The effectiveness of different encoder designs lies in their ability to compress information while retaining salient features indicative of both the clean image content and the superimposed degradations.  

The decoder's role is to invert the encoding process, reconstructing the high-resolution clean image from the compressed latent features. This is typically achieved through upsampling operations at each stage [9]. Techniques like pixel shuffling are used to increase spatial resolution [9], while bilinear upsampling can be employed to reduce reconstruction artifacts [7]. Decoders in multi-stage networks like MPRNet extract features at each scale after merging deep features from previous stages [22], progressively refining the restoration. Decoders can also be guided by additional information, such as text guidance in Diffusion Unet acting as a decoder for denoising [23] or utilizing degradation-aware information [6,25]. DyNet's decoder gradually restores high-resolution outputs based on the extracted latent features [1].​  

A critical component in most encoder-decoder architectures for image restoration is the use of skip connections. These connections bridge the feature maps between corresponding encoder and decoder layers at the same resolution. Their primary function is to bypass the bottleneck and allow the direct flow of low-level, high-frequency details from the encoder to the decoder, preventing their loss during the downsampling process [9,24]. This preservation of fine-grained details is essential for accurate reconstruction of textures and edges in the restored image [24]. Skip connections also help alleviate the semantic gap between the encoder and decoder features [27], sometimes incorporating processing like deep convolutional layers before connecting features [27]. Some models integrate specific modules, such as CAB modules in MPRNet, within their skip connections [7], or enhance them with degradation-aware prompts [1].  

The bottleneck layer, often located at the deepest level of the encoder-decoder structure, processes the most compressed feature representation. Its role is dual: it reduces computational complexity by operating on a lower-dimensional space and aims to capture the most abstract and global features of the input, which are then expanded by the decoder. In some architectures, like HINT, the bottleneck layer unifies the encoder and decoder blocks at the deepest level [14].  

Despite their widespread success, standard encoder-decoder architectures have limitations. A known issue is the potential loss of fine spatial details during the repeated downsampling and upsampling operations, even with skip connections [19]. Semantic differences between encoder and decoder features can also pose challenges [27]. Potential improvements explored in the literature include refined skip connection mechanisms, advanced upsampling techniques (e.g., sophisticated attention or learnable modules instead of simple bilinear upsampling), incorporating degradation-aware information throughout the network [1,25], or integrating elements from other architectures like Transformers within the encoder-decoder framework [9,12,14]. Architectures like DTSD also leverage U-attention blocks within an encoder-decoder structure [4].​  

In summary, encoder-decoder architectures provide a robust framework for all-in-one image restoration by handling information at multiple scales. Variations in encoder/decoder stages, specific module designs within each stage (like DGM in DGRN [25]), and particularly the implementation of skip connections and bottleneck layers, define the specific capabilities and performance of different models in addressing diverse degradations. Continued research focuses on enhancing feature representation and flow within this structure to mitigate detail loss and improve restoration fidelity across various degradation types.​  

# 5.1.2 Attention Mechanism based Architectures  

Attention mechanisms play a crucial role in modern all-in-one image restoration architectures by enabling the network to selectively focus on important image regions and features while suppressing noise and irrelevant information [24]. This selective focus is essential for effectively addressing complex degradations. Beyond local feature processing typical of convolutions, self-attention mechanisms are utilized to capture long-range dependencies within images, which is vital for synthesizing global context during restoration [1]. Attention also facilitates multi-level feature matching and fusion within the restoration process [23].​  

Diverse attention mechanisms have been proposed to optimize feature processing for image restoration tasks. Channel attention, for instance, is employed to aggregate features at the channel level by dynamically weighting channels based on their importance, as seen in models incorporating Channel Attention Blocks (CABs) with CALayers [22] or channel attention within components like the Local-Global Perception Mixer (LGPM) [9]. Other models utilize self-attention within mixed convolutional and attention blocks to aggregate pixel-wise cross-channel context and encode channel-wise spatial information [27]. The fundamental self-attention operation is typically defined as:​  

$$
A t t e n t i o n ( Q , K , V ) = \mathrm { S o f t M a x } \Big ( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \Big ) V
$$  

where $\mathsf { Q } , \mathsf { K } ,$ and $\mathsf { V }$ are the query, key, and value matrices, and $\backslash ( { \mathsf { d } } _ { - } { \mathsf { k } } \backslash )$ is the dimension [27].  

The capability of attention mechanisms to focus on specific features inherently contributes to handling diverse degradation types and levels, as the most informative features for restoration vary depending on the specific degradation characteristics. While the digests do not explicitly detail attention adapting to an external degradation identifier, the strategic integration of attention within restoration architectures aids in managing multiple degradations. For example, attention integrated within restoration modules generally enhances the ability to manage complex degradations [24]. In multi-stage networks like MPRNet [7], Supervised Attention Modules (SAMs) compute attention maps based on the output of the previous restoration stage, using these maps to re-weight local features passed to the next stage [7,22]. This progressive refinement guided by attention derived from intermediate results enables the network to adapt its feature processing based on the specific degradation being addressed at each stage. SAMs also function as useful control signals during training [22].  

Several transformer-based architectures introduce specific attention mechanisms designed to overcome limitations of standard self-attention and improve performance for all-in-one image restoration [2,12,14]. Restormer [12] employs the Multi-Dconv Head Transposed Attention (MDTA). Standard spatial self-attention suffers from quadratic complexity concerning spatial size and does not effectively leverage correlations between feature channels. MDTA addresses this by performing self-attention across feature channels instead of spatial dimensions, achieving linear complexity and efficiently aggregating local and non-local pixel interactions [12]. It incorporates depth-wise convolutions for local context mixing and 1x1 convolutions for cross-channel context before computing feature covariance for channel-wise attention maps [12].  

In contrast, the Attention-Retractable Transformer [2] introduces a combined sparse and dense attention mechanism. While dense attention captures interactions among all tokens, leading to high computational cost, especially for long sequences or high-resolution images, sparse attention allows for more efficient capture of long-range dependencies by enabling interactions primarily between spatially distant tokens [2]. This hybrid approach aims to balance capturing comprehensive context with computational efficiency.​  

HINT [14] introduces Hierarchical Multi-head Attention (HMHA) to mitigate the redundancy often observed between heads in standard Multi-Head Attention (MHA). HMHA allows each head to learn distinct contextual features by operating on differentsized subspaces containing varied information [14]. Prior channel reordering based on similarity further ensures that different heads focus on diverse semantic features, thus enhancing the effectiveness and diversity of the learned representations [14].  

Other variants include the U-attention block utilizing intra-group and inter-group attentions to leverage multi-level features across scales [4], and the use of self-attention in selective kernel fusion for dynamically selecting useful kernels [19].  

# 5.1.3 Contrastive Learning based Approaches  

Contrastive learning has emerged as a promising technique for learning effective representations of degradations in the context of all-in-one image restoration. This approach aims to structure a latent space where representations of images affected by the same degradation are pulled closer together, while representations of images with different degradations are pushed further apart [6]. Specifically, in the Contrastive-Based Degradation Encoder (CBDE) utilized in AirNet, contrastive learning is employed to learn degradation representations by maximizing the consistency between inputs exhibiting identical degradations and minimizing consistency between inputs with distinct degradation types [6,25].  

A critical aspect of contrastive learning is the construction of positive and negative sample pairs, as the effectiveness of the learned representation heavily relies on this sampling strategy. In the approach described, for a given input image x, two randomly cropped patches, $x _ { 1 } \mathsf { q } ,$ and $x _ { 1 } k \boxtimes ,$ , are extracted from it and designated as a positive pair, assuming they share the same underlying degradation [25]. Image patches sourced from other images are treated as negative samples, denoted as $x _ { 1 } k \boxtimes _ { i }$ ₎ [25]. These sampled pairs $( x , 0 , 0 , 1 , 1 , 0 , 1 , 1 )$ are processed through the CBDE to derive intermediate representations $\mathsf { v } _ { ( } \mathsf { q } _ { ) } , \mathsf { v } _ { \mathsf { c } } \mathsf { k } \bigotimes ,$ , and $v _ { 1 } k \boxtimes ,$ , respectively. Subsequently, these representations are projected into a lower-dimensional space using a two-layer Multi-Layer Perceptron (MLP) to obtain q, $\mathsf { k } \boxtimes$ , and $\mathsf { k } \boxtimes$ [25].​  

The learning objective is typically formulated using a contrastive loss function, such as the InfoNCE loss. For a query representation q, the positive counterpart is $\mathsf { k } \boxtimes$ , and negative counterparts are kᵢ⁻. The contrastive loss L₍cl₎ is designed to maximize the similarity between q and $\mathsf { k } \boxtimes$ relative to the similarity between q and any kᵢ⁻. It is expressed as:  

$L _ { c l } = -$   
log   
​lef t(   
f racexplef t(f racqcdotk tauright)sumiN=0​explef t(f racqcdotki−​tauright)   
right)  

where \\(\\tau\\) is a temperature parameter that controls the spread of the representations [6]. This loss function facilitates the learning of a degradation space where different degradation types and levels are distinguishable [25].  

This contrastive learning strategy offers several advantages. By explicitly pushing representations of different degradations apart and pulling same-degradation representations together, the network gains enhanced ability to distinguish between various degradation types and levels [25]. This structured degradation space is crucial for all-in-one restoration models, enabling them to condition the restoration process effectively based on the identified degradation, leading to more accurate and artifact-free results. The specific approach of using patches from the same image as positive pairs and patches from other images as negative pairs is a common strategy in representation learning, leveraging the inherent assumption that different spatial locations within a degraded image share the same overall degradation characteristics, while different images are likely to have different degradations.​  

Comparing this approach to alternative methods for degradation representation learning, the contrastive method explicitly learns an embedding space through similarity comparisons, unlike methods that might rely on direct regression of degradation parameters or classification into predefined degradation types. The task addressed here is learning a unified representation for unknowndegradations, rather than predicting specific known parameters. The definition of positive and negative pairs from within and across images, respectively, is tailored to capture the inherent variability across different degradation instances [6].​  

However, contrastive learning-based approaches also face limitations. The performance can be highly sensitive to the quality and diversity of both positive and negative samples. Generating a sufficiently large and representative set of negative samples can be challenging, especially when dealing with a wide spectrum of potential degradations. Poorly constructed pairs can lead to collapsed representations or failure to effectively separate distinct degradations. Potential solutions might involve more sophisticated negative sampling strategies, such as hard negative mining, or combining contrastive learning with other learning objectives that provide additional structural constraints on the degradation space. Despite these challenges, contrastive learning provides a powerful framework for learning discriminative degradation representations, significantly improving the network's ability to handle the diverse and unknown degradations encountered in all-in-one image restoration.​  

# 5.1.4 Frequency Domain based Architectures  

Frequency domain analysis offers a powerful perspective in image restoration by processing signals based on their constituent frequencies, which can reveal characteristics less apparent in the spatial domain. This approach is particularly adept at handling degradations that manifest distinctly across different frequency bands, such as noise (often highfrequency) or blur (which attenuates high frequencies). Several architectures leverage frequency domain information to enhance restoration performance.  

One notable approach is demonstrated by AdaIR, which explicitly utilizes frequency domain information by mining low- and high-frequency components from input features [10,21]. This mining process is guided by the adaptively decoupled spectra of the degraded image [10,21]. The extracted frequency-specific features are then modulated by a bidirectional operator to facilitate interactions between these distinct frequency components [21]. This selective mining and interaction mechanism allows the model to process information tailored to different frequency characteristics, potentially improving its ability to address diverse degradations simultaneously.  

Another method, seen in the frequency domain branch of the LGPM within SFHFormer, employs 2D Fast Fourier Transform (FFT) to convert spatial features into the frequency domain [9]. Within this domain, specialized modules like Frequencyconditioned Position Encoding (FCPE) and Frequency Dynamic Convolution (FDC) are utilized to extract unique frequency features [9]. The FCPE assigns a distinct identity to each frequency component, while the FDC enables flexible modeling based on the input of individual frequency components [9]. These modulated frequency features are subsequently transformed back to the spatial domain using the 2D Inverse Fast Fourier Transform (IFFT) [9]. This explicit transformation and specialized processing within the frequency domain, combined with techniques like FCPE and FDC, contribute to extracting salient frequency features that can aid in recovery. Similarly, other methods, such as the DM block, also adopt the principle of extracting high-frequency and low-frequency features, albeit in a potentially lower-resolution space, demonstrating the general utility of frequency decomposition [4].  

The enhancement in performance observed in these models can be attributed to their ability to process image characteristics across different frequency scales. Frequency mining techniques, as employed by AdaIR, allow models to focus on specific frequency bands relevant to different degradation types—e.g., recovering fine textures (high frequencies) or restoring global structure (low frequencies) [10,21]. Modulation and frequency-conditioned processing, as seen in AdaIR and SFHFormer respectively, enable adaptive weighting and processing of these frequency components, facilitating better adaptation to varying degradation mixes [9,21]. This frequency-aware processing allows for more nuanced feature extraction and manipulation than purely spatial methods, thus enhancing restoration quality across multiple degradation types.​  

However, the provided digests do not explicitly detail the limitations inherent in frequency domain-based methods. Generally, a potential limitation of frequency domain approaches can be the loss of explicit spatial relationships after transformation, although techniques like FCPE aim to address this [9]. The computational cost associated with FFT and IFFT operations, especially for high-resolution images, can also be a consideration. Furthermore, while frequency analysis is powerful, certain localized spatial degradations might benefit more from processing that retains detailed spatial context. The inclusion of spatial domain branches alongside frequency branches in some architectures, such as SFHFormer's LGPM [9], implicitly suggests that a hybrid approach combining both frequency and spatial domain processing may be necessary to overcome the limitations of relying solely on one domain for comprehensive all-in-one image restoration.​  

# 5.1.5 Hybrid Architectures  

Hybrid architectures in all-in-one image restoration aim to synthesize the strengths of different neural network paradigms to enhance performance across diverse degradation types. A prominent approach involves combining Convolutional Neural Networks (CNNs) with Transformer networks. This combination is motivated by the complementary capabilities of these architectures: CNNs excel at capturing local patterns and spatial hierarchies through convolution operations, while Transformers are adept at modeling long-range dependencies and global context via self-attention mechanisms. By integrating both, hybrid models seek to leverage local feature extraction alongside global context understanding for more comprehensive image restoration [9,27].​  

Several studies have explored different strategies for implementing these CNN-Transformer hybrids. One method is to incorporate mixed blocks that concurrently process features using both convolution and self-attention. For instance, the generator in one approach employs a mixed block specifically designed to integrate convolution and self-attention for extracting image features [27]. This allows the network to simultaneously capture local textural details and global structural information within the same processing unit.  

Another architectural strategy involves using different network types for distinct stages of the restoration pipeline. The SFHformer, for example, utilizes convolutional layers specifically for low-level feature extraction, capitalizing on their efficiency and effectiveness in processing local image details. Subsequently, it employs Transformer-based modules, such as the LGPM and MCFN, to model multi-scale receptive fields and capture broader contextual relationships necessary for holistic image restoration [9]. Similarly, a Restoration Network is described as combining convolutional and Transformer networks, suggesting a potential partitioning of tasks or integration at different levels within the overall architecture [23].  

These hybrid CNN-Transformer architectures theoretically leverage the strengths of both components. CNNs provide a strong inductive bias for image data due to their translation equivariance and local connectivity, making them efficient for initial feature extraction and capturing fine-grained details. Transformers, on the other hand, overcome the limited receptive field of standard CNNs by establishing connections between any two pixels, enabling the model to understand the global structure and context of the image, which is crucial for complex degradations affecting large areas.​  

While the combination of CNNs and Transformers offers promising performance gains by effectively handling both local and global information, it is important to consider potential trade-offs. Transformers, particularly with standard self-attention, can be computationally expensive and require significant memory, especially for high-resolution images, due to the quadratic complexity with respect to the number of tokens. Hybrid designs often attempt to mitigate this by using techniques like windowed attention or integrating CNNs to reduce the spatial dimensions before applying Transformer layers. The specific architectural choices in these hybrid models, such as the design of mixed blocks or the strategic placement of CNN and Transformer components, influence the balance between restoration quality, computational efficiency, and model complexity. However, detailed analyses of the performance and efficiency trade-offs for these specific hybrid approaches are not extensively elaborated in the provided digests.  

# 5.1.6 Other Key Architectural Components  

<html><body><table><tr><td>Component Type</td><td>Description</td><td>Contribution to AlO Performance</td><td>Examples/Associated Architectures</td></tr><tr><td>Multi-stage Processing</td><td>Decomposing restoration into</td><td>improvement, handles complexity</td><td>DGRN, hierarchical structures</td></tr><tr><td></td><td>Integrating features from different</td><td>representations, aids detail recovery,</td><td>Skip connections, CSFF, DM block, Dynamic</td></tr><tr><td></td><td>block to its output.</td><td>enables deeper networks, helps</td><td></td></tr><tr><td>Multi-scale Designs Processing information at receptive fields.</td><td>various spatial resolutions or</td><td>context.</td><td>details and global structures, multi- kernel conv, different patch sizes</td></tr><tr><td>Specialized Modules</td><td>Components for specific tasks (attention maps, dynamic weights,</td><td>Tailors processing to SAM,QKCU, Prompt specific challenges or degradation types.</td><td>Modules, DGM, Hypernetworks</td></tr></table></body></html>  

Beyond the fundamental network architectures, the efficacy of all-in-one image restoration models is significantly enhanced by the strategic incorporation of various key components and techniques. This section discusses architectural elements such as multi-stage processing, feature fusion, Residual Swin Transformer Blocks (RSTB), and multi-scale designs, analyzing their roles in improving performance, handling diverse degradations, and enhancing restoration quality. These components often work in conjunction to facilitate the learning of robust representations and the accurate reconstruction of damaged or degraded image content.​  

Multi-stage processing is a prevalent strategy that decomposes the complex restoration task into a sequence of processing stages, allowing for progressive refinement of image quality [7]. This approach enables networks to iteratively process inputs, addressing different aspects of degradation across various levels. Implementations often involve hierarchical or encoder-decoder structures that handle features at multiple scales or resolutions, as seen in DyNet, Restormer, and SFHformer [1,9,12]. MPRNet, for example, employs a three-stage design for gradual restoration, with early stages learning context features and later stages focusing on high-resolution texture reconstruction [7,22]. The specialization of processing blocks within stages, such as degradation-guided groups (DGG) in DGRN or distinct blocks for initial prediction and refinement in low-light enhancement, underscores the adaptability of this paradigm to diverse degradation types [4,6].  

Feature fusion is another critical mechanism for integrating information across different stages, resolutions, or branches, thereby enriching feature representation and aiding detail recovery [7]. Techniques range from simple concatenation or element-wise operations to sophisticated attention-based methods and dedicated fusion modules. Skip connections are widely used to merge low-level and high-level features, preserving fine-grained details often lost during downsampling [9]. Restormer also utilizes feature fusion implicitly through skip connections and multi-scale feature combination [12]. MPRNet's cross-stage feature fusion (CSFF) mechanism specifically propagates refined multi-scale context features across stages using concatenation [7,22]. Fusion modules like the DM block combine features from different resolutions [4]. More advanced approaches include prompt-to-prompt interaction fusing degradation-aware and basic restoration prompts, or dedicated dynamic aggregation modules [16,23]. Effective feature fusion enhances the model's ability to leverage comprehensive contextual information and fine details, leading to improved reconstruction quality [7,9].  

Residual Swin Transformer Blocks (RSTB), notable in frameworks like SwinIR [31], represent a key component for deep feature extraction. These blocks integrate Swin Transformer layers with residual connections [31]. Residual connections are fundamental in deep networks for improving information flow and enabling models to learn more complex features by facilitating gradient propagation and mitigating degradation issues in deep architectures.  

Multi-scale designs are essential for capturing both fine-grained local details and broader global context, a requirement for effective restoration across various degradation scales [12]. Architectures like MPRNet and Restormer employ multi-stage hierarchical designs or process images with different patch sizes across stages to aggregate multi-scale features [7,12,22]. SFHformer integrates a dual-domain approach and multi-kernel convolutions to model multi-scale receptive fields [9]. Recent methods also focus on preserving original high-resolution features throughout the hierarchy to maintain spatial detail [19]. While the precise impact of varying patch sizes on performance and computational complexity is complex and implementation-dependent, using larger patch sizes or lower resolutions in specific stages is a common technique to manage computational load while capturing broader context. Beyond these core techniques, other specialized components like supervised attention modules (SAM) [7], query-key cache updating (QKCU) [14], prompt generation and interaction modules [1], gated blocks [27], and deformable convolution based modules (DGM) [6] further contribute to tailoring networks for the diverse challenges of all-in-one image restoration.​  

# 5.1.6.1 Multi-Stage Processing  

Multi-stage processing is a prevalent technique in image restoration networks, wherein the complex restoration task is decomposed into a sequence of stages to progressively enhance image quality [7]. This architectural paradigm allows models to refine the restored output iteratively, addressing various aspects of degradation across different levels of processing.​  

A common implementation of multi-stage processing involves hierarchical or encoder-decoder structures that process features at multiple scales or resolutions. For instance, DyNet utilizes a multi-level encoder-decoder to systematically decrease spatial resolution while expanding channel capacity, facilitating the extraction of low-resolution latent features across stages [1]. Similarly, the encoder-decoder architecture of Restormer is designed to support the multi-stage processing of image features [12]. SFHformer also employs a five-stage hierarchical encoder-decoder structure tailored for extracting multi-scale latent features [9].​  

The progressive nature of multi-stage processing is key to handling complex degradations and improving restoration accuracy. By processing the image iteratively through several stages, the network can learn increasingly sophisticated representations and apply specialized operations at each step. MPRNet exemplifies this approach with a three-stage architecture dedicated to the gradual restoration of the image [7,22]. In MPRNet's design, the initial stages, typically employing encoder-decoder subnetworks, focus on learning context features, while the final stage incorporates a highresolution branch specifically for reconstructing the necessary texture details in the output image [22]. This division of tasks allows the network to build a comprehensive understanding of the image content and degradation before generating the final high-quality output.​  

Multi-stage processing plays a crucial role in enhancing restoration performance by enabling both hierarchical feature extraction and progressive refinement [7]. The distinct processing blocks or groups in different stages can be specialized for particular tasks. For example, DGRN implements multi-stage processing through five degradation-guided groups (DGG), suggesting stages potentially adapt or specialize based on the detected degradation characteristics [6]. Another example is a two-stage process used for low-light image enhancement, where distinct blocks are allocated for initial prediction and subsequent quality enhancement, highlighting the role of later stages in refining the output from earlier ones [4]. This systematic decomposition and sequential processing allow multi-stage architectures to effectively tackle the multifaceted challenges of all-in-one image restoration, leading to improved accuracy and visual quality.  

# 5.1.6.2 Feature Fusion  

Feature fusion constitutes a critical mechanism within image restoration networks, enabling the integration of information derived from various processing stages, different resolutions, or distinct network branches to enhance feature representation and recovery of image details [7]. Various fusion strategies are employed, including concatenation, element‐ wise operations, and more sophisticated attention‐based mechanisms or dedicated fusion modules.  

A common approach involves combining features across different network layers. Skip connections are frequently utilized to connect low‐level latent features with high‐level latent features [9]. This facilitates the preservation of fine‐grained structural and textural information from early layers, which is often lost during downsampling or deeper processing, thereby aiding in accurate restoration [9]. Similarly, architectures like Restormer implicitly incorporate feature fusion through skip connections and the combination of features extracted at different scales [12].  

Fusion is also essential in multi‐stage or multi‐resolution architectures. MPRNet, for instance, employs a cross‐stage feature fusion (CSFF) mechanism designed to propagate multi‐scale context features from earlier stages to subsequent ones [7]. In this mechanism, features from one stage are refined, for example, via a $1 \times 1$ convolution, before being aggregated into the processing of the next stage [7]. Specifically, features like the SAM features from Stage 1 are concatenated with shallow features from Stage 2, and similarly, SAM features from Stage 2 are concatenated with shallow features from Stage 3, demonstrating the use of concatenation for combining features across stages and types [22]. In multi‐resolution settings, modules like the DM block are designed to combine deep High‐Resolution (HR) and Low‐ Resolution (LR) features [4]. Research suggests that efficient fusion can be directional; for example, transferring information flow predominantly from the low‐resolution stream to the high‐resolution stream can improve efficiency compared to bidirectional flow [19].​  

Beyond combining features from different layers or resolutions, fusion can integrate information from distinct input sources or processing paths. The prompt‐to‐prompt interaction module, for instance, fuses a degradation‐aware prompt with a basic restoration prompt, leveraging prompt engineering to guide the restoration process [16]. Dedicated modules like the Dynamic Aggregation module are also proposed to explicitly handle feature fusion within specific network designs [23].  

The primary role of these feature fusion techniques is to enhance the network's capacity to capture and leverage comprehensive contextual information and fine details. By effectively combining features across different levels of abstraction and scales, fusion improves the overall feature representation, leading to superior image reconstruction and restoration performance by recovering subtle textures and structures [7,9].  

# 5.1.6.3 Residual Swin Transformer Blocks (RSTB)  

Residual Swin Transformer Blocks (RSTB) serve as a core component for deep feature extraction within the SwinIR framework [31]. These blocks are constructed by integrating Swin Transformer layers with residual connections [31].  

# 5.1.6.4 Multi-Scale Designs  

Multi-scale designs are a fundamental strategy in image restoration networks to effectively capture both fine-grained details and broader global context [12]. By processing information at various spatial resolutions or through different receptive fields, models can simultaneously attend to local structures crucial for precise restoration and larger patterns necessary for structural coherence and global understanding.  

Several architectures employ multi-scale approaches. MPRNet, for instance, utilizes encoder–decoder subnetworks across early stages specifically to capture multi-scale context [7]. This model processes images using different patch sizes across its stages, enabling the aggregation of features at multiple resolutions [7]. Its encoder is designed to extract features at three distinct scales, preparing them for subsequent scale fusion [22]. Similarly, Restormer adopts a multi-scale hierarchical design, which is instrumental in facilitating local–global representation learning [12]. This hierarchical structure allows the network to progressively process features at different spatial granularities, enabling the capture of both detailed textures and overall image structure.​  

Other methods explore different paradigms for multi-scale modeling. The SFHformer integrates a dual-domain hybrid structure (spatial and frequency) to achieve multi-scale receptive field modeling [9]. Within SFHformer, the MCFN component specifically employs multi-kernel convolutions, which inherently extract local information from varying receptive fields corresponding to the different kernel sizes [9]. A recent multi-scale method aims to preserve original highresolution features throughout the network hierarchy, minimizing the loss of precise spatial details that are vital for highfidelity restoration outcomes [19]. These approaches collectively highlight the importance of multi-scale processing— whether through hierarchical structures, multi-stage processing with varying scales, feature fusion from different resolutions, or specialized convolutional kernels—to effectively address the diverse challenges in all-in-one image restoration. While the specific impact of different patch sizes on computational complexity is not explicitly detailed in the provided digests, the use of larger patch sizes or lower resolutions in later stages of hierarchical or multi-stage designs is a common technique to reduce computational load while capturing broader context.​  

# 5.2 Training Strategies and Loss Functions for All-in-One Restoration  

Effective training strategies and carefully designed loss functions are fundamental to the successful development and deployment of all-in-one image restoration models, which aim to address multiple image degradation types within a unified framework. The complexity and diversity of potential degradations necessitate sophisticated approaches to model training and optimization.​  

Various training paradigms are employed in all-in-one image restoration. Supervised learning, relying on paired degraded and clean image datasets, is a common and direct approach [18,29]. Models trained in this manner learn a direct mapping from the degraded input to the clean output by minimizing a defined loss function. However, a significant challenge for supervised all-in-one restoration is the acquisition of large-scale, high-quality datasets that encompass the vast array of realworld degradations [18]. Techniques such as multi-task learning can be integrated within the supervised framework to handle specific restoration tasks concurrently [16].  

To mitigate data limitations and enhance performance, alternative and supplementary strategies are utilized. Unsupervised learning offers an approach when paired data is unavailable, which is particularly relevant for real-world domain migration tasks where synthesizing realistic degradations is difficult [18]. Transfer learning leverages knowledge from models pretrained on large datasets or related tasks, such as diffusion models fine-tuned with anchor images for restoration [11] or integrating pre-trained language models for instruction-based restoration [5]. These approaches can reduce the data requirements and improve generalization.​  

Beyond the main paradigms, specific training techniques are crucial for optimizing all-in-one models. Multi-stage training, often involving pre-training of specific components like degradation estimators [6,24], followed by end-to-end fine-tuning, is widely adopted. Dynamic pre-training strategies, which train multiple model variants concurrently [1], and techniques like input masking can improve efficiency and generalization. Progressive learning, exemplified by Restormer [12,19,20], trains on increasingly larger image patches to effectively learn both local details and global context, improving performance for high-resolution images while managing computational load by adjusting batch sizes accordingly [12]. Standard optimizers like ADAM are commonly used [6,27].​  

The selection of loss functions directly influences the quality and characteristics of the restored images. Pixel-wise losses, such as L1 (Mean Absolute Error) and L2 (Mean Squared Error), are straightforward and measure the direct difference between the restored and ground truth images [5,6,24]. L1 loss is often preferred for its robustness and tendency to produce sharper results compared to L2 loss. However, these losses often fail to align well with human visual perception and traditional metrics like PSNR/SSIM may not fully reflect perceptual quality [18].  

To address this limitation, perceptual loss and adversarial loss functions are widely adopted [7,18,24,27]. Perceptual loss minimizes differences in feature space extracted by pre-trained networks, encouraging perceptual similarity [24]. Adversarial loss from GANs pushes the model to generate images that are visually realistic and difficult for a discriminator to distinguish from real images [24,27]. Perceptual metrics like LPIPS often correlate better with human judgment than pixelwise metrics [18].  

Modern all-in-one restoration models frequently utilize multi-objective loss functions, combining different loss types to balance various aspects of image quality, such as fidelity to ground truth and perceptual realism [6,24,27]. Common combinations include L1, perceptual, and adversarial losses [24,27]. Some models incorporate specific losses like contrastive loss to improve degradation representation learning, e.g., for the CBDE module [6,25], formulated as:  

$$
\mathcal { L } _ { c l } = - \log \frac { \exp \left( \frac { q \cdot k ^ { + } } { \tau } \right) } { \sum _ { i = 0 } ^ { K } \exp \left( \frac { q \cdot k _ { i } ^ { - } } { \tau } \right) }
$$  

The total loss is typically a weighted sum, such as  

$$
L = L _ { R e c } + \lambda L _ { c l }
$$  

[6,25]. Other specialized losses include Charbonnier and Edge loss for structure preservation [7], dual-domain loss combining spatial and frequency components [9], and texture-structure decomposition loss for specific tasks like low-light enhancement [4].  

The trade-offs between different loss functions are crucial. Pixel-wise losses prioritize quantitative fidelity but may lack perceptual quality. Perceptual and adversarial losses improve visual realism but can be harder to train and may introduce artifacts. The weighted combination of these losses allows for balancing these trade-offs. However, a common limitation is the imperfect alignment between objective loss functions and complex, subjective human perception. Future research could explore more sophisticated perceptual models, task-adaptive loss weighting, or novel loss formulations to better bridge this gap.​  

Training all-in-one models presents challenges related to data diversity, model complexity, and generalization across unseen degradations. The discussed strategies, such as utilizing diverse datasets [10,29], employing various pre-training techniques [1,6,24], and carefully selecting and combining loss functions, are employed to address these issues. The effectiveness of these strategies varies depending on the model architecture and the specific mix of degradations targeted [12]. While the provided digests detail the application of these strategies and their perceived benefits, a comprehensive discussion of their inherent limitations and potential solutions remains an important area for further research and systematic comparison.​  

# 5.2.1 Training Strategies  

Effective training strategies are paramount for achieving robust and generalizable performance in all-in-one image restoration models, which aim to handle multiple degradation types within a unified framework. Various paradigms, including supervised, unsupervised, and transfer learning, are employed—each with distinct advantages and challenges.  

Supervised learning represents a common and direct approach when sufficient labeled data pairs (degraded image and corresponding high-quality ground truth) are available [18]. Models trained with supervised signals learn to map degraded inputs directly to their clean counterparts by minimizing a defined loss function. For all-in-one restoration, this often involves training on a large dataset comprising various types and levels of image degradation and their respective clean versions [29]. Some supervised methods incorporate multi-task learning, training the model to handle multiple specific restoration tasks simultaneously [16]. A significant challenge for supervised all-in-one restoration is the acquisition of largescale, high-quality datasets covering the vast and complex space of possible image degradations, especially real-world scenarios which are difficult to synthesize accurately.​  

To enhance performance and generalization within the supervised paradigm, several specific training techniques have been explored. Multi-stage training strategies are prevalent, often involving pre-training of specific components—such as a degradation estimation module—followed by fine-tuning the entire network or jointly training the pre-trained module with the restoration network [6,24]. Dynamic pre-training approaches, such as training multiple network variants with varying depths concurrently, are also utilized to improve efficiency and scalability [1]. Techniques like input masking can further aid generalization by encouraging the model to learn robust features [1]. Progressive learning, which involves training the network on increasingly larger image patches, can be viewed as a form of curriculum learning that potentially stabilizes training and improves results, particularly for high-resolution image restoration [12]. Standard optimization methods like ADAM are typically used to train these models, with specific learning rates often applied to different components, such as the generator and discriminator in GAN-based architectures [27].  

In contrast to supervised methods, unsupervised learning mechanisms are considered essential—particularly for domain migration tasks like image-to-image translation, where paired data is unavailable or difficult to obtain [18]. Unsupervised approaches learn restoration by finding mappings between degraded and clean image distributions without explicit ground truth pairs. A major advantage of unsupervised learning lies in its ability to leverage readily available unpaired data, making it more applicable to diverse real-world degradations that are challenging to model synthetically. However, unsupervised methods can be more challenging to train and evaluate precisely compared to supervised methods, as they lack the direct supervision signal from ground truth images.​  

Transfer learning is another crucial strategy, leveraging knowledge gained from training on one task or dataset to improve performance on a related task. This often involves using pre-trained models as a starting point. For instance, pre-trained diffusion models can be fine-tuned with anchor images for image restoration, effectively transferring the generative capabilities of the diffusion model to the restoration task [11]. Similarly, components like pre-trained language models can be integrated as text encoders in instruction-based restoration models, bypassing the need for extensive pre-training of the core restoration network itself [5]. Transfer learning can mitigate the data requirements for training complex models from scratch and potentially improve generalization by leveraging robust features learned from large auxiliary datasets.​  

In summary, while supervised learning with specialized techniques like multi-stage or progressive training remains a powerful approach for all-in-one restoration when sufficient data is available, the significant challenge of data acquisition necessitates exploration into unsupervised learning for scenarios lacking paired data and transfer learning to leverage  

existing powerful models and datasets [5,16,18]. The choice and combination of training strategies significantly influence a model's ability to achieve robust and generalizable performance across a wide range of degradations.  

# 5.2.2 Loss Functions  

The choice of loss function is paramount in guiding the training process of deep learning models for image restoration, fundamentally influencing the quality of the restored images. Various loss functions have been employed, ranging from simple pixel-wise differences to complex perceptual and adversarial formulations [7].  

Pixel-wise losses, such as L1 loss and L2 loss, quantify the difference between the restored image and the ground truth image directly at the pixel level. L1 loss (Mean Absolute Error) is frequently used for its robustness to outliers and its tendency to produce sharper results compared to L2 loss (Mean Squared Error), which penalizes larger errors more heavily and often leads to smoother, but potentially blurrier, outcomes. Several studies utilize L1 loss as a primary component, either for reconstruction accuracy [6,24] or specifically during pre-training phases [1]. InstructIR, for instance, optimizes its model using L1 loss alongside cross-entropy loss [5].​  

While pixel-wise losses are straightforward and stable for training, they often fail to capture the human perception of image quality. Traditional evaluation metrics like Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), which heavily rely on pixel-wise accuracy, do not always correlate well with perceptual quality [18]. This limitation has driven the adoption of perceptual loss functions and adversarial loss functions.  

Perceptual loss, often computed based on the feature representations extracted by a pre-trained deep convolutional network (like VGG or ConvXNet), aims to minimize the difference between the feature maps of the restored and ground truth images. This encourages the restored image to be perceptually similar to the ground truth, preserving high-level content and structure [24]. Adversarial loss, derived from Generative Adversarial Networks (GANs), trains a discriminator network to distinguish between real and restored images. This pushes the generator (restoration model) to produce outputs that are highly realistic and visually indistinguishable from real images [24]. The integration of adversarial loss can significantly improve the visual realism and texture details of restored images [27]. Perceptual loss functions are particularly advantageous as perceptual metrics like Learned Perceptual Image Patch Similarity (LPIPS) often align better with human judgment than traditional metrics [18].  

Modern image restoration methods frequently combine multiple loss functions to leverage the strengths of each [6,24,27]. For instance, a common practice is to combine L1 loss for basic reconstruction with perceptual loss and adversarial loss for enhanced visual quality and realism [24,27]. MUGAN incorporates adversarial loss, L1 loss, content loss (a form of perceptual loss), and illumination smoothness loss to handle complex degradations [27]. Similarly, All-in-One image restoration models may use L1 loss for reconstruction and a contrastive loss for the degradation encoder [6]. The contrastive loss  

$$
\mathcal { L } _ { c l } = - \log \frac { \exp \left( \frac { q \cdot k ^ { + } } { \tau } \right) } { \sum _ { i = 0 } ^ { K } \exp \left( \frac { q \cdot k _ { i } ^ { - } } { \tau } \right) }
$$  

is formulated to enhance the representation learning for degradation features [25]. In this formulation, $q$ is the query, $k ^ { + }$ is the positive key, $k _ { i } ^ { - }$ ​ are negative keys, and $\tau$ is a temperature hyperparameter [6,25]. The overall loss function is often a weighted sum of these components, e.g.,  

$$
L = L _ { R e c } + \lambda L _ { c l }
$$  

[6].​  

Other specialized loss functions have also been introduced to address specific challenges. MPRNet employs Charbonnier Loss (a robust version of L2 loss) and Edge Loss to preserve image structures and edges [7]. SFHformer utilizes a dualdomain loss, combining a spatial domain loss ( $L _ { s p a }$ ) and a frequency domain loss ( $L _ { f r e q } )$ to leverage information from both domains [9]. For low-light image enhancement, deep texture-structure decomposition networks incorporate texturestructure decomposition loss alongside reconstruction loss [4].  

The trade-offs between different loss functions are evident in their impact on restoration quality. Pixel-wise losses prioritize fidelity to ground truth pixels, potentially sacrificing perceptual pleasantness for quantitative metrics like PSNR/SSIM. Perceptual and adversarial losses, while often yielding visually superior results, can be more challenging to train and may sometimes introduce artifacts or deviate significantly from the ground truth pixels, potentially lowering PSNR/SSIM scores.  

The careful selection and combination of loss functions, often empirically tuned, are crucial for balancing pixel accuracy and perceptual quality to achieve state-of-the-art performance in all-in-one image restoration tasks.  

While diverse loss functions and their combinations address various aspects of image quality, a common limitation remains the perfect alignment of objective functions with complex and context-dependent human visual perception. Future work may explore more sophisticated perceptual models, task-specific losses, or adaptive weighting schemes to further bridge the gap between objective optimization and subjective visual quality.  

# 5.2.3 Pre-training Strategies  

Pre-training strategies serve as a crucial component in enhancing the performance and efficiency of all-in-one image restoration models, particularly when addressing diverse and complex degradations [1,6]. These methods help to initialize network weights to a more favorable state, potentially accelerating convergence and improving final restoration quality by providing valuable prior knowledge.  

A notable approach is the dynamic pre-training strategy employed by DyNet [1]. This technique involves simultaneously training different variants of the model, including lightweight (DyNet-S) and larger (DyNet-L) versions, on extensive datasets such as Million-IRD [1]. The specific pre-training task utilized is a self-supervised masked image reconstruction problem, where the model learns to restore corrupted or masked regions of input images [1]. This dynamic strategy is highlighted for its effectiveness in improving the performance of lightweight models, enabling them to handle complex restoration tasks more efficiently [1].​  

Beyond dynamic pre-training, various methods utilize different pre-training tasks and objectives. Some approaches focus on pre-training specific modules within the restoration pipeline. For instance, the Coupled-Bridge Degradation Estimator (CBDE) is pre-trained separately before the complete network undergoes training [6]. This pre-training phase for CBDE involves optimizing an objective function denoted as $L _ { c l }$ ​ for a specified number of iterations (e.g., 100 iterations) before the entire network is trained end-to-end (e.g., for 1400 iterations) [6]. Similarly, a degradation estimation module can be pretrained independently to enhance its accuracy in predicting degradation parameters from input images [24], a step that is reported to improve the overall performance of the all-in-one restoration model [24]. The impact of such module-specific pre-training is primarily to ensure the robustness and reliability of critical components responsible for analyzing degradation characteristics.  

Another significant pre-training strategy involves leveraging large, general-purpose models pre-trained on vast natural image datasets. For example, text-guided image restoration methods often utilize pre-trained components like the CLIP image encoder and a pre-trained Diffusion Unet [23]. Other methods rely on existing pre-trained diffusion models as a foundation, although the specifics of their initial pre-training might not always be detailed [11]. These general-purpose pretrained models are subsequently fine-tuned for the specific restoration task, sometimes employing techniques like using anchor images to constrain the output space [11]. The impact of using these large pre-trained models lies in transferring rich visual priors and powerful generative capabilities learned from diverse data, which can be beneficial for handling complex and unknown degradations.​  

Based on the provided digests, a comprehensive discussion regarding the specific limitations of these diverse pre-training strategies in the context of all-in-one image restoration and potential solutions to mitigate them is not available. The digests primarily focus on describing the application and perceived benefits of the employed pre-training methods.  

# 5.2.4 Progressive Learning  

Progressive learning is a training strategy employed in models such as Restormer to enhance their capability in handling images across varying resolutions [12]. This approach involves initially training the network on smaller image patches and progressively increasing the patch size in subsequent training stages [12,19,20]. This gradual increase in patch size allows the model to learn context from larger images, which is crucial for effective restoration, particularly for high-resolution inputs [12].​  

The strategy is designed to optimize both learning efficacy and training efficiency. By starting with smaller patches, the model can effectively capture fine-grained local details. As training progresses to larger patches, the network gains exposure to broader spatial contexts, enabling it to understand and restore larger structures and global image properties. This multiscale learning process, facilitated by training on different patch sizes, contributes to improved performance at test time  

[12,20]. The ability to process and learn from varying scales allows the model to generalize better and perform robustly on images of diverse resolutions.  

Furthermore, the implementation of progressive learning considers the computational cost associated with larger patch sizes. As the patch size is increased, the batch size is concurrently decreased [12,20]. This adjustment is made to maintain a relatively similar computational time for each optimization step, thereby managing the overall training time and improving training speed [12,19]. The interplay between patch size and batch size is critical: smaller patches allow for larger batches, facilitating efficient learning of local features, while larger patches necessitate smaller batches to remain computationally feasible while enabling the learning of wider contextual information. This balance ensures that the model benefits from multi-scale learning without incurring excessive training costs, leading to improved performance on images with different characteristics and resolutions [12].​  

# 5.3 Prompt-based Methods  

![](images/2550d53392260970b5956741aa61b35d638f6587397f5cf8dc28bc6b64328957.jpg)  

Prompt‐based methods represent a significant advancement in all‐in‐one image restoration by leveraging external textual information—often in the form of natural language instructions or structured prompts—to guide the restoration process. This approach allows for greater flexibility and control, moving beyond traditional methods that rely solely on image data and pre‐defined degradation models. Prompts can be designed in various ways; for instance, InstructIR utilizes human-written instructions (including those generated by large language models like GPT-4) covering a multitude of restoration tasks to allow users control via natural language [5]. Other methods, such as PIP, employ structured prompts by combining a degradation-aware prompt encoding high-level degradation knowledge with a basic restoration prompt containing low-level information, and then fusing them into a universal restoration prompt [16]. Furthermore, prompts can be learned implicitly within the network architecture, as demonstrated by DyNet’s use of Prompt Generation and Interaction Modules integrated at skip connections to dynamically learn degradation-aware prompts [1].​  

The integration of textual information offers several advantages over traditional image restoration techniques. Text guidance allows models like SUPIR and InstructIR to condition restoration on specified criteria, facilitating the handling of multiple degradation types and potentially leading to more intelligent and visually appealing results that align with semantic intent [20]. Users can provide explicit instructions, enabling a level of control previously unavailable in automated restoration pipelines [5]. This paradigm shifts restoration from merely reversing physical degradation processes to also incorporating high-level semantic understanding.  

However, incorporating text introduces its own set of challenges. A primary concern is that degradation present in the image can contaminate the textual representations derived from it or used alongside it, thereby embedding degradation information within the guiding text features [23]. This contamination can confuse the restoration model. To address this, techniques such as the Textual Restoration module are proposed to purify textual features by removing degradation information, ensuring the text primarily conveys desired semantic content relevant to the clean image [23]. Complementary methods focus on mitigating degradation’s impact within the image processing path, for example, by using antidegradation encoders [20].​  

Different methods exist for incorporating text information into image restoration models. A common approach involves mapping image features to a shared feature space with textual features—often leveraging models like CLIP—followed by modules such as an Image-to-Text module to facilitate this mapping [23]. Dynamic aggregation techniques are then employed to fuse information from both modalities, enabling the restoration process to be guided by the textual input [23]. Additionally, integrating prompts directly into the network architecture, for instance through prompt blocks at skip connections [1], allows for dynamic modulation of feature processing based on the learned or provided prompt. The use of text prompts to guide advanced generative models like diffusion models also represents a key strategy [20]. Moreover, the architectural design of the restoration network itself—while not solely text-driven—is crucial for effectively utilizing text guidance, with examples such as InstructIR’s encoder-decoder structure [20]. Overall, prompt-based methods offer a powerful avenue for enhancing all-in-one image restoration by injecting semantic guidance, although effectively handling the interaction between image degradation and textual representation remains a critical area of research.  

# 5.3.1 Textual Representation Degradation Removing  

Image restoration tasks often benefit from leveraging external information, and textual descriptions or guidance have emerged as a powerful modality. Encoding both images and text into a shared feature space, commonly facilitated by models such as CLIP, allows for the alignment of visual and semantic information, enabling text prompts to guide the restoration process. However, degradation present in the image — such as noise, blur, or compression artifacts — can inadvertently influence the textual representation derived from, or associated with, the degraded image. This can lead to the degradation information being embedded within the textual features themselves, potentially confusing the restoration model and hindering its ability to reconstruct the clean image content.​  

Addressing this challenge necessitates techniques capable of identifying and removing or mitigating the influence of degradation information within these textual representations. One direct approach is the development of specialized modules designed explicitly for this purpose. For instance, a Textual Restoration module has been proposed, engineered with the specific objective of eliminating degradation information from textual representations [23]. The goal is to purify the textual features, ensuring they primarily capture the desired semantic content relevant to the original, undegraded image rather than encoding artifacts or degradation cues.  

Complementary strategies focusing on mitigating the impact of degradation on the overall restoration process are also explored. For example, some methods incorporate anti-degradation mechanisms within the image processing pipeline, such as employing an anti-degradation encoder. Such encoders work to reduce the adverse effects of degradation on the model responsible for generating the restored image, preventing it from misinterpreting artifacts as legitimate image content [20]. While this latter approach primarily targets the image processing path, the concept of explicitly handling degradation information—whether in the image features or the guiding text features—is crucial for robust all-in-one image restoration. Techniques focused on purifying textual representations [23] directly address the potential for degradation to contaminate the semantic guidance signal, ensuring that text prompts effectively guide the restoration towards a highquality output.  

# 5.3.2 Dynamic Aggregation and Restoration Network  

In the context of text-guided image restoration, dynamic aggregation techniques and dedicated restoration networks are employed to facilitate image recovery [23]. Specifically, dynamic aggregation is utilized to fuse information derived from different modalities, namely the image content and the guiding text description [23]. This multimodal fusion is crucial for enabling the restoration process to be informed by textual instructions, thereby achieving results that align with the desired textual attributes or corrections. However, the provided digest does not elaborate on the specific mechanisms or architectures underpinning this dynamic aggregation process for fusing image and text modalities [23].  

The restoration network component is integral to reconstructing the degraded image [23]. While the digest related to textguided restoration mentions the use of such a network, it does not detail its architecture or training methodology [20]. As an example of restoration network architectures employed in related image restoration tasks, InstructIR utilizes a novel image model featuring a 4-level encoder-decoder structure [20]. This architecture incorporates four intermediate blocks positioned between the encoder and decoder, designed to enhance feature representation throughout the network [20]. A notable architectural choice in InstructIR is the use of addition for skip connections within the decoder, diverging from the more common concatenation approach and representing a novel design decision [20]. The specific training procedures for these restoration networks, including loss functions or optimization strategies, are not detailed in the provided digests.  

# 6. Datasets and Evaluation Metrics  

The effectiveness and generalization capabilities of image restoration models, particularly those designed for all-in-one scenarios, are intrinsically linked to the quality, diversity, and scale of the datasets utilized for both training and evaluation [5]. Datasets commonly employed in image restoration research can be broadly categorized into synthetic and real-world types [13]. Synthetic datasets are constructed by artificially applying degradation processes, such as noise, blur, rain, or haze, to clean, high-quality images [13]. This controlled generation allows for the creation of large volumes of paired data (degraded and clean), which is beneficial for supervised training [13]. However, synthetic degradations may oversimplify the complexity and variability encountered in natural images, leading to a significant "sim-to-real" generalization gap when models are applied to real-world scenarios [13].​  

Conversely, real-world datasets capture images in authentic environments, reflecting the intricate and often combined degradations present in practice [13,19]. While providing a more accurate representation of the challenges models must address, real-world datasets are typically smaller and often lack precise pixel-aligned clean ground truth images, complicating supervised learning and quantitative evaluation. For instance, datasets like SIDD and DND provide real noise but necessitate specialized training approaches due to the absence of ideal ground truth [2]. The Million-IRD dataset [1] represents an effort to bridge this gap by providing a massive collection of real images for pre-training, demonstrating the trend towards leveraging large-scale, diverse datasets for initial model development, followed by fine-tuning on taskspecific benchmarks [1].​  

A wide array of datasets serves as benchmarks for evaluating image restoration algorithms, encompassing diverse degradation types. Popular datasets for denoising include BSD400, BSD68, WED, Urban100, SIDD, DND, DIV2K, Flickr2K, CBSD68, Kodak24, McMaster, Classic5, and LIVE1 [1,2,5,6]. Deraining is commonly evaluated on datasets like Rain100L and SPA-Data [1,5,6,9]. Dehazing performance is often assessed using the RESIDE dataset, particularly the SOTS subset [1,5,6,9,14]. Motion deblurring utilizes datasets such as GoPro and HIDE [5,9]. For low-light enhancement, LOL-v1 and LOL-v2 are standard benchmarks [5,9,14]. Underwater image enhancement studies often rely on UIEB, LSUI [9] and EUVP [27]. Other tasks use specialized datasets, including Snow100K for desnowing [14] and datasets like Set5, Set14, BSD100,  

Urban100, Manga109, DIV2K, and Flickr2K for super-resolution and related tasks [1,2,3,6]. Face restoration may use generated albums or specific datasets like AFHQ for cross-category evaluations [11].  

The characteristics of these datasets, including image content diversity, resolution, the nature and severity of degradations, and the availability of high-quality ground truth, significantly impact the evaluation of All-in-One image restoration methods [5]. Training all-in-one models often requires combining multiple datasets to cover a broad spectrum of tasks and degradations, posing challenges due to dataset heterogeneity [5]. The limited scale and realism of many benchmark datasets, particularly for complex multi-degradation scenarios, remain a challenge for evaluating model generalization in the wild.​  

Evaluating the performance of image restoration algorithms necessitates rigorous quantitative and qualitative assessment using appropriate metrics. Objective evaluation metrics aim to numerically quantify the similarity between the restored image and a reference ground truth image. A foundational metric is the Peak Signal-to-Noise Ratio (PSNR), calculated based on the Mean Square Error (MSE) between the images [18,28]. The formulas are:  

$$
\begin{array} { l } { { \displaystyle M S E = \frac { 1 } { M N } \sum _ { i = 0 } ^ { M - 1 } \sum _ { j = 0 } ^ { N - 1 } \left( I ( i , j ) - K ( i , j ) \right) ^ { 2 } } } \\ { { \displaystyle P S N R = 1 0 \log _ { 1 0 } \left( \frac { M A X _ { I } ^ { 2 } } { M S E } \right) } } \end{array}
$$  

where $I ( i , j )$ and $K ( i , j )$ are the pixel values of the original and restored images respectively, $M \times N$ are the image dimensions, and $M A X _ { I }$ ​ is the maximum possible pixel value. PSNR is widely used [1,2,6,9,27] but is primarily a pixel-wise fidelity measure and does not correlate strongly with human visual perception [18].  

To better align with human perception, the Structural Similarity Index Measure (SSIM) was developed, considering structural information, luminance, and contrast [18]. SSIM is often reported alongside PSNR [2,6,9,12,14,27]. Other objective metrics include VIF, FSIM, IWSSIM [4], and task-specific metrics like UCIQE for underwater enhancement [9]. Objective metrics enable quantitative comparison of different algorithms [30].​  

However, a significant challenge in evaluation is the imperfect correlation between objective metrics (like PSNR and SSIM) and subjective human assessment of visual quality [18,30]. This highlights the critical importance of subjective evaluation, where human observers provide judgments on the visual quality of restored images [18,30]. Subjective studies are considered the most reliable for assessing perceived quality but are time-consuming and costly [18].​  

The limitations of traditional objective metrics have driven the development of new metrics designed to better capture perceptual quality, such as MUSIQ [11], and metrics for generative quality like FID [11]. Furthermore, non-reference metrics, such as MANIQA, are increasingly important for evaluating performance on real-world images where ground truth is unavailable [12,14].  

The choice of evaluation metrics and datasets significantly impacts the assessment of image restoration methods [18]. Different metrics have varying strengths and weaknesses depending on the degradation type and the desired outcome [30]. Relying solely on one or two metrics provides an incomplete picture. Therefore, a comprehensive evaluation strategy for Allin-One image restoration models typically involves a combination of multiple quantitative objective metrics (including fidelity-based and perception-based), potentially non-reference metrics, and qualitative visual inspection or formal subjective studies to gain a holistic understanding of performance and generalization across diverse degradation types and real-world conditions. Analyzing performance necessitates considering these multiple metrics and datasets, revealing the ongoing need for more realistic datasets and evaluation metrics that accurately reflect human perception, particularly for the complex and varied outputs of all-in-one models.  

# 6.1 Datasets  

<html><body><table><tr><td>Dataset Type</td><td>Examples</td><td>Primary Use Case(s)</td><td>Key Characteristics</td></tr><tr><td>Synthetic</td><td>Rain100L, SOTS (part of RESIDE)</td><td>Training/Evaluating specific degradations (rain, haze)</td><td>Paired degraded/clean data,controlled degradation</td></tr><tr><td></td><td></td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td>Real-world</td><td>SIDD,DND,EUVP</td><td>Evaluating real- world noise, underwater degradation</td><td>Authentic degradation,often lack perfect ground truth</td></tr><tr><td>General/Mixed</td><td>BSD400, BSD68, WED,Urban100, DIV2K, Flickr2K</td><td>General image quality, various tasks (denoising, SR)</td><td>Diverse content, sometimes used with synthetic deg.</td></tr><tr><td>Large-Scale</td><td>Million-IRD</td><td>Pre-training for generalization</td><td>Massive size, diverse real images</td></tr><tr><td>Task Specific</td><td>GoPro (Deblur), LOL- v1/v2 (Low-light), UIEB/LSUI (Underwater), Snow100K (Desnowing), AFHQ (Face)</td><td>Benchmarking performance on specific degradation types</td><td>Tailored to specific challenges</td></tr></table></body></html>  

The effectiveness of image restoration models, particularly those designed for all-in-one scenarios, is critically dependent on the quality, diversity, and scale of the datasets used for training and evaluation [5]. Datasets for image restoration can be broadly categorized into synthetic and real-world types [13]. Synthetic datasets are generated by simulating degradation processes on clean images, offering controlled environments for training and testing [13]. Real-world datasets, conversely, consist of images captured in natural settings, providing a more accurate reflection of complex, real-world degradations and serving to validate the generalization capabilities of models [13,19].  

A variety of datasets are commonly employed to evaluate image restoration algorithms, covering a spectrum of degradation types. Popular benchmark datasets include BSD400 (400 clean images) and BSD68 (68 clean images) [1,6], often used for tasks like denoising and general image quality assessment. WED (Waterloo Exploration Database), containing 4,744 natural images, is also frequently utilized [1,2,6]. For specific degradations, Rain100L provides 200 training and 100 test pairs for deraining [1,5,6,9]. Dehazing is commonly evaluated using the RESIDE dataset, specifically the SOTS (Stress-Testing Outdoors and Indoors) subset, which includes foggy-clean image pairs [1,6,9,14]. Motion deblurring performance is often gauged on datasets like GoPro and HIDE [5,9]. For low-light enhancement, LOL-v1 and LOL-v2 are standard benchmarks [5,9,14]. Underwater image enhancement frequently uses UIEB, LSUI [9], and EUVP, the latter featuring images collected under varied visibility conditions for ocean exploration [27]. Other tasks utilize specialized datasets, such as Snow100K for image desnowing [14], SIDD and DND for real image denoising [2], DIV2K and Flickr2K for super-resolution and denoising [2], and specific sets like Set5, Set14, BSD100, Urban100, Manga109, CBSD68, Kodak24, McMaster, Classic5, and LIVE1 for evaluating various restoration tasks [1,2,6]. Face restoration methods may utilize generated or personal albums for realworld testing and datasets like AFHQ for cross-category scenarios [11].  

Synthetic datasets, while offering large quantities of paired data (degraded and clean), have limitations. The simulated degradations may not fully capture the complexity and variability of real-world noise, blur, or atmospheric effects, leading to a "sim-to-real" gap. This can impact the generalization ability of models trained solely on synthetic data [13]. Real-world datasets, conversely, present authentic degradation but are often smaller in scale and may lack precisely aligned clean ground truth images, making training more challenging. For instance, datasets like SIDD and DND provide real noise but require specialized training strategies [2].  

For training all-in-one image restoration models capable of handling multiple degradations, the diversity and combination of datasets are crucial [5]. Training on a collection of datasets covering various tasks (e.g., denoising, deraining, deblurring) helps models learn degradation-agnostic features and improve generalization across different types of image degradation [5]. However, combining datasets designed for specific tasks can be complex due to differing image characteristics, resolutions, and degradation levels.  

The size and diversity of datasets significantly impact model generalization. Larger, more diverse datasets allow models to learn a wider range of degradation patterns and image content, improving performance on unseen data. This is particularly true for advanced models like Transformers [12]. Creating large-scale, high-quality datasets that encompass the full  

spectrum of real-world degradations remains a significant challenge. Manual annotation and collection are resourceintensive, while synthetic generation struggles with realism.  

The Million-IRD dataset [1] exemplifies an effort to create a large-scale resource for image restoration, particularly for pretraining all-in-one models. It comprises 2 million high-quality, high-resolution images, from which 8 million 512x512 nonoverlapping patches are extracted after filtering out flat areas [1]. This massive scale is leveraged for pre-training, followed by fine-tuning on smaller, task-specific datasets like BSD400, WED, BSD68, Urban100, Rain100L, and SOTS [1]. This strategy highlights the current trend of using massive datasets for initial training stages to build robust feature representations, followed by fine-tuning on benchmark datasets to tailor performance for specific tasks and evaluations. Despite advancements, developing comprehensive datasets that accurately represent the complexity and variability of real-world multi-degradation scenarios at scale remains an open area of research.​  

# 6.2 Evaluation Metrics  

Evaluating the performance of image restoration algorithms is crucial for comparing different methods and understanding their effectiveness in recovering degraded images. This process involves the use of various metrics, which can be broadly categorized into objective and subjective measures. A fundamental objective evaluation metric is the Peak Signal-to-Noise Ratio (PSNR), which quantifies the difference between the restored image and a reference ground truth image based on the Mean Square Error (MSE) [18,28]. The formula for MSE is given by:  

$$
M S E = \frac { 1 } { M N } \sum _ { i = 0 } ^ { M - 1 } \sum _ { j = 0 } ^ { N - 1 } \left( I ( i , j ) - K ( i , j ) \right) ^ { 2 }
$$  

where $I ( i , j )$ is the original image, $K ( i , j )$ is the restored image, and $M \times N$ denotes the image dimensions. PSNR is then calculated as:  

$$
P S N R = 1 0 \log _ { 1 0 } \left( \frac { M A X _ { I } ^ { 2 } } { M S E } \right)
$$  

where $M A X _ { I }$ is the maximum possible pixel value of the image. While PSNR is simple and widely used across various studies [1,2,6,9,27], it primarily measures pixel-wise differences and does not correlate well with human visual perception of quality [18].​  

To address the limitations of PSNR in capturing perceptual quality, the Structural Similarity Index Measure (SSIM) was introduced [18]. SSIM assesses image quality by considering structural information, luminance, and contrast, aiming to better approximate the human visual system's quality assessment [18]. SSIM is often used in conjunction with PSNR to provide a more comprehensive objective evaluation [2,6,9,12,14,27]. Other objective metrics employed in the literature include VIF, FSIM, and IWSSIM, particularly in tasks like low-light image restoration [4], and UCIQE for specific applications such as underwater enhancement [9].  

Despite the prevalence of objective metrics, a significant challenge in evaluating image restoration methods lies in the discrepancy between these quantitative scores and the perceived visual quality by humans [18,30]. Objective metrics can quantitatively compare different restoration algorithms, but their correlation with subjective perception is not always strong [30]. This highlights the importance of subjective evaluation, where human observers assess the visual quality of restored images [30]. Some methods are shown to yield significant advantages in perceptual quality through such assessments [20].  

Consequently, there is a growing need for more comprehensive evaluation metrics that align better with human perception. Researchers have introduced metrics specifically designed to capture perceptual quality, such as MUSIQ (Multi-scale Structural Image Quality) [11], and metrics for assessing generative quality, like Fréchet Inception Distance (FID) [11]. Furthermore, non-reference metrics, such as MANIQA, are increasingly used to evaluate restoration performance on realworld inputs where a ground truth image is unavailable [12,14].  

The strengths and limitations of different metrics vary depending on the degradation type and the desired outcome. While PSNR and SSIM are useful for measuring fidelity to a reference image, they may not adequately capture artifacts or structural distortions that are visually apparent but have minimal impact on pixel-wise differences. Metrics like MANIQA and MUSIQ offer advantages in assessing perceptual quality without a reference or by considering multi-scale structural information.  

Given the advantages and disadvantages of different performance metrics [30], it is evident that relying solely on one or two metrics provides an incomplete picture of a restoration method's performance. The performance analysis of image  

restoration techniques necessitates considering multiple metrics, as seen in studies that report a combination of scores [7]. Therefore, evaluating image restoration models effectively requires using a combination of quantitative objective metrics, perceptual metrics, and subjective evaluation to gain a holistic understanding of their capabilities and limitations across various degradation types and desired output characteristics. Analyzing the performance metrics used to evaluate the quality of restored images reveals the ongoing challenge in finding metrics that universally correlate well with human perception, underscoring the need for continued research into improved evaluation methods.  

# 6.3 Benchmarking and Comparisons  

Evaluating the performance of all-in-one image restoration methods necessitates rigorous benchmarking against both single-task specialists and other multi-task or all-in-one approaches. These evaluations typically rely on quantitative metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), complemented by qualitative visual inspections to assess perceptual quality [5,7,16]. PSNR, measured in decibels (dB), quantifies the ratio between the maximum possible power of a signal and the power of corrupting noise, with higher values indicating better objective fidelity. SSIM, conversely, measures the structural similarity between the restored image and the ground truth, accounting for luminance, contrast, and structure, with values closer to 1 signifying higher similarity. While these metrics provide valuable objective comparisons, they do not always perfectly align with human visual perception, making qualitative analysis essential [5,7,16].​  

Quantitative comparisons across various studies highlight the advancements in all-in-one models. For instance, DyNet, an efficient and scalable dynamic network, demonstrates substantial PSNR improvements in all-in-one settings. DyNet-L and DyNet-S achieve average PSNR gains of 0.82 dB and 0.59 dB, respectively, over PromptIR [1]. Beyond average performance, DyNet-L also exhibits superior results in single-task evaluations for dehazing, deraining, and denoising when compared to PromptIR and Restormer [1]. HINT, a hierarchical multi-head attention transformer, consistently achieves state-of-the-art PSNR and SSIM results across diverse tasks [14]. Specifically, on the LOL-v2 dataset for low-light enhancement, HINT improves average PSNR by 0.9 dB relative to Retinexformer [14]. For image desnowing on Snow100K, it shows a significant PSNR increase of 1.64 dB over AST [14], and on the SOTS dehazing dataset, it surpasses other methods by at least 0.35 dB in PSNR [14]. ART (Attention Retractable Transformer) similarly reports superior PSNR and SSIM on various benchmark datasets for super-resolution, denoising, and JPEG compression artifact reduction, citing specific results like 27.77 PSNR and 0.8321 SSIM on Urban100 for image SR [2]. MUGAN achieves a PSNR of 27.304 dB for underwater enhancement, outperforming other methods by at least 0.29 dB [27]. Restormer also demonstrates state-of-the-art performance across tasks including deraining, deblurring, and denoising, achieving a 0.26 dB PSNR gain over standard feedforward networks at high noise levels [12,20]. SwinIR, based on the Swin Transformer, outperforms state-of-the-art methods in super-resolution, denoising, and JPEG artifact reduction, showing improvements of up to 0.14–0.45 dB while reducing parameters by up to $6 7 \%$ in some settings [3,31]. SFHformer also reports state-of-the-art PSNR and SSIM across multiple tasks, including dehazing, deraining, deblurring, low-light enhancement, underwater enhancement, and snow/rain drop removal, often with reduced parameters and FLOPs compared to prior methods like DRSformer [9]. AdaIR is also reported to achieve state-ofthe-art performance across different tasks [10]. An analysis comparing sparsity learning techniques noted that a hybrid sparsity learning technique performed better than other state-of-the-art techniques based on evaluation parameters [28].  

Qualitative evaluations provide insights into the visual fidelity and presence of artifacts in restored images. MPRNet provides visual comparisons on benchmark datasets for deraining, deblurring, and denoising, showcasing its progressive restoration capabilities [7]. InstructIR is noted for surpassing traditional and deep learning methods, offering improved visual quality through a more intuitive, instruction-guided approach [5]. HINT demonstrates visually satisfactory restored images on real-world datasets without ground truth references, indicating robust performance in practical scenarios [14]. MUGAN is reported to produce images with higher contrast, richer colours, and more details compared to other methods for underwater enhancement [27]. SwinIR excels in real-world image super-resolution, producing sharper and clearer results compared to models like ESRGAN and Real-ESRGAN [3].  

Performance comparisons across diverse tasks and datasets reveal the strengths and weaknesses of different models. AirNet, for instance, was extensively compared against a wide array of baselines for denoising (CBM3D, DnCNN, IRCNN, FFDNet, BRDNet), deraining (DIDMDN, UMRL, SIRR, MSPFN, LPNet), dehazing (DehazeNet, MSCNN, AOD-Net, EPDN, FDGAN), as well as a general image restoration method (MPRNet) and an IRMD baseline (Decoupled Learning) [6]. DyNet demonstrates superior performance in specific single tasks (dehazing, deraining, denoising) compared to PromptIR and Restormer, while also achieving significant efficiency gains, reducing parameters by $5 6 . 7 5 \%$ and GFlops by $3 1 . 3 4 \%$  

compared to PromptIR [1]. HINT achieves high PSNR scores while maintaining low model complexity compared to CNNbased MIRNet and Transformer-based IPT and Restormer [14]. SFHformer also achieves superior performance with significantly reduced parameters and FLOPs, particularly noted against DRSformer for deraining [9]. MIRNet-v2 demonstrates state-of-the-art performance on six real image datasets and shows effectiveness across tasks including dualpixel defocus deblurring, denoising, super-resolution, and image enhancement [19]. Factors contributing to the performance of these models often include architectural innovations, such as the hierarchical attention mechanisms in HINT [14], the efficient transformer designs in Restormer [12] and SwinIR [3], the dynamic processing in DyNet [1], the adaptive frequency analysis in AdaIR [10], and the frequency-aware processing in SFHformer [9]. The ability to effectively model diverse degradation patterns and capture multi-scale features appears crucial for achieving robust performance across varied restoration tasks and datasets. The success of models like InstructIR also suggests the potential benefit of incorporating explicit guidance signals, such as human instructions, to improve restoration quality and controllability [5,20].  

# 7. Applications of All-in-One Image Restoration  

<html><body><table><tr><td>Application Area</td><td>Common Degradations Addressed</td><td>Benefits of All-in-One Approach</td></tr><tr><td>Old Photo Restoration</td><td>Scratches,spots, tears, stains,fading (often mixed)</td><td>Removes diverse defects simultaneously,enhances overall quality.</td></tr><tr><td>Image Compression</td><td>Compression artifacts (e.g., JPEG)</td><td>Mitigates artifacts within compression,potentially improves visual quality.</td></tr><tr><td>Underwater Enhancement</td><td>Color distortion, low contrast,blur (mixed)</td><td>Improves visibility,recovers natural appearance in challenging environment.</td></tr><tr><td>Security & Surveillance</td><td>Noise, blur, low light, weather effects (rain, haze)</td><td>Enhances clarity for analysis in varied conditions.</td></tr><tr><td>Remote Sensing</td><td>Atmospheric effects,sensor noise, blur</td><td>Improves image quality for analysis of Earth surface.</td></tr><tr><td>Medical Imaging</td><td>Noise,blur,artifacts, low light</td><td>Enhances image quality for diagnosis and analysis.</td></tr><tr><td>Digital Media</td><td>Various aesthetic or technical imperfections</td><td>General quality improvement for visual content.</td></tr><tr><td>Law Enforcement</td><td>Blur, noise,low light in forensic images</td><td>Restores details for analysis.</td></tr></table></body></html>  

All-in-one image restoration techniques offer a unified approach to address multiple types of image degradation simultaneously, providing significant benefits across a range of application domains. By handling diverse imperfections such as noise, blur, rain, haze, and low light within a single framework, these methods not only enhance image quality but also facilitate downstream tasks. The utility of all-in-one restoration extends from preserving cultural heritage through old photo recovery to improving critical data visualization in fields like medical imaging and remote sensing [8,18,28,29].  

A prominent application is the restoration of old photographs, which commonly suffer from mixed degradations such as scratches, spots, tears, stains, and fading [8,29]. All-in-one methods aim to remove these defects and enhance overall image quality concurrently. Modern AI-driven tools have been designed to automate this complex process and improve visual appearance [8,29]. When evaluating techniques for old photo restoration, key considerations include restoration quality (i.e., the efficacy in removing defects and enhancing appearance), computational efficiency, and user-friendliness. For instance, online tools simplify access for end-users [29]. Crucially, successful old photo restoration requires preserving the historical context and artistic style, balancing defect removal with maintaining the image’s original character—a critical challenge for automated techniques [8,29].  

All-in-one image restoration can also be integrated into practical image compression workflows to improve the visual quality of compressed images [17]. This integration involves embedding restoration capabilities within the compression process to mitigate artifacts, such as those introduced by JPEG compression [2]. However, it does present a trade-off between the compression ratio achieved, the effectiveness of the restoration in enhancing visual quality, and the added computational complexity of the combined process [17].  

Furthermore, these restoration techniques are well-suited to address the unique challenges of underwater image enhancement. Underwater images are often severely degraded by light absorption and scattering, resulting in color distortion, low contrast, and blurring [27]. Methods such as MUGAN [27] and SFHformer [9] have demonstrated efficacy in tackling these complex, mixed degradations to improve visibility and recover natural appearance, which is vital for marine applications [27]. Other specialized imaging fields, such as fluorescence imaging and computerized tomography superresolution, also benefit from deep learning-based image restoration [18].  

Beyond these specific examples, all-in-one techniques find applications in security and surveillance, remote sensing, digital media, and law enforcement [28], as well as in general tasks like deblurring, denoising, super-resolution, de-raining, dehazing, de-snowing, rain drop removal, and low light enhancement [2,9,19,22]. The ability of all-in-one methods to handle scenarios with unknown or mixed degradation types makes them promising for real-world deployment across various domains [16].​  

Despite their demonstrated potential, current all-in-one image restoration methods face limitations. One significant challenge is developing algorithms that are both robust and efficient enough for real-time processing—particularly in applications that require low latency. Future research directions include exploring novel network architectures, training strategies, and loss functions to further improve performance and efficiency. Addressing the specific requirements and challenges of diverse application areas is crucial, as advances in all-in-one restoration could also enhance the performance of other computer vision tasks by providing cleaner, higher-quality input data.​  

# 7.1 Old Photo Restoration  

Restoring old photographs presents unique challenges due to various forms of degradation accumulated over time— including scratches, spots, tears, stains, and fading. All-in-one image restoration techniques aim to address these multiple imperfections simultaneously while potentially enhancing overall image quality. Modern approaches increasingly leverage AI-driven tools specifically tailored for this task [8,29].​  

Different tools employ AI to automate the complex process of removing these diverse defects. For instance, AI-driven methods are highlighted for their ability to effectively remove imperfections such as scratches, stains, and fading, thereby enhancing the overall appearance of the image and making it more visually appealing [29]. Similarly, other specialized tools are designed to address specific issues like scratches, spots, and tears while concurrently improving details, enhancing colors, and boosting the overall quality of aged images [8]. These approaches emphasize comprehensive damage removal and aesthetic enhancement as key aspects of restoration quality.​  

When comparing these techniques, factors such as restoration quality, computational efficiency, and user-friendliness are crucial. Based on the available information, both methods prioritize addressing common physical damages and enhancing visual appeal. One significant aspect related to user-friendliness is the availability of these tools online, which simplifies the restoration process for end-users without requiring specialized software or technical expertise [29]. Although the descriptions focus on defect removal and enhancement, detailed comparisons regarding quantitative quality metrics or computational efficiency (e.g., processing speed, resource requirements) are not provided. However, the emergence of tailored AI restorers [8] and online tools [29] indicates a movement toward accessible and efficient solutions in the field of old photo restoration.​  

Beyond merely removing damage and enhancing appearance, preserving the historical context and artistic style of old photographs is paramount. Restoration should aim to recover the image’s original state without introducing modern artifacts or altering its historical authenticity. This involves retaining the original color palette, film grain, and overall aesthetic unique to the era in which the photo was taken. While current descriptions primarily focus on defect removal and general enhancement [8,29], the delicate balance between removing degradation and preserving historical fidelity remains a critical consideration—and a potential challenge—for all-in-one automated methods. Ensuring that AI-driven tools achieve high restoration quality while maintaining the image’s original character is essential for truly successful restoration.  

# 7.2 Image Compression  

All-in-one image restoration techniques offer a promising avenue for integration into practical image compression workflows [17]. This integration aims to enhance the visual quality of compressed images by embedding the capability to address various degradation types directly within the compression process itself [17]. A notable application area is the reduction of artifacts commonly introduced during compression, such as JPEG compression artifacts [2]. By incorporating restoration capabilities, systems can potentially mitigate these visual degradations, leading to improved perceptual quality of the reconstructed image. While the potential benefits in visual quality are evident, the practical implementation necessitates a careful analysis of the inherent trade-offs between compression ratio, the effectiveness of the restoration (restoration quality), and the additional computational complexity introduced by the integrated restoration module.​  

# 7.3 Underwater Image Enhancement  

Underwater imaging presents significant challenges for visual perception and subsequent analysis due to inherent environmental factors. Light absorption and scattering in water lead to issues such as color distortion, often manifesting as a greenish or bluish cast, reduced contrast, and overall image degradation [27]. All-in-one image restoration techniques offer a promising approach to simultaneously address these complex degradations within a unified framework.​  

Several methods demonstrate the applicability of these techniques to underwater image enhancement. For instance, MUGAN has been specifically applied to enhance underwater images, directly targeting and alleviating common problems like color distortion and low contrast [27]. By integrating multiple restoration aspects, MUGAN aims to recover a more natural appearance and improved visibility in underwater scenes. Another approach, the SFHformer, has also been deployed for underwater image enhancement [9]. This model has achieved state-of-the-art results on challenging datasets such as UIEB and LSUI, demonstrating its efficacy in restoring degraded underwater images [9].  

# 8. Challenges and Future Directions  

Despite significant progress in the field, all-in-one image restoration faces several remaining challenges that hinder its robustness, efficiency, and widespread applicability in real-world scenarios [18]. A primary limitation lies in effectively handling complex, unknown, or multiple types of degradations simultaneously [5,13,16,17]. Current methods often exhibit sensitivity to specific degradation characteristics and struggle to generalize to unseen combinations of distortions, such as images captured under extremely low-light conditions or those with combined blur and snow [6,14]. Furthermore, limitations exist in traditional pipelines' ability to balance precise spatial detail preservation with rich contextual representation, and challenges persist in applying methods designed for clean images to degraded inputs or handling diverse degradations in tasks like joint compression and restoration [17,19]. The dynamic nature of real-world degradations necessitates continuous learning and adaptation of algorithms over time [29].​  

Another critical challenge is the high computational cost and resource requirements associated with many deep learningbased approaches [1,18,25]. Existing all-in-one methods can be computationally intensive, limiting their flexibility and contributing to environmental concerns [1,6]. Issues like slow inference speeds due to per-image fine-tuning and high storage costs for multiple models also pose practical barriers [11,16]. These factors underscore the urgent need for more efficient architectures and training strategies [6].​  

Data dependency and generalization capability also remain significant hurdles [18]. Deep learning methods typically rely on large, well-labeled datasets, which may not accurately represent the complexity of real-world degradations, leading to overfitting and poor generalization to unseen data [18,25]. The performance evaluation limited to specific image categories also raises concerns about broader applicability [11]. Architectural limitations, such as restricted receptive fields in some Transformer designs, can also impact performance [2]. Deploying these methods in real-world applications is further complicated by a lack of robustness to variations in input quality and environmental conditions, alongside insufficient user interactivity and flexibility [16].  

Based on these limitations, future research directions are centered on developing more robust, efficient, and generalizable all-in-one image restoration methods [13]. Key areas for exploration include novel network architectures, such as advanced Transformer models, the latest Generative Adversarial Networks, dynamic network designs, and architectures that optimize efficiency and receptive fields through mechanisms like sparse attention [1,2,12,13,25]. Developing more effective and efficient training strategies, including dynamic large-scale pre-training, algorithm optimization, and methods that avoid perimage fine-tuning, is crucial for improving performance and reducing computational burden [1,11,13].  

Gathering larger, more diverse, and realistic real-world datasets, especially for challenging scenarios like low-light conditions and complex combined degradations, is essential for training more robust models [11,13,14,29]. Incorporating prior knowledge about image content and degradation processes through the use of multiple image priors or integrating it into network designs represents a vital direction to guide the restoration process more effectively [1,5,13,16,25]. Addressing the complexities of different combined degradations and developing adaptive algorithms capable of handling a wide range of degradation types and severities, including images with multiple distortions, are necessary steps towards achieving true universality [5,6,13,16].​  

Exploring unsupervised or self-supervised learning methods, such as Noise2Siamese and adversarial training, offers promising avenues to reduce reliance on large labeled datasets and improve generalization ability [5,16,18,25]. Enhancing efficiency and real-time performance, particularly for practical applications, is a key objective [13]. Furthermore, expanding the applicability to a wider range of image categories and scenarios, as well as developing more interpretable models, will increase trust and facilitate real-world adoption [5,11,16].​  

Future research can also explore integrating all-in-one restoration models with other image processing tasks, such as image super-resolution, image colorization, object detection, and segmentation, potentially by combining deep learning with traditional techniques [18]. Advances in these directions hold significant potential impact across various fields, including medical imaging, remote sensing, and computer vision, by providing more accurate and efficient tools for analyzing and utilizing visual data [18]. Overcoming the current limitations and pursuing these proposed research directions will be fundamental to advancing the field of all-in-one image restoration.  

# 8.1 Limitations of Current Methods  

Despite significant advancements, current all-in-one image restoration methods still face substantial limitations, hindering their widespread applicability and performance in diverse real-world scenarios [5,16,17,19]. A primary challenge lies in their sensitivity to specific degradation types and a lack of robustness when encountering the complex, unknown, or multiple distortions prevalent in real-world images [5,13,16]. For instance, the performance of methods like AirNet under specific corruptions such as blur and snow remains unclear [6]. Similarly, the HINT model encounters challenges when tasked with restoring images captured in extremely low-light conditions [14]. Traditional image restoration and enhancement pipelines are often limited to fulfilling only one of two competing requirements: preserving precise spatial details or providing richer contextual representation [19]. Furthermore, image compression methods designed for clean images perform unsatisfactorily on degraded inputs [17], and existing joint compression and restoration techniques typically address only a single degradation type, failing to handle the variety encountered in practice [17]. The ability to effectively handle a wide variety of image degradations often requires continuous learning and refinement of AI algorithms [29].  

Another significant limitation is the high computational complexity and resource requirement associated with many current deep learning-based restoration approaches [1,18,25]. Existing all-in-one methods frequently suffer from high computational costs and reduced flexibility in adapting to different degradation types [1]. While methods like AirNet can adapt to different corruptions without requiring multiple task-specific models, they necessitate significant computational resources for optimization, contributing to carbon emissions [6]. The need to fine-tune models for individual images, as seen in some generative approaches, can result in slower inference speeds [11]. Moreover, current deep learning methods often incur high storage costs due to the need for storing numerous task-specific models [16]. These computational demands highlight the need for more efficient architectures and training strategies [6].​  

Data requirements and generalization capabilities also pose challenges. Deep learning methods often require large, welllabeled datasets for training, which may not accurately reflect real-world conditions [18]. Supervised learning, commonly employed, is susceptible to overfitting and exhibits poor generalization, particularly when dealing with complex underlying mapping functions and limited training data [18]. The generalization ability and robustness to unseen degradations remain a limitation for methods like AirNet, shared with other existing all-in-one techniques [25]. Furthermore, the evaluation of some methods has been limited to specific image categories, such as faces and pets, raising questions about their performance on broader datasets [11].​  

Architectural constraints can also impact performance. For instance, some Transformer-based networks utilizing dense attention strategies suffer from restricted receptive fields [2]. Deploying these methods in real-world applications is further complicated by their lack of robustness to variations in input quality and environmental conditions, as well as a general lack of interactivity and flexibility in user control [16]. Although models like InstructIR attempt to address this by interpreting human instructions [5], the general limitation persists for many methods.  

Based on these limitations, key areas for future research include the development of more efficient architectures and training strategies to reduce computational costs [6], the design of more robust loss functions, and the improved handling of complex and unknown degradation types [13]. Addressing the challenge of restoring images with multiple distortions and creating more robust and generalizable models remains a primary goal in the field [13,25].​  

# 8.2 Future Research Directions  

Future research in all-in-one image restoration should explore potential advancements in network architectures, training strategies, and the incorporation of prior knowledge into deep learning models [13]. Significant improvements in performance are anticipated through the development of novel deep learning architectures, enhanced training methodologies, and innovative loss functions. Potential research directions include investigating new network designs [1,2,13] and exploring more efficient training strategies [1,13]. Specific architectural considerations include the utilization of Transformer models [13] and the latest Generative Adversarial Networks (GANs) [13], as well as developing more efficient architectures overall [25]. The integration of improved attention mechanisms can also enhance image restoration capabilities [2].​  

Developing more effective training strategies is crucial. This involves methods such as dynamic large-scale pre-training to achieve higher accuracy [1] and optimizing algorithms, including genetic algorithms [13]. A significant area for future work lies in the collection of large-scale real-world datasets to facilitate further training, particularly for challenging scenarios like extremely low-light conditions [14] and diverse, complex degradations [29]. Expanding dataset size for neural networks and CNN models is broadly recommended [13].​  

ncorporating prior knowledge into deep learning models represents another vital direction [1,5,13,16,25]. Future work could explore the use of multiple image priors [13] to guide the restoration process more effectively.  

Addressing the complexities of degradation remains a key challenge. Future research needs to delve deeper into understanding the effects of different combined degradations on single tasks and why these combinations yield varying results [6]. Developing adaptive algorithms capable of handling a wide range of degradation types and severities is essential [5,16]. Handling images with multiple distortions [13] is a specific focus area.  

Exploring self-supervised learning methods presents a promising path to reduce reliance on large pairs of degraded and clean images [5,16,25]. Examples include frameworks like Noise2Siamese for denoising, which derives a self-supervised upper bound for typical supervised losses and proposes a new self-supervised loss [18]. The use of adversarial training could further improve generalization ability.​  

Efficiency and real-time performance are critical for practical applications [13]. Future research should aim to develop methods that do not require per-image fine-tuning to enhance efficiency [11] and address the specific challenges of realtime image restoration [13]. Expanding the applicability of current methods, such as the PIP method, to real-world scenarios and improving their performance is also important [16]. Extending methods to a wider range of image categories and larger generative spaces is a potential direction [11].  

Future research can also explore the potential of all-in-one restoration models to address other related image processing tasks, such as image super-resolution and image colorization [18]. In image super-resolution, research is focusing on scenarios like Zero-Shot Super-Resolution (ZSSR) using internal image data and the application of Graph Neural Networks (GNNs) [18]. Furthermore, combining traditional image processing techniques with deep learning and machine learning methods may offer novel solutions [18].  

Ultimately, advances in all-in-one image restoration will have significant impacts on various fields, including medical imaging, remote sensing, and computer vision. Developing more interpretable models [5,16] will be essential for increasing trust and adoption in sensitive applications. Addressing the limitations of existing models and pursuing these outlined research directions will drive the field forward.​​  

# References  

[1] DyNet：用于高效可扩展一体化图像恢复的动态预训练网络 https://cloud.tencent.com/developer/article/2416742 [2] Attention Retractable Transformer for Accurate Ima https://github.com/gladzhang/ART [3] SwinIR: 基于 Swin Transformer 的图像恢复 https://cloud.tencent.com/developer/article/2398287 [4] Deep Texture-Structure Decomposition for Low-Light https://www.sciencedirect.com/science/article/abs/pii/S0925231222015454 [5] InstructIR：基于人类指令的高质量图像修复 https://cloud.tencent.com.cn/developer/article/2465300 [6] All-In-One图像恢复：未知退化的统一解决方案 https://blog.csdn.net/a486259/article/details/139559389 [7] MPRNet: 多阶段渐进式图像恢复网络 https://blog.csdn.net/qq_41251963/article/details/120794807 [8] AI Old Photo Restoration: Revive Your Memories https://vanceai.com/old-photo-restoration/  

[9] SFHformer：快速傅里叶变换与Transformer结合用于高效图像恢复 https://mp.weixin.qq.com/s? _biz $: =$ MzU0NjgzMDIxMQ $\scriptstyle 1 = =$ &mid=2247630596&idx $\mathop { : = }$ 3&sn=93870175a8959542657eebe8e47f3d4d&chksm=fa780e992dab1e87   
68c51b85cb1569c5e1a50445eedc5127725e017851154ac52cebb09caac6&scene=27   
[10] AdaIR: Adaptive Frequency-Aware All-in-One Image R https://github.com/c-yn/AdaIR​   
[11] 约束生成空间的生成式图像修复：一种图像复原新方法 https://blog.csdn.net/weixin_36829761/article/details/135300230​   
[12] Restormer：用于高分辨率图像恢复的高效Transformer https://cloud.tencent.com.cn/developer/article/2398294​   
[13] 2023图像复原技术综述：基于图像处理、机器学习和深度学习的方法   
https://blog.csdn.net/m0_60350022/article/details/136375169​   
[14] HINT：用于图像恢复的分层多头注意力Transformer https://mp.weixin.qq.com/s?   
__biz $\mathrel { \mathop : } =$ MzI1MjQ2OTQ3Ng==&mid=2247656019&idx $\circleddash$ 1&sn=0474f680e9ad27a0f2881a3a781891c7&chksm $\scriptstyle | =$ e88ccac5599b54163   
20d020516459905e33ae0ccf6ab0ce4a2494f8766dc9e2725417290e7f4&scene=27   
[15] ${ \mathsf { D } } ^ { 3 }$ Net: Dynamic Degradation Decomposition Networ http://www.paperreading.club/page?id $\ c =$ 287541   
[16] Prompt-In-Prompt Learning for Universal Image Rest http://www.paperreading.club/page?id $\ c =$ 198526​   
[17] All-in-One Image Compression and Restoration Frame http://www.paperreading.club/page?id $\mathbf { \Psi } = \mathbf { \Psi }$ 282447   
[18] 深度学习图像恢复方法综述(2022) https://blog.csdn.net/weixin_39653948/article/details/124455382   
[19] 特斯联：基于丰富特征学习的快速图像复原与增强技术突破 https://www.geekpark.net/news/309589​   
[20] 图像复原新突破：SUPIR、Restormer引领智能逼真修复新高度 https://www.bilibili.com/read/cv32071397/   
[21] AdaIR: 基于频率挖掘和调制的自适应全合一图像恢复 https://blog.csdn.net/u014546828/article/details/141302989   
[22] MPRNet图像修复论文笔记：多阶段渐进式图像复原与代码解析   
https://blog.csdn.net/xian0710830114/article/details/131698256​   
[23] 文本引导的图像修复：消除文本表示中的退化信息 https://blog.csdn.net/sgr011215/article/details/142618190​   
[24] 曹相湧的出版物及工作成果 https://gr.xjtu.edu.cn/en/web/caoxiangyong/publication​   
[25] AirNet：面向未知损坏的一体化图像修复网络 https://blog.csdn.net/m0_61163395/article/details/146541762   
[26] 图像复原与重建：退化模型、噪声模型与滤波方法 https://www.cnblogs.com/surfzjy/p/6200526.html​   
[27] MUGAN: Mixed Generative Adversarial Network for Un   
https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.12702   
[28] Image Restoration: Techniques, Applications, and D https://www.sciencedirect.com/topics/computer-science/image  
restoration   
[29] Free AI Old Photo Restoration Online Tool by Spyne http://www.spyne.ai/tools/photo-restoration​   
[30] 图像恢复与识别基础方法：退化模型、算法与模糊识别 https://blog.csdn.net/snowleafzf/article/details/85730520   
[31] 计算机视觉学术速递：Transformer与检测论文精选 (8.24) https://cloud.tencent.com/developer/article/1867803  