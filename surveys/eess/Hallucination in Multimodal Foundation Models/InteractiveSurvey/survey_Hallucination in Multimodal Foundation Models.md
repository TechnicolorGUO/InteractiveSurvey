# A Survey of Hallucination in Multimodal Foundation Models

# 1 Abstract


Multimodal foundation models (MFM) have revolutionized artificial intelligence by integrating multiple data modalities, such as text, images, and audio, to solve complex tasks. However, one of the most significant challenges in the development and deployment of these models is the issue of hallucination, where the model generates outputs that are inconsistent with the input data, leading to errors and reduced reliability. This survey paper focuses on the problem of hallucination in multimodal large language models (MLLMs), a subset of MFMs that combine large language models with visual and other sensory inputs. The paper provides a comprehensive overview of the techniques and methods developed to mitigate hallucinations in MLLMs, exploring both training-based and training-free approaches, as well as integrated and hybrid methods. Key findings include the effectiveness of contrastive and autoregressive methods, dynamic focus and vision amplification techniques, and advanced integrated approaches like decoupling contrastive decoding and retrieval and reasoning for personalization. The survey also delves into evaluation frameworks and benchmarks, emphasizing the importance of systematic and rigorous testing. Finally, the paper highlights the integration of cognitive and sensory modalities to enhance the performance and reliability of MLLMs, paving the way for future research and development in this rapidly evolving area.

# 2 Introduction
Multimodal foundation models (MFM) have emerged as a transformative force in artificial intelligence, enabling the integration of multiple data modalities such as text, images, and audio to solve complex tasks. These models, built on the foundation of large-scale pre-training, have demonstrated remarkable capabilities in tasks ranging from image captioning and visual question answering to audio-visual understanding [1]. However, one of the most significant challenges in the development and deployment of MFMs is the issue of hallucination. Hallucination refers to the generation of outputs that are inconsistent with the input data, leading to errors and reduced reliability. This phenomenon is particularly problematic in safety-critical applications, such as medical diagnosis and autonomous systems, where the accuracy of the model's outputs is paramount.

This survey paper focuses on the problem of hallucination in multimodal large language models (MLLMs), a subset of MFMs that combine large language models with visual and other sensory inputs [2]. The paper aims to provide a comprehensive overview of the techniques and methods developed to mitigate hallucinations in MLLMs [3]. We explore both training-based and training-free approaches, as well as integrated and hybrid methods that combine multiple strategies to enhance the reliability and accuracy of MLLM outputs. The survey also delves into the evaluation frameworks and benchmarks used to assess the performance of these models, highlighting the importance of systematic and rigorous testing in the development of robust MLLMs [4].

The paper begins by examining the techniques for reducing hallucinations in MLLMs, including contrastive and autoregressive methods, such as Contrastive-Autoregressive Fine-Tuning (CAFe), Learning to InstrucT (LIT), and Temporal Attention Realtime Accumulative Connection (TARAC) [5]. These methods are designed to enhance the model's alignment with input modalities and maintain a balanced attention distribution, thereby reducing the likelihood of hallucinations [6]. We then discuss training-free methods, such as Dynamic Focus (DyFo) and Vision Amplification Fusion (VAF), which leverage dynamic adjustments and feature amplification to improve the model's fine-grained visual understanding and reduce the risk of spurious correlations [7].

Next, we explore integrated and hybrid approaches that combine multiple techniques to address the multifaceted nature of hallucinations [8]. Decoupling Contrastive Decoding (DCD) and Multi-Frequency Perturbations (MFP) are presented as advanced methods that decouple the learning of positive and negative samples and leverage both low-frequency and high-frequency features, respectively. These methods are complemented by Retrieval and Reasoning for Personalization (R2P), which enhances the model's ability to generate contextually relevant and personalized outputs by integrating external knowledge from a knowledge base [9].

The paper also covers the evaluation and benchmarking of MLLMs, including frameworks such as Instruction-Oriented Preference Alignment (IPA) and Grounded Chain-of-Thought (GCoT), which provide systematic and scalable approaches to aligning MLLMs with human preferences and grounding their reasoning processes in visual and spatial information. Additionally, we discuss data-driven and task-specific evaluations, such as TASKANYTHING and JUDGEANYTHING, which offer comprehensive testbeds for assessing the multimodal understanding and generation capabilities of MLLMs [10].

Finally, we examine the integration of cognitive and sensory modalities to enhance MLLMs, including vision-enhanced preference optimization, user-centric and interactive enhancements, and sensory and modality-specific enhancements. These sections highlight the importance of adaptive and context-aware methods in improving the reliability and user-friendliness of MLLMs.

The contributions of this survey paper are multifaceted. First, it provides a comprehensive review of the state-of-the-art techniques for reducing hallucinations in MLLMs, offering a valuable resource for researchers and practitioners in the field [8]. Second, it introduces and evaluates a range of evaluation frameworks and benchmarks, emphasizing the need for rigorous and systematic testing in the development of robust MLLMs [4]. Lastly, it highlights the importance of integrating cognitive and sensory modalities to enhance the performance and reliability of MLLMs, paving the way for future research and development in this exciting and rapidly evolving area.

# 3 Techniques for Reducing Hallucinations in MLLMs

## 3.1 Contrastive and Autoregressive Methods

### 3.1.1 Contrastive-Autoregressive Fine-Tuning CAFe
Contrastive-Autoregressive Fine-Tuning (CAFe) is a novel framework designed to enhance the capabilities of Large Vision-Language Models (LVLMs) by addressing the limitations of existing methods [11]. Unlike traditional fine-tuning approaches that often focus on either retrieval or generation, CAFe aims to achieve a balanced improvement in both areas [11]. The framework introduces a unified contrastive-autoregressive mechanism that leverages contrastive loss to align multimodal representations during fine-tuning, while maintaining the autoregressive nature of the model for coherent generation. This dual approach ensures that the model can effectively learn from both text and image pairs, leading to more accurate and contextually relevant outputs.

CAFe's architecture is built upon the foundation of contrastive learning, which has been widely successful in aligning different modalities by minimizing the distance between positive pairs (e.g., corresponding text and image) and maximizing the distance between negative pairs. During the fine-tuning phase, CAFe incorporates a contrastive loss term that encourages the model to generate embeddings that are semantically aligned with the input modalities. This alignment is crucial for tasks such as image captioning, where the model needs to generate text that accurately describes the visual content. Additionally, the autoregressive component of CAFe ensures that the generated text is coherent and contextually appropriate, avoiding the common issue of hallucinations that can arise from over-reliance on language priors.

To validate the effectiveness of CAFe, extensive experiments were conducted across a variety of multimodal tasks, including image captioning, visual question answering, and text-to-image generation. The results demonstrate that CAFe significantly outperforms existing methods in terms of both retrieval accuracy and generation quality. Specifically, the model shows improved performance in tasks that require a deep understanding of visual content, such as object recognition and scene description. Furthermore, CAFe's ability to maintain high-quality generation while enhancing retrieval capabilities makes it a versatile framework for a wide range of applications, from content creation to information retrieval systems [11].

### 3.1.2 Learning to InstrucT LIT
Learning to InstrucT (LIT) is a novel approach designed to enhance the capabilities of multimodal large language models (MLLMs) by focusing on the generation of instructions for images [12]. Unlike traditional visual instruction tuning (VIT), which primarily emphasizes the generation of responses to instructions, LIT introduces a dual-learning mechanism. This mechanism not only trains the model to generate appropriate responses but also to create meaningful instructions for images. By doing so, LIT aims to improve the model's understanding and representation of visual content, thereby enhancing its performance across various multimodal tasks [13].

The core idea behind LIT is to treat the generation of instructions for images as a form of regularization. This approach ensures that the model develops a deeper and more nuanced understanding of visual inputs, which is crucial for tasks that require fine-grained visual understanding, such as optical character recognition (OCR), captioning, and hallucination mitigation. During training, LIT encourages the model to focus on the most salient aspects of an image, generating instructions that highlight key features and relationships. This process helps to reduce the reliance on spurious cues and minimizes the risk of object hallucinations, a common issue in MLLMs [3].

Extensive experimental evaluations across 16 diverse tasks have demonstrated the effectiveness of LIT. Compared to conventional VIT, LIT achieves a significant relative improvement of up to 6% in overall multimodal task performance [13]. Notably, LIT shows particular strength in tasks that heavily depend on visual content, such as OCR and captioning [13]. Additionally, LIT has proven to be highly effective in mitigating hallucinations, as evidenced by its performance across four distinct hallucination benchmarks. These results underscore the potential of LIT as a robust and versatile method for enhancing the capabilities of MLLMs in fine-grained visual understanding tasks.

### 3.1.3 Temporal Attention Realtime Accumulative Connection TARAC
Temporal Attention Realtime Accumulative Connection (TARAC) is a novel approach designed to mitigate hallucinations in large multimodal models (LMMs) by maintaining a cumulative attention distribution over image tokens during the generation process [14]. Specifically, TARAC continuously updates the attention of the current generating token on image tokens, ensuring that the model maintains a consistent focus on visual information [14]. This mechanism is crucial because, as observed in Figure 1, LMMs tend to allocate less attention to image tokens as more tokens are generated, leading to a dilution of visual context and an increased likelihood of hallucinations [15].

At each time step, the accumulated attention is injected with a scaling factor to enhance the model's focus on image tokens, thereby counteracting the natural decay of visual attention [14]. This injection of accumulated attention helps to maintain a balanced attention distribution, ensuring that the model does not overly rely on language priors and remains grounded in the visual input. The scaling factor is dynamically adjusted based on the current attention distribution, allowing the model to adapt its focus as needed. This dynamic adjustment is particularly important in tasks where the visual context is critical, such as medical VQA, where misinterpretation of the input image can have severe consequences.

The effectiveness of TARAC is demonstrated through extensive experiments, where it has been shown to significantly reduce hallucinations while maintaining or even improving the overall quality of generated content. By continuously updating the attention distribution, TARAC ensures that the model remains attentive to the visual input, even as the sequence length grows. This approach not only mitigates hallucinations but also enhances the coherence and relevance of the generated responses, making it a valuable addition to the toolkit of techniques for improving the reliability of LMMs in multimodal tasks [15].

## 3.2 Training-Free Methods

### 3.2.1 Dynamic Focus DyFo
Dynamic Focus (DyFo) is a training-free method designed to enhance the fine-grained visual understanding capabilities of large multimodal models (LMMs) by simulating human-like dynamic focusing adjustments [7]. DyFo operates by establishing a bidirectional interaction between the LMM and a visual expert, enabling the model to selectively amplify critical visual information and filter out irrelevant input. This approach is particularly useful in tasks where the model needs to focus on specific regions of an image, such as object recognition or detailed scene description.

The core mechanism of DyFo involves an action space that simulates human focusing adjustments, allowing the LMM to dynamically adjust its attention to different parts of the image. This is achieved through a consensus-based reward mechanism, where the LMM and the visual expert collaboratively determine the most relevant visual regions. Unlike other methods that require additional dependencies on instruction-tuned localization or heatmap modules, DyFo is designed to be lightweight and easily integrable into existing LMM architectures. By leveraging Monte Carlo Tree Search (MCTS), DyFo can efficiently explore and refine the focus on key visual regions, ensuring that the model's attention is directed towards the most salient features [7].

Experimental results have shown that DyFo significantly enhances the performance of LMMs in fine-grained visual understanding tasks, while also reducing the incidence of visual hallucinations [7]. The method's ability to dynamically adjust focus without requiring additional training or complex modifications makes it a versatile tool for improving the reliability and accuracy of multimodal models in various applications, from medical imaging to autonomous driving. DyFo's effectiveness in mitigating hallucinations and enhancing fine-grained visual understanding highlights its potential as a valuable addition to the toolkit of multimodal model developers [16].

### 3.2.2 Vision Amplification Fusion VAF
Vision Amplification Fusion (VAF) is a novel technique designed to mitigate hallucinations in multimodal large language models (MLLMs) by enhancing the model's reliance on visual information during modality fusion [17]. The core idea of VAF is to amplify the visual features extracted from the input image, ensuring that the model's response is more closely aligned with the visual content. This is achieved through a two-step process: Multi-Frequency Feature Extraction and Fine-Grained Frequency Feature Fusion. In the first step, Gaussian high-pass and low-pass filters are applied to the original image to obtain its raw high-frequency and low-frequency features. These raw features are then fed into the visual encoder, producing high-frequency and low-frequency feature token sequences [3].

The second step involves a fine-grained fusion of these high-frequency and low-frequency token sequences with the original visual token sequence derived from the image encoder [3]. This fusion is performed at the token level using a cross-attention mechanism, which allows the model to integrate the enhanced visual features more effectively. During inference, a decay is applied to both high-frequency and low-frequency feature perturbations, reducing redundant features and ensuring that the final visual token sequences are more robust and coherent [3]. Experimental results have shown that VAF significantly improves the model's performance on object hallucination benchmarks, with notable gains of approximately 3% on POPE and 7% on MME [17].

VAF not only enhances the accuracy and coherence of generated responses but also maintains content quality without negative impacts, unlike other methods such as VCD, which can cause a decrease in performance on benchmarks like NoCaps [17]. By amplifying the visual influence, VAF addresses the issue of hallucinations caused by the degradation of visual attention, making it a promising approach for improving the reliability and trustworthiness of MLLMs in various applications [8]. This method is particularly useful in scenarios where the visual content is critical, such as medical imaging and complex scene understanding, where hallucinations can have significant consequences [8].

### 3.2.3 Divide-Then-Aggregate Approach
The divide-then-aggregate approach is a method designed to enhance the robustness and accuracy of multimodal large language models (MLLMs) by breaking down complex images into manageable patches and then aggregating the processed information [18]. This method is inspired by the way the human brain processes visual information, where basic features are extracted in parallel, and attention is sequentially directed to specific regions to form a coherent perception [16]. In the context of MLLMs, the approach begins by dividing the input image into smaller, non-overlapping patches. Each patch is then processed independently to extract local features, such as edges, textures, and colors, which are crucial for understanding the content of the image.

Once the local features are extracted, the next step involves a hierarchical aggregation of these features. This process starts at the patch level, where the extracted features are combined to form a more comprehensive representation of each patch. To ensure that the aggregated information is semantically consistent, a semantic filtering strategy is applied. This strategy classifies the descriptions of each patch into three categories: same, contradictory, and unique. Contradictory descriptions are resolved by selecting the most probable or consistent interpretation, while unique descriptions are retained to capture the diversity of the image. This filtering process helps to reduce inconsistencies and hallucinations that can arise from misaligned or conflicting information.

Finally, the aggregated patch-level information is combined to form a global representation of the image. This global representation is then used to generate a coherent and accurate caption or response. The divide-then-aggregate approach has been shown to improve the quality of image captions and reduce hallucinations by ensuring that the model focuses on relevant and consistent features [16]. Additionally, this method is training-free, making it applicable to both open-source and closed-source models, and it can be easily integrated into existing MLLM architectures to enhance their performance across various multimodal tasks.

## 3.3 Integrated and Hybrid Approaches

### 3.3.1 Decoupling Contrastive Decoding DCD
Decoupling Contrastive Decoding (DCD) represents a significant advancement in mitigating object hallucinations in multimodal large language models (MLLMs) [17]. Unlike traditional contrastive decoding methods that adjust the output distribution in a unidirectional and blunt manner, DCD introduces a more nuanced approach by decoupling the learning of positive and negative samples. This separation aims to alleviate the issue of likelihood displacement, a phenomenon where the model's output distribution shifts unfavorably due to the aggressive adjustment of contrastive logits. By decoupling the learning process, DCD ensures that the model can more effectively balance the influence of positive and negative samples, thereby reducing the risk of hallucinations.

A key component of DCD is the introduction of a vision-aware negative image projector. Traditional contrastive decoding methods often employ agnostic image perturbations, which do not account for the specific visual characteristics of the input [17]. In contrast, DCD learns a negative image projector from negative samples, enabling the model to generate perturbations that are more aligned with the visual context. This approach not only enhances the relevance of the negative samples but also ensures that the model's attention remains focused on the visual input, rather than being overly influenced by language priors. The vision-aware negative image projector is trained to produce perturbations that are semantically meaningful and visually plausible, thus providing a more robust mechanism for contrastive learning.

Extensive experimental evaluations on a variety of MLLMs and benchmark datasets have demonstrated the effectiveness of DCD in reducing hallucinations while preserving the model's general capabilities [3]. The method has been shown to increase the fine-grained details in generated captions and improve the overall quality of the outputs [19]. For instance, when applied to models like LLaVA-1.5 and Mini-Gemini, DCD consistently outperformed traditional contrastive decoding methods in terms of both hallucination reduction and caption quality metrics such as CIDEr. These results highlight the potential of DCD as a robust and versatile solution for enhancing the reliability and trustworthiness of MLLMs in multimodal tasks.

### 3.3.2 Multi-Frequency Perturbations MFP
Multi-Frequency Perturbations (MFP) is a novel method designed to address the issue of spurious bias in Multi-Modal Large Language Models (MLLMs) by leveraging both low-frequency and high-frequency features of images. MFP operates by partitioning an image into its high-frequency and low-frequency components, which are then processed separately to extract their corresponding features. The raw high-frequency and low-frequency features are obtained by applying a Fourier transform to the original image, followed by a visual encoder that produces the respective visual token sequences [3]. This approach ensures that the model can focus on both the fine-grained details and the broader structural elements of the image, thereby reducing the reliance on spurious correlations.

In the second step, MFP employs a fine-grained frequency feature fusion mechanism to integrate the high-frequency and low-frequency token sequences with the original visual token sequence [3]. This fusion is achieved using a cross-attention mechanism, which allows the model to dynamically weigh the importance of each frequency component. The cross-attention mechanism ensures that the final perturbed visual token sequences are a balanced representation of both the high-frequency and low-frequency features [3]. By applying a decay to the perturbations during inference, MFP effectively reduces the influence of redundant high-frequency and low-frequency features, leading to more robust and accurate visual feature representations.

Experimental results demonstrate that MFP significantly mitigates spurious bias in MLLMs, improving their performance on various benchmarks. The method's simplicity and cost-effectiveness make it a practical solution for enhancing the robustness of MLLMs in real-world applications [4]. By explicitly suppressing redundant frequency-domain features, MFP helps the model focus on the essential attributes of the input, thereby reducing the risk of spurious correlations and enhancing overall model reliability. This approach not only addresses the issue of spurious bias but also contributes to the broader goal of developing more transparent and trustworthy MLLMs.

### 3.3.3 Retrieval and Reasoning for Personalization R2P
Retrieval and Reasoning for Personalization (R2P) in multimodal large language models (MLLMs) is a critical component that enhances the model's ability to generate contextually relevant and personalized outputs [20]. R2P leverages retrieval-augmented generation (RAG) techniques to incorporate external knowledge from a knowledge base (KB) into the model's generative process [21]. This approach ensures that the generated content is grounded in factual information, reducing the likelihood of hallucinations and improving the overall accuracy and reliability of the model's responses [22]. By integrating RAG, MLLMs can dynamically retrieve and incorporate relevant information from a KB, which is particularly useful in domains such as medical diagnosis, where precision and accuracy are paramount.

The reasoning component of R2P is essential for aligning the retrieved information with the user's query and context. This involves a multi-step process where the model first identifies the key elements of the user's query, retrieves relevant information from the KB, and then reasons over this information to generate a coherent and contextually appropriate response. For example, in medical VQA, the model might retrieve radiology reports and clinical guidelines related to a specific condition and then reason over these to provide a detailed and accurate diagnosis. This reasoning process is often enhanced through techniques such as reinforcement learning from human feedback (RLHF), which helps the model learn to align its outputs with human preferences and reduce hallucinations.

To further improve the personalization and relevance of the generated content, R2P systems often employ advanced techniques such as prompt ensembling and reasoning-based prompting. Prompt ensembling involves combining multiple queries with different phrasings to take a majority vote, which can help mitigate the impact of individual query biases and improve the robustness of the model's responses [12]. Reasoning-based prompting, on the other hand, involves the model first describing the input (e.g., an image) before answering the query, which can reduce hallucination rates but may also lower object recognition accuracy [12]. These techniques, when combined with RAG and RLHF, enable MLLMs to generate highly personalized and accurate outputs, making them valuable tools in a variety of applications, from healthcare to customer service.

# 4 Evaluation and Benchmarking of MLLMs

## 4.1 Frameworks and Benchmarks

### 4.1.1 Instruction-Oriented Preference Alignment IPA
Instruction-Oriented Preference Alignment (IPA) is a framework designed to enhance the alignment of Multi-Modal Large Language Models (MLLMs) with human preferences through a systematic and scalable approach [6]. The core of IPA lies in its automated preference construction mechanism, which operates through three sequential stages: Sampling, Revision, and Selection. In the Sampling Stage, generator models are employed to produce multiple response candidates for a given instruction. This stage ensures a diverse set of potential outputs, which is crucial for capturing the nuances of human preferences.

At the Revision Stage, these sampled responses undergo a refinement process where they are evaluated and modified to better align with the intended instruction. This stage leverages both automated and human-in-the-loop methods to identify and correct errors, ensuring that the responses not only meet the criteria of the instruction but also adhere to broader standards of coherence and relevance. The Selection Stage involves the final curation of responses, where the most aligned and coherent outputs are chosen. This stage is critical for generating high-quality preference data that can be used to train and fine-tune MLLMs.

The contributions of IPA are significant in advancing the field of preference alignment. First, it provides a scalable framework for transforming existing instruction data into alignment signals, which can be used to steer the alignment of MLLMs towards improved general comprehension capabilities [6]. By leveraging diverse instruction data, IPA ensures that the alignment process is grounded in the model's ability to fulfill a wide range of instructions accurately and coherently. Second, IPA introduces an instruction-oriented verification paradigm that identifies core underlying factors through clear decision boundaries, enabling the model to distinguish between preferred and dispreferred responses. This paradigm enhances the model's ability to generalize and adapt to new instructions, ultimately leading to more robust and reliable performance in real-world applications.

### 4.1.2 Grounded Chain-of-Thought GCoT
Grounded Chain-of-Thought (GCoT) is a novel approach designed to enhance the reasoning capabilities of multi-modal language models (MLLMs) by grounding their reasoning processes in both visual and spatial information [2]. Unlike traditional chain-of-thought (CoT) methods that primarily focus on textual reasoning, GCoT integrates visual and spatial elements to provide a more comprehensive and accurate reasoning framework [2]. This is particularly important for tasks that require understanding and reasoning about visual content, such as chart interpretation and geometric problem-solving.

GCoT achieves this by systematically breaking down the reasoning process into multiple steps, each of which is grounded in specific visual and spatial attributes [2]. For instance, in a chart reasoning task, GCoT would first identify and extract key elements from the chart, such as the values and positions of data points, and then use this information to construct a step-by-step reasoning process. Each step in the GCoT is designed to be transparent and verifiable, ensuring that the model's reasoning is not only accurate but also interpretable. This approach significantly reduces the risk of hallucinations and ensures that the model's outputs are grounded in the input data [16].

To further enhance the effectiveness of GCoT, the method leverages a collaborative data filtering and generation pipeline. This pipeline involves using a smaller language model to filter out low-quality or irrelevant samples, followed by a more advanced model to generate high-quality reasoning processes. The generated reasoning steps are then refined and validated using a combination of automated and human evaluations. This multi-step process ensures that the GCoT data is both diverse and high-quality, making it an invaluable resource for training and evaluating MLLMs in complex reasoning tasks.

### 4.1.3 Semantics and MUltimodal Document Grounded Evaluation SMuDGE
In the realm of multimodal large language models (MLLMs), the evaluation of model outputs remains a significant challenge, particularly when these outputs are grounded in both textual and visual contexts [18]. To address this, we introduce Semantics and MUltimodal Document Grounded Evaluation (SMuDGE), a novel evaluation framework designed to assess the groundedness and semantic accuracy of MLLM outputs [23]. SMuDGE is unique in its ability to evaluate outputs based on their expected type—whether numeric, textual, or a hybrid of both—and to incorporate a multimodal grounding score that verifies the alignment of the output with the input context [23]. This grounding score is crucial for ensuring that the model's responses are not only semantically accurate but also contextually relevant, thus providing a more comprehensive evaluation metric.

The design of SMuDGE is modular and configurable, making it adaptable to various downstream applications. It consists of two primary components: a semantic similarity scorer and a multimodal grounding checker. The semantic similarity scorer evaluates the textual coherence and relevance of the model's output to the input query, using techniques such as cosine similarity and semantic role labeling. The multimodal grounding checker, on the other hand, assesses whether the model's output is correctly aligned with the visual elements of the input, such as images or videos, by verifying the spatial and contextual consistency of the generated content. This dual evaluation ensures that the model's responses are both linguistically and visually grounded, which is essential for tasks requiring precise and context-aware outputs.

To validate the effectiveness of SMuDGE, we conducted extensive experiments across a range of multimodal benchmarks, including visual question answering (VQA), multimodal classification, and document-grounded reasoning tasks. The results demonstrate that SMuDGE provides a more nuanced and accurate assessment of MLLM performance compared to traditional evaluation metrics. Specifically, SMuDGE highlights areas where models may excel in semantic understanding but fall short in multimodal grounding, or vice versa. This granular evaluation is particularly valuable for identifying and addressing specific weaknesses in MLLMs, thereby guiding future model improvements and ensuring that these models are more reliable and effective in real-world applications.

## 4.2 Data-Driven and Task-Specific Evaluations

### 4.2.1 TASKANYTHING and JUDGEANYTHING
The construction of TASKANYTHING and JUDGEANYTHING follows a systematic approach designed to evaluate and enhance the multimodal understanding and generation (MMU and MMG) capabilities of Multimodal Large Language Models (MLLMs) [12]. TASKANYTHING is a comprehensive benchmark that compiles open-ended any-to-any instructions from various existing benchmarks and datasets. This compilation ensures a wide range of tasks, from simple perceptual questions to complex reasoning problems, thereby providing a robust testbed for MLLMs [24]. The benchmark is further enriched through rigorous human annotation, ensuring the diversity and quality of the samples. Each task in TASKANYTHING is meticulously designed to cover different modalities and reasoning chains, making it a versatile tool for evaluating MLLMs' performance across various domains [25].

Building on TASKANYTHING, JUDGEANYTHING is introduced to assess the judging capabilities of MLLMs [25]. This benchmark focuses on the evaluation of model responses using human-annotated judgments and fine-grained checklists. The evaluation principles developed through a Human-MLLM collaborative process ensure that the assessment is both comprehensive and nuanced. JUDGEANYTHING employs two primary evaluation metrics: Score Evaluation and Pair Comparison. Score Evaluation assigns a numerical score to each model response based on predefined criteria, such as coherence, relevance, and logical consistency. Pair Comparison, on the other hand, involves comparing two model responses to determine which one better fulfills the task requirements. This dual approach provides a balanced and detailed evaluation of the MLLMs' performance, highlighting their strengths and weaknesses in different contexts [13].

To further enhance the reliability and utility of these benchmarks, a series of validation and filtering steps are applied. Initial model responses are subjected to rule-based filtering to correct obvious errors and inconsistencies. Manual inspection is then conducted to refine the samples, ensuring that the final dataset of 1,000 samples is of high quality and representative of the diverse tasks and reasoning chains required. TASKANYTHING and JUDGEANYTHING together form a robust framework for evaluating and improving the MMU and MMG capabilities of MLLMs, providing a valuable resource for researchers and practitioners in the field of multimodal AI [25].

### 4.2.2 3D Reasoning Segmentation
3D reasoning segmentation is a critical task in the domain of multimodal large language models (MLLMs), particularly when dealing with complex 3D scenes [26]. Traditional 2D reasoning models often struggle to generalize their capabilities to 3D environments due to the increased complexity and the need for spatial awareness. To address this challenge, recent research has focused on developing frameworks that can effectively transfer 2D reasoning capabilities to 3D scenarios. One notable approach involves the use of multiview pseudo segmentation masks, which are refined through a spatial consistency strategy to reduce hallucinations and improve the accuracy of 3D segmentations.

The proposed framework, MLLM-For3D, demonstrates significant advancements in 3D reasoning segmentation by integrating a novel alignment mechanism that binds token embeddings to specific queries [26]. This mechanism ensures that the model can accurately map visual features to their corresponding linguistic descriptions, thereby enhancing its ability to handle ambiguous queries. The framework is evaluated on three challenging benchmarks, where it achieves state-of-the-art performance, with an mIoU score approximately 55% higher than previous methods. This improvement is attributed to the model's robustness in handling complex 3D scenes and its ability to maintain consistency across multiple views.

To further validate the effectiveness of MLLM-For3D, a pilot study was conducted using the ScanNet++ dataset. The study involved processing different views of a 3D scene with an implicit language instruction, generating both reasoning responses and 2D segmentations [26]. The results showed that the model could effectively reason about the 3D environment and produce accurate segmentations without the need for explicit 3D annotations. This finding highlights the potential of MLLM-For3D in real-world applications, where 3D annotations are often costly and time-consuming to obtain. The framework's ability to leverage 2D reasoning capabilities and adapt them to 3D tasks opens new avenues for research and practical deployment in areas such as autonomous navigation and augmented reality.

### 4.2.3 Geometric Problem Solving with Symbolic Systems
Geometric problem solving (GPS) is a critical domain where the integration of symbolic systems with large language models (LLMs) and multimodal large language models (MLLMs) can significantly enhance reasoning capabilities [27]. Traditional LLMs, while adept at generating coherent and contextually relevant text, often struggle with the precise and structured nature of geometric problems. This is where symbolic systems, which are designed to handle formal logic and mathematical operations, come into play. Symbolic systems can provide a structured framework for geometric reasoning, enabling the systematic application of geometric theorems and axioms to derive solutions.

The combination of symbolic systems with LLMs allows for the creation of hybrid models that leverage the strengths of both approaches. Symbolic systems can handle the precise and deterministic aspects of geometric reasoning, such as the application of theorems and the manipulation of geometric objects, while LLMs can provide the necessary context and natural language understanding to interpret and generate problem statements and solutions [27]. This hybrid approach can effectively bridge the gap between human-like reasoning and computational capabilities, addressing the limitations of LLMs in handling complex geometric problems. For instance, a symbolic system can decompose a geometric problem into a series of logical steps, which the LLM can then interpret and explain in natural language, making the reasoning process more transparent and understandable.

However, the integration of symbolic systems with LLMs and MLLMs is not without challenges. One of the primary challenges is the data scarcity issue, as geometric problems often require a deep understanding of specific theorems and principles that may not be well-represented in the training data of LLMs. To overcome this, symbolic systems can be used to generate a wide range of potential geometric relationships from a single diagram, effectively expanding the training data. Additionally, the symbolic system can apply search algorithms to explore multiple solution paths, which can then be evaluated and refined by the LLM [27]. This collaborative approach not only enhances the robustness of the reasoning process but also improves the accuracy and reliability of the solutions generated.

## 4.3 Adaptive and Latency-Aware Evaluations

### 4.3.1 AdaLLaVA Adaptive Inference Framework
The AdaLLaVA Adaptive Inference Framework represents a significant advancement in the field of Multimodal Large Language Models (MLLMs) by addressing the critical challenge of adapting to varying computational budgets during inference [12]. This framework introduces a dynamic reconfiguration mechanism that allows the base MLLM to adjust its operations in real-time, ensuring optimal performance while adhering to strict latency constraints. By integrating this mechanism with token selection techniques, AdaLLaVA not only enhances the efficiency of the model but also maintains high levels of accuracy and reliability, making it a versatile solution for adaptive inference [4].

At the core of AdaLLaVA is a probabilistic modeling approach that incorporates hard latency constraints during the training phase. This ensures that the model learns to operate efficiently within predefined time limits, which is crucial for real-world applications where computational resources are often limited. During inference, AdaLLaVA dynamically reconfigures the operations of the base MLLM based on the input query and the available latency budget [4]. This adaptive mechanism allows the model to generate appropriate responses while minimizing performance degradation, even under stringent time constraints. The framework's ability to balance computational efficiency and performance is particularly valuable in scenarios where rapid response times are essential, such as in interactive systems and real-time applications.

Extensive experimental evaluations have demonstrated the effectiveness of AdaLLaVA in both training and inference phases. The framework has shown consistent performance improvements across a range of latency requirements, maintaining high accuracy and reducing the computational overhead. Additionally, the integration of AdaLLaVA with token selection techniques further enhances the model's efficiency, making it a robust solution for adaptive inference in MLLMs [4]. These findings highlight the potential of AdaLLaVA to significantly advance the capabilities of MLLMs in practical applications, paving the way for more efficient and responsive multimodal systems.

### 4.3.2 Systematic Review of LLM Reasoning Methods
In the realm of large language models (LLMs), systematic reviews of reasoning methods have become increasingly critical as these models are applied to complex tasks requiring multi-step logical processing [28]. Recent studies have highlighted the importance of Chain-of-Thought (CoT) reasoning, where LLMs are prompted to break down problems into smaller, manageable steps, thereby enhancing transparency and accuracy in their outputs [10]. This method not only improves the interpretability of LLMs but also allows for more precise error detection and correction. For instance, the use of CoT reasoning in visual-linguistic tasks has shown that LLMs can effectively integrate visual and textual information to perform tasks such as image captioning and visual question answering, where the reasoning process is crucial for generating coherent and contextually relevant responses [27].

However, the evaluation of LLM reasoning capabilities remains a significant challenge. Existing benchmarks, while incorporating reasoning questions, often lack the complexity needed to thoroughly assess the models' abilities. To address this, researchers have proposed more comprehensive benchmarks that include a variety of reasoning tasks, such as mathematical reasoning, hallucination understanding, and multi-image understanding [29]. These benchmarks aim to evaluate not just the final output but also the intermediate reasoning steps, ensuring that the models' decision-making processes are robust and logically sound. For example, the VLRMBench benchmark introduces a diverse set of tasks that require LLMs to reason about visual and textual data simultaneously, providing a more holistic assessment of their capabilities.

Moreover, recent advancements in LLM reasoning methods have focused on enhancing the models' ability to perform self-reflection and self-correction [30]. Techniques such as retrieval-augmented generation, where LLMs are provided with external knowledge sources to ground their reasoning in factual information, have shown promise in improving the accuracy and reliability of the models' outputs. Additionally, the integration of symbolic reasoning systems with LLMs has been explored to leverage the strengths of both approaches, combining the general knowledge and language capabilities of LLMs with the structured and rule-based reasoning of symbolic systems [27]. These hybrid methods aim to create more robust and versatile reasoning frameworks that can handle a wide range of complex tasks.

### 4.3.3 MCTS-Based Automated Critique Generation
The MCTS-based automated critique generation method represents a significant advancement in enhancing the interpretability and reliability of multimodal large language models (MLLMs) through fine-grained supervision [30]. This method leverages the Monte Carlo Tree Search (MCTS) algorithm to systematically explore and refine reasoning paths, generating high-quality critiques without the need for extensive manual annotations. The core of this approach is the multimodal actor–critic framework, where the actor model iteratively refines its reasoning process based on the feedback provided by the critic model [30]. The critic model, trained on the MCTS-based Multimodal Critique (MMC) dataset, is designed to identify reasoning errors and provide targeted corrections, thereby improving the overall performance of the actor model [30].

The MMC dataset is constructed through an automated process that simulates the MCTS algorithm to explore a wide range of reasoning paths and their potential errors. Each path is evaluated for correctness and logical consistency, and the results are used to generate critiques that highlight specific issues and suggest corrective actions. This automated generation of critiques not only reduces the dependency on manual annotations but also ensures a comprehensive coverage of potential reasoning errors. The tree-based organization of the MCTS algorithm allows for a detailed exploration of the reasoning space, enabling the identification of subtle errors that might be overlooked in a linear or heuristic-based approach. The iterative nature of the actor–critic interaction ensures that the actor model continuously learns from its mistakes, leading to a more robust and accurate reasoning process.

By integrating the MCTS-based critique generation method into the training and evaluation pipeline of MLLMs, this approach addresses the limitations of traditional manual annotation methods, which are often time-consuming and costly. The effectiveness of the critic model in identifying and correcting reasoning errors is demonstrated through extensive experiments, showing significant improvements in the performance of MLLMs across various tasks, including process understanding, outcome judgment, and critique generation. This method not only enhances the interpretability of MLLMs but also provides a scalable solution for fine-grained supervision, making it a valuable tool for the development and evaluation of advanced multimodal reasoning systems [10].

# 5 Enhancing MLLMs with Cognitive and Sensory Modalities

## 5.1 Vision-Enhanced Preference Optimization

### 5.1.1 Adaptive Vision-Enhanced Preference AdaViP
Adaptive Vision-Enhanced Preference optimization (AdaViP) represents a significant advancement in the integration of visual and linguistic preferences within multimodal large language models (MLLMs) [31]. By extending the traditional preference learning framework to incorporate vision-based preference pairs, AdaViP aims to enhance the model's ability to generate outputs that are both contextually relevant and aligned with human preferences [31]. The core idea behind AdaViP is to leverage visual cues to guide the model's attention towards key elements in the input, thereby improving the accuracy and relevance of the generated responses.

The AdaViP framework introduces a novel adaptive mechanism that dynamically balances the influence of visual and language-based preferences during the optimization process [31]. This is achieved through a dual-path architecture where the model simultaneously processes visual and textual inputs, allowing it to learn from both modalities in a coordinated manner. The adaptive component of AdaViP ensures that the model can adjust its focus based on the complexity and nature of the input, leading to more nuanced and contextually appropriate outputs. For instance, in scenarios where visual context is critical, such as image captioning or visual question answering, the model places greater emphasis on visual features, whereas in text-heavy tasks, it relies more on linguistic cues.

Experimental evaluations of AdaViP have demonstrated its effectiveness in enhancing the performance of MLLMs across various low-level vision tasks. The introduction of vision-based preference pairs has led to significant improvements in tasks such as object recognition, scene understanding, and visual question answering, where the model's ability to accurately interpret and respond to visual stimuli is crucial. Furthermore, the adaptive integration of preferences has shown to reduce the occurrence of hallucinations and improve the overall coherence and reliability of the generated outputs, making AdaViP a promising approach for advancing the capabilities of MLLMs in multimodal applications.

### 5.1.2 Reflective Perception RePer
Reflective Perception (RePer) is a novel perceptual mechanism designed to enhance the visual cognition capabilities of Large Vision-Language Models (LVLMs) [22]. Inspired by human cognitive processes, RePer employs a dual-model architecture consisting of a policy model and a critic model. The policy model is responsible for the initial perception of visual content, while the critic model evaluates the quality of this perception and provides feedback. This feedback loop allows LVLMs to iteratively refine their understanding of visual scenes, much like humans do through repeated observation and reflection [22]. The policy model generates initial perceptual outputs, which are then critiqued by the critic model. The critic model assesses the accuracy and completeness of these outputs, identifying areas for improvement. This process is repeated over multiple rounds, allowing the LVLM to gradually build a more precise and comprehensive understanding of the visual content.

RePer's dual-model architecture addresses a critical limitation of existing LVLMs, which often struggle with fine-grained visual perception and context understanding. By separating the perception and evaluation processes, RePer enables LVLMs to develop a more nuanced and detailed representation of visual scenes. The policy model focuses on capturing the initial visual features, while the critic model ensures that these features are accurately and comprehensively represented. This separation of concerns allows for a more systematic and iterative refinement of visual perception, leading to improved performance in tasks that require detailed and context-aware visual understanding. For instance, in visual question answering (VQA) tasks, RePer can help LVLMs better interpret complex scenes and provide more accurate and contextually relevant answers [18].

Comprehensive ablation studies have been conducted to evaluate the effectiveness of RePer across various dimensions, including data construction, training strategies, reflection rounds, and critic designs. These studies demonstrate that RePer significantly enhances the perceptual capabilities of LVLMs from both discriminative and generative perspectives. The results show that RePer can effectively generalize to a wide range of visual tasks, making it a robust and versatile framework for advancing multimodal perception [22]. The iterative refinement process enabled by RePer not only improves the accuracy of visual perception but also enhances the model's ability to handle complex and ambiguous visual scenes, thereby paving the way for more sophisticated and reliable visual assistants.

### 5.1.3 Prompt-Aware Multi-Instance Learning PaMi-VDPO
To address the limitations of traditional preference optimization methods in multimodal large language models (MLLMs), we propose Prompt-Aware Multi-instance Learning VDPO (PaMi-VDPO) [32]. This framework dynamically constructs multiple candidate responses for a given prompt, allowing the model to adjust its preference learning based on the context of the prompt while mitigating the impact of false-rejected noise. Unlike traditional methods that require costly manual annotation or preconstructed preference data, PaMi-VDPO operates in a close-to-far strategy, where it first generates a set of rejected candidates and then refines the preference learning process by focusing on the most relevant samples [32].

PaMi-VDPO employs a multi-instance learning (MIL) approach to integrate prompt-dependent augmentation selection into preference learning [32]. This method allows the model to dynamically select the most informative samples for optimization, while down-weighting less relevant or noisy samples. By doing so, PaMi-VDPO ensures that the model's learning process is more aligned with human preferences and less influenced by irrelevant or incorrect data. Importantly, PaMi-VDPO does not require any additional parameters or architectural modifications, making it a lightweight and efficient solution that can be easily integrated into existing MLLM training pipelines.

Experimental results on various benchmarks, including language-prior and hallucination tasks, demonstrate the effectiveness of PaMi-VDPO in improving the alignment and robustness of MLLMs. The framework's ability to dynamically adjust preference learning based on prompts and context leads to more accurate and contextually relevant outputs, thereby enhancing the overall performance and reliability of multimodal models in real-world applications. PaMi-VDPO's adaptive and efficient nature makes it a promising approach for advancing the capabilities of MLLMs in complex multimodal tasks.

## 5.2 User-Centric and Interactive Enhancements

### 5.2.1 User Surveys for Visual Assistants
User surveys for visual assistants are essential for understanding the practical challenges and user expectations in the deployment of multimodal large language models (MLLMs) as assistive technologies [33]. These surveys typically focus on evaluating the accuracy, reliability, and user-friendliness of visual assistants, which are critical for applications such as automated captioning systems and smart devices. The surveys often include a range of tasks, from object and face recognition to scene and video description, to gauge the assistant's performance across different visual contexts. Key findings from these surveys highlight the importance of context-awareness and the ability to handle diverse and complex visual environments, which are crucial for enhancing user satisfaction and trust in the technology.

One of the primary challenges identified through user surveys is the need for visual assistants to maintain a balance between language and visual understanding. Many existing MLLMs tend to overemphasize language preferences, leading to a neglect of key visual details [31]. This can result in inaccurate or irrelevant responses, particularly in scenarios where visual context is paramount. For instance, users often report that visual assistants fail to recognize subtle but important visual cues, such as the emotional state of individuals in images or the specific details of complex scenes. To address this, user surveys suggest the need for adaptive vision-enhanced preference optimization (AdaViP) methods that can dynamically balance the influence of visual and language-based preferences, ensuring that the assistant's responses are both contextually accurate and linguistically coherent [31].

Furthermore, user surveys emphasize the importance of iterative feedback and continuous improvement in the development of visual assistants. Users often reflect on their interactions with the assistant, providing valuable insights into areas that require enhancement, such as the assistant's ability to handle low-light conditions, recognize rare or unfamiliar objects, and understand the nuances of human interactions in visual content. By incorporating these user insights, researchers can refine the training data and algorithms used in MLLMs, leading to more robust and user-centric visual assistants. This iterative process is crucial for building systems that not only meet but exceed user expectations, ultimately driving the adoption and effectiveness of visual assistants in real-world applications.

### 5.2.2 Cognitive Architecture Integration
Cognitive architecture integration in multimodal large language models (MLLMs) is a critical aspect that enhances their ability to process and understand complex, real-world scenarios [12]. This integration involves the development of sophisticated mechanisms that allow MLLMs to leverage cognitive signals, such as eye tracking (ET) data, to improve their alignment with human cognitive processes. By incorporating these signals, MLLMs can better simulate human-like attention and perception, leading to more accurate and contextually relevant responses. For instance, ET data can guide the model to focus on specific regions of interest in images or videos, thereby reducing the likelihood of hallucinations and improving the overall quality of generated content.

One of the key approaches in cognitive architecture integration is the implementation of a reflective perception (RePer) mechanism, which mimics the human perception-feedback loop [22]. This mechanism involves a dual-model architecture, consisting of a policy model and a critic model. The policy model is responsible for initial perception tasks, such as object grounding and structure identification, while the critic model evaluates the accuracy and relevance of these perceptions. Through iterative refinement, the RePer mechanism enables MLLMs to gradually build a more precise and nuanced understanding of visual content. This approach not only enhances the model's ability to handle complex scenes but also improves its robustness against hallucinations and other forms of misalignment [16].

Another important aspect of cognitive architecture integration is the use of visual chain-of-thought (CoT) reasoning. This paradigm involves breaking down complex visual tasks into a series of fine-grained perceptual subtasks, each of which is executed sequentially. For example, a model might first identify objects in an image, then determine their spatial relationships, and finally synthesize a coherent description. By structuring the perception process in this way, MLLMs can more effectively manage the complexity of multimodal data and produce more accurate and contextually appropriate responses. This approach also facilitates the integration of cognitive signals, such as ET data, by allowing the model to dynamically adjust its focus based on the salience of different visual elements. Overall, these cognitive architecture integrations are essential for advancing the capabilities of MLLMs in real-world applications, particularly in domains requiring high levels of visual and linguistic understanding [20].

### 5.2.3 Zero-Shot Natural Multi-Modal HRI NVP-HRI
Zero-Shot Natural Multi-Modal Human-Robot Interaction (NVP-HRI) represents a significant advancement in the integration of multimodal inputs for enhancing the intuitiveness and efficiency of human-robot interaction (HRI). NVP-HRI leverages pre-trained Multimodal Large Language Models (MLLMs) to process and interpret a combination of voice and posture commands, enabling robots to perform complex tasks with minimal user training. The core of NVP-HRI lies in its ability to segment and infer unknown objects in real-time, a capability achieved through the integration of the Segment Anything Model (SAM) for point cloud inference and object recognition [34].

The NVP-HRI framework is designed to handle the dynamic and unpredictable nature of real-world environments by fusing parallel multi-modal inputs into a coherent control sequence. This fusion is critical for tasks that require precise and adaptive manipulation, such as object sorting, assembly, and navigation. By leveraging the natural language processing capabilities of MLLMs, NVP-HRI allows users to issue commands in a more intuitive and less structured manner, reducing the cognitive load and enhancing the overall user experience. The system's zero-shot learning capability ensures that it can adapt to new objects and scenarios without the need for extensive retraining, making it highly versatile and user-friendly [35].

To evaluate the effectiveness of NVP-HRI, extensive benchmarking was conducted against state-of-the-art HRI methods. The results demonstrated that NVP-HRI outperformed existing systems in terms of input speed, accuracy, and user satisfaction, particularly in scenarios involving unknown objects. The system's ability to rapidly process and act on multimodal inputs, combined with its robust zero-shot learning capabilities, positions NVP-HRI as a promising approach for advancing the field of HRI. The open-source release of the NVP-HRI framework is expected to facilitate further research and development in this area, potentially leading to more intuitive and efficient human-robot collaboration in various applications.

## 5.3 Sensory and Modality-Specific Enhancements

### 5.3.1 Human-Object Interaction Video Generation HOIGen-1M
Human-Object Interaction (HOI) video generation represents a critical yet challenging domain within multimodal large language models (MLLMs) [36]. The primary goal is to generate realistic and contextually accurate videos that depict interactions between humans and objects, a task that requires a deep understanding of both the visual and linguistic aspects of the content. To this end, we introduce HOIGen-1M, a large-scale and high-quality dataset specifically designed for HOI video generation [36]. This dataset comprises over 1 million video clips, each meticulously annotated to ensure the accuracy and richness of the interactions depicted.

HOIGen-1M addresses several limitations observed in existing datasets and models. Prior datasets often lack the scale and diversity needed to capture the full spectrum of human-object interactions, leading to suboptimal performance in real-world applications. Additionally, many existing models suffer from hallucination, where the generated content diverges from the actual visual input, undermining the reliability of the generated videos. By curating a vast and varied collection of video clips, HOIGen-1M provides a robust training ground for MLLMs to learn and generalize from a wide array of HOI scenarios. Each video clip in the dataset includes detailed annotations that describe the interactions, settings, and contextual elements, ensuring that the models can generate videos that are not only visually coherent but also semantically accurate.

The construction of HOIGen-1M involved a rigorous process to ensure the quality and relevance of the data. All video clips were manually verified to confirm the accuracy of the annotations and the clarity of the interactions. This meticulous curation process helps mitigate the risk of hallucination and ensures that the generated videos remain faithful to the input prompts. Initial experiments with state-of-the-art MLLMs, such as CogVideoX-5B, demonstrate significant improvements in HOI video generation when fine-tuned on HOIGen-1M. These results highlight the dataset's value in advancing the field and pave the way for more sophisticated and reliable HOI video generation models [36].

### 5.3.2 RGB-Thermal Image Pair Benchmark RGB-Th-Bench
RGB-Th-Bench represents a pioneering effort to establish a comprehensive benchmark for evaluating the performance of vision-language models (VLMs) in handling RGB-Thermal image pairs [37]. This benchmark is critical given the increasing relevance of thermal imaging in various applications, such as surveillance, security, and environmental monitoring. The dataset is meticulously curated to include a diverse range of scenes and conditions, ensuring that it captures the complexities and nuances inherent in real-world scenarios. Each image pair consists of a high-resolution RGB image and its corresponding thermal counterpart, annotated with detailed labels and descriptions to facilitate a thorough analysis of model performance.

The primary objective of RGB-Th-Bench is to assess the ability of VLMs to accurately understand and describe the content of both RGB and thermal images simultaneously [37]. Initial evaluations have revealed significant performance gaps, particularly in tasks that require cross-modal reasoning and alignment. Most models, despite their advanced capabilities in RGB-only tasks, struggle to generalize their understanding to the thermal domain. This discrepancy underscores the unique challenges posed by thermal imaging, such as variations in thermal signatures, occlusions, and the lack of color information. The benchmark also highlights the importance of developing specialized architectures and training methodologies that can effectively bridge the gap between RGB and thermal modalities.

To drive future research and development, RGB-Th-Bench provides a suite of evaluation metrics tailored to the specific requirements of RGB-Thermal tasks. These metrics include cross-modal alignment scores, semantic consistency measures, and error rates for object detection and classification. By offering a standardized framework for performance assessment, RGB-Th-Bench aims to foster innovation and collaboration within the research community. Ultimately, the benchmark serves as a foundational resource for advancing multimodal research and enhancing the practical applicability of VLMs in real-world RGB-Thermal applications [37].

### 5.3.3 Audio-Visual Understanding Enhancement
Audio-Visual Understanding Enhancement (AVUE) is a critical area of research within multimodal large language models (MLLMs), focusing on the integration of audio and visual modalities to improve the models' comprehension and response accuracy [1]. By leveraging both auditory and visual cues, AVUE aims to provide a more holistic and contextually rich understanding of the environment, which is essential for tasks such as audio-visual question answering (AVQA), source localization, and event localization. The integration of these modalities not only enhances the robustness of the models but also enables them to handle complex, real-world scenarios more effectively, where multiple sensory inputs are often intertwined.

To systematically evaluate and enhance the capabilities of MLLMs in AVUE, researchers have developed specialized benchmarks and datasets. One such benchmark, AVU-Bench, is designed to assess the performance of audio-visual large language models (AV-LLMs) across various tasks, including AVQA, AV source localization, and AV segmentation [1]. AVU-Bench emphasizes the importance of interaction between the audio and visual modalities, as shown in Fig [1]. 1 (b), and provides a comprehensive framework for evaluating the models' ability to align and integrate these modalities. This benchmark highlights the need for models to not only understand the content of each modality but also to effectively combine them to form a coherent and accurate representation of the scene.

In addition to benchmarking, several methods have been proposed to enhance the audio-visual understanding capabilities of MLLMs. One approach involves the use of adaptive vision-enhanced preference optimization (AdaViP), which extends traditional language-based preference pairs by incorporating vision-based preference pairs [31]. This method ensures that the model pays closer attention to key visual elements, leading to more accurate and contextually relevant outputs. Furthermore, the integration of dynamic noise assessment techniques allows the models to adapt their noise robustness during training, improving their performance in noisy or ambiguous environments. These advancements in AVUE are crucial for developing more reliable and user-friendly multimodal systems, particularly in applications such as human-robot interaction (HRI) and assistive technologies for individuals with sensory impairments.

# 6 Future Directions


Despite the significant progress made in reducing hallucinations in multimodal large language models (MLLMs), several limitations and gaps remain. Current techniques, while effective in many scenarios, often struggle to maintain robust performance across diverse and complex tasks. For instance, the integration of visual and linguistic information remains a challenge, particularly in tasks requiring fine-grained understanding and context-awareness. Additionally, the reliance on large-scale pre-training datasets can introduce biases and spurious correlations, leading to hallucinations in real-world applications. Moreover, the evaluation frameworks, although comprehensive, may not fully capture the nuanced aspects of model performance, especially in safety-critical domains such as medical diagnosis and autonomous systems.

To address these limitations, several directions for future research are proposed. First, there is a need to develop more sophisticated and adaptive training methods that can dynamically adjust to the complexity and variability of multimodal data. This could involve the integration of reinforcement learning (RL) techniques to optimize the model's behavior in real-time, ensuring that it remains aligned with the input data and user preferences. Additionally, exploring the use of unsupervised and semi-supervised learning approaches could help mitigate the reliance on large labeled datasets, reducing the risk of biases and spurious correlations.

Second, enhancing the model's fine-grained visual understanding is crucial for reducing hallucinations. This can be achieved through the development of advanced visual feature extraction and attention mechanisms that can focus on specific regions of interest and maintain a consistent alignment with the input data. Techniques such as dynamic focus (DyFo) and vision amplification fusion (VAF) can be further refined to improve the model's ability to handle complex and ambiguous visual scenes. Moreover, the integration of cognitive signals, such as eye-tracking data, can provide valuable insights into human attention patterns, guiding the model to focus on the most relevant visual elements.

Third, the development of more comprehensive and diverse evaluation frameworks is essential for a thorough assessment of MLLM performance. This includes the creation of benchmarks that cover a wide range of tasks and scenarios, including those that require multi-step reasoning and context-awareness. Additionally, incorporating user feedback and human-in-the-loop evaluations can help identify and address specific weaknesses in the models, ensuring that they are more reliable and user-friendly. The use of adaptive and latency-aware evaluation methods, such as the AdaLLaVA framework, can also enhance the efficiency and responsiveness of MLLMs in real-world applications.

The potential impact of the proposed future work is substantial. By addressing the current limitations and gaps, the next generation of MLLMs can achieve higher levels of reliability and accuracy, making them more suitable for safety-critical applications. Enhanced fine-grained visual understanding and robust evaluation frameworks will lead to more trustworthy and context-aware models, improving their performance in complex and diverse tasks. Furthermore, the integration of cognitive and sensory modalities will enable MLLMs to better simulate human-like perception and reasoning, paving the way for more intuitive and user-friendly assistive technologies. Overall, these advancements will contribute to the broader goal of developing more transparent and trustworthy AI systems, ultimately benefiting a wide range of industries and applications.

# 7 Conclusion



The survey of hallucination mitigation techniques in multimodal large language models (MLLMs) has provided a comprehensive overview of the current state-of-the-art methods. Key findings include the effectiveness of contrastive and autoregressive methods, such as Contrastive-Autoregressive Fine-Tuning (CAFe) and Learning to InstrucT (LIT), in enhancing the alignment of MLLMs with input modalities and reducing the likelihood of hallucinations. Training-free methods, including Dynamic Focus (DyFo) and Vision Amplification Fusion (VAF), have also shown promise in improving fine-grained visual understanding and reducing spurious correlations. Integrated and hybrid approaches, such as Decoupling Contrastive Decoding (DCD) and Multi-Frequency Perturbations (MFP), further enhance the reliability of MLLM outputs by combining multiple strategies. The survey also highlights the importance of retrieval and reasoning techniques, such as Retrieval and Reasoning for Personalization (R2P), in generating contextually relevant and personalized responses.

The significance of this survey lies in its comprehensive review of the techniques and methods developed to mitigate hallucinations in MLLMs. By providing a detailed analysis of both training-based and training-free approaches, the survey serves as a valuable resource for researchers and practitioners in the field. The introduction and evaluation of various frameworks and benchmarks, such as Instruction-Oriented Preference Alignment (IPA) and Grounded Chain-of-Thought (GCoT), emphasize the need for systematic and rigorous testing to ensure the robustness and reliability of MLLMs. These evaluation tools are essential for advancing the development of MLLMs and addressing the challenges of hallucination in safety-critical applications.

In conclusion, the field of hallucination mitigation in MLLMs is rapidly evolving, with a growing body of research and innovative methods being developed. The findings from this survey underscore the importance of continued research and development to enhance the reliability and performance of MLLMs. We call on the research community to further explore the integration of cognitive and sensory modalities, such as vision-enhanced preference optimization and user-centric enhancements, to create more robust and user-friendly models. Additionally, the development of more comprehensive and diverse benchmarks is crucial for evaluating the real-world applicability of these models. By addressing these areas, we can pave the way for more advanced and trustworthy MLLMs that can be effectively deployed in a wide range of applications.

# References
[1] Aligned Better, Listen Better for Audio-Visual Large Language Models  
[2] Grounded Chain-of-Thought for Multimodal Large Language Models  
[3] Mitigating Object Hallucinations in MLLMs via Multi-Frequency  Perturbations  
[4] Learning to Inference Adaptively for Multimodal Large Language Models  
[5] Decoupling Contrastive Decoding  Robust Hallucination Mitigation in  Multimodal Large Language Model  
[6] Instruction-Oriented Preference Alignment for Enhancing Multi-Modal  Comprehension Capability of MLL  
[7] DyFo  A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in  Fine-Grained Visual Underst  
[8] Vision-Amplified Semantic Entropy for Hallucination Detection in Medical  Visual Question Answering  
[9] Training-Free Personalization via Retrieval and Reasoning on  Fingerprints  
[10] The Future of MLLM Prompting is Adaptive  A Comprehensive Experimental  Evaluation of Prompt Enginee  
[11] CAFe  Unifying Representation and Generation with  Contrastive-Autoregressive Finetuning  
[12] Seeing What's Not There  Spurious Correlation in Multimodal LLMs  
[13] Learning to Instruct for Visual Instruction Tuning  
[14] TARAC  Mitigating Hallucination in LVLMs via Temporal Attention  Real-time Accumulative Connection  
[15] Exploring Hallucination of Large Multimodal Models in Video  Understanding  Benchmark, Analysis and  
[16] Patch Matters  Training-free Fine-grained Image Caption Enhancement via  Local Perception  
[17] ClearSight  Visual Signal Enhancement for Object Hallucination  Mitigation in Multimodal Large langu  
[18] Identifying Multi-modal Knowledge Neurons in Pretrained Transformers via  Two-stage Filtering  
[19] The Power of Context  How Multimodality Improves Image Super-Resolution  
[20] Integrating Cognitive Processing Signals into Language Models  A Review  of Advances, Applications a  
[21] One Pic is All it Takes  Poisoning Visual Document Retrieval Augmented  Generation with a Single Ima  
[22] Perception in Reflection  
[23] Where is this coming from  Making groundedness count in the evaluation  of Document VQA models  
[24] Chain of Functions  A Programmatic Pipeline for Fine-Grained Chart  Reasoning Data  
[25] Judge Anything  MLLM as a Judge Across Any Modality  
[26] MLLM-For3D  Adapting Multimodal Large Language Model for 3D Reasoning  Segmentation  
[27] Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via  Symbolic-Neural Integration  
[28] Applications of Large Language Model Reasoning in Feature Generation  
[29] VLRMBench  A Comprehensive and Challenging Benchmark for Vision-Language  Reward Models  
[30] MMC  Iterative Refinement of VLM Reasoning via MCTS-based Multimodal  Critique  
[31] AdaViP  Aligning Multi-modal LLMs via Adaptive Vision-enhanced  Preference Optimization  
[32] PaMi-VDPO  Mitigating Video Hallucinations by Prompt-Aware  Multi-Instance Video Preference Learning  
[33] Evaluating Multimodal Language Models as Visual Assistants for Visually  Impaired Users  
[34] NVP-HRI  Zero Shot Natural Voice and Posture-based Human-Robot  Interaction via Large Language Model  
[35] LRSCLIP  A Vision-Language Foundation Model for Aligning Remote Sensing  Image with Longer Text  
[36] HOIGen-1M  A Large-scale Dataset for Human-Object Interaction Video  Generation  
[37] RGB-Th-Bench  A Dense benchmark for Visual-Thermal Understanding of  Vision Language Models  