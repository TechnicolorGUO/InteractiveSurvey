# 5/1/2025, 6:04:23 PM_Hallucination in Multimodal Foundation Models  

# 0. Hallucination in Multimodal Foundation Models  

# 1. Introduction  

Foundation Models (FMs), characterized as large AI models trained on extensive unlabeled data using self-supervised learning, have demonstrated remarkable performance across a diverse range of tasks, including image classification, natural language processing, and complex creative endeavors [1,4,20]. Among these, Multimodal Foundation Models (MMFMs), such as Multimodal Large Language Models (MLLMs) and Large Vision-Language Models (LVLMs), represent a significant advancement by integrating and processing information from multiple modalities like text, vision, and audio [7,8,9]. These models exhibit capabilities approaching human-level cognition and learning, opening novel possibilities for Artificial General Intelligence (AGI) and possessing broad application prospects in areas such as image, video, and audio understanding [7,9].​  

Despite their impressive capabilities, MMFMs, like their unimodal counterparts, are plagued by a critical issue known as "hallucination" [7,8]. In the multimodal context, hallucination refers to the generation of content that appears plausible and is confidently asserted by the model but is inconsistent with the provided input data, established world knowledge, or realworld facts [9,13,14]. This phenomenon is not indicative of intentional deception but rather arises inherently from the model's architecture and training data, where models learn patterns that can lead to the generation of fictitious, misleading, or entirely fabricated details [1,4,14,20]. For instance, LVLMs may generate object descriptions that are not present in the visual input, a specific type of object hallucination [3,8,10].  

The prevalence and potential consequences of hallucination make it a significant concern for the widespread and safe deployment of MMFMs [7,9,13,18]. Hallucinated content directly affects the accuracy and reliability of model outputs [7], undermining user trust [5,6,11,12,22]. In critical applications such as healthcare, finance, or legal services, where factual accuracy is paramount, hallucinations can lead to severe real-world consequences, including lawsuits or penalties, as illustrated by instances involving fabricated legal cases [2,4,11]. The generation of false or misleading information also facilitates the spread of misinformation [9]. While MMFMs promise increased efficiency, the necessity to verify AI-generated content due to hallucinations can diminish these gains [11]. The demonstrated performance on datasets specifically designed to expose hallucinations, such as the AutoHallusion framework showing only $6 6 . 0 \%$ accuracy on models like GPT4V, Gemini, and Claude, further underscores the severity of this problem in current models [10].  

Addressing multimodal hallucination presents substantial challenges. Current research on evaluating LVLM hallucination is limited [8]. The development of comprehensive evaluation standards and benchmarks capable of assessing hallucinations across different modalities is crucial but complex [1,7,18]. Furthermore, the lack of large, representative datasets specifically designed for hallucination research hinders progress in both detection and mitigation efforts [10]. The intrinsic nature of how these models learn and generate content also makes root cause analysis and the development of universally effective mitigation strategies difficult [2].​  

This survey aims to provide a comprehensive overview of the phenomenon of hallucination in Multimodal Foundation Models. We begin by formalizing the definition and categorization of multimodal hallucinations. Subsequently, we explore the potential causes underlying these issues. The survey then delves into existing methods and proposed benchmarks for detecting and evaluating hallucinations in MMFMs. Finally, we review current strategies for mitigating hallucinations and discuss promising avenues for future research in this critical area. By synthesizing recent advancements and identifying key challenges, this survey contributes to a better understanding of multimodal hallucination and motivates further research towards building more reliable and trustworthy MMFMs.  

# 2. Background on Multimodal Foundation Models  

Foundation Models (FMs) represent a significant advancement in artificial intelligence, typically trained via self-supervised learning on extensive, unlabeled datasets and capable of adaptation to a wide range of downstream tasks [1]. Generative AI, a subset of AI, focuses specifically on creating novel content that resembles human-generated output [22]. Building upon the success of large language models (LLMs), such as ChatGPT, research has expanded to develop models that can process and integrate information from multiple modalities, leading to the emergence of Multimodal Foundation Models (MMFMs) [8,22]. These models aim to perform tasks requiring the joint understanding and generation across different data types, including text, images, audio, and video [1,17].​  

The fundamental architecture of MMFMs, particularly Multimodal Large Language Models (MLLMs) or Large Vision Language Models (LVLMs), typically involves combining modality-specific encoders with powerful LLMs [8,21]. For instance, LVLMs often integrate visual encoders, capable of processing image or video data, with pre-trained LLMs [8,21]. Information from different modalities is fused and processed through various mechanisms. Common approaches include aligning the embeddings generated by modality encoders with the embedding space of the LLM or employing attention mechanisms to facilitate interaction between representations from different modalities [21]. Some models, like LLaVA-Mini, explore efficient fusion strategies such as using a single vision token to represent the entire visual input, thereby reducing computational complexity [17]. Other architectures might leverage visual experts integrated with LLMs like ChatGPT, while others focus on aligning visual tokens derived from visual encoders with pre-trained LLMs [8].​  

Training methodologies for MMFMs often involve pre-training on large-scale multimodal datasets, such as paired visual-text data, enabling modality encoders to achieve strong zero-shot generalization capabilities [1,21]. A crucial training technique is instruction tuning, which fine-tunes the models on datasets of instructions and desired outputs to make them more responsive and sensitive to human prompts [8,22]. This process helps align the model's output with specific user requirements and diverse interaction styles [8]. Strategies to enhance performance through training include increasing the diversity of instructions and constructing larger instruction fine-tuning datasets [8].​  

MMFMs have demonstrated versatility across numerous applications. Examples include image captioning (implied by the focus on visual output description), visual question answering (VQA), content generation, autonomous driving, and robotics [10]. In bioinformatics, transformer-based foundation models, including specialized textual models like BioBERT and BioGPT, are applied to sequence and non-sequence data analysis [19]. Other applications encompass image restoration, video understanding, cross-modal retrieval, open-vocabulary object detection (e.g., LaMI-DETR leveraging VLMs like CLIP), reasoning video object segmentation (e.g., VISA utilizing multimodal LLMs), and seamless speech interaction [17,21]. The development of universal multimodal embeddings, such as those proposed by E5-V, further highlights the potential of MMFMs in representing complex multimodal inputs for various tasks [21].  

Despite their impressive capabilities, developing and scaling MMFMs presents significant challenges. A key difficulty lies in the effective integration and alignment of different modalities [21]. Furthermore, MMFMs, particularly LVLMs, frequently exhibit the phenomenon of "hallucination," where they generate information not supported by the input data [3,10]. This issue can manifest as generating descriptions of non-existent objects or omitting present features in visual inputs, termed object hallucination [3]. Hallucination in LVLMs is inherited from limitations in both LLMs (e.g., incorrect knowledge from training data) and visual language models (e.g., challenges in accurately representing visual information) [8]. A significant contributing factor is the language module's tendency to over-rely on its vast prior knowledge, sometimes leading it to disregard or misinterpret the visual input [10]. This can lead to misleading outputs in critical applications like robotics and medical imaging [3]. Addressing hallucination is compounded by the challenge and expense of manually creating comprehensive hallucination cases and benchmarks for evaluation and fine-tuning [10].  

# 3. Defining Hallucination in Multimodal Models  

Hallucination in Multimodal Foundation Models (MMFMs) is a critical phenomenon characterized by the generation of content that appears plausible or convincing but contradicts the input data, established real-world knowledge, or logical reasoning [2,4,5,6,7,8,9,10,11,12,13,14,15,18,22]. Unlike simple factual errors or grammatical mistakes, hallucination involves the confident assertion of untruthful information, making it particularly insidious and difficult for users to detect [11,12,13]. It represents a significant deviation from factual reality or fidelity to the provided input or context [2,14,18].  

Due to the diverse ways MMFMs can fail to generate accurate or grounded outputs, researchers have proposed various taxonomies to categorize hallucinations [2,8,9,12,13]. These categorizations help in understanding the underlying causes and developing targeted mitigation strategies. Commonly identified types in the literature include Object Hallucination,  

Attribute Hallucination, Factual Hallucination, Relationship Hallucination, and Multimodal Conflict Hallucination, among others like Faithfulness and Bias hallucinations.  

Factual Hallucination occurs when a model generates content that contradicts verifiable real-world facts, historical records, or established knowledge [2,6,11,13,14]. This can range from asserting untrue facts confidently instead of admitting ignorance to fabricating information entirely ("无中生有") [6,12]. Examples include providing incorrect dates, attributing achievements to the wrong individuals [13,14], making errors in domains requiring precise factual recall (e.g., legal questions) [5], or being misled by untruthful context provided in the input [17].  

Object Hallucination, particularly prevalent in Vision-Language Models (VLMs), involves generating descriptions of objects that are not present in the visual input or omitting essential objects that are present [3,8,10]. This directly violates the constraint posed by the visual modality.  

Attribute Hallucination relates to generating incorrect descriptions of the properties or attributes of objects present in the input, such as misidentifying the color, size, or state of an object [7,9]. While sometimes discussed as part of Object Hallucination or Multimodal Conflict, it represents a distinct type of error focused on properties rather than existence.  

Relationship Hallucination pertains to errors in describing or understanding the spatial, temporal, or logical relationships between different entities or objects within the multimodal input [10,12]. This can manifest as misrepresenting how objects are positioned relative to each other in an image or failing to correctly infer implications from changes in a visual scene [10,21].​  

Multimodal Conflict Hallucination is a broad category specifically addressing issues arising from the interaction between different modalities. It occurs when the model's output is inconsistent with information presented in one or more of the input modalities [7,9]. This can overlap with Object and Attribute hallucinations when language output conflicts with visual input. Other related phenomena include Unimodal Dominance Hallucinations, where the model overly relies on one modality (e.g., language priors in LVLMs) even when contradicted by others [7,10], and hallucinations stemming from spurious correlations learned between modalities during training [7].​  

Beyond these core types, other forms of hallucination like Faithfulness Hallucination describe output inconsistent with user instructions or internal logic [6,11], while Bias Hallucinations arise from the model's data-driven biases, leading to prejudiced or stereotypical outputs [5,6]. Understanding this taxonomy is essential for comprehensively evaluating and addressing the challenges posed by hallucinations in MMFMs across various applications.​  

# 3.1 Types of Hallucinations  

Hallucinations in multimodal foundation models manifest in various forms, reflecting deviations from factual accuracy, consistency with input, or unbiased representation [3,9,13,18]. Researchers have proposed different categorizations based on the nature of the error, the modality involved, or the task context [2].  

<html><body><table><tr><td>Type</td><td>Description /Nature</td></tr><tr><td>Factual Hallucination</td><td>Contradicts verifiable real-world facts, fabricates information,asserts untrue facts.</td></tr><tr><td>Faithfulness Hallucination</td><td>Inconsistent with user instructions,context, or internal logic.</td></tr><tr><td>Bias Hallucinations</td><td>Output skewed by data biases, introducing prejudice or stereotypes.</td></tr><tr><td>Modal Conflict Hallucination</td><td>Output conflicts with information from other input modalities (e.g., text vs.image).</td></tr><tr><td>Unimodal Dominance Hallucination</td><td>Over-reliance on one modality (e.g., language) even when contradicted by others.</td></tr><tr><td>Perceptual Hallucinations</td><td>Errors in generating visual details (e.g., incorrect body parts in generated images).</td></tr><tr><td>Relationship Hallucinations</td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Errors in describing spatial, temporal,or logical relationships between entities.</td></tr></table></body></html>  

Despite varying terminologies, several core types are commonly identified across the literature.  

One prevalent type is Factual Hallucination, characterized by the generation of content that contradicts verifiable realworld facts or fabricates information confidently [11,14]. This includes asserting untrue facts instead of admitting ignorance [6], fabricating information ("无中生有") which has been observed as a dominant type in some studies [12], or providing incorrect factual details such as historical dates or attributing achievements to the wrong individuals [13,14]. Factual hallucinations can also arise from the model being misled by untruthful context [17] or by contradicting established factual knowledge (Factual Conflict Hallucination) [9]. Examples include large language models making errors in legal questions by failing to understand precedential relationships [5] or generating narratives in image-to-text tasks that deviate from actual content by introducing irrelevant facts [9].​  

Another significant category is Faithfulness Hallucination, where the generated output is inconsistent with the user's instructions, the provided context, or exhibits internal logical contradictions [11]. This type encompasses ignoring translation requests (instruction inconsistency), contradicting context provided in the prompt (context inconsistency), or presenting logically flawed solutions (logical inconsistency) [11]. In multimodal contexts, this extends to inconsistency with the image description or failure to provide consistent answers when presented with varying levels of supplementary information (Consistency) [10]. The failure of Large Vision-Language Models (LVLMs) to generate outputs consistent with visual content or instructions also falls under this category [21]. Logical fallacies inherent in the model's reasoning process can also lead to incorrect answers [6].  

Bias Hallucinations occur when model outputs are skewed by the prevalence of certain data patterns or introduce subjective assumptions not present in the input, leading to erroneous or prejudiced results [5,6,21]. A notable example is the generation of biased images based on text prompts, such as depicting specific ethnicities or genders in historically inaccurate or stereotypical roles [5]. This can manifest as data-driven bias skewed by training data or subjective bias introduced by the model [6,21].  

Specific to multimodal models are hallucinations related to the interaction and integration of different modalities. Modal Conflict Hallucination arises when the model generates outputs that conflict with information from other modal inputs [7,9]. This can include incorrect descriptions of objects, attributes (like color), or scene text based on visual input. This type is often linked to limitations in achieving fine-grained alignment between modalities [9]. A related phenomenon is overreliance on unimodal priors, leading to Unimodal Dominance Hallucinations where the model excessively favors information from one modality (language, vision, or audio) even when contradicted by others [7]. This can also involve hallucinations based on spurious inter-modal correlations learned during training, such as hallucinating a visual element based on a commonly associated audio cue [7].  

Furthermore, multimodal models are susceptible to Perceptual and Relationship Hallucinations. Perceptual errors include generating visual details that are incorrect, such as drawing hands with the wrong number of fingers or depicting subjects with missing or extra limbs [13]. Relationship hallucinations involve errors in understanding or describing the spatial relationships or interactions between objects in a scene [10]. Detection frameworks specifically target hallucinations related to object existence and spatial relationships within images [10]. Metrics evaluating a model's ability to infer the implications of changes in a scene also relate to assessing understanding of relationships [21].​  

These hallucination types manifest distinctly across various multimodal tasks. In Visual Question Answering (VQA) and image captioning, LVLMs may exhibit object hallucinations by describing objects not present in the target image [2]. Text-toimage models can produce visuals inconsistent with factual knowledge in the text prompts [9]. Summarization systems can suffer from intrinsic hallucinations distorting source information or extrinsic hallucinations adding unverifiable details [2]. Dialogue systems may produce uncooperative or incorrect responses [2]. Question Answering systems face issues with incomplete or incorrect answers due to deficiencies in knowledge or reasoning [2]. Machine translation can show deviations from the target language or over-generation [2]. Knowledge graph generation can result in redundant or factually incorrect details [2]. The characteristics unique to each type highlight distinct failure modes: factual hallucinations represent a break from external reality, faithfulness hallucinations indicate a failure to adhere to constraints or internal logic, bias hallucinations reveal harmful societal prejudices encoded in data, and modal conflicts underscore challenges in robust cross-modal understanding and fusion.​  

# 4. Causes and Contributing Factors  

The phenomenon of hallucination in multimodal foundation models (MMFMs) is a complex issue stemming from a confluence of factors spanning different stages of model development and application.  

![](images/95c951b51c3e19e6a5af8f2328278b2402f6104e65a982a1cb1ce87cc3d830dd.jpg)  

These underlying causes can be broadly categorized into data‐related issues, limitations inherent in model architecture, specifics of the training process and objectives, and challenges encountered during inference and prompting [7,11,15]. Understanding these distinct yet often interrelated contributors is crucial for developing effective mitigation strategies.  

A primary source of hallucination lies within the characteristics and quality of the training data [6,12]. Data biases, whethe inherited from human sources or arising from collection methodologies, can lead models to generate skewed or discriminatory outputs [5,11,15,22]. Furthermore, issues such as data incompleteness, contradictions, untruthful information, and the presence of fictional content in training sets contribute to models generating factually incorrect or nonsensical outputs [2,6,13,14,17]. A particularly salient data‐related factor in MMFMs is the existence of spurious correlations, both within and across modalities. Models may learn to associate objects or concepts based purely on their co‐occurrence frequency in the training data, rather than genuine semantic or visual relationships [7,10,11,15].  

Model architecture also imposes inherent limitations that can contribute to hallucinations. The probabilistic and amorphous nature of large models, coupled with a lack of explicit logical reasoning frameworks, can lead to increased randomness and conjectural outputs [5]. Specific architectural choices, such as residual connections or limitations in attention mechanisms, may introduce noise or hinder the model's ability to capture fine‐grained details and complex contextual dependencies [11,21].  

The procedures and objectives employed during model training significantly shape its propensity for hallucination [1,7,11,17]. Imperfect training objectives, exposure bias, and an over‐reliance on statistical prediction without robust world knowledge can lead to the generation of plausible but incorrect information [2,11,14]. High model complexity without adequate regularization can result in overfitting, while insufficient learning leads to underfitting, both compromising accuracy [13,15]. Data compression techniques during training can also result in models imperfectly "filling in the blanks" [6]. Critically, in multimodal contexts, training can lead to an over‐reliance on unimodal priors, where the model prioritizes information from one modality (e.g., language) over others (e.g., vision), especially when faced with conflicting signals [7,10].​  

Finally, issues arising during the inference stage, including decoding strategies and prompt design, play a direct role in eliciting hallucinations [11]. Randomness in decoding methods, such as sampling strategies, can introduce errors and lead to fabrications [2,11]. A notable phenomenon is the snowballing effect, where initial inaccuracies compound during autoregressive generation [2,3]. Model confidence levels and biases towards generating affirmative responses, even when uncertain, can pressure models to fabricate information [8,14]. Furthermore, models exhibit significant sensitivity to prompts [6,12]. Queries about topics outside the training distribution, ambiguous inputs, or prompts designed to test specific behaviors can trigger hallucinatory responses [10,14]. Prompt engineering techniques, such as incorporating clear constraints or encouraging staged thinking, are actively explored to mitigate these inference‐time hallucinations [6,12]. The interplay of these factors underscores the intricate nature of hallucination, requiring multifaceted approaches for understanding and mitigation.  

# 4.1 Data-Related Factors  

The characteristics and quality of the data used for training Multimodal Foundation Models (MMFMs) significantly influence their propensity for generating hallucinations. Issues such as data biases, noise, and spurious correlations can directly lead models to produce outputs that are factually incorrect, nonsensical, or discriminatory [6,7,12,15].  

One primary factor is the presence of biases within the training data [5,11,13,15,22]. These biases, often reflecting human perspectives and prejudices [5], can be inherited by the model, resulting in skewed or discriminatory outputs [5,22]. A notable example cited is the Gemini model generating racially biased images, illustrating how biases in data or model design manifest as problematic outputs [5]. Furthermore, biases can arise from how models learn from input context or from challenges in handling low-resource languages in multilingual models [2]. Specific to certain models, a "binding" phenomenon has been observed where AI assistants disproportionately cite content from platforms affiliated with their developers, introducing a source-specific bias into the generated information, such as Doubao favoring Toutiao and Douyin Baike content, Tencent Yuanbao leaning towards WeChat public accounts, and Wenxin Yiyan frequently referencing Baijiahao and Baidu Baike [12].  

Data quality issues, encompassing incompleteness, contradictions, and outdated information, also contribute substantially to hallucinations [6,13]. Datasets that do not cover all possible information may force models to guess in less-represented areas [14]. Conflicting information present within the training data or on the internet can confuse models, leading to inconsistent or contradictory outputs [6,14]. The inclusion of fictional content, such as novels, can blur the lines between fact and fiction for the model, causing it to present speculative information as factual [14]. Moreover, the presence of untruthful information in training or input contexts can directly mislead the model, causing it to incorporate these falsehoods into its generated text [17]. When relevant, accurate knowledge is unavailable, models may resort to basic heuristics based on factors like term frequency, potentially generating plausible but incorrect responses. The challenge of quality assurance for vast and dynamic datasets means models are often trained on outdated or unreliable sources, hindering accurate knowledge acquisition [2,6,11,13].​  

Spurious correlations within the training data represent another critical cause of hallucinations [11,15,21]. Models may learn to rely on surface patterns and shortcuts rather than performing rigorous fact-checking or logical consistency checks [21]. This over-reliance on statistical associations can lead models to incorrectly identify non-existent patterns or features [15]. In the context of MMFMs, particularly Large Visual Language Models (LVLMs), spurious inter-modal correlations are a significant issue [2,7]. These arise from: (1) Global Occurrence Frequency, where the frequent appearance of certain objects or events in the dataset leads the model to generate them even when absent in the specific input; and (2) Co-occurrence Frequency, where objects or events frequently appearing together during training cause the model to predict the existence of one based solely on the presence of the other [7]. For instance, if "grass" and "sky" often appear together in training data, the model may develop a spurious association and generate descriptions mentioning both even if only grass is visible [3]. LVLMs have been observed to hallucinate common objects specifically because they frequently co-occur in visual instruction datasets [2]. Over-reliance on these spurious correlations can prevent models from recalling accurate knowledge, especially in complex or long-tail scenarios [11]. Real-world datasets, with their inherent complexity in object semantic relationships, can be particularly effective at exposing models' susceptibility to such spurious correlations, leading to hallucinations [10]. Thus, the quality, comprehensiveness, and correlation structure of the training data are fundamental determinants of hallucination frequency and severity in MMFMs.​  

# 4.2 Model and Training-Related Factors  

The intrinsic characteristics of model architecture and the specifics of the training process are fundamental contributors to the phenomenon of hallucination in multimodal foundation models. Architectural limitations can constrain a model's ability to accurately perceive, interpret, and generate content. For instance, inherent complexities and the absence of explicit logical frameworks in large language models (LLMs) may lead to increased randomness or conjectural outputs, as they lack transparent rules of inference for reliable conclusions [5]. Specific architectural choices, such as residual connections in models like CLIP, have been identified as potential sources of noise that degrade performance, particularly in tasks requiring fine-grained detail like segmentation [21]. Furthermore, inadequate unidirectional representation and limitations in attention mechanisms can hinder a model's capacity to capture complex contextual dependencies across modalities or within sequences [11]. General-purpose architectures may also lack the domain-specific knowledge or reasoning capabilities required for specialized tasks, implicitly leading to hallucination when applied outside their core expertise [19].​  

Training objectives and regularization techniques significantly influence a model's susceptibility to hallucination. Imperfect training objectives and exposure bias, where the model only sees ground truth during training and not its own potentially erroneous outputs, can result in overconfidence and the generation of inaccurate content [11]. Models trained primarily on statistical prediction based on data, without developing a true understanding of the real world, are prone to generating seemingly plausible but factually incorrect outputs if the training data contains biases or inaccuracies [14]. High model complexity, when not adequately managed through regularization, can lead to overfitting, where the model learns noise and irrelevant patterns, causing errors on new data [13,15]. Conversely, underfitting, resulting from a failure to learn underlying patterns, also leads to a lack of accuracy and depth [13]. The process of compressing training data, where  

models store mathematical representations rather than verbatim inputs, can reduce fidelity and cause models to inaccurately "fill in the blanks" [6]. In the context of multimodal learning, an over-reliance on unimodal priors—depending too heavily on knowledge from a single modality—is a critical training-induced factor contributing to hallucinations when the model fails to effectively integrate information from other modalities [7].​  

Specific training strategies, such as pre-training and fine-tuning, also play a crucial role. Instruction tuning, a common finetuning approach aimed at aligning models with human instructions, can inadvertently create deviations between the training and target data distributions, contributing to hallucination [8]. Knowledge gaps and inconsistencies often arise from differences in input formats or data characteristics between the pre-training and fine-tuning stages [2]. Models may struggle to effectively match questions with stored knowledge, facing challenges in balancing memorized information with retrieved evidence; ignoring retrieved evidence can introduce biased model knowledge, while incorrect overrides can disrupt behavior [2]. In transfer learning processes, such as adapting vision-language models to detection tasks, knowledge can become biased towards base categories learned during pre-training, leading to overfitting on these specific concepts [21]. Misalignment during the final alignment phase, if not carefully managed, can train the model to generate content beyond its actual knowledge boundaries or that conflicts with its learned internal representations [11]. The uncertainty inherent in the model's generation process, such as the iterative prediction using beam search, is also linked to hallucination, with tokens generated with higher uncertainty being more likely to be inaccurate.​  

Comparing different model and training approaches reveals trade-offs. While increased model size can enhance capabilities, it may also correlate with increased randomness without a corresponding increase in logical coherence [5]. Specialized models and training strategies are proposed as a means to overcome the limitations of general-purpose models in specific domains [19]. Different training objectives, like the contrastive training in CLIP focusing on global features, may be effective for certain tasks but detrimental for others requiring local distinguishability [21]. The transition from pre-training to finetuning presents distinct challenges, including managing distribution shifts and knowledge inconsistencies, which require careful design of the adaptation strategies [2,8]. Ultimately, even well-trained models exhibit a propensity for hallucination, indicating that these issues are deeply rooted in current model architectures and training paradigms [22].  

# 4.3 Inference and Prompting Factors  

The process by which foundation models generate output, encompassing decoding strategies and interaction via prompts, significantly influences the likelihood and characteristics of hallucinations. Decoding strategies, in particular, introduce variability and potential errors during inference. Randomness inherent in certain decoding methods and imperfections in the decoding representation can directly contribute to the generation of hallucinatory content [11]. Specifically, random sampling strategies, such as top-k or top-p sampling, increase the probability of selecting low-frequency words or less probable tokens, which can derail the generation process and lead to fabrications [2,11]. Furthermore, issues like insufficient attention to contextual information during decoding or limitations imposed by the softmax bottleneck can restrict the accurate representation and selection of diverse output probabilities, resulting in inaccurate or nonsensical predictions [11]. Training methodologies like maximum likelihood estimation and teacher forcing can also contribute by encouraging models to mimic training data patterns without deep understanding, a phenomenon termed stochastic imitation [2]. The discrepancy between training and testing environments, known as exposure bias, further exacerbates the issue, particularly when generating longer sequences [2]. Additionally, hallucinations can exhibit a "snowballing" effect, where initially incorrect information accumulates and propagates through the generated text, leading to more pronounced hallucinations later in the output [2,3]. For instance, object hallucinations in descriptions have been observed to become more dominant in the latter half of the output [3].​  

Model confidence also plays a role in hallucination generation. Requiring a model to provide a response with an overly confident tone, particularly when it is uncertain, can pressure it to fabricate information rather than admitting lack of knowledge [14]. Similarly, some models, especially Large Vision-Language Models (LVLMs), demonstrate a bias towards providing affirmative descriptions when faced with judgmental queries, potentially influenced by instruction fine-tuning data that rewards catering to user requests [8]. This behavioral bias might be exploited by certain evaluation methods, potentially overestimating object-based hallucinations [8].​  

Prompt design and variations in input are critical factors influencing hallucination generation [6,12]. Models are prone to hallucinate when queried about topics or events outside their training data or knowledge boundary [14]. For example, a model trained on data up to a specific date may generate incorrect predictions about future events [14]. Ambiguous or vague inputs can also lead to hallucinations as the model attempts to fill in missing information with fabricated details [14].  

The specific formulation of a prompt can significantly impact the model's output and propensity for hallucination. Studies have shown that optimizing prompts can help reduce AI hallucinations [12]. Techniques such as using precise language and defining clear boundaries, including temporal and reality constraints, are suggested for prompt construction [12]. Incorporating annotation mechanisms within the prompt, requiring the model to distinguish facts from speculation and potentially cite sources, can also improve output reliability [12,22]. Prompting models to engage in staged thinking processes, such as chain-of-thought reasoning, can enable them to break down problems, think step-by-step, and perform self-verification, thereby mitigating hallucinations [6,12]. Different levels of detail in question prompts have been shown to affect model responses [10]. Furthermore, specific prompt structures, such as existence questions (probing the presence of an object) or spatial relationship questions (probing object location), can be designed to test a model's reliance on prior knowledge versus visual evidence, exposing hallucination tendencies [10]. In multimodal contexts, user text prompts can directly influence the generation of biased or hallucinatory visual content, as evidenced by issues encountered in tools like Google's Gemini [5]. The importance of prompt design is also highlighted in tasks like zero-shot anomaly segmentation, where visual context prompting can activate specific model capabilities crucial for performance [21]. Beyond prompt engineering, techniques like cross-validation of model outputs against authoritative sources or outputs from different models are recommended to identify potential hallucinations [12].​  

# 5. Detection and Evaluation Methods  

Evaluating and detecting hallucinations in multimodal foundation models are critical tasks for ensuring their reliability and trustworthiness. This section provides a comprehensive overview of the methodologies employed for quantifying and identifying hallucinatory outputs. Effective evaluation relies on suitable metrics and representative benchmark datasets, while detection utilizes various automated techniques often complemented by human assessment.​  

Evaluation metrics for quantifying hallucinations range from general measures assessing correctness and consistency, conceptually related to traditional metrics like precision and recall, to specialized metrics designed to capture the nuances of multimodal outputs [4,6,13]. Key specialized metrics include Perception Accuracy (PA) and Hallucination Resistance (HR), used in benchmarks like CMM to evaluate models' ability to correctly interpret input modalities and avoid generating nonexistent elements [7]. More recent metrics like True Understanding (TU), Implication Gap (IG), Subjectivity Bias (SB), and Indefiniteness (ID), introduced with the BEAF framework, assess deeper cognitive aspects such as understanding scene changes, inferring implications, quantifying bias, and measuring uncertainty [21]. Object-level hallucinations in image captions can be specifically quantified using metrics like CHAIRI and CHAIRS [3]. Despite this variety, metrics face limitations in scalability, granularity, and capturing the diverse manifestations and underlying causes of hallucinations [4,6,13].  

Benchmark datasets are fundamental tools for systematically evaluating multimodal hallucination. While some studies adapt existing datasets like MSCOCO, the field increasingly relies on dedicated benchmarks tailored for multimodal hallucination assessment [3,8]. Notable examples include CMM, which incorporates audio and video modalities alongside structured probing questions [7]; HaELM, which leverages LLMs for automated evaluation based on manual annotations [8]; MHaluBench, specifically designed for evaluating hallucination detectorsacross image-to-text and text-to-image tasks [9]; and BEAF, which uses image pairs to assess responses to scene changes [10]. Creating comprehensive benchmarks is challenging, requiring diversity across modalities, tasks, and hallucination types, alongside costly manual annotation or careful synthetic data generation [1,7,9,10].  

Detection methods can be broadly categorized into automated approaches and human evaluation protocols [4,6,13]. Automated methods include knowledge-based verification, which checks model outputs against external facts or ground truth [4,13]; self-consistency checks, which identify inconsistencies within the model's own outputs or internal states [2]; cross-modal checks, which verify coherence across different input modalities or their manipulations [9]; and toolaugmented methods, which integrate external tools or other AI models for evidence gathering and verification, such as the UNIHD framework [9]. Human evaluation, while labor-intensive, remains crucial for capturing subtle or context-dependent hallucinations and often involves detailed annotation, user training, and iterative review processes [4,6,13,22]. Automated methods offer scalability but may lack the nuance of human judgment and depend on the quality of external resources, while human evaluation provides high fidelity but is resource-intensive [4,9]. The field is moving towards hybrid approaches that combine these methods to balance efficiency, accuracy, and comprehensive coverage. This necessitates continued research into developing more robust, scalable, and fine-grained evaluation metrics, diverse and representative benchmarks, and advanced automated detection techniques capable of handling the complexity of multimodal hallucinations.​  

# 5.1 Evaluation Metrics  

Evaluating hallucination in multimodal foundation models necessitates the use of appropriate metrics that can quantify the deviation of model output from factual reality or expected behavior based on multimodal inputs [4,6]. Various metrics have been proposed, each focusing on different facets of the hallucination phenomenon.​  

Some metrics target the factual correctness and consistency of the model's response. The AutoHallusion framework, for instance, employs correctness to verify if the model's answer aligns with basic facts and consistency to ensure coherent responses across varying levels of supplementary information [10]. The effectiveness of this framework in eliciting hallucinations is measured by the average number of questions that induce hallucinations per sample [10]. For evaluating visual output quality in multimodal models, metrics like Inception Score (IS) and Frechet Inception Distance (FID) are utilized, though their direct link to halucinationin linguistic output might be indirect, primarily assessing the generated visual component's realism or diversity [10].  

Specific metrics have been developed to quantify object-level hallucinations in captions. CHAIRI (Caption Hallucination Rate of Individual Instances) and CHAIRS (Caption Hallucination Rate of Sentences) measure the proportion of hallucinated object instances relative to the total number of object instances in a caption and the proportion of sentences containing hallucinated objects relative to the total number of sentences, respectively [3]. These metrics provide fine-grained quantitative measures for a common type of visual hallucination.​  

Broader multimodal benchmarks introduce metrics assessing perceptual accuracy and resistance to hallucination. The CMM benchmark utilizes Perception Accuracy (PA) to evaluate the model's ability to correctly perceive objects or events present in the audio or video modalities and Hallucination Resistance (HR) to measure its ability to avoid hallucinating objects or events absent from the input [7].  

Newer metrics aim to capture more nuanced aspects of understanding and bias in multimodal contexts. The "BEAF" paper proposes True Understanding (TU) to measure the model's accuracy in comprehending scenes before and after modifications, Implication Gap (IG) to assess its ability to infer implications from scene changes, Subjectivity Bias (SB) to quantify the introduction of subjective assumptions not explicit in the visual input, and Indefiniteness (ID) to measure uncertainty or vagueness in descriptions [21]. These metrics move beyond simple factual correctness to evaluate deeper cognitive aspects of the model's processing.​  

Evaluation methods also encompass different technical approaches. Hallucination detection accuracy, defined as correctly identifying statements as hallucinatory or not, serves as a primary metric in datasets like MHaluBench [9]. Manually annotated hallucination responses are frequently used as ground truth for evaluation benchmarks [8]. For language models, relevant evaluation concepts include metrics like relevance and toxicity [5], which can sometimes overlap with hallucination (e.g., irrelevant generation could be seen as a form of hallucination). LLM evaluation technologies investigate various signals such as checking data sources and response provenance, identifying contradictions between input and output embeddings, and detecting "outlier" responses [5]. Furthermore, inference classifiers can be employed, utilizing models to assess hallucination likelihood, such as using state-of-the-art LLMs for end-to-end text generation detection or employing different classifier models [2]. Uncertainty measures, which examine the correlation between hallucination and output quality using probabilistic outputs, represent another approach [2]. Examples include ASTSN, which calculates uncertainty from logit outputs, BARTSCORE for transforming text into references, and KoK, which evaluates answer uncertainty based on subjectivity, ambiguity, and textual uncertainty [2].​  

Despite the array of proposed metrics, limitations persist in accurately and comprehensively evaluating multimodal hallucination. Reliance on manual annotation, while providing ground truth, is labor-intensive, subjective, and may not scale to large-scale model evaluation [8]. Metrics like CHAIRI/CHAIRS are specific to object hallucination in captions and may not capture other forms, such as factual inaccuracies or biased generation. Broader metrics like PA and HR are valuable but may not fully dissect the causesor typesof hallucination. The introduction of nuanced metrics like TU, IG, SB, and ID suggests recognition that hallucination is not solely about factual error but can involve misinterpretation, flawed inference, and undesirable biases, indicating a need for evaluation paradigms that go beyond simple correctness. Furthermore, methods relying on other LLMs or classifiers for evaluation introduce dependency and potential biases from the evaluation model itself [2]. Future directions include developing more automated, scalable, and fine-grained metrics that can capture the diverse manifestations of hallucination across different modalities and model outputs, moving towards evaluation frameworks that assess not just the presence but also the severity, type, and potential impact of hallucinations.​  

# 5.2 Benchmark Datasets  

Evaluating hallucination in multimodal foundation models necessitates the development of diverse and representative benchmark datasets. These datasets serve as crucial tools for assessing model capabilities and identifying specific failure modes. Early approaches often leverage existing datasets like MSCOCO, which provides a large collection of images with detailed annotations [3,8]. For instance, variations of MSCOCO 2014 have been used, selecting subsets for training hallucination revisors or evaluating models on specific tasks like image captioning or visual question answering [3,8]. While rich in visual content and annotation, these general-purpose datasets are not inherently designed to probe for or specifically identify multimodal hallucinations, requiring careful selection, modification, and supplementary annotation to adapt them for this purpose [8].​  

Recognizing the limitations of adapting general datasets, researchers have developed benchmarks specifically targeting multimodal hallucination. CMM is a multimodal benchmark that extends beyond images to include video and audio modalities [7]. It employs structured probing questions, categorized into "Existence Questions" and "Non-Existence Questions," to systematically assess whether a model hallucinates objects or events that are either present or absent in the given audio or video samples [7]. Each subcategory within CMM contains 200 samples, totaling 1,200 samples and 2,400 questions, offering a focused evaluation on different modalities [7].  

Another dedicated benchmark, MHaluBench, is designed for the rigorous evaluation of multimodal hallucination detectors, covering both image-to-text and text-to-image generation tasks [9]. It includes a balanced distribution of examples across tasks like image captioning, visual question answering, and text-to-image generation, providing 200 tasks each for image description and VQA, and 220 for text-to-image generation [9]. Similarly, MHalDetect1 focuses on detecting hallucinations specifically in detailed image descriptions and VQA examples, containing a substantial 16,000 detailed annotations [1,7]. The BEAF (BEfore-AFter Hallucination Dataset) benchmark employs image pairs depicting scenes before and after modifications to evaluate how VLMs respond to visual changes, probing their sensitivity to subtle visual-textual inconsistencies [21]. The AutoHallusion framework generates a dataset comprising synthetic and real-world samples, specifically aiming to create examples that successfully elicit hallucinations [10]. Its diversity is assessed by the variety of scenes and objects, while effectiveness is measured by the rate at which questions per sample provoke hallucination [10]. This contrasts with datasets focusing on detection by concentrating on the generation of challenging instances for evaluation. Furthermore, datasets targeting specific multimodal tasks, such as LP-MusicCaps for audio-text music captioning or TVQA-long for reasoning over long videos, offer evaluations within particular domains and content types [1,21].​  

While the focus is on multimodal hallucination, benchmarks developed for large language models (LLMs) provide relevant context and methodologies [1,6,7,20]. HaluEval, for instance, is a comprehensive LLM benchmark evaluating hallucination across general queries and task-specific settings like question answering, dialogue, and summarization [6,7,20]. Med-HALT specializes in the medical domain, employing a multinational dataset and various testing methods to assess LLMs' hallucination in specialized contexts [1,6,7,20]. Other LLM benchmarks like TruthfulQA, KoLA, Hallucination Snowballing (which uses datasets designed to test rationalization of fixed incorrect answers), and REVERIE (focused on instruction following and reasoning with explicit rationales) highlight different facets of text-based hallucination and evaluation strategies [6,20,21]. Although valuable, these text-only benchmarks do not capture the unique challenges and complexities of hallucinations arising from the interaction and misalignment of multiple modalities.​  

The development of these diverse benchmarks highlights the challenges inherent in creating representative datasets for multimodal hallucination evaluation. Key challenges include ensuring diversity across modalities (images, video, audio, text), different domains and scenarios, various types of hallucinations (object existence, attribute errors, relational errors, temporal inconsistencies), and the tasks being performed (captioning, VQA, generation, reasoning) [7,9,10]. Manually annotating multimodal data for fine-grained hallucination detection is labor-intensive and costly, as seen in datasets like MHalDetect1 [1]. Generating synthetic data, as explored by AutoHallusion, offers a way to scale and target specific hallucination types, but requires careful validation to ensure ecological validity [10]. The importance of creating diverse and representative datasets cannot be overstated, as they are fundamental for accurately assessing model performance, understanding the root causes of hallucinations, and driving the development of more robust multimodal foundation models. Continued research is needed to develop larger, more varied, and systematically constructed benchmarks that can comprehensively capture the multifaceted nature of multimodal hallucination.​  ​  

# 5.3 Detection Methods  

![](images/8a11c2bf6d065cb53d4056b8e3f0591c6fba68e2ac004d34eb6206c0670d0e8a.jpg)  

Effective detection of hallucinations in multimodal foundation models is paramount for ensuring their reliability and trustworthiness. Detection methodologies broadly fall into automated approaches and human evaluation.  

Automated detection methods employ computational techniques to identify hallucinated content without manual intervention. These methods can be categorized based on the principles they utilize. Knowledge-based verification involves comparing the model's output against established facts, external knowledge bases, or ground truth information [4,13]. Frameworks like AutoHallusion assess correctness by verifying answers against basic facts [10]. The "Truth-Aware Context Selection" mechanism aims to filter misleading information by discerning truthful contexts [17]. Specific metrics and tools have been developed, such as FactVC for evaluating video captioning factuality [1], Evidence Retrieval methods, FActScore for quantifying factual support, and FacTool for integrating various tools for evidence gathering [2].  

Cross-modal consistency checks evaluate the coherence of the model's output with information presented or implied across different input modalities or their manipulations [9]. The BEAF approach modifies visual scenes using image editing models and assesses the VLM's response consistency relative to these altered inputs [21]. Similarly, the CMM benchmark employs probing questions designed to test the model's perception of both present and absent elements within the input modalities [7].​  

Self-consistency checks rely on the model's internal state or the relationships between its own generated outputs to detect inconsistencies, often without requiring external information [2]. The principle of Self-Assessment involves sampling multiple responses from the model and measuring the informational consistency between them to identify false statements [2]. SELFCHECKGPT is a representative zero-resource black-box method based on this concept [1,4]. Hallucinations can also be detected by identifying contradictions within the model's internal states or between input and output embeddings, or by identifying outliers in the generated responses [5,16]. AutoHallusion incorporates consistency evaluation by checking if the model provides coherent answers across different levels of supplementary information related to the same input [10].  

Tool-augmented methods enhance detection by integrating external resources or specialized models [9]. The UNIHD framework provides a systematic approach: it extracts core claims, selects appropriate external tools based on the claims, executes these tools in parallel (including object-oriented tools like Grounding DINO, attribute-oriented tools like GPT-4V and Gemini, scene text-oriented tools like MAERec, and fact-oriented tools like Google Search API), and performs hallucination verification using an MLLM (GPT-4V or Gemini) based on the gathered evidence [9]. Another form of tool augmentation involves using one AI system to evaluate the output of another, exemplified by tools like Arize’s Phoenix which uses an LLM for evaluation [5]. FacTool, previously mentioned, is also a tool-integrated framework for evidence-based detection [2]. The HaELM framework utilizes a fine-tuned LLM as an automated judge for hallucinations against reference captions, offering an alternative to traditional matching methods for complex responses [8].  

While automated methods offer scalability, human evaluation remains an indispensable component for comprehensive hallucination detection, particularly for complex, subjective, or subtle errors [4,6,13]. It involves detailed assessment protocols, user awareness training, and iterative review processes where human annotators provide oversight and feedback [22]. Human evaluators are crucial for tasks such as annotating specific hallucinated entities like objects [3].  

Each detection approach involves trade-offs. Automated methods, while efficient and scalable, may lack the nuanced understanding of human judgment and can be limited by the scope and accuracy of their underlying knowledge sources or integrated tools [9]. Knowledge-based methods rely heavily on external data validity. Self-consistency methods are zeroresource but might fail to detect factual inaccuracies if the model consistently generates the same incorrect information. Cross-modal checks are powerful for multimodal inconsistencies but require specific test designs. Tool-augmented methods offer broad coverage by combining different tools but introduce complexity in integration and potential dependencies on tool reliability [9]. Human evaluation, conversely, provides high-fidelity assessment and captures subtle hallucinations but is inherently labor-intensive, slow, and expensive, making it unsuitable for real-time or large-scale  

monitoring [9]. Consequently, a hybrid approach combining various automated techniques with targeted human review is often employed to balance efficiency, coverage, and accuracy.  

# 6. Mitigation Strategies  

Addressing hallucination in multimodal foundation models necessitates a multifaceted approach, targeting various stages of the model lifecycle and interaction. This section provides an overview of established and emerging strategies aimed at mitigating the generation of factually incorrect or fabricated content.  

![](images/65f57dac82f632e1ad35ae0577140e2f2c514044da2091562d9fe3e616887209.jpg)  

Mitigation efforts can be broadly categorized into several key areas, each focusing on distinct aspects of model development, deployment, and interaction: data-centric approaches, model and training-centric modifications, techniques applied during inference and decoding, integration of external knowledge and post-hoc corrections, and strategic prompt engineering, alongside other complementary methods. The subsequent subsections delve into each of these categories in detail, outlining specific techniques and their underlying principles.  

Data-centric strategies focus on enhancing the quality, diversity, balance, and truthfulness of the data used for training and fine-tuning foundation models [13,14,15,17]. By ensuring the training data accurately reflects the intended reality and covers a wide range of scenarios, the model's foundational understanding is improved, reducing its propensity to hallucinate due to data noise, bias, or insufficiency [7,13,14,15]. Techniques such as data cleaning, careful curation, augmentation, and domain-specific dataset creation fall under this umbrella [3,4,13,14,15,19,21].​  

Model and training-centric approaches involve modifications to the model architecture, training objectives, or optimization procedures [1,2,4,11,13,15]. These methods aim to build more robust internal representations and enhance the model's ability to learn and retain factual information reliably [1,4,13]. Examples include regularization techniques, architectural adjustments, improved training strategies like instruction tuning, parameter adaptation methods, and the development of domain-specific models [1,2,4,11,13,15,19,21].  

Inference and decoding techniques focus on controlling the model's output generation process [6,15]. By adjusting decoding parameters or implementing constraints, these methods steer the model towards producing more factual and less speculative content [6,15]. This category also includes post-inference verification and correction mechanisms that review and potentially revise the model's initial output to mitigate identified hallucinations [2,3,14,22].  

Knowledge integration and post-hoc methods augment the model's internal knowledge with external, verifiable information or apply correctional steps after generation [2,3,4,5,6,11,14,20,22]. Retrieval-Augmented Generation (RAG) systems, integration with knowledge graphs, and leveraging external sources for verification are prominent examples of knowledge integration [4,5,6,14,20]. Post-hoc methods, such as automated revisors or human review processes, are applied to refine or correct generated output based on external criteria or statistical analysis [3,5,11,14,22].  

Prompt engineering involves crafting input queries and instructions to guide the model's behavior, encouraging it to generate factual and coherent responses [6,12,14]. Techniques such as Chain-of-Thought prompting, providing specific context, and requiring evidence or sources can effectively reduce hallucination by structuring the model's reasoning and grounding its output [1,4,6,12,14]. Complementary strategies, including neuro-symbolic methods and dynamic modality weighting in multimodal models, offer alternative or integrated approaches to enhance reliability and address modalityspecific hallucination issues [5,7].​  

While each category offers valuable techniques, their effectiveness, trade-offs, and limitations vary [3,5]. Data-centric methods are foundational but require significant resources for data collection and curation. Model-centric approaches can improve inherent model capabilities but may be computationally expensive and require extensive retraining. Inference techniques provide runtime control but can impact output diversity or fluency. Knowledge integration enhances factuality but relies on the availability and quality of external sources. Post-hoc methods offer a final safeguard but may not capture all errors and can add latency. Prompt engineering is flexible but may not be universally effective across all tasks or model types. Often, a combination of these strategies is necessary to achieve robust hallucination mitigation, highlighting the need for integrated frameworks [6,11,15]. Current research continues to explore the synergies between these approaches and  

address challenges such as balancing factuality with fluency and creativity, particularly in complex, multimodal scenarios [7,15].  

# 6.1 Data-Centric Approaches  

Data-centric approaches constitute a fundamental strategy for mitigating hallucinations in foundation models by focusing on improving the quality, quantity, and characteristics of the training data [13,15]. This paradigm posits that the nature of the data significantly influences a model's propensity to generate erroneous or fabricated content.  

A primary focus within data-centric methods is enhancing data quality and implementing rigorous cleaning procedures. Utilizing higher quality and carefully screened data during model training is crucial for reducing the impact of noise, chaotic, and fictional content that can contribute to hallucinations [14]. This involves actively filtering out irrelevant or misleading information [14]. Furthermore, collecting more comprehensive and accurate training data directly addresses hallucinations stemming from data insufficiency [13]. For models operating in specialized domains, such as scientific knowledge, sourcing data from authoritative outlets like scientific journals and research reports is essential for grounding the model in factual information and minimizing the generation of fabricated claims [13]. Selecting truthful contexts is another data-centric technique proposed to mitigate models being misled by untruthful information [17].​  

Beyond quality, ensuring data balance and diversity is critical to prevent biases that can lead to skewed or incorrect outputs [13,15]. Avoiding data bias ensures that the model does not disproportionately represent certain concepts or relationships, which can cause hallucinations when encountering less represented scenarios [13]. For multimodal foundation models, creating balanced datasets that encompass diverse modalities and include temporal annotations can help reduce visual bias and enhance the model's understanding of dynamic events, thereby mitigating related hallucinations [7]. The use of high-quality, diverse, balanced, and well-structured training data is broadly recognized as a foundational data-centric approach to reducing AI hallucinations [15].​  

Data augmentation and enrichment techniques also play a significant role. This involves not merely expanding the dataset size but enhancing its informational content or structural properties. For instance, the REVERIE dataset introduces reasoning annotations, providing explanations for why specific answers are correct or incorrect. This enrichment provides models with more detailed information about the underlying reasoning process, offering a data-centric method to mitigate hallucinations by guiding the model towards more logical outputs [21]. Another approach involves strategically generating training data tailored to address specific hallucination types. The LURE method, for instance, creates a dataset for training a hallucination revisor by modifying accurate descriptions to simulate common object hallucinations based on factors like cooccurrence and object properties [3]. This method leverages generative models (e.g., GPT-3.5) to create more naturalistic cooccurrence scenarios, avoiding potential biases present in existing datasets [3]. Techniques like Hallucinator, which generate additional positive samples, can also be seen as data augmentation methods aimed at improving representation learning, which indirectly contributes to model robustness [4].  

Furthermore, for domain-specific foundation models, the meticulous curation and utilization of large, domain-specific datasets during pre-training are paramount [19]. Examples include using extensive genomic sequences for bioinformatics models, which ensures the model is grounded in the specific patterns and relationships of that domain, reducing out-ofdomain hallucinations [19].​  

Collectively, these data-centric approaches directly impact data quality and model robustness. By providing models with cleaner, more accurate, balanced, diverse, and relevant data, the likelihood of generating false or misleading information is reduced. Enhanced data quality leads to more reliable pattern recognition, while diversity and balance build robustness against distributional shifts and biases. Augmentation and enrichment provide the model with richer signals and examples of correct reasoning or error correction, further strengthening its ability to produce truthful and accurate outputs.  

# 6.2 Model and Training-Centric Approaches  

Addressing hallucinations inherently involves modifications to the foundation models themselves or their training procedures. Model and training-centric approaches focus on enhancing the internal mechanisms and learned representations of models to improve their factual accuracy and robustness. These strategies range from fundamental architectural design choices and regularization techniques to sophisticated training paradigms and post-training parameter adjustments.  

Architectural modifications can play a crucial role in improving model performance and reducing the propensity for generating hallucinatory content. For instance, increasing the number of neural network layers or adjusting the intricate connections between neurons can enhance the model's capacity to learn complex patterns and potentially mitigate hallucinations [13]. In multimodal models, dynamic cross-modal fusion, where weights for different modalities are contextually adjusted, has been shown to improve understanding and reduce hallucinations [7]. The development of domain-specific models with tailored architectures and training objectives also reflects this approach, as seen in bioinformatics applications [19]. Regularization and optimization techniques applied during training further contribute to model robustness. Utilizing an appropriate learning rate is essential to control training speed and prevent issues like overfitting or underfitting, which can exacerbate hallucinations [13]. Employing advanced optimization algorithms helps tune model parameters more effectively, enabling the model to better capture true data patterns [13].  

Beyond architecture and basic optimization, various parameter-level strategies have been developed to guide models towards generating more truthful content. These "Parameter Adaptation" techniques involve adjusting, editing, and optimizing parameters to steer the model's knowledge recall and generation. Examples include optimizing parameters via contrastive learning to decrease the probability of generating incorrect spans (e.g., CLR), reducing the influence of prior knowledge using context-aware decoding methods (e.g., TYE), introducing noise and fine-tuning compact editors that denoise text by incorporating relevant evidence (e.g., PURR), and using sequence information contrastive learning to bolster the reliability of memory parameters (e.g., HISTALIGN) [2]. A related approach is the direct editing of model parameters to align internal knowledge with factual information, as demonstrated by methods like TruthX [17].​  

Training methodology itself is another key area for hallucination mitigation. Instruction tuning, a widely adopted technique, involves fine-tuning models on datasets of instructions and desired outputs to improve their ability to follow commands and generate relevant responses. A specific training-centric technique, "Reflective Instruction Tuning," modifies the training process by requiring the model to predict the reasoning behind its answers, thereby encouraging more careful and accurate inferential steps [21]. Fine-tuning can also be applied to create specialized components; for example, LURE employs a "hallucination revisor," an LVLM fine-tuned on data containing hallucinated and corrected descriptions. This revisor is trained to identify and re-evaluate uncertain or trailing objects in descriptions, prompting correction [3].​  

Hybrid approaches combining neural capabilities with symbolic logic also fall under this umbrella. By using neural networks for pattern recognition and then layering logical reasoning or transparency mechanisms, models can potentially correct or explain their outputs, mitigating errors [5]. Elemental Cognition's "neuro-symbolic reasoner" builds a logical model from LLM output, adding logic and explicability to the neural component [5].  

Furthermore, strategies involving external knowledge and output control mechanisms are often implemented alongside or through training adjustments. Knowledge Retrieval Augmented Generation (RAG) links the model to external databases, providing real-time factual support during generation and reducing reliance on potentially faulty internal knowledge [14]. Enhancing constraints or "Guardrails" within the model's output mechanism allows it to express uncertainty or admit "I don't know" when appropriate, preventing fabrication [14]. The focus on evaluating and mitigating hallucinations specifically in smaller open-source LLMs underscores the importance of applying these techniques across different model scales [1].​  

The primary benefit of these model and training-centric approaches is their potential to fundamentally improve the model's internal representations and generation process, leading to a reduction in hallucination frequency and severity. Drawbacks can include computational expense, the difficulty of guaranteeing factual accuracy across all contexts, potential trade-offs between factuality and fluency, and the need for large, high-quality datasets for training and fine-tuning, especially for parameter editing methods which require precise knowledge [1,4,11,13,15]. Despite these challenges, ongoing research continues to refine these techniques to build more reliable and truthful foundation models.  

# 6.3 Inference and Decoding Techniques  

Mitigating hallucinations during the inference and decoding phases represents a critical avenue for enhancing the factual reliability of multimodal foundation models. Techniques applied at this stage directly influence the final output generated by the model. One set of approaches involves adjusting decoding parameters to steer the generation process towards more factual outcomes. For instance, modifying the temperature parameter allows balancing adherence to learned data distributions with the generation of diverse responses [6] and lowering the temperature typically reduces randomness, favoring more probable and often more factual tokens, albeit potentially at the cost of diversity [6].​  

Similarly, flexible sampling strategies are employed to maintain fidelity while ensuring generation quality and diversity [2] and the fact core sampling algorithm, for example, is introduced to address observations that the randomness of sampling can have a greater impact on factuality, particularly in the latter parts of a sentence [2]. Constrained decoding techniques, such as limiting the AI output range through filtering tools or setting clear probability thresholds, also serve to prevent the model from producing low-probability or factually incorrect tokens, thereby reducing the incidence of hallucinations [15].​  

Beyond adjusting the core decoding process, post-inference verification and correction methods offer additional layers of defense. These techniques process the model's initial output to identify and correct potential hallucinations. Postprocessing and calibration steps may involve using fact checkers or manual review mechanisms to filter out content that is obviously incorrect before it is presented to the user [14] and a more sophisticated approach involves multi‐model verification, where outputs from multiple models are compared for consistency; results with high agreement are considered more reliable [14]. This can be conceptually related to re‐ranking, where outputs are implicitly ranked based on consistency or confidence derived from multiple sources. Another targeted method is the use of a trained revisor model applied during inference [3] which is specifically designed to correct generated descriptions. By integrating placeholder tags into the generated text, the revisor is prompted to re‐evaluate objects associated with high uncertainty or those appearing later in the sequence, allowing for targeted correction of potential object‐related hallucinations [3]. Furthermore, a cautious strategic approach at the inference stage involves limiting the application of AI outputs in critical tasks where misinformation could lead to significant consequences, effectively mitigating risk by avoiding high‐stakes scenarios [22].  

Despite their potential, inference and decoding techniques face several challenges and limitations. Adjusting decoding parameters like temperature and sampling often involves a trade-off between factual accuracy and the diversity or creativity of the output. Aggressively constraining decoding or lowering temperature too much can lead to repetitive or generic responses. Post‐processing methods like manual review or external fact‐checking are resource‐intensive and may not scale effectively for high‐throughput applications [14]. Multi‐model verification increases computational cost and relies on the assumption that multiple models are less likely to hallucinate in the same way, which may not always hold true, especially for complex or novel inputs. Trained revisors require additional training data and infrastructure and might introduce their own biases or errors. Identifying subtle or complex hallucinations remains difficult for automated verification systems, limiting the effectiveness of these post‐processing steps [14]. Ultimately, while these inference and decoding techniques can significantly reduce hallucinations, they are often part of a broader mitigation strategy rather than a complete solution, requiring careful consideration of the specific application and the acceptable balance between accuracy, cost, and output characteristics.  

# 6.4 Knowledge Integration and Post-hoc Methods  

Mitigating hallucinations in large language models, particularly in multimodal contexts, often necessitates going beyond the inherent capabilities encoded within the model's parameters. Two primary strategies involve integrating external knowledge during or before generation and employing post-hoc methods to revise outputs after they are produced.  

Integrating external knowledge serves to ground model responses in verifiable information, thereby reducing reliance on potentially erroneous internal knowledge representations. A prominent approach in this category is Retrieval-Augmented Generation (RAG), which combines the strengths of information retrieval and sequence generation [6,20]. RAG systems retrieve relevant document fragments from large databases based on the input query and condition the language model's generation on this retrieved information [5]. This "retrieve $^ +$ generate" paradigm [20] is suggested to enhance model accuracy and specificity [5,19]. Specific techniques like RETRO retrieve similar documents based on input sequence chunks, while IRCoT interweaves Chain-of-Thought generation with document retrieval steps to guide the model [2]. Beyond unstructured text retrieval, integrating structured knowledge sources, such as knowledge graphs, also holds potential [4,6]. Frameworks termed "knowledge chains" aim to link language models with structured knowledge bases to improve performance [1,4]. General strategies involve leveraging external knowledge sources to transform generation tasks into simpler search or summarization problems grounded in provided data [6]. Systems like LLMAUGMENTER combine external knowledge sources and automated feedback to enhance output accuracy [1,4], and interactive question-knowledge alignment allows users to guide models towards accurate information by aligning generated text with factual knowledge [1]. Utilizing external knowledge is also fundamental to cross-verifying AI-generated data with trusted sources for validation [5,22].​  

In contrast to proactive knowledge integration during generation, post-hoc methods focus on identifying and correcting hallucinations after the model has produced its output. These methods often involve monitoring and evaluation tools that function as a checksum against the model's responses [5]. An example of a post-hoc revision method is LURE, designed for Large Vision-Language Models (LVLMs) [3]. LURE revises generated descriptions by utilizing lists of potential co-occurring objects and uncertain objects, which are generated based on analysis (implied to involve statistical analysis based on the description's guidance and the digest's content) and then used to modify the original description [3]. Another form of posthoc verification involves using external search engines to check the veracity of model-generated answers, exemplified by features like Google's "check this answer" in Gemini [11]. Furthermore, iterative review processes involving human oversight represent a straightforward, albeit labor-intensive, post-hoc approach to validate and correct outputs [22].​  

# 6.5 Prompt Engineering and Other Approaches  

Prompt engineering serves as a crucial strategy for mitigating hallucination in foundation models, particularly large language models and multimodal variants [6,12,14]. By carefully designing the input queries and instructions, researchers aim to guide the model's generation process, encouraging it to produce factual and grounded outputs rather than fabricated content. A variety of prompt engineering techniques have been proposed and explored.​  

One category of techniques focuses on explicitly structuring the model's reasoning process. Chain of Thought (CoT) prompting, for instance, instructs the model to "think step by step," thereby breaking down complex problems into intermediate steps and making the reasoning path explicit [6,14]. Building upon CoT, Self-Consistency with CoT (CoT-SC) involves generating multiple reasoning paths and selecting the most consistent answer through voting, enhancing reliability [6]. Tree of Thoughts (ToT) further expands this by allowing the model to explore and self-evaluate intermediate reasoning steps, navigating a tree-like search space for better problem-solving and potentially reducing errors [6]. Guiding the AI's thought process in a specific sequence, such as providing facts first, followed by analysis, and then conclusions, is another effective strategy to improve logical flow and reduce speculation [12]. Decomposing long and complex questions into smaller, manageable parts and prompting the model to answer them sequentially can also prevent it from deviating into erroneous responses [14].​  

Another critical approach involves providing clear context and demanding external grounding or evidence. Using precise wording and defining clear boundaries for answers, such as specifying timeframes or required factual bases, helps constrain the model's output space [12]. Providing specific context, like asking for Nobel Prize winners "Based on 2023 data" rather than a general query, significantly reduces ambiguity and potential for hallucination [14]. Tagged Context Prompts incorporate source links or factual references directly into the prompt, compelling the model to utilize provided information [6]. Similarly, explicitly requesting sources or evidence ("please provide factual evidence," "based on what data") forces the model to rely on existing knowledge and prevents fabrication [12,14]. Implementing annotation mechanisms to distinguish certain facts from speculation or requiring source citations also promotes verifiable outputs [12]. The CRITIC framework combines external information retrieval with self-checking prompts to enhance accuracy [6].  

Iterative prompt optimization is also suggested as a method to guide generation and remove hallucinations, thereby enhancing output accuracy through refinement cycles [1,4,20]. Self-Correcting prompts encourage the model to doublecheck and critique its own outputs, while employing multiple language model instances to debate and reach a consensus offers another avenue for internal verification and error reduction [6].  

For multimodal foundation models, prompt engineering extends to incorporating visual and other sensory information. Visual context prompting models like VCP-CLIP embed global visual information into text prompts to activate modalityspecific awareness for tasks like zero-shot anomaly segmentation [21]. Leveraging fine-grained visual features to adjust text embeddings is explored in post-VCP modules [21]. Quantizing prompts has also been investigated as a method to regularize vision-language models and suppress overfitting and catastrophic forgetting by using quantization error as noise [21].  

Beyond prompt engineering, other approaches target hallucination mitigation, particularly in multimodal contexts. One significant strategy involves addressing language priors and dynamically adjusting the influence of different modalities. Mitigating language priors can be achieved through techniques such as context-diversified fine-tuning and implementing visual or audio verification mechanisms [7]. Dynamically adjusting modality weights based on the input can help ensure that the model relies appropriately on the most reliable or relevant modality for a given task, potentially reducing hallucinations that arise from an over-reliance on linguistic information when visual or audio cues contradict it [7].  

# 7. Applications and Impact  

The pervasive deployment of multimodal foundation models across diverse sectors underscores their transformative potential; however, this widespread application also brings to the forefront significant concerns regarding the impact of model hallucinations [11,22]. Hallucinations, where models generate content that is factually incorrect or does not align with the input, pose substantial risks that can undermine the reliability and trustworthiness of these systems in real-world applications [6,8,10].​  

The risks associated with hallucinations are particularly acute in safety-critical domains [15], where errors can have severe and irreversible consequences [1,4,14,20]. Beyond direct operational risks, hallucinations also raise significant ethical implications. These include the potential for generating and disseminating misinformation, which can erode public trust and hinder effective responses, especially during emergencies [12,13,15]. Furthermore, hallucinations can manifest as bias, amplifying societal prejudices, as exemplified by biased image generation by models [5]. Addressing accountability for harm caused by hallucinating AI systems remains a complex challenge. The criticality of factual accuracy and reliability in these high-stakes environments necessitates careful consideration and mitigation strategies [1,4,14,20,22].  

To illustrate the tangible consequences of hallucinations, numerous case studies and examples across various domains have been documented. Subsequent sections will delve into specific instances within fields such as healthcare, autonomous driving, finance, legal services, bioinformatics, and education. These analyses detail the distinct ways hallucinations manifest and the particular risks and consequences inherent to each domain [1,4,15,19,22].  

Case studies highlight the potential for serious harm in healthcare, where inaccurate medical information generated by models can lead to misdiagnosis or inappropriate treatment [4,15,18]. In the legal domain, hallucinations have resulted in the fabrication of case precedents, leading to tangible professional penalties and undermining legal processes [5,11]. While bioinformatics is increasingly leveraging foundation models for complex tasks like sequence analysis and protein design, the specific manifestations and consequences of hallucinations in this field are less documented in the provided literature, though the potential for generating biologically implausible results exists [19]. Examples extend to customer service, where incorrect information provided by chatbots has led to financial losses and legal action [11], and travel advice, where dangerously inaccurate AI-generated content has been found [11]. The proliferation of face forgery videos also represents a form of generated content with significant societal impact [21]. While the potential for impact in autonomous driving, finance, and education is recognized [15,22], detailed case studies from the provided sources are limited, underscoring the need for further investigation into how hallucinations specifically affect these critical areas. The varied nature of these impacts across different applications necessitates domain-specific evaluation and tailored mitigation strategies to ensure safe and trustworthy deployment of multimodal foundation models [3,10].​  

# 7.1 Case Studies in Specific Domains  

The impact of hallucinations in multimodal foundation models manifests distinctly across various application domains, presenting unique challenges and severe consequences.  

<html><body><table><tr><td>Domain</td><td>Example / Impact</td></tr><tr><td>Healthcare</td><td>Medically inaccurate info, misdiagnosis, inappropriate treatment (e.g., misclassifying skin lesions).</td></tr><tr><td>Legal Services</td><td>Fabricating case precedents, resulting in dismissed complaints and fines for lawyers.</td></tr><tr><td>Bioinformatics</td><td>Potential for generating biologically implausible sequences,incorrect structures, erroneous data interpretations.</td></tr><tr><td>Travel</td><td>Dangerous advice (e.g., mushroom foraging instructions) leading to physical harm.</td></tr><tr><td>Customer Service</td><td>Providing inaccurate information leading to financial loss/lawsuit (e.g., Air Canada bereavement fare).</td></tr><tr><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td>General (Bias)</td><td>Biased or inappropriate image generation reflecting/amplifying societal biases (e.g., Google Gemini).</td></tr><tr><td>General (Misinfo)</td><td>Al-generated news bots spreading unverified info, proliferation of face forgery videos.</td></tr></table></body></html>  

This section details specific case studies illustrating these effects in critical fields such as healthcare, legal services, and bioinformatics, alongside other significant examples identified in the literature.  

In healthcare, hallucinations pose a considerable risk due to the critical nature of medical decisions. Foundation models can generate information that appears plausible but is medically inaccurate, potentially leading to harmful outcomes [4,18]. A specific concern is the misidentification of medical conditions, such as an AI system incorrectly classifying a benign skin lesion as malignant, which could result in unnecessary and potentially harmful medical interventions [15]. The unique challenge here lies in ensuring absolute factual accuracy and reliability, as errors can directly impact patient health and safety.​  

The legal domain has also experienced significant disruptions due to AI hallucinations. Studies indicate that generative AI can make errors when answering legal questions, with one assessment reporting a $7 5 \%$ error rate in such contexts [5]. A notable instance involved lawyers using a large language model to find relevant case precedents, only for the model to hallucinate several non-existent cases. Citing these fictional cases led to the dismissal of the complaint and resulted in a fine for the law firm, starkly illustrating how hallucinations can undermine legal processes and professional integrity [11]. The development of specialized legal language models, such as ChatLaw, which incorporates retrieval mechanisms like vector database and keyword retrieval, represents an effort to mitigate these inaccuracies by grounding responses in verified information [4,18,20].  

In bioinformatics, foundation models are increasingly applied to tasks like genome analysis (e.g., DNABERT), protein design, and single-cell RNA-seq data analysis (e.g., scBERT) [19]. While the application of these models is highlighted, the provided literature does not contain specific case studies detailing how hallucinations manifest within these complex biological tasks or their precise consequences [19]. However, the potential for hallucination to generate biologically implausible sequences, incorrect protein structures, or erroneous interpretations of genetic data poses a significant implicit challenge, potentially misleading research or therapeutic development.  

Beyond these core domains, hallucinations have been observed with harmful consequences in other areas. In travel and tourism, AI-generated content, such as guidebooks on sensitive topics like mushroom foraging, has contained dangerously incorrect information that could lead to severe physical harm or even fatality [11]. Customer service applications have also faced issues, exemplified by an Air Canada chatbot providing inaccurate information about bereavement fares, which resulted in a lawsuit against the airline [11]. Furthermore, hallucinations in multimodal models can manifest as biased or inappropriate outputs, such as the generation of biased images by models like Google's Gemini, reflecting and potentially amplifying societal biases [5]. AI-generated news bots spreading unverified information during emergencies and the proliferation of face forgery videos highlight how hallucinations contribute to the broader spread of misinformation, eroding public trust and hindering effective response efforts [15,21].  

The provided materials do not contain specific case studies detailing the impact of hallucinations in autonomous driving, finance, or education, although these domains are recognized areas where foundation models are being deployed and thus face potential risks from hallucinatory behavior [15]. The diverse examples presented underscore the critical need for domain-specific evaluation and mitigation strategies tailored to the unique challenges and potential consequences of hallucinations in each field.  

# 8. Challenges and Future Directions  

Addressing the phenomenon of hallucination remains a fundamental challenge in the development and application of multimodal foundation models [13]. This complexity stems from intrinsic model properties, data limitations, and the difficulty in reliable evaluation. Current mitigation strategies face significant limitations, lacking fundamental solutions to fully eliminate hallucinations and ensure the inherent stability and reliability of AI systems [11,15,22]. Challenges include balancing model creativity with reliability [14], ensuring transparency and controllability [15], and overcoming biases stemming from training data and a lack of clear logical frameworks [5]. From a model perspective, issues such as  

imbalances in modality integration, spurious data correlations, and over-reliance on language priors contribute to the problem [7]. For vision-language models, specifically, reducing excessive reliance on prior knowledge while enhancing attention to visual input, particularly for complex real-world scenes, is crucial [10].  

A concurrent major challenge is the need for improved evaluation metrics and benchmarks [7,21]. Existing evaluation frameworks may not accurately capture real-world hallucination phenomena, sometimes exploiting judgmental biases or relying on superficial patterns rather than rigorously verifying factuality and logical consistency [8,21]. Data biases within datasets used for training and evaluation further complicate accurate assessment [21]. Consequently, developing more effective and sophisticated monitoring and evaluation technologies is essential [5].  

Addressing these challenges necessitates exploring promising future research directions [1,11,15]. Developing more robust detection and mitigation strategies remains a primary focus. This involves optimizing training data and methods, modelcentric techniques like fine-tuning and safety alignment, and external mechanisms such as fact-checking and bias detection [1,4]. Process-oriented improvements, including engineering safeguards, refined review processes, and human feedback loops, are also vital [5,11,22].  

Exploring new architectures and training methods is fundamental. This includes developing models better equipped for nuanced visual-linguistic integration and handling complex semantic relationships [10]. Advancements in reasoning modules, such as Chain-of-Thought prompting and reasoning-aware training, are crucial for mitigating hallucinations linked to faulty reasoning [14]. Creating comprehensive and balanced benchmarks is indispensable, alongside advancing automated evaluation methods, potentially leveraging other large language models [4,5,7,8].  

Integrating knowledge graphs and external knowledge bases represents a significant opportunity to enhance factuality and provide richer context for generation [1,4,6,14]. Leveraging Explainable AI (XAI) can improve transparency and understanding of hallucination causes [19], while addressing cognitive dissonance in models requires improving their ability to accurately perceive and semantically represent complex scenes [10]. Furthermore, improving automated evaluation methods is a critical area for progress [4].  

Finally, from an emerging perspective, the potential role of hallucination in fostering creativity warrants further exploration, provided factual accuracy can be controlled [4]. The complexity of hallucination underscores the need for continued vigilance and multifaceted research efforts across model development, evaluation, and application [15].  

# 8.1 Open Challenges  

Despite significant advancements in multimodal foundation models, addressing the phenomenon of hallucination remains a major open challenge.  

<html><body><table><tr><td>Challenge Area</td><td>Description / Issues</td></tr><tr><td>Mitigation Limitations</td><td>Lack of fundamental solutions for full elimination; balancing creativity vs. reliability; ensuring transparency/controllability.</td></tr><tr><td>Model Architecture</td><td>Imbalances in modality integration; over- reliance on language priors; enhancing attention to visual/complex input.</td></tr><tr><td>Evaluation Robustness</td><td>Existing methods may not capture real-world phenomena; reliance on superficial patterns; data biases in benchmarks.</td></tr><tr><td>Knowledge Integration</td><td>Effectively integrating external knowledge for grounded generation; handling complex semantic relationships.</td></tr><tr><td>Data Dependency</td><td>Addressing biases and spurious correlations in training data.</td></tr><tr><td>Task Alignment</td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Ensuring models align with specific downstream application requirements.</td></tr></table></body></html>  

This challenge manifests in several critical areas, including the inherent limitations of current mitigation strategies and the substantial difficulties associated with developing truly robust evaluation methodologies.  

Current mitigation approaches face limitations in ensuring model reliability and stability. Modern AI systems can intrinsically generate errors, directly impacting their dependability [11]. Hallucinations, where models produce false or misleading information even after extensive training, represent a significant unresolved issue [22]. A fundamental difficulty lies in striking a balance between fostering AI's "creativity" and ensuring its "reliability" [14]. Furthermore, ensuring the transparency, credibility, and controllability of AI systems remains a key hurdle [15]. Biases inherent in generative AI platforms, often originating from biases in training data and the absence of clear logical frameworks, pose a significant challenge to mitigation efforts [5]. The lack of transparent rules of inference makes it difficult to guarantee reliable conclusions from these models [5].  

From a model architecture perspective, challenges include imbalances in modality integration, the presence of spurious correlations within training data, and an over-reliance on language priors, which contribute to hallucinations [7]. For large vision-language models (LVLMs), a specific challenge is reducing their propensity to rely excessively on prior knowledge while enhancing their attention to visual input, particularly in complex, real-world environments [10]. This necessitates improving the models' understanding of object semantic relationships within real-world images [10]. While strategies like leveraging high-quality data and constructing entity-centric fine-tuning instructions hold promise [2], deviations from downstream application requirements persist, leading to various hallucinations and highlighting the need for better task alignment, symbolic reasoning, and faithful external knowledge injection [2].​  

Developing truly robust methods for evaluating hallucinations also presents significant difficulties [7]. Existing object-based evaluation frameworks may not accurately capture real-world hallucination phenomena, as they can inadvertently exploit judgmental biases present in LVLMs rather than assessing genuine factuality [8]. Current evaluation techniques have been noted to sometimes rely on superficial patterns rather than rigorously verifying the factuality and logical consistency of multimodal outputs [21]. Consequently, the development of reliable methods specifically designed to evaluate the factuality of multimodal generations remains an ongoing challenge [21]. Furthermore, biases or shortcuts embedded within existing datasets utilized for training and evaluation can be exploited by models, complicating accurate assessment [21]. Creating more robust and unbiased datasets is thus a concurrent challenge in improving evaluation [21]. Overall, there is a clear need for more effective and sophisticated monitoring and evaluation technologies to accurately assess and track hallucination in generative AI systems [5].  

# 8.2 Promising Future Research Avenues  

Despite significant advancements in mitigating hallucination in multimodal foundation models, it remains a persistent challenge requiring dedicated future research efforts [1,15].  

<html><body><table><tr><td>Research Area</td><td>Focus /Examples</td></tr><tr><td>Robust Detection/Mitigation</td><td>Optimizing data/training; model-centric techniques (fine-tuning,safety alignment); external fact-checking/bias detection.</td></tr><tr><td>Model Architectures</td><td>Improved visual-linguistic integration; reasoning modules (CoT, Reflective Tuning); dynamic modality weighting.</td></tr><tr><td>Evaluation Advancement</td><td>Comprehensive/balanced benchmarks; sophisticated automated evaluation; leveraging LLMs for assessment.</td></tr><tr><td>Knowledge Integration</td><td>Deeper KG/external knowledge base integration; interactive knowledge</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>alignment; active learning for knowledge updates.</td></tr><tr><td>Transparency (XAl)</td><td>Improving model transparency to understand hallucination causes.</td></tr><tr><td>Cognitive Dissonance</td><td>Enhancing perception and semantic representation of complex scenes.</td></tr><tr><td>Controlled Creativity</td><td>Exploring potential role of hallucination in creativity while maintaining factual control.</td></tr></table></body></html>  

Promising avenues encompass a multifaceted approach involving improvements across model development, evaluation, and deployment.  

A primary focus for future research lies in developing more robust detection and mitigation strategies [1,11,15]. This involves optimizing training data by identifying and filtering out untruthful information and adjusting training methods [11,17]. Model-centric mitigation techniques include fine-tuning strategies, mitigating language priors, and optimizing safety alignment strategies [1,7]. External mechanisms such as fact-checking, validation models, and bias detection are crucial complements [1,4]. Process-oriented improvements like enhancing verification protocols, refining review processes, setting up engineering safeguards, incorporating formal languages or guardrails, and integrating human feedback loops are also essential [5,11,22]. Furthermore, techniques exploring the editing of a model's internal representations to promote truthfulness present a novel direction [17].​  

Exploring new architectures and training methods is fundamental. This includes developing models better equipped for nuanced visual-linguistic integration, reducing over-reliance on prior knowledge, and improving their ability to handle the complexity of semantic relationships in real-world images [10]. Advancements in reasoning modules, such as Chain-ofThought Prompting or incorporating reasoning-aware training like Reflective Instruction Tuning, are seen as crucial for mitigating hallucinations linked to faulty reasoning [2,14,21]. Other architectural considerations include dynamic crossmodal fusion and developing specialized platforms trained on specific data with reduced bias [5,7]. Enhancing the understanding of visual common sense and potentially penalizing attention deviations from relevant image regions could help address challenges in accurately transferring visual encoding to semantic space [2].​  

Creating more comprehensive and balanced benchmarks is indispensable for evaluating the progress of mitigation techniques [7]. While new metrics like those in the BEAF paper have been introduced, there is still considerable room for developing more nuanced metrics that capture diverse aspects of hallucination [21]. Advancing automated evaluation methods is also a critical area [4]. Approaches using Large Language Models (LLMs) for evaluation, such as the HaELM framework, offer advantages like low cost and repeatability [8]. Developing sophisticated monitoring, evaluation, and observability tools, alongside adversarial testing, will be vital for continuous assessment [1,5]. Using other LLMs to judge the output of different models is another viable strategy [5].​  

Integrating knowledge graphs and external knowledge bases represents a significant opportunity [1,6,14]. Deeper integration can enhance fact-checking, support validation models, and provide a richer context for generation. Continually updating these knowledge sources through active learning is also suggested [1,4]. Establishing clear ethical guidelines for utilizing external knowledge sources in AI development is a necessary accompanying step [1].  

Leveraging Explainable AI (XAI) to improve the transparency and explainability of AI systems is crucial for building user trust and understanding whyhallucinations occur, which in turn can guide mitigation efforts [19,22]. Relatedly, addressing cognitive dissonance in visual-linguistic models by improving their ability to accurately perceive and semantically represent complex scenes remains a challenge [10].​  

Finally, from a different perspective, the observed phenomenon of hallucination sometimes yielding creative results warrants further exploration [1]. Understanding and potentially harnessing this aspect, while strictly controlling factual accuracy, could open novel applications.  

# 9. Conclusion  

This survey has systematically reviewed the critical phenomenon of hallucination in Multimodal Foundation Models (MMFMs), providing a comprehensive overview of its various types, underlying causes, proposed detection methods, and emergent mitigation strategies [4,6]. As highlighted throughout this work, hallucination remains a significant challenge inherent in generative AI models, stemming from factors such as limitations in training data, model architecture design, and the complex interplay between modalities [7,11,14].  

Addressing hallucination is paramount for ensuring the reliability, trustworthiness, and safe deployment of MMFMs across diverse applications, from academic research and education to critical business processes [5,13,15,22]. The manifestation of hallucinations in real-world scenarios, such as customer service or legal contexts, underscores the tangible risks associated with unreliable AI outputs [11]. Therefore, continuous research and development in this area are not merely academic pursuits but necessities for realizing the full, responsible potential of MMFMs [7,13,18].​  

Significant progress has been made in developing methods to tackle hallucinations. Evaluation and detection frameworks, such as AutoHallusion for generating hallucination benchmarks, UNIHD and MHaluBench for unified detection, and HaELM for evaluation using LLMs, provide crucial tools for understanding model limitations and diagnosing issues [8,9,10]. These efforts reveal that even advanced models remain susceptible to generating false information, particularly in complex realworld contexts [10]. Concurrent research has explored various mitigation strategies, ranging from prompt engineering and instruction optimization during inference to improvements in training data quality, architectural modifications incorporating reasoning, and post-hoc correction mechanisms like LURE [3,6,11,12,21]. These approaches, while offering promise, often involve trade-offs and are not foolproof, emphasizing the complexity of achieving robust hallucination reduction [6,12]. Key insights point towards the importance of enhancing cross-modal alignment and reducing over-reliance on unimodal priors in multimodal models [7].  

Looking ahead, the future of MMFMs hinges on the ability to mitigate hallucinations effectively. Promising directions include the development of more sophisticated automatic evaluation techniques, the integration of controlled external knowledge sources for grounded generation, and fostering a stronger synergy between models and external knowledge bases to create reliable interactive systems [2,4,18]. Ultimately, building trustworthy AI systems requires not only technological advancements in detection and mitigation but also a commitment to transparency, verifiability of AI-generated content, and user education regarding AI's inherent limitations [13,15,22]. As the field matures, continued interdisciplinary efforts are needed to navigate these challenges and ensure that the transformative potential of MMFMs is realized responsibly and reliably.​  

# References  

[1] AI基础模型“幻觉”现象研究综述 https://blog.csdn.net/weixin_44292902/article/details/137010237   
[2] 大型语言模型幻觉现象综述：认知幻觉的分析与展望 https://www.bilibili.com/read/cv26949833   
[3] LVLM幻觉修正器：基于统计分析的事后物体幻觉减轻方法 https://blog.csdn.net/Mars_prime/article/details/134652936   
[4] 基础模型“幻觉”现象研究综述 https://cloud.tencent.com/developer/article/2332943   
[5] AI Hallucination Mitigation: Monitoring and Evalua https://www.computerworld.com/article/3714290/ai-hallucination  
mitigation-two-brains-are-better-than-one.html   
[6] 大模型幻觉探究与缓解策略 https://blog.itpub.net/70018536/viewspace-2990346/   
[7] 多模态幻觉诅咒：达摩院发布CMM基准，评估多模态大模型幻觉问题 https://it.sohu.com/a/822836033_121119001   
[8] 大视觉语言模型幻觉评估与分析：基于大型语言模型的HaELM框架   
https://blog.csdn.net/Mars_prime/article/details/134712493   
[9] UNIHD：统一的多模态幻觉检测框架与MHaluBench基准 https://c.m.163.com/news/a/J88IEH1O0511CQLG.html   
[10] 视觉大模型易陷认知失调，马里兰大学提出幻觉自动生成框架AutoHallusion https://baijiahao.baidu.com/s?   
id=1815411200001322547&wfr=spider&for=pc​   
[11] AI幻觉：一本正经说瞎话，会捅多大的娄子？ https://36kr.com/p/3210091336942470   
[12] AI幻觉：实验揭示成因与应对 https://beta.huxiu.com/article/4274404.html​   
[13] AI幻觉：定义、成因、实例及缓解方法 https://baijiahao.baidu.com/s?id $\ c =$ 1817397048023793741&wfr=spider&for=pc   
[14] AI“幻觉”现象解析与应对之道 https://cloud.tencent.com/developer/article/2507907   
[15] AI幻觉：挑战与机遇 https://baijiahao.baidu.com/s?id $=$ 1822863120708825793&wf $\mathbf { \bar { \rho } } = \mathbf { \rho }$ spider&for=pc   
[16] Yiqun Liu: Web Search, User Behavior Analysis, and http://www.thuir.cn/group/\~YQLiu/   
[17] 论文论著 - 发表论文 https://nlp.ict.ac.cn/lwlz/fblw/​   
[18] 大型基础模型幻觉现象综述 https://blog.csdn.net/c_cpp_csharp/article/details/133030651   
[19] Foundation Models in Bioinformatics: A Perspective https://journal.hep.com.cn/qb/EN/10.1002/qub2.69   
[20] 大模型幻觉现象综述与缓解策略 https://blog.csdn.net/qq_35166730/article/details/133648588   
[21] 多模态视觉语言模型VLMs论文速览 (2024.07.15-2024.07.20)   
https://blog.csdn.net/weixin_44362044/article/details/140620768​   
[22] Generative AI Guidance for University Use https://www.smu.edu/oit/ai  