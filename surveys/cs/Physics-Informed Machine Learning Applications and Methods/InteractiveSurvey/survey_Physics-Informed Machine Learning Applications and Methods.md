# A Survey of Physics-Informed Machine Learning Applications and Methods

# 1 Abstract


The integration of physical laws and data-driven techniques, known as Physics-Informed Machine Learning (PIML), represents a significant advancement in computational modeling, enhancing accuracy and computational efficiency. This survey aims to provide a comprehensive overview of the current state of PIML, focusing on its core motivations, governing equations, and integration methods. The paper explores the advantages and limitations of PIML, emphasizing its enhanced generalization capability and the ability to handle noisy or sparse data, while also addressing computational challenges and the importance of accurate physical laws. Key findings include the effectiveness of PIML in various applications such as fluid dynamics, materials science, and control systems, and the potential for future developments in algorithmic techniques and the integration of PIML with advanced technologies like digital twins and edge computing. This survey serves as a valuable resource for researchers and practitioners, offering a detailed guide to the state-of-the-art in this rapidly evolving field.

# 2 Introduction
The integration of physical laws and data-driven techniques, often referred to as Physics-Informed Machine Learning (PIML), represents a significant advancement in the field of computational modeling [1]. PIML models leverage the strengths of both data-driven and physics-based approaches to achieve higher accuracy and computational efficiency [2]. By incorporating physical laws, such as conservation of mass, energy, and momentum, into the training of machine learning models, PIML ensures that the predictions adhere to the fundamental principles governing the system [1]. This integration not only reduces the reliance on large datasets but also enhances the model's ability to generalize to unseen scenarios, making it particularly useful in domains where data is scarce or expensive to obtain. The ability to reduce the dimensionality of the problem by constraining the solution space to physically feasible outcomes is a key benefit of PIML, as it improves the robustness and interpretability of the models. This approach is especially valuable in engineering applications, where the accuracy and reliability of predictions are critical for decision-making.

The research topic of this survey paper is the application and methodology of Physics-Informed Machine Learning (PIML) across various scientific and engineering domains [3]. This survey aims to provide a comprehensive overview of the current state of PIML, highlighting its core motivations, governing equations, and integration methods [3]. The paper explores the advantages and limitations of PIML methodologies, emphasizing the enhanced generalization capability and the ability to handle noisy or sparse data [2]. However, it also addresses the computational challenges and the importance of accurate physical laws in the success of PIML models [2]. The survey further examines the effectiveness of PIML in specific applications, such as fluid dynamics, materials science, and control systems, and discusses the potential for future developments, including advances in algorithmic techniques and the integration of PIML with other advanced technologies like digital twins and edge computing.

The paper delves into the integration of physical constraints into the loss function and model architecture, which are critical aspects of PIML. Loss function integration ensures that the predictions adhere to known physical laws, enhancing the robustness and generalizability of the models. The paper discusses the use of specialized tools like Google JAX, which offer advanced automatic differentiation capabilities, making the training of PIML models more efficient and scalable. Model architecture integration, on the other hand, involves embedding physical equations directly into the neural network layers or designing hybrid architectures that combine traditional physics-based models with data-driven components. These strategies are essential for achieving physically plausible and interpretable predictions, particularly in safety-critical applications.

The survey also covers the integration of training strategies in PIML, focusing on the effective incorporation of physical constraints into the training process. This includes the use of custom loss functions, adaptive sampling techniques, and domain decomposition methods, which help in efficiently exploring the solution space and reducing the computational burden. The paper discusses the role of meta-learning and network architecture modifications in enhancing the training process, ensuring that the models are robust, efficient, and reliable for a wide range of scientific and engineering applications.

The contributions of this survey paper are multifaceted. It provides a detailed and structured overview of the current landscape of Physics-Informed Machine Learning, synthesizing insights from a broad range of applications and methodologies [4]. The paper identifies key challenges and limitations in PIML and offers a critical analysis of the approaches used to overcome these challenges [3]. Additionally, it highlights the potential for future developments, such as the integration of PIML with emerging technologies and the development of more efficient and scalable algorithms. This survey serves as a valuable resource for researchers and practitioners interested in the application and advancement of PIML, offering a comprehensive guide to the state-of-the-art in this rapidly evolving field.

# 3 Physics-Informed Model Development and Integration

## 3.1 Core Motivations and Governing Equations

### 3.1.1 Integration of Physical Laws and Data-Driven Techniques
The integration of physical laws and data-driven techniques, often referred to as Physics-Informed Machine Learning (PIML), represents a significant advancement in the field of computational modeling [1]. PIML models leverage the strengths of both data-driven and physics-based approaches to achieve higher accuracy and computational efficiency [2]. By incorporating physical laws, such as conservation of mass, energy, and momentum, into the training of machine learning models, PIML ensures that the predictions adhere to the fundamental principles governing the system [3]. This integration not only reduces the reliance on large datasets but also enhances the model's ability to generalize to unseen scenarios, making it particularly useful in domains where data is scarce or expensive to obtain.

One of the key benefits of PIML is its ability to reduce the dimensionality of the problem by constraining the solution space to physically feasible outcomes [3]. This is achieved by embedding physical constraints directly into the loss function of the machine learning model, ensuring that the learned mappings are consistent with the underlying physics [5]. For instance, in the context of fluid dynamics, PIML models can be trained to satisfy the Navier-Stokes equations, leading to more accurate and physically meaningful predictions of flow behavior [6]. This approach not only improves the robustness of the model but also enhances its interpretability, as the predictions can be directly linked to known physical laws and principles [5].

Moreover, the integration of physical laws in PIML models helps mitigate the issues associated with traditional data-driven models, such as overfitting and sensitivity to noise [2]. By incorporating prior knowledge from physics, PIML models can better handle noisy or sparse data, leading to more reliable and stable predictions [7]. This is particularly important in engineering applications, where the accuracy and reliability of predictions are critical for decision-making. Additionally, the use of PIML in solving inverse problems and parameter estimation tasks has shown promising results, as the physical constraints guide the optimization process towards physically plausible solutions, thereby reducing the computational burden and improving the overall performance of the model [6].

### 3.1.2 Advantages and Limitations of PIML Methodologies
Physics-Informed Machine Learning (PIML) methodologies offer several advantages over traditional data-driven and purely physics-based models [7]. One of the primary benefits is the enhanced generalization capability, particularly in scenarios with limited or noisy data. By integrating physical laws and domain-specific knowledge, PIML models can extrapolate beyond the training data, providing robust predictions even in regions where data is scarce [7]. This is particularly valuable in safety-critical applications where data collection is challenging or expensive. Additionally, the incorporation of physical constraints acts as a form of regularization, reducing the risk of overfitting and improving the model's stability and reliability.

However, PIML methodologies also come with notable limitations. The integration of physical laws into the learning process can significantly increase the computational complexity and training time. This is especially true for problems involving complex, nonlinear dynamics or high-dimensional state spaces. The need to solve partial differential equations (PDEs) or other computationally intensive tasks during training can make PIML models less scalable compared to pure data-driven approaches [6]. Moreover, the effectiveness of PIML models heavily depends on the accuracy and relevance of the physical laws used [2]. If the underlying physics is poorly understood or incorrectly specified, the model's performance can degrade, leading to biased or unreliable predictions.

Despite these challenges, PIML methodologies offer a balanced approach that leverages the strengths of both data-driven and physics-based modeling [7]. They provide a framework for incorporating domain expertise and physical constraints, enhancing the interpretability and trustworthiness of the models. This makes PIML particularly suitable for applications in engineering, environmental science, and other fields where physical principles play a crucial role [3]. Nevertheless, careful consideration must be given to the trade-offs between computational cost, model complexity, and the availability of accurate physical laws to ensure that PIML models are effectively utilized [2].

### 3.1.3 Effectiveness and Future Developments
The effectiveness of Physics-Informed Machine Learning (PIML) models in engineering applications, particularly in aircraft design and control, is evident through their ability to integrate physical laws and constraints directly into the learning process [1]. This integration not only enhances the accuracy and reliability of the models but also ensures that the predictions are physically plausible, thereby reducing the risk of generating unrealistic or unsafe outcomes. By leveraging the known physics of the system, PIML models can provide more robust and generalizable solutions compared to purely data-driven approaches, which often suffer from overfitting and lack of interpretability [2]. This is especially crucial in safety-critical domains where the consequences of model failure can be severe.

Future developments in PIML are likely to focus on addressing the computational challenges and improving the scalability of these models [2]. Current PIML approaches, such as Physics-Informed Neural Networks (PINNs) and physics-informed neural operators (PINOs), have shown promise but are still limited by the computational cost associated with solving partial differential equations (PDEs) and ensuring that the solutions adhere to the physical constraints [8]. Advances in algorithmic techniques, such as adaptive sampling, domain decomposition, and meta-learning, are expected to reduce these costs and make PIML more accessible for real-time applications [8]. Additionally, the development of more efficient and scalable neural network architectures that can better capture the complex dynamics of physical systems will be a key area of research.

Moreover, the integration of PIML with other advanced technologies, such as digital twins and edge computing, holds significant potential for enhancing the predictive capabilities and operational efficiency of engineering systems. Digital twins, which are virtual replicas of physical systems, can benefit greatly from PIML by providing a more accurate and dynamic representation of the system's behavior under various conditions. Edge computing, on the other hand, can enable the deployment of PIML models closer to the data source, reducing latency and improving the responsiveness of the system. These advancements will not only enhance the effectiveness of PIML in current applications but also open up new opportunities for its use in emerging fields such as autonomous systems and smart cities.

## 3.2 Integration Methods Across Applications

### 3.2.1 Loss Function Integration
Loss function integration is a critical aspect of Physics-Informed Machine Learning (PIML) models, particularly in the context of sequential hybrid-ML frameworks [7]. By incorporating physical constraints directly into the loss function, these models ensure that the predictions adhere to the known physical laws and principles [9]. This approach not only enhances the robustness and reliability of the model but also improves its generalizability to unseen data. For instance, in the case of partial differential equations (PDEs), the loss function can be designed to penalize deviations from the PDE solutions, thereby ensuring that the model's predictions remain physically consistent. This is especially important in mission-critical applications where the violation of physical constraints can lead to significant errors or failures.

One of the primary challenges in integrating physical constraints into the loss function is the computational complexity associated with the evaluation of these constraints during training. Traditional machine learning frameworks like PyTorch, while powerful, are not optimized for the numerical methods required to efficiently compute these constraints. To address this, researchers have turned to more specialized tools such as Google JAX, which offers advanced automatic differentiation capabilities in both forward and backward modes. JAX's ability to handle complex mathematical operations and its seamless integration with Python make it an ideal choice for implementing physics-informed loss functions. This not only simplifies the development process but also significantly reduces the computational overhead, making the training of PIML models more efficient and scalable [2].

Furthermore, the integration of physical constraints into the loss function allows for the development of more interpretable and trustworthy models. By explicitly incorporating domain-specific knowledge, PIML models can provide insights into the underlying physical processes, which is crucial for applications in fields such as materials science, fluid dynamics, and structural engineering [7]. This approach also helps in reducing the risk of overfitting to the training data, as the physical constraints act as a form of regularization. As a result, PIML models are better equipped to handle noisy or sparse data, making them more robust and reliable in real-world scenarios [2]. The use of JAX and similar tools further enhances this capability by enabling the efficient computation of gradients, which is essential for the optimization of the loss function during training.

### 3.2.2 Model Architecture Integration
Model architecture integration in PIML frameworks is a critical aspect that determines the effectiveness of combining physical principles with machine learning [5]. This integration can be achieved through various strategies, such as embedding physical equations directly into the neural network layers, using physics-informed loss functions, or designing hybrid architectures that combine traditional physics-based models with data-driven components [3]. Each approach has its own advantages and challenges. For instance, embedding physical equations directly into the network layers ensures that the model adheres to known physical laws, which can improve the interpretability and reliability of the predictions. However, this approach requires a deep understanding of the underlying physics and can be computationally intensive.

Another approach is to use physics-informed loss functions, which incorporate physical constraints into the training process. This method allows the model to learn from both data and physical principles, leading to better generalization and robustness, especially in scenarios with limited or noisy data. The physics-informed loss function can be designed to penalize deviations from known physical laws, ensuring that the model's predictions remain physically plausible. However, the design of such loss functions can be complex and may require careful tuning to balance the contributions of the data and the physical constraints.

Hybrid architectures, which combine traditional physics-based models with data-driven components, offer a flexible and scalable approach to PIML [2]. These architectures can leverage the strengths of both paradigms, using physics-based models to capture the underlying physical processes and data-driven models to handle the complexities and uncertainties that are difficult to model with physics alone. For example, a hybrid architecture might use a physics-based model to simulate the core dynamics of a system and a neural network to correct for any discrepancies or to model the effects of unobserved variables [2]. This approach can lead to more accurate and reliable predictions, particularly in safety-critical applications where adherence to physical principles is essential.

### 3.2.3 Training Strategy Integration
Training strategy integration in physics-informed machine learning (PIML) is crucial for ensuring that the model not only learns from the available data but also adheres to the underlying physical principles [1]. One of the primary challenges in this integration is the effective incorporation of physical constraints into the training process. Traditional machine learning models often struggle with generalizing to unseen data and maintaining physical consistency, especially in complex and dynamic systems. To address these issues, various training strategies have been developed, including the use of custom loss functions, adaptive sampling, and domain decomposition. Custom loss functions, for instance, can explicitly penalize deviations from known physical laws, thereby guiding the model towards physically plausible solutions.

Another significant aspect of training strategy integration is the adaptive adjustment of training parameters and conditions. Adaptive sampling techniques dynamically select training points that are most informative for improving the model's accuracy and adherence to physical constraints. This approach helps in efficiently exploring the solution space and reducing the computational burden associated with training on large datasets. Similarly, domain decomposition methods break down the problem into smaller, more manageable subproblems, each of which can be solved independently or in parallel. This not only accelerates the training process but also allows for better handling of spatial and temporal variations in the data.

Finally, the integration of meta-learning and network architecture modifications further enhances the training process in PIML. Meta-learning techniques enable the model to learn how to learn, adapting its parameters and hyperparameters based on the specific characteristics of the physical system being modeled. This adaptability is particularly useful in scenarios where the physical laws or system dynamics change over time. Additionally, modifying the network architecture to incorporate physical characteristics, such as conservation laws or symmetry properties, ensures that the model's predictions are inherently consistent with the underlying physics. These advanced training strategies collectively contribute to the robustness, efficiency, and reliability of PIML models, making them suitable for a wide range of scientific and engineering applications [7].

# 4 Physics-Informed Optimization and Inversion

## 4.1 Physics-Informed Bayesian Optimization

### 4.1.1 Enhancing Optimization with Gaussian Processes
Gaussian Processes (GPs) have emerged as a powerful tool for enhancing optimization in various scientific and engineering domains, particularly in scenarios where the objective functions are expensive to evaluate or exhibit complex behaviors. GPs provide a flexible framework for modeling unknown functions by treating them as random processes, allowing for the incorporation of prior knowledge and the quantification of uncertainty. This is particularly advantageous in optimization contexts where the goal is to identify the global optimum with a limited number of function evaluations. By constructing a probabilistic model of the objective function, GPs enable the optimizer to balance exploration (searching unexplored regions) and exploitation (focusing on promising areas) effectively.

In the context of design optimization, GPs are often integrated with Bayesian optimization (BO) algorithms to form a robust and efficient optimization strategy. BO leverages the GP model to sequentially select the next evaluation points, guided by an acquisition function that balances the trade-off between exploration and exploitation. Common acquisition functions include Expected Improvement (EI), Probability of Improvement (PI), and Upper Confidence Bound (UCB). These functions guide the search towards regions that are likely to yield significant improvements in the objective function while also exploring uncertain areas of the design space. This approach is particularly effective in high-dimensional and multimodal optimization problems, where traditional gradient-based methods may struggle due to the presence of local optima and the curse of dimensionality.

Moreover, the integration of GPs with BO has been successfully applied to a wide range of applications, from hyperparameter tuning in machine learning models to the optimization of complex engineering systems. In these applications, GPs not only help in finding optimal solutions but also provide valuable insights into the underlying function landscape, such as the sensitivity of the objective function to different input parameters. This capability is crucial for understanding the robustness of the optimized solutions and for making informed decisions in the design process. Additionally, the ability of GPs to handle noisy and sparse data makes them particularly suitable for real-world optimization problems where data collection is costly or time-consuming.

### 4.1.2 Maximum Entropy Sampling Algorithm
The Maximum Entropy Sampling Algorithm (MESA) is a sophisticated technique designed to enhance the efficiency and accuracy of sampling in the context of optimization and design, particularly in scenarios where the underlying function or system is complex and computationally expensive to evaluate. MESA operates by selecting the next sample point that maximizes the information gain, thereby reducing uncertainty in the model predictions. This is achieved by optimizing the entropy of the predictive distribution, which is a measure of the uncertainty in the predictions. By iteratively selecting points that maximize this entropy, MESA ensures that the most informative data points are included in the training set, leading to a more robust and accurate model.

In the context of Gaussian Processes (GPs), MESA is particularly effective due to the probabilistic nature of GPs, which naturally provide a measure of uncertainty for predictions. The algorithm begins with an initial set of data points and iteratively selects new points by evaluating the entropy of the GP's predictive distribution at candidate points. The point with the highest entropy is chosen for the next sample, and this process is repeated until a stopping criterion is met, such as a maximum number of iterations or a threshold on the reduction in entropy. This approach is particularly useful in Bayesian optimization, where the goal is to find the global optimum of a black-box function with as few evaluations as possible.

MESA has been successfully applied in various fields, including the design optimization of electric machines, where it has been used to optimize the fill factor of stator slots in electrically excited synchronous machines (EESMs). By efficiently exploring the design space and identifying the most promising configurations, MESA has demonstrated its ability to significantly reduce the computational cost and improve the performance of the final design. The algorithm's adaptability and effectiveness in handling high-dimensional and non-linear problems make it a valuable tool in the toolkit of design engineers and researchers working on complex optimization tasks.

### 4.1.3 Application to Electromagnetic Performance
The application of advanced computational methods to enhance electromagnetic performance is a critical area of research, particularly in the design and optimization of electric machines. One notable approach is the use of physics-informed Bayesian optimization (PIBO) algorithms, which integrate physical principles into the optimization process to improve the slot fill factor (SFF) of electric machines. By optimizing the SFF, these algorithms aim to maximize the electromagnetic performance, leading to higher efficiency and better thermal management. The PIBO algorithm, when combined with the Multi-Objective Evolutionary Algorithm based on Decomposition and Adaptive Neighborhood (MESA), has shown significant improvements in computational efficiency and performance outcomes, reducing computation time by approximately 45% compared to conventional methods.

In the context of magnet-free electric motors, the PIBO-MESA algorithm has been particularly effective in optimizing the design of the recently invented EESM (Electromagnetic Energy Storage Motor) [10]. The elimination of permanent magnets in these motors not only reduces costs and enhances sustainability but also presents unique challenges in achieving comparable performance to traditional PM traction motors. The PIBO-MESA algorithm addresses these challenges by optimizing the internal structure and geometry of the EESM, leading to a 20% improvement in SFF. This enhancement results in better electromagnetic performance, including higher torque density and efficiency, making the EESM a viable alternative for traction applications [10].

The integration of these advanced optimization techniques with electromagnetic simulations, such as those performed using finite element methods (FEM), has further validated the effectiveness of the PIBO-MESA approach. The use of FEM allows for detailed analysis of the electromagnetic fields within the motor, ensuring that the optimized design meets the desired performance criteria. This combined approach not only accelerates the design process but also ensures that the final product is optimized for real-world operating conditions, thereby enhancing the overall electromagnetic performance and reliability of electric machines.

## 4.2 Parameter Identification and Gain Estimation

### 4.2.1 PINN-Based Inverse Models
Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving inverse problems, particularly in scenarios where the underlying physical laws are known but the parameters or initial/boundary conditions are unknown [3]. PINNs integrate the governing partial differential equations (PDEs) and physical constraints directly into the loss function of the neural network, allowing the model to learn the system dynamics while adhering to the physical laws [3]. This approach is particularly advantageous in inverse scattering problems, where the goal is to reconstruct the interaction potential from the observed scattering data [11]. By incorporating the PDEs that describe the scattering process, PINNs can effectively regularize the solution space, leading to more accurate and physically meaningful reconstructions.

In the context of inverse scattering, PINNs have been applied to various types of potentials, including nonlocal separable potentials, where the traditional methods often struggle due to the complexity of the problem. The use of PINNs in these scenarios not only simplifies the computational process but also enhances the robustness of the solution, especially when dealing with noisy or limited data. The ability of PINNs to handle small datasets is particularly valuable in inverse scattering, where experimental data is often scarce and expensive to obtain. By leveraging the physical constraints, PINNs can guide the model towards solutions that are consistent with the underlying physics, thereby reducing the risk of overfitting and improving the generalization of the model [12].

Moreover, PINNs have been successfully applied to inverse problems in other domains, such as structural dynamics and heat transfer, where they have demonstrated the capability to accurately estimate parameters and initial conditions from limited observational data. In structural dynamics, for example, PINNs have been used to estimate the dynamic response of structures subjected to seismic ground motions, providing a robust alternative to traditional physics-based models [13]. Similarly, in heat transfer applications, PINNs have been employed to estimate convective heat transfer coefficients in photovoltaic arrays, showcasing their versatility and effectiveness in handling complex physical systems. The integration of physical laws into the training process of PINNs not only enhances the accuracy of the solutions but also provides physical interpretability, making these models a valuable tool for both theoretical and practical applications in inverse problems.

### 4.2.2 Reducing Data Requirements and Improving Accuracy
Reducing data requirements and improving accuracy in machine learning (ML) models, particularly in the context of physical simulations, is a critical challenge due to the scarcity of high-quality data in many scientific domains. Traditional ML approaches often rely heavily on large datasets to achieve satisfactory performance, which can be a significant bottleneck in fields such as materials science, fluid dynamics, and quantum mechanics, where experimental data is limited or expensive to generate. To address this issue, researchers have explored various strategies that integrate physical principles into the training process, thereby reducing the dependency on extensive data. One such approach is physics-informed machine learning (PIML), which leverages the known physical laws and constraints to guide the learning process and enhance the model's predictive capabilities [1].

In PIML, the integration of physical laws is achieved by incorporating these laws directly into the loss function of the ML model [3]. For instance, partial differential equations (PDEs) that govern the behavior of physical systems can be used to regularize the training process, ensuring that the model's predictions adhere to the underlying physics. This approach not only helps in reducing the amount of data needed for training but also improves the robustness and generalizability of the model. By constraining the solution space to physically plausible outcomes, PIML can effectively handle scenarios with limited or noisy data, making it particularly useful in applications where experimental data is scarce [2]. Moreover, the use of physical constraints can help in mitigating overfitting, a common issue in data-driven models, by providing a form of regularization that aligns with the intrinsic properties of the system being modeled.

Another key aspect of reducing data requirements and improving accuracy is the development of hybrid models that combine the strengths of both data-driven and physics-based approaches. These hybrid models leverage the flexibility and adaptability of ML algorithms while maintaining the interpretability and physical consistency of traditional physics-based models [14]. For example, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) can be modified to incorporate physical insights, such as conservation laws or material properties, during the training process. This integration can lead to more accurate and reliable predictions, especially in complex environments where the data is limited or of low fidelity. Additionally, the use of synthetic data generated from physics-based simulations can augment real-world datasets, further enhancing the training process and improving the overall performance of the ML models.

### 4.2.3 Handling Discontinuities with Smooth Approximations
Handling discontinuities in computational models is a critical issue, particularly in the context of partial differential equations (PDEs) and their numerical solutions. One effective approach to addressing these discontinuities is through the use of smooth approximations, which can significantly enhance the stability and accuracy of numerical simulations. For instance, the Dirac delta function, commonly encountered in moving load problems, poses a significant challenge due to its instantaneous and discontinuous nature. Traditional methods often struggle to accurately represent such functions, leading to numerical instabilities and inaccuracies in the solution.

To mitigate these issues, researchers have proposed approximating the Dirac delta function with a smooth Gaussian function. This approach not only provides a continuous representation but also allows for the effective incorporation of the discontinuity into the loss function of physics-informed neural networks (PINNs) [15]. By doing so, the PINN architecture can more effectively learn the underlying physical processes, even in the presence of abrupt changes [12]. The Gaussian approximation effectively smooths out the discontinuity, making the problem more tractable for the neural network and improving the overall convergence and accuracy of the solution.

Furthermore, the use of smooth approximations extends beyond the Dirac delta function to other areas where discontinuities are prevalent, such as in the modeling of material interfaces or phase transitions. In these cases, the introduction of smooth transitions between different regions can significantly reduce the computational complexity and improve the robustness of the numerical methods. For example, in the context of finite element methods (FEM), smooth approximations can help in accurately capturing the behavior of materials with varying properties, thereby enhancing the fidelity of the simulations. Overall, the adoption of smooth approximations represents a powerful strategy for handling discontinuities, enabling more accurate and reliable solutions in a wide range of computational models.

# 5 Physics-Informed Neural Networks and Frameworks

## 5.1 Data-Driven Crowd Simulation

### 5.1.1 Integrating Navigation Potential Fields
Integrating navigation potential fields into machine learning frameworks represents a significant advancement in modeling complex systems, particularly in scenarios involving dynamic environments and autonomous agents. Navigation potential fields are a powerful tool for path planning and obstacle avoidance, as they enable the representation of the environment as a continuous field where the potential is lower at the target and higher around obstacles. This approach facilitates the computation of optimal paths by allowing agents to follow the gradient of the potential field towards the goal. By integrating these fields with machine learning models, particularly deep learning architectures, the framework can leverage the strengths of both approaches to achieve more robust and adaptive behavior.

The integration of navigation potential fields with physics-informed machine learning (PIML) has been particularly effective in enhancing the realism and efficiency of agent movements in dynamic environments. PIML methods, such as physics-informed neural networks (PINNs), can learn the underlying physical laws governing the system, ensuring that the generated paths are not only optimal but also physically plausible [12]. This is crucial in applications such as robotics, where adherence to physical constraints is essential for safety and performance. The use of PINNs in this context allows for the incorporation of partial differential equations (PDEs) that describe the dynamics of the environment, leading to more accurate and reliable predictions of agent movements. Additionally, the ability to handle high-dimensional observational data and complex constraints makes this approach highly versatile and applicable to a wide range of scenarios.

Moreover, the combination of navigation potential fields with deep learning models has shown promise in addressing the challenges of long-term crowd simulation and global path planning [16]. Traditional rule-based models often suffer from low heterogeneity and poor interpretability, while purely data-driven methods can struggle with generalization and long-term predictions. By integrating navigation potential fields, the framework can learn movement trends from data while ensuring that the generated paths are consistent with physical laws and environmental constraints. This hybrid approach not only improves the realism and efficiency of crowd simulations but also enhances the ability to handle dynamic and complex environments, making it a valuable tool for applications in urban planning, emergency response, and autonomous systems [16].

### 5.1.2 PI-STGCN for Heterogeneous Behaviors
Physics-informed Spatio-temporal Graph Convolutional Networks (PI-STGCN) represent a significant advancement in modeling heterogeneous behaviors in dynamic systems, particularly in the context of pedestrian movement and crowd management [16]. Unlike traditional models that rely solely on empirical data, PI-STGCN integrates physical constraints and spatio-temporal dependencies to enhance the accuracy and robustness of predictions. This integration allows the model to capture the intricate dynamics of individual and collective behaviors, ensuring that the predictions remain physically plausible and consistent with the underlying physical laws governing the system.

The core architecture of PI-STGCN leverages graph convolutional layers to model the spatial relationships between pedestrians, while recurrent neural network (RNN) components handle the temporal evolution of these interactions. By incorporating physics-informed terms, such as conservation of momentum and energy, the model can better simulate the natural movement patterns of individuals within a crowd. This is particularly important in scenarios where the behavior of pedestrians is influenced by environmental factors, such as obstacles, exits, and other pedestrians. The physics-informed terms act as regularizers, guiding the learning process to adhere to the known physical principles, thereby reducing the risk of overfitting to noisy or biased data.

Moreover, PI-STGCN addresses the challenge of heterogeneity in pedestrian behaviors by dynamically adjusting its parameters based on the observed data and the physical constraints. This adaptability is crucial in real-world applications, where the behavior of individuals can vary significantly due to personal preferences, cultural norms, and situational contexts. By effectively combining the strengths of machine learning and physical modeling, PI-STGCN provides a powerful tool for real-time crowd management, enabling more accurate and reliable predictions of pedestrian movements, which can be used to inform decision-making processes in urban planning, emergency response, and public safety.

### 5.1.3 Enhancing Accuracy and Interpretability
Enhancing the accuracy and interpretability of machine learning models is a critical challenge, especially in scientific and engineering applications where the stakes are high and the need for reliable, explainable models is paramount [5]. Traditional data-driven models, while highly effective in many domains, often suffer from limitations such as overfitting, poor generalization, and a lack of transparency. These issues are particularly pronounced when the models are applied to complex systems governed by intricate physical laws and constraints [3]. To address these challenges, researchers have increasingly turned to physics-informed machine learning (PIML) approaches, which integrate domain-specific knowledge and physical principles into the learning process [3]. By doing so, PIML models can achieve higher accuracy and better interpretability, as they are guided by the underlying physics of the system rather than relying solely on empirical data [2].

Physics-informed neural networks (PINNs) represent a prominent example of PIML, where the governing equations of the system are embedded directly into the loss function of the neural network [12]. This integration ensures that the model's predictions are not only data-consistent but also physically plausible. PINNs have been successfully applied to a wide range of problems, from fluid dynamics and heat transfer to materials science and structural mechanics. By constraining the model to adhere to known physical laws, PINNs can generalize more effectively to unseen data and provide insights into the underlying mechanisms of the system [12]. This is particularly valuable in scenarios where data is scarce or expensive to obtain, as the physical constraints help regularize the model and prevent overfitting.

Moreover, the interpretability of PIML models is significantly enhanced by their ability to provide clear, physically meaningful explanations for their predictions [2]. Unlike traditional black-box models, PIML frameworks can highlight which physical principles are most influential in the model's decision-making process, making it easier for domain experts to validate and trust the model's outputs [2]. This is crucial for applications in fields such as medicine, where the ability to explain a model's reasoning can have significant ethical and practical implications. As a result, the development and application of PIML techniques are expected to play a pivotal role in advancing the state-of-the-art in machine learning, particularly in areas where interpretability and robustness are essential [5].

## 5.2 Diagnosing Battery Degradation

### 5.2.1 PINNs for Late Aging Stage Predictions
Physics-Informed Neural Networks (PINNs) have emerged as a promising approach for predicting the late aging stages of various systems, particularly in scenarios where direct experimental data is scarce or expensive to obtain [17]. PINNs integrate physical laws, expressed as partial differential equations (PDEs), into the loss function of neural networks, ensuring that the learned models adhere to known physical principles [3]. This integration is particularly beneficial in the context of late aging predictions, where the underlying physical processes can be highly complex and nonlinear. By leveraging the physics-informed constraints, PINNs can extrapolate beyond the available training data, providing more reliable predictions of system behavior in the later stages of aging.

One of the key advantages of using PINNs for late aging stage predictions is their ability to handle the scarcity of late-life data. Traditional machine learning models often require extensive datasets covering the entire lifespan of a system, which may not be feasible or practical. PINNs, however, can be trained with limited early-stage data and still produce accurate predictions for later stages by incorporating the relevant physical equations [12]. This is achieved by penalizing the model during training if its predictions deviate from the solutions of the governing PDEs. For instance, in battery degradation studies, PINNs can predict the evolution of capacity fade and internal resistance over extended periods using only a few cycles of early-stage data, thereby reducing the need for extensive and time-consuming experiments.

Despite their advantages, PINNs face several challenges in the context of late aging predictions. One major issue is the potential mismatch between the assumed physical models and the actual underlying processes, especially when dealing with complex systems where the governing equations are not fully known or are subject to significant uncertainties. Additionally, the accuracy of PINNs can be sensitive to the quality and quantity of the available data, as well as the choice of hyperparameters and the architecture of the neural network [12]. To address these challenges, recent research has focused on developing hybrid approaches that combine PINNs with other techniques, such as co-kriging and data augmentation, to improve the robustness and reliability of predictions. These hybrid methods aim to leverage the strengths of multiple approaches to better capture the complexities of late aging phenomena.

### 5.2.2 Co-Kriging and Delta Learning
Co-Kriging and Delta Learning represent two advanced methodologies within the Physics-Informed Machine Learning (PIML) paradigm, designed to address the challenges of limited and noisy data in scientific and engineering applications [3]. Co-Kriging, a form of Gaussian process regression, extends traditional Kriging by incorporating multiple correlated outputs, allowing for the modeling of complex, multi-fidelity data. This method is particularly useful in scenarios where high-fidelity data is scarce but can be complemented with more abundant low-fidelity data. By leveraging the correlation between different levels of data fidelity, Co-Kriging can provide more accurate and reliable predictions, reducing the overall computational cost and improving the robustness of the model.

Delta Learning, on the other hand, focuses on learning the differences or "deltas" between model predictions and observed data, rather than the absolute values. This approach is particularly effective in scenarios where the underlying physical processes are partially known, and the goal is to refine the model to better fit the observed data. By learning these deltas, the model can be adjusted to account for discrepancies between the physics-based model and the real-world observations, thereby enhancing the model's predictive accuracy and generalization capabilities. Delta Learning can be further enhanced by incorporating regularization techniques, such as the elastic net, which help in preventing overfitting and improving the model's stability.

In the context of battery aging, both Co-Kriging and Delta Learning have shown significant potential in diagnosing cell degradation, especially in the late aging stage where data is often limited [12]. Co-Kriging can effectively integrate high-fidelity data from a few cells with low-fidelity data from a larger population, providing a more comprehensive understanding of the degradation process. Delta Learning, meanwhile, can refine the predictions of physics-based models by learning the deviations between the model outputs and the actual cell performance, thus improving the accuracy of degradation predictions without the need for extensive late-life data. Together, these methods offer a robust framework for enhancing the reliability and accuracy of PIML models in complex, data-scarce environments [2].

### 5.2.3 Data Augmentation for Limited Data
Data augmentation is a crucial technique in machine learning, particularly when dealing with limited data, as it helps to artificially expand the size of a dataset by generating modified versions of existing data points. This approach is essential in scenarios where collecting additional data is either impractical or prohibitively expensive. By applying various transformations such as rotations, translations, and noise injection, data augmentation can enhance the diversity of the training set, thereby improving the generalization ability of the model. In the context of limited data, data augmentation serves not only to increase the quantity of available data but also to introduce variability that can help the model learn more robust features and reduce overfitting.

In specific domains like medical imaging and material science, where data scarcity is a common issue, sophisticated data augmentation techniques have been developed to better mimic the natural variations found in the data. For instance, in medical imaging, augmentations such as elastic deformations and intensity adjustments can simulate the variations in patient anatomy and imaging conditions, making the model more resilient to these variations during deployment. Similarly, in material science, data augmentation can involve simulating different environmental conditions or defect configurations to ensure that the model can handle a wide range of scenarios. These domain-specific augmentations require careful design to ensure that the synthetic data remains realistic and relevant to the task at hand.

Moreover, recent advances in generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), have opened up new possibilities for data augmentation. These models can generate synthetic data that closely resembles the real data distribution, providing a powerful tool for expanding datasets in a controlled manner. By training on a combination of real and synthetic data, models can achieve better performance and stability, especially in tasks with limited training examples. However, the success of these generative models depends heavily on the quality of the generated data, and careful validation is necessary to ensure that the synthetic data does not introduce biases or artifacts that could degrade model performance.

# 6 Future Directions


The current landscape of Physics-Informed Machine Learning (PIML) has demonstrated significant advancements in integrating physical laws with data-driven techniques, offering enhanced accuracy, robustness, and interpretability. However, several limitations and gaps remain. One of the primary challenges is the computational complexity and training time, especially for models involving complex, nonlinear dynamics or high-dimensional state spaces. The effectiveness of PIML models also heavily depends on the accuracy and relevance of the physical laws used, and if these laws are poorly understood or incorrectly specified, the model's performance can degrade. Additionally, the integration of physical constraints into the loss function and model architecture can be computationally intensive, and there is a need for more efficient and scalable algorithms to handle these challenges.

To address these limitations, several directions for future research are proposed. First, the development of more efficient and scalable algorithms is crucial. Advances in algorithmic techniques, such as adaptive sampling, domain decomposition, and meta-learning, can significantly reduce the computational burden and make PIML more accessible for real-time applications. For instance, adaptive sampling techniques can dynamically select the most informative data points, while domain decomposition methods can break down the problem into smaller, more manageable subproblems. Meta-learning approaches can also enable models to adapt their parameters and hyperparameters based on the specific characteristics of the physical system being modeled.

Second, the integration of PIML with emerging technologies holds significant potential. Digital twins, which are virtual replicas of physical systems, can benefit greatly from PIML by providing a more accurate and dynamic representation of the system's behavior under various conditions. Edge computing can enable the deployment of PIML models closer to the data source, reducing latency and improving the responsiveness of the system. These advancements can enhance the predictive capabilities and operational efficiency of engineering systems, opening up new opportunities for their use in emerging fields such as autonomous systems and smart cities.

The potential impact of the proposed future work is substantial. By developing more efficient and scalable algorithms, PIML can be applied to a broader range of complex and dynamic systems, leading to more accurate and reliable predictions. The integration of PIML with digital twins and edge computing can revolutionize the way we model and manage physical systems, enabling real-time decision-making and optimization. This can have far-reaching implications in various fields, including aerospace, materials science, and environmental monitoring, where the accuracy and reliability of predictions are critical for safety and performance. Moreover, the enhanced interpretability and trustworthiness of PIML models can foster greater adoption and confidence in these technologies, driving innovation and progress in scientific and engineering applications.

# 7 Conclusion



The survey on Physics-Informed Machine Learning (PIML) has provided a comprehensive overview of the current state and applications of this interdisciplinary field. Key findings include the integration of physical laws and data-driven techniques to enhance model accuracy and generalization, particularly in scenarios with limited or noisy data. The survey highlights the advantages of PIML, such as reduced reliance on large datasets, improved robustness, and enhanced interpretability, while also addressing the computational challenges and the importance of accurate physical laws. The effectiveness of PIML is demonstrated across various domains, including fluid dynamics, materials science, and control systems, with a focus on the integration of physical constraints into loss functions, model architectures, and training strategies. The survey also explores the use of specialized tools like Google JAX and advanced techniques like adaptive sampling and domain decomposition to improve the efficiency and scalability of PIML models.

The significance of this survey lies in its contribution to the growing body of knowledge on PIML, providing a structured and detailed analysis of the methodologies and applications. By synthesizing insights from a broad range of studies, the survey identifies key challenges and limitations in PIML and offers critical analyses of the approaches used to overcome these challenges. It serves as a valuable resource for researchers and practitioners, offering a comprehensive guide to the state-of-the-art in PIML. The survey also highlights the potential for future developments, such as the integration of PIML with emerging technologies like digital twins and edge computing, and the development of more efficient and scalable algorithms. These advancements are expected to enhance the predictive capabilities and operational efficiency of engineering systems, opening up new opportunities for innovation in various scientific and engineering fields.

In conclusion, the field of Physics-Informed Machine Learning is poised for significant growth and impact. Researchers and practitioners are encouraged to explore the integration of PIML in their work, leveraging the insights and methodologies discussed in this survey. Future research should focus on addressing the computational challenges, improving the accuracy and efficiency of PIML models, and expanding their application to new and emerging domains. The continued development and refinement of PIML techniques will not only advance the state-of-the-art in computational modeling but also drive innovation in a wide range of scientific and engineering applications.

# References
[1] When Physics Meets Machine Learning  A Survey of Physics-Informed  Machine Learning  
[2] Physics-Infused Machine Learning Based Prediction of VTOL Aerodynamics  with Sparse Datasets  
[3] A Review of Physics-Informed Machine Learning Methods with Applications  to Condition Monitoring and  
[4] Physics-Informed Machine Learning On Polar Ice  A Survey  
[5] Physics-Informed Machine Learning  A Survey on Problems, Methods and  Applications  
[6] A Gaussian Process Framework for Solving Forward and Inverse Problems  Involving Nonlinear Partial D  
[7] Physics Informed Machine Learning (PIML) methods for estimating the  remaining useful lifetime (RUL)  
[8] Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning  (PIML) Methods  Towards Robust  
[9] Physics-Informed Machine Learning for Modeling and Control of Dynamical  Systems  
[10] A physics-informed Bayesian optimization method for rapid development of  electrical machines  
[11] Machine Learning Approach to Study of Low Energy Alpha-Deuteron Elastic  Scattering using Phase Func  
[12] Physics-Informed Machine Learning for Battery Degradation Diagnostics  A  Comparison of State-of-the  
[13] Physics-Informed Machine Learning for Seismic Response Prediction OF  Nonlinear Steel Moment Resisti  
[14] Enhancing predictive skills in physically-consistent way  Physics  Informed Machine Learning for Hyd  
[15] Plug-and-Play Physics-informed Learning using Uncertainty Quantified  Port-Hamiltonian Models  
[16] A Data-driven Crowd Simulation Framework Integrating Physics-informed  Machine Learning with Navigat  
[17] Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics  