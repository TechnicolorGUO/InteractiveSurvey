# 5/1/2025, 6:01:23 PM_Large Language Model Based Multi-Agent Systems  

0. Large Language Model Based Multi-Agent Systems  

# 1. Introduction  

The advent of Large Language Models (LLMs) has catalyzed a rapid evolution in artificial intelligence, propelling the field towards increasingly sophisticated autonomous systems known as AI agents [8,17,24]. Building upon the foundation of traditional single-agent systems, the focus is now shifting towards Large Language Model-based Multi-Agent Systems (LLMMAS), positioned as the next frontier in AI agents and agentic automation [7,14,15,28,30,33]. This transition is motivated by the inherent limitations of single agents, such as constrained context windows and the lack of diverse expertise required for complex tasks [15]. LLMs fundamentally enhance multi-agent systems by endowing individual agents with advanced capabilities in planning, reasoning, and communication [2,4,12,23]. Leveraging the extensive knowledge base and sophisticated inference abilities of LLMs, these multi-agent systems can overcome issues prevalent in standalone GenAI models, including context handling, reasoning errors, hallucinations, and biases [14,20,35]. By facilitating collaborative planning, discussion, and decision-making among agents with distinct roles and perspectives, LLM-MAS mirror human teamwork dynamics and leverage collective intelligence to address complex problems and simulate intricate real-world environments [11,12,18,22].  

The capabilities of LLM-MAS have garnered significant interest across a multitude of disciplines, highlighting the interdisciplinary nature of this rapidly evolving field [2,3,27]. Potential applications span diverse domains, including creative generation, resource scheduling, decision optimization in AIGC [3], enterprise operation and maintenance [25], connected autonomous driving [13], engineering design [5], content knowledge identification in professional development systems [32], scientific debates, and automated GUI testing [30].  

Despite the promising advancements and broad applicability, the deployment and management of LLM-based multi-agent systems present substantial challenges [1,2,17]. Key concerns include the high computational cost associated with largescale LLM inference and significant token consumption, which can hinder large-scale deployment and usage in resourceconstrained environments [6]. Ensuring scalability, robustness, and safety in production environments remains a critical hurdle [31].​  

Given the rapid proliferation of research and the lack of comprehensive, systematic reviews summarizing these developments [2], this survey aims to provide an in-depth overview of LLM-based multi-agent systems. We consolidate fundamental concepts, research trends, and applications. By addressing existing research gaps, highlighting key contributions from the literature, and identifying open challenges and future opportunities, this survey seeks to establish a structured understanding of the field and guide future research directions [2,12,23].  

# 2. Foundations of LLM-Based Multi-Agent Systems  

Understanding Large Language Model (LLM)-Based Multi-Agent Systems (MAS) necessitates a foundational grasp of both constituent technologies and their synergistic integration. This section provides an overview of LLMs, the principles of MAS, and the various approaches to integrating LLMs into MAS architectures.​  

Large Language Models, rooted in deep learning architectures like the Transformer [34], are characterized by their vast scale and training on extensive text data, which endows them with sophisticated language understanding, generation, and emergent reasoning capabilities [7,34]. Key to their advanced functionality are architectural elements that facilitate complex pattern recognition and contextual processing. While powerful, practical considerations involve trade-offs, such as balancing model size and complexity with computational cost and accessibility [26].​  

Complementing the capabilities of LLMs are Multi-Agent Systems, which embody the principles of distributed intelligence and collaborative problem-solving [11]. MAS comprise multiple autonomous agents interacting to achieve complex objectives [3,7,15]. These systems emphasize the division of complex problems into manageable sub-tasks and the  

coordination of individual agent actions through mechanisms such as cooperation, negotiation, and competition [3,5,7,15]. The benefits of MAS include enhanced efficiency, robustness through distributed processing, and the emergence of complex system-level behaviors from simple agent interactions [3,7,15,22,33]. Different architectural patterns exist within MAS, tailored to specific application domains and interaction requirements [7].  

The integration of LLMs into MAS significantly enhances the capabilities of individual agents and the performance of the collective system [2,4]. LLMs serve as cognitive engines, bolstering agents' perception, reasoning, and action-taking abilities [3,4]. They improve agent communication, enable sophisticated problem decomposition, and provide a foundation for complex reasoning and decision-making by allowing agents to process unstructured data and utilize external knowledge and tools [2,3,7,17,22,26,28]. Moreover, LLM-based MAS can mitigate some limitations of single-LLM systems, such as hallucination, through collaborative mechanisms like multi-agent debate [20,35].​  

Approaches to integrating LLMs into MAS architectures vary, including assigning specialized roles to different LLMs or LLMpowered agents, facilitating collaborative dialogues and debate structures, and developing structured frameworks that manage agent state, communication, and interaction with the environment [2,5,6,7,12,18,20,25,35]. The development and deployment of these systems also introduce new considerations for traditional software engineering principles and system software [1].​  

The subsequent sections will delve into these aspects in more detail, exploring the characteristics of LLMs, the concepts of MAS, and the specifics of their integration.  

# 2.1 Large Language Models (LLMs)  

Large Language Models (LLMs) form the foundational layer of many modern AI systems, including multi-agent systems (MAS) [7]. At their core, LLMs are AI models designed to comprehend and generate human language, typically based on deep learning architectures, notably the Transformer model [34]. Trained on massive datasets, they consist of billions of parameters, enabling them to perform a wide array of natural language processing tasks [34]. These models, such as GPT-4, Claude, LLaMA, and various open‐source alternatives like Qwen2.5-7B-Instruct, StarCoder2, and Magicoder, serve as the central processing unit for agents—facilitating the understanding of intricate instructions and the generation of human-like outputs [1,6,17,22,24,26]. They function by processing natural language inputs, discerning nuanced meanings, contextual cues, and implicit information through deep learning techniques—surpassing traditional NLP in handling linguistic subtlety [7,33].​  

The capabilities of LLMs are particularly relevant to the functionality and sophistication of MAS. Their robust language understanding allows agents to interpret complex prompts, infer necessary actions, and process unstructured data effectively [8,17,33]. Concurrently, their powerful generative capacity enables the creation of coherent, contextually appropriate, and novel text—crucial for agent communication and output generation [7,8,10]. Beyond basic language tasks, LLMs empower agents with higher-level cognitive functions. They facilitate the breakdown of complex problems into manageable steps, enable systematic reasoning, and allow agents to learn from experience for improved decision-making [12]. LLMs also grant agents the ability to utilize external tools and integrate external knowledge sources or memory mechanisms like vector databases, thereby enhancing their capacity for complex tasks and contextual learning over time [12,25].​  

Emerging capabilities include sophisticated reasoning, in-context learning, and role-playing [4]. Techniques like Chain-ofThought (CoT) prompting have been developed to further improve reasoning performance, as simple scaling proves insufficient [4]. Specific models like GPT-4 have demonstrated superior performance in critical areas for MAS collaboration, such as Theory of Mind and contextual reasoning—achieving near human-level scores in specialized tests [18]. These capabilities collectively contribute to autonomous agents capable of generalizing across diverse tasks and adapting in real time [8,22].​  

Despite their significant advancements, LLMs present certain limitations within the MAS context. A prevalent challenge is the issue of “hallucination,” where models generate factually incorrect or nonsensical information with confidence [20,35]. This directly impacts the factuality and reliability of their outputs and reasoning processes, particularly when handling complex tasks or making inferences—sometimes leading to illogical jumps in reasoning chains [20,35]. Furthermore, LLMs are fundamentally passive tools that respond to input, requiring precisely formulated prompts to yield accurate and desired results; this constraint can hinder the achievement of fully autonomous and unpredictable agent behaviors [10]. Addressing these limitations—particularly concerning factual accuracy and reasoning consistency—remains a critical area of research for deploying reliable LLM-based multi-agent systems.  

# 2.2 Multi-Agent Systems (MAS)  

Multi-Agent Systems (MAS) are conceptualized as distributed systems comprising multiple autonomous agents that interact and collaborate to accomplish complex tasks [3,7,15]. Drawing inspiration from the collaborative structures found in human society, MAS enable well-organized groups of agents to handle larger workloads and complete intricate objectives with enhanced efficiency compared to single-agent approaches [15,22,33].  

The fundamental principles of MAS include the division of complex problems into sub-tasks that are distributed among agents for collaborative completion [7,15]. Each agent within the system possesses autonomy, allowing it to make independent decisions and execute actions [7,15]. Agents are intelligent entities capable of perceiving their environment, processing information, and utilizing tools to perform assigned tasks [10]. Key interaction mechanisms facilitating collaboration include coordination, cooperation, and negotiation [3,5]. Agents engage in flexible interactions, potentially cooperating or competing, to achieve shared system goals through collaborative decision-making and action execution in a distributed, parallel manner [7,22]. This modular approach, where agents undertake specialized roles based on their unique domain knowledge, functional algorithms, and tool resources, allows for a highly specialized division of labor, contrasting with the centralized responsibility of single-agent systems [15]. The collective interaction of simple agents can give rise to complex, system-level emergent behaviors [3,7].​  

MAS architectures are diverse and tailored to the specific requirements of various applications [7]. One architectural pattern involves a managing agent coordinating multiple specialized models to tackle complex tasks, such as in Retrieval Augmented Generation (RAG) systems [26]. Another approach structures agents into networks based on their respective expertise and responsibilities, enabling efficient collaborative workflows [33]. For tasks requiring improved reasoning and factuality, debate-based architectures simulate interactions where agents iteratively generate, review, and critique each other's outputs to refine their responses [20,35]. Domain-specific problems, like intelligent operation and maintenance, leverage architectures composed of multiple sub-domain agents collaborating on tasks such as autonomous decisionmaking and repair [25]. Furthermore, specific agent architectures, such as those for generative agents in simulated environments, incorporate components like memory streams crucial for decision-making and cognitive activities [29]. These varied architectures demonstrate the adaptability of MAS for applications ranging from enhancing LLM reasoning and automating complex operations to simulating environments and managing distributed workflows. Real-world examples, such as the Manus product, further illustrate the potential suitability of these architectures in practical settings [4].  

However, designing and managing complex MAS presents significant challenges. Key difficulties include establishing and maintaining efficient communication protocols among numerous agents [4]. Coordinating the actions of autonomous agents effectively to ensure harmonious operation and progress towards common goals is also complex [4]. Ensuring agents achieve consensus on final outputs or decisions, particularly in tasks requiring high accuracy or agreement, constitutes another critical challenge [4]. Moreover, while emergent behaviors are a characteristic feature, predicting, understanding, and controlling these often unpredictable system-level phenomena remains a significant management challenge [7]. Managing the dynamic adaptation of agents to constantly changing environments adds further complexity to the design and management process [7].​  

# 2.3 Integration of LLMs in MAS  

The integration of Large Language Models (LLMs) into Multi-Agent Systems (MAS) represents a significant advancement, fundamentally enhancing the capabilities of individual agents and the collective system [2,4]. LLMs serve as the cognitive core or "brain" for autonomous agents, complemented by essential components such as planning, memory, and tool use [17,23]. This integration leverages the inherent strengths of LLMs, including their sophisticated natural language understanding and generation, extensive world knowledge, and emergent reasoning abilities, to endow agents with greater intelligence and autonomy [2,22].​  

One primary enhancement is the improvement of communication within MAS. LLMs enable more complex, flexible, and natural language interactions between agents [7], simulating human conversations and facilitating efficient, satisfactory services in applications like customer support and e-commerce [31]. This natural language capability also provides intuitive interfaces for users, allowing systems to better understand and adapt to new requirements [7].​  

Beyond communication, LLMs significantly augment agent reasoning and problem-solving capabilities. Agents can utilize LLMs to perform sophisticated reasoning [2,4], analyze unstructured data, recognize patterns, and make autonomous decisions [28]. This includes interpreting complex contexts and ambiguous information, even under information overload [33]. LLMs facilitate the decomposition of complex tasks into smaller sub-tasks manageable by different agents [3,7,26]. They also act as shared knowledge repositories, providing agents with extensive background information crucial for understanding tasks and enabling dynamic role assignment based on this understanding [7].  

Multi-agent systems integrated with LLMs effectively overcome several limitations inherent in single-agent LLM systems. While a single LLM might interact with tools, MAS involve explicit communication and collaboration among multiple agents [4]. A key limitation addressed is the "hallucination" problem; multi-agent debate mechanisms, where agents propose, critique, and refine responses, have been shown to enhance factuality and improve collective reasoning [20,35]. Furthermore, MAS can reduce the need for extensive fine-tuning and potentially lower computational costs compared to relying on a single, monolithic LLM for diverse tasks [35]. Unlike single agents limited by their context window and reliance on simple RAG over documents, MAS allow multiple agents to share context for shared goals [15], and extend knowledge access beyond documents to include databases and knowledge graphs [15]. MAS also provide a framework where agents can handle planning and memory, functionalities sometimes considered proxy requirements that future, more capable single LLMs might fulfill, but which are currently critical for autonomous behavior [15,23].​  

Different strategies are employed for integrating LLMs into MAS, each with implications for performance, scalability, and robustness. One common strategy involves specializing LLMs or assigning distinct LLM agents to represent different expert perspectives or handle specific sub-domains [2,5,12]. For example, a "chief" agent LLM can bridge anomalies detected by a system with specialized sub-domain agents for efficient operation and maintenance [25]. This specialization can improve performance and scalability by distributing tasks. Another strategy utilizes collaborative dialogues or debate mechanisms, simulating human teams to facilitate holistic problem-solving or enhance factual accuracy through critical refinement [5,20]. These debate structures, sometimes modeled as spatio-temporal graphs where agents communicate across rounds [6], prioritize robustness and reasoning quality, though they may introduce communication overhead. Frameworks like LLMCo provide structure for integration by supplying agents with state information and translating outputs into actions, enabling participation in collaborative tasks like games [18]. Other frameworks simplify complex engineering tasks such as managing memory streams and external interactions with APIs or databases [17,24,29].  

Despite the significant enhancements, integrating LLMs into MAS introduces key challenges. These include system-level complexities such as designing effective workflows and coordinating the potentially conflicting actions of multiple autonomous agents [4]. The emergence of LLM-powered systems also poses new challenges for established software engineering principles and system software [1]. Potential solutions are often embedded within the integration strategies themselves, such as developing structured coordination frameworks [18], hierarchical agent architectures with clear roles and communication protocols [25], and robust planning and memory components to guide agent behavior [17,23]. Enabling the system to understand and adapt to new scenarios and requirements through flexible LLM-driven interfaces also contributes to addressing the dynamic nature of real-world problems [7]. Research continues to explore emergent behaviors in such collaborative LLM-driven systems and refine the technical architectures necessary for stable, scalable, and robust deployments.​  

# 3. Agent Capabilities and Techniques  

The advancement of Large Language Model (LLM)-based multi-agent systems is fundamentally driven by enhancements in the core capabilities of individual agents and their ability to interact effectively as a collective [2,9,14,19,28].  

![](images/3955d189084c621ed32cc4399dbc659030b5b4b4d421eba0ef537f6531f121ab.jpg)  

These capabilities—which include reasoning, decision-making, planning, acting, memory, reflection, adaptation, learning, and tool use—empower agents to perceive environments, formulate strategies, execute actions, and pursue complex goals proactively and autonomously [10,33]. Analyzing the progress in each area and their interplay is crucial for understanding the current state and future trajectory of LLM-based multi-agent research.  

Reasoning and decision-making form the cognitive core of agents, enabling information processing, strategy formulation, and action selection in diverse settings [33]. Progress has been spurred by prompting techniques like Chain-of-Thought (CoT) and its variants (Few-Shot CoT, Zero-Shot CoT), which make the model's thinking process explicit and improve accuracy on multi-step problems, though they require more computation [4,23]. More sophisticated structures like Tree of Thoughts (ToT) and Graph of Thoughts (GoT) explore branching and iterative reasoning paths [23]. Evaluation of reasoning often employs mathematical and multi-step QA benchmarks [21]. Multi-agent systems enhance reasoning through collaborative dynamics like debate and negotiation, where agents critique and refine each other's outputs, leading to improved accuracy and rationality [20,35]. Hierarchical structures and specialized agents coordinated by a central LLM further improve decision-making for complex tasks like intelligent operations and maintenance [9,25]. Foundational multiagent reasoning skills like Theory of Mind and Contextual Reasoning are evaluated using specialized test sets, with advanced models demonstrating near human-level performance [18].​  

Planning and acting translate reasoning into purposeful behavior aimed at achieving goals [9,33]. Techniques like ReAct integrate reasoning and action in a feedback loop, allowing agents to adapt plans based on observations [23,26]. Hierarchical planning approaches manage complexity by decomposing tasks into multiple levels [15]. Actions can be defined in various ways, from executable code (CodeAct) to medium-level verb-based commands interpreted by action managers [18,26]. Specific algorithms like ${ \mathsf { A } } ^ { \star }$ search and Monte Carlo tree search are explored for navigating planning spaces [9]. While explicit planning offers interpretability compared to end-to-end methods, challenges remain in decomposing complex problems, mapping intentions to low-level actions, and handling large state/action spaces [13,23]. In multi-agent settings, planning includes anticipating teammates' actions, as seen in collaboration reasoning frameworks like ProAgent [34].​  

Memory and reflection endow agents with the capacity to learn from history and improve behavior autonomously [9,23,29]. Memory systems, inspired by human cognition, often categorize information into sensory, short-term (context window), and long-term types, with external storage leveraging techniques like Retrieval Augmented Generation (RAG) and vector databases for scalability [19,21,23]. Reflection allows agents to self-assess, critique past actions (e.g., in multi-agent debate), generate insights from memory, and correct errors, contributing to adaptability and “self-evolution” [4,20,21,29]. The synergy between memory and reflection is vital; memory provides the basis for learning, while reflection facilitates analysis and behavioral modification [9,23]. Challenges include balancing memory system trade-offs (capacity vs. speed vs. cost) and effectively designing and evaluating reflection processes [21,23,29].​  

Adaptation and learning enable agents to operate effectively in dynamic environments and continuously improve performance [33]. Techniques like Learning through Communication (LTC) utilize interaction logs and environmental feedback to generate training data for fine-tuning LLMs, facilitating continuous adaptation [9,12]. Feedback mechanisms and evaluation modules, as seen in frameworks like AGENTVERSE, provide crucial signals for adjusting strategies and agent composition [22]. Learning can also involve adjusting the LLM's prompts (Retroformer) or refining internal reasoning skills through dedicated training strategies [4,9]. Significant challenges include continual learning from non-stationary data and mitigating catastrophic forgetting [12]. Methods like CLIN and dynamic agent adjustment aim to address these issues, allowing adaptation without constant model retraining or by changing the system structure itself [9,22].  

Tool use significantly expands agents' functional capabilities by integrating external resources like APIs, databases, and code interpreters [4,23]. This allows agents to access real-time information, perform complex calculations, and interact with the external world beyond their training data [24,33]. Architectures like MRKL and HuggingGPT frame the LLM as a router for specialized tools or models [23]. The process typically involves judging tool necessity, planning API calls, parameter filling, execution, and synthesizing results [23]. Evaluation benchmarks like API-Bank and BFCL specifically assess tool utilization prowess, focusing on API selection and parameter handling [23]. A primary challenge is the effective management and selection from a potentially large set of tools, particularly in single-agent systems.​  

These capabilities are not isolated but are highly interdependent and synergistic. Reasoning informs planning and decisionmaking; memory provides the context and history necessary for robust reasoning and decision-making; reflection leverages memory to facilitate learning and adaptation; planning translates reasoning into actions which are executed, potentially with tools, and the outcomes are stored in memory and used for reflection and learning. Multi-agent systems amplify these synergies; collaboration enhances reasoning and decision-making through debate and negotiation [20], allows for distributed planning and action execution, supports shared memory or knowledge bases, enables learning through communication and coordination, and facilitates the coordinated use of specialized tools by different agents [25]. Evaluating these integrated capabilities and their emergent behaviors in complex multi-agent scenarios remains a significant area of research. While significant progress has been made across all fronts, challenges persist in areas such as reliable long-term planning, efficient memory management for vast information, robust and interpretable reflection, continuous adaptation without forgetting, and seamless, scalable tool integration, particularly in dynamic, open-ended multi-agent environments.​  

# 3.1 Reasoning and Decision-Making  

Reasoning and decision-making are foundational capabilities enabling large language model (LLM)–based agents to process information, formulate strategies, and select actions in complex environments [28,33]. Research in this area explores various techniques to enhance agent intelligence, ranging from internal cognitive processes stimulated via prompting to external interactions within multi-agent systems [9].​  

A significant focus in improving LLM reasoning has been on prompting techniques [4,23,29]. Chain-of-Thought (CoT) is a prominent method that guides LLMs to generate intermediate reasoning steps in natural language before arriving at a final answer. This technique, inspired by the observation that arithmetic reasoning benefits from explicit steps, improves accuracy, particularly for multi‐step problems [4,23]. Variants include Few-Shot CoT, which provides examples of input– thought–output sequences, and Zero-Shot CoT, which uses simple phrases like "Let’s think step by step" [23]. CoT also offers interpretability, allowing users to inspect the model’s thinking process for debugging [23]. However, it requires additional computational resources to generate these steps [23]. Self-Consistency Sampling (CoT-SC) enhances CoT by generating multiple reasoning paths and selecting the most consistent answer, often via majority voting, providing a robustness improvement over single-path CoT [23]. Building upon CoT, more complex structures like Tree of Thoughts (ToT) and Graph of Thoughts (GoT) model reasoning as interconnected steps or nodes, enabling exploration of different paths (ToT) or the combination, refinement, and enhancement of ideas through feedback loops (GoT) [23,34]. Other techniques for improving reasoning include optimizing LLMs with task-specific instructions (Agent Instructs), leveraging external knowledge graphs (Think-on-Graph), or exploring solutions to similar problems (THOUGHT PROPAGATION) [9]. Evaluating reasoning abilities often involves classical test questions such as mathematical problems (GSM8K) and multi‐step question answering (HotpotQA) [21].​  

Decision-making processes in agents involve simulating consequences, predicting outcomes, and weighing options to make comprehensive choices [28,33]. Techniques like Outcome Reward Models (ORMs) and Process Reward Models (PRMs) address the challenge of evaluating candidate solutions generated during reasoning [4]. Integrated frameworks like Language Agent Tree Search (LATS) unify planning, acting, and reasoning, while others like REX focus on exploring and exploiting the action space [9]. Asking Before Acting (ABA) allows agents to proactively seek necessary external information before committing to an action [9]. Some systems combine reasoning and reflection modules, such as DiLu for autonomous driving, or learn to ask relevant questions to guide action learning, as seen in Bilevel-LLM [9]. An introspective reasoner combined with an enhanced controller can instill behavioral rationality [9]. Furthermore, generative AI with multi-modal capabilities is being explored for training autonomous vehicle decision-making by processing multi-modal sensor data [13].  

Planning, a crucial form of reasoning [19], requires agents to decompose complex tasks into smaller, manageable steps, mirroring human problem-solving approaches [21]. While agents show proficiency in short-term planning, they currently face difficulties with long-term strategic planning, such as complex travel itineraries [21]. The ReAct paradigm integrates chain-of-thought reasoning with actions in a tight feedback loop, enabling dynamic generation of reasoning paths and iterative problem-solving [17,26]. The “plan and execute” method involves the agent outlining a complete sequence of actions before beginning execution [17]. The effectiveness of prompting techniques, such as CoT and its extensions, directly impacts planning quality by enabling models to better structure the task decomposition and sequencing steps [23].  

Multi-agent systems offer a powerful paradigm for enhancing reasoning and decision-making through interaction and collaboration [9,22]. Multi-agent debate is a notable approach where multiple LLM instances generate responses and iteratively critique each other [20]. This process of challenge and refinement encourages the construction of more truthful and rational answers, significantly improving overall reasoning and decision-making quality [20,35]. Frameworks like AGENTVERSE employ cooperative decision-making stages where agents discuss strategies and reach a consensus before acting [22]. Multi-agent dialogue is also used in frameworks like AutoGen for collaborative task completion [9]. Interdisciplinary reasoning and negotiation among agents further contribute to solving complex problems [5]. Hierarchical agent systems can utilize self-collaboration and self-correction among agents for complex tasks (e.g., OKR-Agent), while in other systems, a central LLM acts as a decision-maker coordinating specialized sub-domain agents, as seen in intelligent operations and maintenance [9,25]. Foundational skills for effective multi-agent collaboration, such as Theory of Mind and Contextual Reasoning, are essential and have been evaluated using test sets like LLM-ToM-Reasoning, demonstrating near human-level performance for advanced models like GPT-4 [18]. LLMs with fine-tuning and external knowledge bases can also enable fault analysis and localization capabilities, with the LLM serving as a central decision-maker coordinating subdomain agents [25].​  

In summary, research into LLM-based agent reasoning and decision-making is exploring diverse avenues [9]. Prompting techniques like CoT and its successors enhance internal reasoning by structuring thought processes but can be computationally intensive [23]. Integrated approaches like ReAct combine reasoning and action, while specific methods target exploration, information seeking, or strategic planning [9,17]. Multi-agent collaboration introduces external dynamics, leveraging debate, negotiation, and consensus-building to improve the accuracy, robustness, and overall quality of reasoning and decisions [20,22,35], particularly for complex and collaborative tasks. While significant progress has been made, especially in short-term planning and foundational reasoning capabilities, challenges remain in areas such as complex long-term planning and robust evaluation of intricate decision processes [21].  

# 3.2 Planning and Acting  

Enabling large language model (LLM) based agents to effectively pursue and achieve long-term goals necessitates robust planning and acting capabilities [9]. Agentic AI, characterized by proactivity and goal‐orientation, actively sets objectives and formulates plans to reach them, moving beyond passive response to external requests by identifying problems and proposing solutions [33].​  

A prominent technique for integrating reasoning and action is ReAct (Reason and Action), which guides LLMs to decompose intricate problems through step‐by‐step reasoning ("Reason") interleaved with actions ("Action"), incorporating an observation phase [23]. This approach allows agents to update their plans based on environmental feedback [26]. Few-shot examples, crafted by manually writing reasoning trajectories in ReAct format, are employed to guide the LLM, demonstrating the versatility of free-form thinking for tasks like problem decomposition, information extraction, and synthesizing results [23]. The effectiveness of ReAct has been leveraged in frameworks like XAgent, where it supports the inner loop of a dual-loop mechanism for low-level task execution [15].  

Hierarchical structures also play a significant role in managing complexity for long-term tasks. The XAgent framework utilizes an outer loop for high-level task management and planning, complementing the inner execution loop [15]. The PlanAgent within this framework can dynamically optimize plans through operations such as subtask splitting, deletion, modification, and addition [15]. Similarly, generating and refining daily schedules by breaking down high-level plans into smaller segments allows for coherent and believable action sequences over time [29].​  

Different approaches exist for defining and executing actions. In CodeAct, each action corresponds to executable code, facilitating complex operations [26]. Frameworks like AGENTVERSE include a dedicated action execution stage for agents to interact with their environment [22]. The LLM-Co framework employs an action manager to interpret the LLM's chosen  

actions, selected from a medium-level, verb-based action space (e.g., "pick", "place", "move"), and translate them into the necessary low-level execution commands [18].  

Furthermore, specific planning algorithms are explored to enhance LLM agents' capabilities. ToolChain utilizesanAsearchbased approach, while DORAEMONGPT employs Monte Carlo tree search for effective exploration of the planning space using various tools [9]. For specific challenges such as planning in embodied environments, techniques like MultiReAct have been developed [9]. In multi-agent settings, planning also involves anticipating teammates' decisions, as demonstrated by the ProAgent framework, which improves collaboration reasoning.​  

Despite these advancements, planning in complex environments presents significant challenges [23]. Decomposing complex problems, mapping high-level intentions to low-level actions, and navigating large state and action spaces are central difficulties. Techniques like ReAct address problem decomposition through explicit reasoning steps [23], while hierarchical methods help manage the scope of planning at different levels [15]. Action managers are crucial for bridging the gap between abstract LLM outputs and concrete environmental interactions [18]. Search-based methods offer strategies for exploring the complex planning landscape [9]. Compared to end-to-end schemes that directly map sensor inputs to actions, explicit planning techniques offer greater interpretability and ease of maintenance, although end-to-end methods can offer simplicity and consistent optimization goals [13]. Ensuring the coherence and believability of sequential actions remains a challenge, addressed by methods for schedule generation and refinement [29]. The ability to learn and adapt planning strategies from experience and handle unexpected situations in dynamic, complex environments are ongoing areas of research.​  

# 3.3 Memory and Reflection  

The development of sophisticated Large Language Model (LLM)-based agents necessitates robust mechanisms for retaining and processing information over time and for evaluating and refining their own behaviors. Memory and reflection are two crucial components that significantly impact an agent's adaptability and robustness, enabling them to learn from past experiences, maintain coherence across interactions, and correct errors autonomously [9,23,29].  

Memory systems in LLM agents are often designed inspired by human cognitive models, distinguishing between different temporal and functional types [19,23]. Common categorizations include sensory memory, short-term memory, and longterm memory [23]. Sensory memory pertains to the transient retention of raw sensory inputs, analogous to embedding representations in LLMs [23]. Short-term memory corresponds to the information held within the LLM's context window, facilitating real-time processing of current interactions like dialogue, though limited by finite context length [21,23]. Longterm memory extends retention over longer periods, from days to decades, and is typically implemented using external storage and fast retrieval mechanisms [21,23]. This external memory can store explicit facts and events as well as implicit skills [23]. Some models also structure memory as a stream comprising observations, reflections, and plans, further differentiating between event memory (actions and states) and thought memory (summarized reflections and plans) [29]. Multi-agent frameworks often integrate built-in memory functions to support sustained interaction histories [24].​  

Implementing long-term memory commonly leverages techniques like Retrieval Augmented Generation (RAG), involving database retrieval, vector databases, and embedding [23]. Information is embedded and stored in vector databases optimized for fast similarity search, alleviating the limitations of context windows and mitigating issues like hallucination [23]. Various nearest neighbor algorithms such as LSH, ANNOY, HNSW, FAISS, and ScaNN are employed for efficient retrieval from these databases [23]. The strategic use of memory systems, particularly the combination of short-context models with effective memory, has been shown to achieve performance comparable to models with much larger context windows [21]. Several agent architectures demonstrate the practical application of memory, including GITM which combines LLMs with text-based knowledge and memory, Synapse which uses trajectory-as-exemplar prompting with memory for computer control, DT-Mem which features an internal memory module, and MemWalker for interactive long-text reading [9].  

Reflection mechanisms enable agents to self-assess their actions and outputs, facilitating adjustments without direct external supervision [33]. This self-reflection capability is crucial for learning and improvement [33]. Reflection can manifest in various forms, such as critiquing earlier generations in a multi-agent debate setting [20]. A detailed reflection process involves retrieving relevant memory records, summarizing them, posing high-level questions about past experiences, and generating insights based on the answers, which are then integrated back into the memory stream [29]. Reflection allows agents to explore alternative problem-solving strategies, contributing to emergent "self-evolution" phenomena observed in models like DeepSeek-R1 [4]. It is also vital for error correction; when an agent receives feedback, such as a code error  

message, reflection enables it to understand the mistake and regenerate the correct output [21]. Prospector is an example of an agent that utilizes self-asking and trajectory ranking for reflection [9].  

The synergy between memory and reflection significantly enhances agent adaptability and robustness [9,23,29]. Memory provides the agent with a historical context and a basis for learning, while reflection allows the agent to analyze this history, derive insights, and modify future behavior accordingly. This enables agents to maintain performance in dynamic environments, handle complex, multi-step tasks, and recover from errors [4,33].​  

However, the effective implementation of memory and reflection presents several challenges. Designing memory systems involves trade-offs between capacity, speed of access, and computational cost [21,23]. For instance, relying solely on the LLM's context window (short-term memory) is simple but limited, while external storage (long-term memory) offers scalability but introduces retrieval latency and complexity [23]. The choice of storage method (e.g., vector database implementation) and retrieval algorithm also involves trade-offs in terms of performance characteristics [23]. For reflection, challenges include defining the optimal process for generating meaningful insights, determining when and how often reflection should occur, and integrating the results back into the agent's operational flow [29]. Furthermore, evaluating the quality and effectiveness of reflection mechanisms is challenging, often relying on coarse-grained final task outcomes rather than fine-grained assessment of the reflection process itself, highlighting a need for more refined evaluation methodologies [21].​  

# 3.4 Adaptation and Learning  

The capacity for adaptation and learning is paramount for Large Language Model (LLM)-based multi-agent systems to operate effectively across diverse and dynamic environments. These capabilities enable agents to generalize from past experiences, adjust their strategies in response to real-time information, and continuously improve their performance without constant human intervention [28,33].  

Various techniques have been explored to endow LLM agents with adaptation and learning abilities, enhancing their effectiveness and generalization [9]. Learning through Communication (LTC) is a prominent paradigm where agents leverage multi-agent communication logs or continuous interaction with the environment and other agents to generate datasets for training or fine-tuning the underlying LLMs [9,12]. This approach facilitates continuous adaptation and improvement, potentially overcoming the limitations of static context learning or supervised fine-tuning by incorporating dynamic feedback from interactions and tool use [12]. For instance, dialogue-based interactions, such as agents engaging in debates with differing opinions, can drive them towards a more accurate consensus, demonstrating adaptation through collaborative communication [20]. Similarly, frameworks like ProAgent enable agents to dynamically adapt their behavior to improve cooperation within a team [34].​  

Feedback mechanisms and evaluation modules are critical for learning and adaptation. The AGENTVERSE framework, for example, includes an evaluation module that assesses the outcome of an action against the desired goal, providing feedback to dynamically adjust agent composition and guide subsequent collaboration rounds [22]. Retroformer employs policy gradients based on environmental feedback to refine an agent's language prompt, demonstrating a learning process that directly adjusts the LLM's input to improve performance [9]. Agentic AI systems, in general, learn from experience and dynamically generate strategies based on real-time data and environmental changes [33]. Techniques like Adaptive Environmental Modeling allow agents to autonomously build models of new environments, contributing significantly to their ability to generalize [9]. Moreover, methods such as DGS iteratively refine the information agents utilize, discarding irrelevant details while retaining effective knowledge, thereby enhancing learning efficiency and generalization [9]. Beyond behavioral adaptation, enhancing core capabilities like reasoning through dedicated training strategies, such as supervised fine-tuning, preference learning (e.g., DPO), and reinforcement learning (e.g., GRPO), also constitutes a form of learning that improves agents' ability to generalize their reasoning skills, reducing reliance on complex test-time computations [4]. However, training reasoning skills faces challenges, such as the scarcity of high-quality annotated reasoning trajectory data [4].​  

Despite advancements, continual learning in dynamic environments presents significant challenges [12]. Agents must learn from continuous streams of potentially non-stationary data, integrate new knowledge without forgetting previously acquired skills (catastrophic forgetting), and adapt to evolving tasks and environmental dynamics. Techniques like LTC, by leveraging ongoing interaction feedback for training [12], offer a pathway for continuous improvement. Furthermore, methods such as CLIN aim to address these challenges by enabling continuous learning within the environment even as  

tasks and conditions change, notably without requiring parameter updates, which can help mitigate stability issues inherent in constant model retraining [9]. The dynamic adjustment of agent roles or composition, as seen in AGENTVERSE's expert recruitment module responding to problem-solving progress [22], is another strategy to adapt the system itself to the evolving demands of the task and environment.​  

# 3.5 Tool Use  

The integration of external tools represents a critical advancement in the capabilities of Large Language Model (LLM)-based agents. By leveraging external tools and resources, agents can overcome the inherent limitations of their training data and enhance their functional scope significantly [23,33]. This allows agents to access dynamic, real-time information such as weather data, perform complex calculations, query databases, execute code, interact with practical applications like calendars and maps, and call specific services that were not part of their original training [4,23,33].​  

The benefits of tool use are manifold, enabling agents to undertake complex tasks that would be impossible for standalone LLMs [23]. For example, agents can interact with external systems for tasks such as web search, online payments, or database queries [24]. Specific instances of tool-enhanced agents highlighted in the literature include AiAgent.app for user interaction, GPT Researcher for generating research reports, and AutoGPT, capable of multi-step activities like ordering food or booking tickets [10]. Furthermore, the integration with technologies like the Internet of Things (IoT) through tool use promises transformative applications in managing physical infrastructure [28]. Code Agents, utilizing Python functions as tools, exemplify another domain where external capabilities are integrated to enhance performance, often supported by frameworks providing modular tools [26].  

The integration of LLMs with external APIs and tools typically involves a structured workflow. A common approach utilizes formatted output, such as JSON, to invoke the necessary tool API interfaces [23]. This process often follows a sequence of steps: first, the agent judges whether a user query necessitates external tool interaction; then, task planning occurs, involving API retrieval and selection based on input and context, parameter judgment, and finally, parameter filling to assemble the API call; subsequently, the action is executed via the API call, potentially repeating the process; finally, an answer is generated based on the aggregated outputs [23]. Architectural paradigms like MRKL (Modular Reasoning, Knowledge and Language) propose a neuro-symbolic structure where an LLM acts as a router directing queries to specialized "expert" modules, which can be symbolic tools like calculators or external APIs [23]. Similarly, HuggingGPT employs an LLM as a task planner to select and execute models from platforms like HuggingFace as tools [23].​  

Evaluating an agent's ability to effectively utilize tools is crucial. Benchmarks like API-Bank are designed to assess this capability in practical settings, evaluating decisions on whether to call an API, selecting the correct API, processing results, and planning complex multi-API tasks [23]. Evaluation often focuses on accurate API matching based on user intent and handling complex scenarios where parameters are ambiguous or multiple steps are required [21]. The Berkeley Function Calling Leaderboard (BFCL) serves as a dynamic evaluation platform specifically targeting tool use prowess [21].  

Despite the significant advantages, integrating LLMs with external tools presents challenges. A notable limitation, particularly for single-agent systems, is the difficulty in managing and selecting the appropriate tool from a potentially vast array of available options [15]. While the subsection description calls for discussing challenges related to the reliability and security of integrating LLMs with external APIs, the provided digests do not offer specific details or analyses regarding these particular challenges in the integration process itself. They primarily focus on the functional aspects, mechanisms, and evaluation of tool use. In the enterprise realm, platforms are emerging to facilitate tool integration, offering pre-built adapters and frameworks for connecting agents to business-critical applications like CRM and project management systems [17].​  

# 4. Communication and Interaction  

Effective communication and interaction mechanisms are paramount for harnessing the collective intelligence and enabling coordinated behaviors in Large Language Model (LLM) based multi-agent systems (MAS) [4,12,24,34]. These systems facilitate complex interaction abilities, ranging from understanding requests and planning workflows to delegating responsibilities and collaborating with humans [14]. Unlike traditional systems, interaction is increasingly semantic rather than purely syntactic, allowing for greater flexibility and responsiveness to emerging needs [14].  

Different communication protocols and structures underpin inter-agent interaction, defining how information is exchanged and coordinated [12]. Communication paradigms vary widely, encompassing cooperative strategies where agents  

collaborate towards shared goals, competitive scenarios involving conflicting objectives, adversarial protocols, and debatebased interactions where agents refine ideas through argumentation to reach consensus [4,12,20,23]. Alongside paradigms, communication structures dictate the flow of information, including layered, decentralized, and centralized architectures [15]. A notable structure is the shared message pool, where agents broadcast and subscribe to relevant messages, promoting flexible information exchange [12,15]. Message passing, often implemented via function calls, is a common mechanism within these architectures [26]. The effectiveness of different approaches varies; comparative studies highlight the performance benefits, even from limited bandwidth communication, over systems lacking explicit protocols [30]. Frameworks facilitating awareness of partner states and intentions also contribute to effective communication [18].​  

The integration of LLMs significantly enhances agent interaction by enabling the use of natural language as a primary communication medium, allowing for more complex and flexible exchanges compared to rigid, traditional methods [5,7,32]. Natural language communication allows agents to articulate reasoning, critique others, and refine responses, which is crucial for paradigms like debate [20]. It also improves system transparency by enabling agents to describe environments, states, and explain their actions in human-readable form [18]. Furthermore, natural language is central to guiding individual agent behavior through sophisticated prompt engineering [26], and the content of communication is highly adaptable to specific application domains [12]. Advances in Natural Language Processing (NLP) models further streamline the generation of communication languages tailored for agent interaction [3].  

Despite the advantages, natural language communication between agents presents significant challenges, particularly in message processing [3]. The inherent ambiguity and variability of natural language complicate consistent and reliable interpretation across diverse agents, especially when precise meaning is required [3]. The computational cost associated with parsing, interpreting, and generating natural language can impact system scalability and real-time performance [3]. Ensuring factual accuracy, relevance, and preventing repetitive or nonsensical content remain complex tasks. Robustness to linguistic variations, grammatical imperfections, and managing conversational context and pragmatics are ongoing areas of research [3].​  

Optimizing communication is crucial for mitigating the high computational costs associated with LLMs, particularly concerning context window processing and message generation. Various approaches aim to reduce the volume or complexity of communication [6]. Techniques include limiting historical context passed between agents, wrapping agents as tools for better prompt control, and summarizing agent responses before incorporation [20,26]. Structural optimizations like the shared message pool improve efficiency by managing information flow through subscription [12]. Explicit link pruning methods, such as AgentPrune's "one-shot pruning" to construct a communication subgraph by retaining top-K edges, aim to reduce token consumption while preserving performance [6]. These optimization techniques often involve a trade-off between communication efficiency and message quality or completeness, which must be carefully considered in system design. Coordination frameworks also leverage LLMs as central hubs for communication orchestration [25].  

# 4.1 Communication Protocols  

Efficient communication protocols are fundamental to the performance and coordination of multi-agent systems, enabling agents to exchange information, coordinate actions, and resolve conflicts [4]. Various communication paradigms and structures exist to facilitate inter-agent interaction. Communication paradigms can range from cooperative strategies, where agents work collaboratively towards shared objectives, to competitive scenarios, where agents pursue conflicting goals [12]. Adversarial protocols, cited as examples, also fall within this spectrum [4]. Another significant paradigm is debate, where agents engage in argumentation, defending their viewpoints to arrive at a consensus or optimal outcome [12].​  

Beyond interaction paradigms, communication structures define how information flows between agents. These structures include layered, decentralized, and centralized architectures [15]. A notable structure is the shared message pool, where agents broadcast messages to a common repository and subscribe only to relevant messages, facilitating flexible information exchange [15]. Specific implementations of communication involve mechanisms like message passing, which can be facilitated through function calls between agents within a system architecture [26].  

The effectiveness of different communication approaches can significantly impact system performance. Comparative evaluations highlight the distinction between systems with and without explicit communication. Methods like IDQN and COMA, which lack explicit communication protocols, can be contrasted with variants of DIAL that employ limited bandwidth data transfer, suggesting that even constrained communication can offer performance benefits [30].  

Large Language Models (LLMs) play a crucial role in enhancing the effectiveness and sophistication of these communication protocols, particularly those relying on complex natural language interaction [3]. For instance, the debate paradigm is highly reliant on agents' ability to generate, understand, and critique natural language responses. In this protocol, agents iteratively refine their answers based on critiques from others, striving for a more accurate and reliable consensus [20]. LLMs facilitate such intricate, iterative communication by enabling agents to engage in human-like discourse, express nuanced arguments, and synthesize diverse perspectives, thereby improving the potential for complex coordination and joint problem-solving. While traditional protocols might focus on structured data exchange, LLMs enable protocols centered around rich, contextual, and persuasive communication.  

# 4.2 Natural Language for Agent Communication  

The integration of Large Language Models (LLMs) has positioned natural language as a primary and highly flexible medium for communication within multi-agent systems. This paradigm shift enables agents to interact in ways previously limited by more rigid communication protocols [7].  

Compared to traditional, often symbolic or structured communication methods, natural language facilitates more expressive and nuanced exchanges among agents [32].  

A significant advantage of using natural language is its capacity to enhance agent capabilities and system interpretability. Agents can leverage natural language to articulate their reasoning processes, critically evaluate the logic presented by other agents, and subsequently refine their own perspectives or responses. This dynamic, debate-like interaction underscores the flexibility and expressiveness of natural language in fostering effective communication and collaboration [20].  

Furthermore, natural language improves the transparency and intuitiveness of multi-agent systems. For instance, frameworks like LLM-Co employ natural language to describe the environment and agent states, providing human-readable representations of complex scenarios, which in turn supports more intuitive reasoning and action generation by the LLMbased agents. A crucial aspect is the agents' ability to articulate and explain their actions clearly via natural language, promoting transparency and understanding of their behavior [18].​  

Beyond direct inter-agent dialogue, natural language is fundamental in guiding individual agent behavior, notably through sophisticated prompt engineering techniques [26].  

The content exchanged in this text-based communication is highly adaptable to the specific domain of application, ranging from the discussion of code segments in collaborative software development to the analysis of strategies in simulated game environments [12].​  

The development of advanced natural language processing (NLP) models, such as GPT, further streamlines the generation of ommunication languages tailored for agent interaction, thereby facilitating seamless natural language communication [3].  

Despite these compelling advantages, the use of natural language for inter-agent communication presents notable challenges, particularly in message processing [3]. The inherent ambiguity and variability characteristic of natural language pose significant difficulties in ensuring consistent and reliable interpretation across a diverse set of agents. Achieving identical understanding of a natural language message by different agents, especially in contexts requiring precise meaning or subtle nuances, necessitates advanced and robust natural language understanding capabilities. The computational resources required for parsing, interpreting, and generating natural language messages can be substantial. This computational burden can impact the scalability and real-time performance of multi-agent systems, particularly as the number of agents or the complexity and volume of their communications increase. Moreover, ensuring the factual accuracy and relevance of agent communications, and preventing the generation of repetitive or nonsensical content, remains a complex challenge. Agent communication systems must also demonstrate robustness to linguistic variations, grammatical imperfections, or non-standard language use. Effectively managing conversational context and handling the pragmatic dimensions of language, such as inferring intent or understanding idiomatic expressions, continue to be areas requiring significant research and development in automated natural language processing for multi-agent systems.  

# 4.3 Communication Optimization  

Optimizing communication within Large Language Model (LLM) based multi-agent systems is critical for mitigating high computational costs and improving efficiency, primarily associated with processing extensive context windows and generating messages. Various techniques have been explored to address this challenge by reducing the volume or complexity of inter-agent communication. One approach focuses on reducing the historical context provided to agents. This includes strategies such as limiting the amount of historical information passed between agents and wrapping agents as tools to enhance prompt control and shorten prompt length, which aims to improve calculation speed [26]. Similarly, summarizing the responses of other agents before incorporating them into an agent's context can significantly reduce the required context length, potentially leading to improved performance [20].  

Beyond context and content reduction, other methods focus on structural or connectivity-based optimizations. The "shared message pool" communication structure, for instance, enhances communication efficiency by allowing agents to post messages to a common pool and subscribe only to the information relevant to their specific configuration [12]. This contrasts with direct point-to-point communication or broadcasting, streamlining information flow. A more explicit approach to optimizing communication links is demonstrated by AgentPrune, a framework designed for economic and efficient communication [6]. AgentPrune employs a "one-shot pruning" method to construct a high-quality communication subgraph, utilizing this compressed representation for subsequent inference rounds. Specifically, it retains the Top-K communication edges in the adjacency matrix, achieving information compression compared to the original complete graph [6]. This method has been shown to reduce token consumption while striving to maintain performance [6].  

These techniques highlight a fundamental trade-off between efficiency and message quality or completeness. Reducing context length through summarization [20,26] risks omitting potentially crucial details, although the aim is often to retain the most salient points. Structural methods like the shared message pool [12] manage volume by routing, but the quality depends on agent configuration and subscription rules. AgentPrune's link pruning [6] directly sacrifices potential communication paths for token reduction; its effectiveness relies on the pruning strategy successfully identifying and retaining only the most essential connections to maintain performance [6]. Analyzing these trade-offs is crucial for designing multi-agent systems that are both computationally feasible and capable of complex, high-quality interactions.​  

# 5. Architectures and Frameworks  

Designing Large Language Model (LLM)-based multi-agent systems involves critical decisions regarding their underlying architectures and the frameworks that support their implementation. These choices fundamentally impact system efficiency, scalability, robustness, and adaptability. At a high level, multi-agent system architectures can be broadly categorized based on their control and communication structures: centralized, decentralized (or distributed), and hybrid [3,4,7]. Centralized architectures, while potentially simplifying global coordination, introduce a single point of failure and can face challenges in scaling. Decentralized architectures enhance robustness and scalability by distributing control and processing but may complicate the achievement of global optima. Hybrid architectures seek to balance these trade-offs by combining elements of both approaches [3].​  

Beyond these fundamental structures, specific patterns govern how agents communicate (e.g., hierarchical, decentralized, centralized messaging) [12] and how LLMs are orchestrated for task processing. Common LLM orchestration patterns include Prompt Chaining for sequential processing, Routing for task distribution, Parallelization (via Sectioning or Voting) for concurrent execution, Orchestrator-worker models for dynamic task delegation, and Evaluator-optimizer loops for iterative refinement [23]. These patterns, often combined within a single system, contribute to modularity and offer different trade-offs in terms of efficiency, robustness, and complexity [15].  

Developing scalable and robust LLM-based multi-agent systems presents significant challenges, stemming from the inherent complexity of coordinating multiple intelligent entities, managing dynamic interactions, and ensuring reliable performance under varying loads. The choice of architecture and patterns directly influences the ability to address these challenges. For instance, distributed patterns can mitigate single points of failure and improve load handling, while careful design of communication and task execution patterns is crucial for efficient resource utilization and robust error recovery [17,23].​  

The field is supported by a growing ecosystem of frameworks and platforms designed to facilitate the creation and deployment of such systems [11,15,24,31]. These range from frameworks emphasizing role-based interactions like MetaGPT, simulating human group dynamics like AgentVerse, or using Standard Operating Procedures (SOPs) like Agents, to versatile open-source toolkits like Autogen and integration-rich platforms like CrewAI [11,12,22,24]. Concepts like Multi-Agent-as-aService (MAaaS) propose leveraging cloud infrastructure and practices such as containerization, service discovery, monitoring, and message queues to explicitly enhance scalability, high availability, and maintainability [31].  

Evaluating the strengths and weaknesses of these frameworks and platforms involves considering factors such as the types of applications they support, ease of use for developers, flexibility in defining agent behaviors and interactions, scalability under increasing demand, and maintainability over time [10,15,24,31]. For example, frameworks with extensive integrations support a wider range of applications [24], while those built on modular or service-oriented principles offer better maintainability and scalability [31,33]. Recommendations for selecting a framework or platform are therefore contingent upon the specific requirements of the intended application, including the complexity of tasks, the need for human-agent collaboration, desired level of autonomy, performance criteria, and operational considerations for deployment and management [11].  

# 5.1 Core Architectural Patterns  

![](images/788273d2237ea19eff977348705680a86e369b72d2e7d5aa0a95b8a074e192ca.jpg)  

Designing Large Language Model (LLM)-based multi-agent systems necessitates careful consideration of the underlying architectural patterns, which significantly impact system efficiency, scalability, and robustness. Broadly, these architectures can be categorized based on their control and communication structures. Centralized architectures, characterized by a single control node, offer simplified global optimization but are susceptible to a single point of failure, potentially hindering robustness and scalability [3]. In contrast, distributed architectures lack a central node, enhancing fault tolerance and potentially improving scalability by distributing load, although achieving global optima can pose challenges [3]. Hybrid architectures aim to leverage the benefits of both by combining centralized coordination with distributed decision-making [3].​  

Within these overarching structures, various communication patterns are employed. Hierarchical communication organizes agents into levels, with interactions occurring within or between adjacent tiers, suitable for structuring complex task decomposition [12]. Decentralized communication operates on a peer-to-peer basis, often found in world simulation applications, offering inherent robustness through the lack of central dependency [12]. Centralized communication involves a single coordinator, which can be made more efficient by utilizing a shared message pool for agent interactions [12].  

Beyond these structural and communication paradigms, specific patterns govern how LLMs process tasks and interact within the system. These include mechanisms for task decomposition, execution, and refinement [23]. Prompt Chaining, for instance, serializes LLM calls, where the output of one becomes the input for the next. This pattern is straightforward and allows for program checks at intermediate steps to ensure quality [23]. Its sequential nature, however, limits efficiency for tasks that could be parallelized. Routing involves classifying incoming tasks and directing them to the most appropriate downstream processes, prompts, or tools. This can significantly enhance efficiency by directing simple queries to smaller, less computationally expensive models and complex ones to more powerful models, thereby improving resource utilization and scalability [23]. The robustness of this pattern depends on the accuracy of the classification mechanism.​  

Parallelization patterns leverage concurrent LLM processing. Sectioning divides a task into independent subtasks executable simultaneously, while Voting runs the same task multiple times to aggregate diverse outputs, which can improve robustness by mitigating individual model errors [23]. These patterns offer substantial efficiency gains for suitable tasks. The Orchestrator-worker pattern, structurally similar to Routing and Parallelization, features a central LLM dynamically decomposing and delegating tasks to worker LLMs. This offers dynamic orchestration capabilities beyond predefined workflows, enhancing flexibility, though the orchestrator can represent a performance bottleneck and potential single point of failure, impacting scalability and robustness [23]. An example of this is seen in intelligent operations frameworks where a central LLM agent coordinates specialized agents for detection, root cause analysis, and fault analysis [25]. The Evaluatoroptimizer pattern employs one LLM to generate content and another to critique and provide feedback iteratively until requirements are met. While potentially less efficient due to iteration, it significantly enhances the robustness and quality of the final output by incorporating validation loops [23]. This pattern aligns with the concept of autonomous agents incorporating environmental feedback for continuous iteration [23].  

These core patterns are frequently combined to create sophisticated multi-agent systems. Hybrid architectures inherently blend centralized and distributed elements [3]. Hierarchical structures can integrate centralized coordination or a shared message pool at higher levels for global oversight, while employing decentralized communication or parallel execution among worker agents at lower tiers [12,26]. An Orchestrator-worker setup might utilize Routing to select appropriate workers or task types, incorporate Parallelization for speed, and employ an Evaluator-optimizer mechanism for output refinement. The inherent modularity of multi-agent systems facilitates the composition of these patterns, making systems easier to maintain and extend compared to monolithic single-agent designs [15]. Selecting and combining these architectural and task execution patterns depends critically on the specific requirements for efficiency, scalability, and robustness of the target application.  

# 5.2 Specific Frameworks and Platforms  

<html><body><table><tr><td>Framework/Platform</td><td>Key Approach/Focus</td><td>Distinguishing Features</td><td>Examples of Use</td></tr><tr><td>MetaGPT</td><td>Role-based, SOPs</td><td>Defined Roles, Shared Environment, Actions, Embeds</td><td>Simulating Software Companies</td></tr><tr><td>AgentVerse</td><td>Human Group Dynamics Simulation</td><td>Human Workflow Expert Recruitment, Collaborative Decision-Making, Dynamic Agent</td><td>Simulation, Collaborative Problem Solving</td></tr><tr><td>Agents Framework</td><td>SOPs for State Transitions</td><td>Agents, Environment, SOP Module, Human- Agent Interaction</td><td>General Task Execution, Interaction Scenarios</td></tr><tr><td>AutoGen</td><td>Versatile Open- Source</td><td>Multi-Agent Conversation Framework, Natural Language/Code Interaction</td><td>Collaborative Task Completion,LLM Workflows</td></tr><tr><td>CrewAl</td><td>Integration-Rich Platform</td><td>Extensive Integrations (700+ Apps), Feature-rich</td><td>Enterprise Applications, Business Workflows</td></tr><tr><td>Phidata</td><td>Python-based, MAaaS Focus</td><td>Supports various LLMs/DBs, Designed for MAaaS Deployment</td><td>MAaaS, Business Applications</td></tr><tr><td>LangGraph</td><td>Node-based</td><td>Graph Structure for Complex Systems, Enterprise Adaptability Streaming,</td><td>Building Complex Agentic Workflows</td></tr><tr><td>CAMEL</td><td>Initial Prompting Alignment</td><td>Aligns Dialogue Agents with Human Objectives</td><td>Dialogue Generation, Role- Playing</td></tr><tr><td>XAgent</td><td>Dual-Loop Mechanism</td><td>Outer (Planning) + Inner (Execution)</td><td>Complex Task Execution</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>Loops,Specialized Agents (PlanAgent, ToolAgent)</td><td></td></tr><tr><td>LLM-Coordination (LLM-Co)</td><td>Coordination Evaluation</td><td>Translates Scenarios to Text, Action Manager, Evaluates</td><td>Studying Multi- Agent Coordination Tasks</td></tr><tr><td></td><td>Deployment</td><td>Service Discovery, Monitoring, Message Queues</td><td>Scalable, Highly Available, Maintainable Deployments</td></tr></table></body></html>  

The development and deployment of Large Language Model (LLM)-based multi-agent systems necessitate robust frameworks and platforms that provide the necessary infrastructure for agent definition, interaction, and orchestration. Several prominent frameworks have emerged, each offering distinct architectural approaches and capabilities [11,12,15,24].  

Among the notable frameworks, MetaGPT is structured around defined roles, where agents possess specific profiles and goals [11,12]. Its architecture includes core components such as RoleSettings, storing agent configurations; RoleContext, managing runtime states like memory and actions; an Environment, shared by all roles for communication; and Actions, representing stages involving LLM calls or tool usage [11]. The operational cycle involves each role observing the environment, reacting through thought and action, and publishing messages back to the environment [11]. Customization is achieved by implementing specific Action and Role classes [11]. MetaGPT specifically embeds human workflow processes and standard operating procedures (SOPs) into agent operations [12].  

In contrast, AgentVerse simulates human group dynamics and decision-making processes, employing stages like expert recruitment, collaborative decision-making, action execution, and evaluation [11,22,34]. Its environment defines agents and interaction rules, utilizing components such as Describer, Order, Selector, Updater, and Visibility to manage agent interactions [11]. A key feature is the dynamic adjustment of agent composition to achieve goals [22,34].​  

The Agents framework also leverages SOPs to manage state transitions and inter-agent interactions [11]. Its core modules are Agents, Environment, and SOP, where the SOP class defines states and transitions, with each state node containing modular prompts and tools [11]. An LLM-based controller determines state transitions and the next agent to act [11]. Notably, this framework supports human users participating as agents, facilitating human-agent interaction scenarios [11].  

Other frameworks address different aspects of multi-agent system development. Autogen is recognized as a versatile, opensource framework supporting multi-agent collaboration and LLM workflows, allowing developers to define agent interactions using natural language and code [12,15,24]. CrewAI is highlighted for its rich feature set and extensive integration capabilities, supporting connections with over 700 applications, making it suitable for use by large companies [15,24]. Phidata is a Python-based framework supporting various LLMs and databases, exemplified in MAaaS deployments [24,31]. LangGraph, a node-based framework, is designed for constructing complex multi-agent systems with features like streaming support and adaptability for enterprise use [24]. CAMEL utilizes an initial prompting technology to align dialogue agents with human objectives [12]. XAgent employs a dual-loop mechanism with specialized PlanAgent and ToolAgent roles [15]. The LLM-Coordination framework (LLM-Co) focuses on evaluating and enabling LLM performance in multi-agent coordination tasks by translating game scenarios into text objectives and managing actions via an action manager [18]. LLMAgentCK represents a domain-specific framework tailored for applications like content knowledge identification in educational contexts [32]. Frameworks like Hugging Face Transformer Agent and platforms such as LangChain (used with FAISS for vector databases) are also utilized in building specific agent systems like Code Agents or Page Search Agents [26]. FMArts is mentioned as a comprehensive platform supporting the entire lifecycle of foundation model development and operation [1]. Examples of enterprise systems include Dust.tt's platform, which emphasizes creating custom connectors for fine-grained control over data flows in integrations with systems like Notion, Slack, and GitHub [17]. Other platforms noted include Grok 3 Deep Search, OpenAI Deep Research, and Manus [4], as well as the experimental OpenAI Swarm framework using agents and handoffs for orchestration [15,24]. Multi-modal anomaly detection systems may utilize an LLM主管 Agent to manage sub-domain agents processing different data types, facilitating autonomous decision-making [25].  

Evaluating these frameworks and platforms involves considering their strengths and weaknesses across several dimensions, including supporting applications, ease of use, flexibility, scalability, and cost [10,15,24,31]. Frameworks like CrewAI excel in supporting diverse applications through extensive integrations [24]. Flexibility and ease of use are influenced by the level of abstraction and customization offered, such as MetaGPT's customizable classes [11] or Autogen's natural language/codebased interaction definition [12]. The requirement for sophisticated process orchestration is critical for agentic automation, necessitating capabilities for dynamic workflow execution and seamless multi-agent collaboration [28].​  

Scalability and maintainability are significantly addressed by platforms adopting a Multi-Agent as a Service (MAaaS) paradigm [31]. MAaaS components contribute to these aspects by leveraging cloud infrastructure and services. For instance, deploying multi-agent systems on platforms like AWS EKS provides high availability and scalability through container orchestration [31]. Utilizing monitoring tools such as AWS CloudWatch enhances maintainability by providing visibility into system performance and issues [31]. Service discovery mechanisms within environments like EKS facilitate seamless communication between agents [31]. The incorporation of message queues ensures resilience against failures by buffering communication, contributing to overall system robustness and maintainability [31]. While cost is a factor, the explicit analysis of cost structures for these frameworks and platforms is not extensively detailed in the provided digests. However, cloud-based MAaaS deployments inherently involve variable operational costs depending on resource utilization [31].  

In summary, the landscape of LLM-based multi-agent systems is supported by a variety of frameworks and platforms offering different models for agent definition, interaction, and orchestration. Frameworks like MetaGPT, AgentVerse, and Agents provide structured approaches based on roles, simulated human dynamics, and SOPs, respectively [11]. Others like Autogen and CrewAI prioritize versatility and integration [12,15,24]. The MAaaS model, by integrating cloud technologies, specifically addresses the critical requirements of scalability, high availability, and maintainability through components like container orchestration, monitoring, service discovery, and message queues [31]. The choice of framework or platform depends on the specific application requirements, desired level of abstraction, and operational considerations regarding scalability and maintenance.​  

# 6. Evaluation and Benchmarking  

Rigorous evaluation and benchmarking are crucial for assessing the capabilities, limitations, and progress of Large Language Model (LLM) based multi-agent systems [21]. The performance of these complex systems is multifaceted, requiring a variety of metrics and environments to capture their diverse behaviors and emergent properties.  

Evaluation metrics commonly employed include accuracy, efficiency, robustness, and characteristics related to human-like interaction and coordination. Accuracy metrics quantify the correctness of the agent's output or final task completion [21]. Examples include the ratio of successful responses to requests [31], factual accuracy in tasks such as generating historical biographies or answering knowledge questions [20,35], the validity of generated steps or actions, such as moves in chess games [20], or domain-specific performance like Content Knowledge (CK) identification [32] and speech-to-text translation BLEU scores [34]. Efficiency metrics address resource usage and speed, such as cost efficiency [21], latency, throughput [31], and communication bandwidth or token reduction [6,30]. Robustness evaluates an agent's ability to perform consistently under varying conditions, including robustness to different partners in a multi-agent setting [18] or resilience against adversarial attacks [6]. Evaluation also considers aspects of collaboration and human-likeness, such as Theory of Mind, Contextual Reasoning, Sustained Cooperation, and Explicit Assistance in collaborative tasks [18]. A significant challenge in evaluation, particularly for complex reasoning tasks, lies in assessing the rationality of intermediate processes, which can be difficult even for human evaluators [4].​  

To provide standardized evaluation across different systems, various benchmarks have been developed. General agent capabilities are assessed by multi-dimensional benchmarks like AgentBench, which comprises 8 environments evaluating reasoning and decision-making [9]. Task-specific benchmarks target particular domains: WebArena simulates real website interactions for tasks like shopping and information retrieval [21], while SWE-bench uses real GitHub issues to test bug fixing and function addition capabilities [21]. For collaborative scenarios, benchmarks utilize interactive environments such as Collab Escape, Collab Capture, and Overcooked [18], sometimes comparing LLM agents against reinforcement learning methods [18]. Specific reasoning and knowledge tasks are evaluated using datasets like MMLU for factual knowledge [20] or the LLM-ToM-Reasoning test set for Theory of Mind and contextual reasoning [18]. Benchmarks also exist for evaluating agents' performance on machine learning tasks (MLAgentBentch) [9], compositional web automation (CompWoB) [9], planning for home services (LoTa-Bench) [9], and the ability to use and select tools (MetaTool, ToolEmu) [9]. Real-world  

problem sets like GAIA, featuring 466 problems covering reasoning and multi-modal understanding [21], and evaluations on engineering and computer science proposals [5], assess agents in more complex, integrated scenarios. The MaCKT dataset, a real-world mathematics CK dataset, is used to validate knowledge identification frameworks [32]. Dynamic interactionbased evaluations, such as DynaEval and SmartPlay, are being developed to assess agents in more realistic, interactive environments [9]. Other studies utilize custom or smaller sets of benchmarks tailored to specific method evaluations, such as the six benchmark tests used for AgentPrune [6].​  

Evaluation frameworks provide tools and infrastructure for monitoring and scoring agent performance. Platforms like LangSmith and Google Vertex AI offer functionalities such as real-time monitoring to record operational steps and tool calls, multi-dimensional scoring for final answer correctness, step rationality, and cost efficiency, and manual intervention capabilities to review controversial results [21]. These frameworks facilitate systematic testing and debugging of agent behaviors. Synthetic probing can also be used to simulate user requests and monitor relevant performance data [31].  

Despite advancements, current evaluation methods face limitations. Many benchmarks rely on static datasets, which may not fully capture agent performance in dynamic, real-world scenarios [9,21]. Evaluating the safety and potential biases of agent decisions requires rigorous testing and validation in diverse, often complex, scenarios, which current benchmarks may not adequately cover [21,28]. Cost considerations, while emerging as a scoring dimension in some frameworks [21], are not universally integrated into evaluations. Furthermore, assessing the transparency and interpretability of multi-agent system reasoning processes remains a significant challenge [4]. Future research needs to focus on developing more dynamic, interactive, and comprehensive benchmarks that better simulate real-world complexities and explicitly incorporate metrics for safety, cost, and transparency [4,21,28].  

# 7. Applications of LLM-Based Multi-Agent Systems  

Large Language Model (LLM)-based multi-agent systems (MAS) are demonstrating suitability across a diverse spectrum of applications, leveraging their collaborative reasoning and complex task-handling capabilities [2,3,12]. These systems are positioned to significantly impact various industries by automating existing processes and enabling the development of novel workflows [14,28,33]. Compared to traditional automation methods, agentic automation represents an evolution, enabling more complex, context-aware, and collaborative task execution [28]. This transformation is expected to reshape business strategies and operational paradigms across sectors [8,33].  

<html><body><table><tr><td>Application Domain</td><td>Key Capabilities Leveraged</td><td>Examples/Use Cases</td></tr><tr><td>Software Development</td><td>Collaboration, Planning, Tool Use (Code)</td><td>Programming Assistance, Bug Fixing, Automated Testing, Simulated Dev Teams</td></tr><tr><td>Customer Service</td><td>Natural Language Interaction, Reasoning, Information Retrieval</td><td>Chatbots, Virtual Assistants, Conversational Banking, Personalized Support</td></tr><tr><td>Content Creation</td><td>Generation, Collaboration (Multi-modal)</td><td>Multimedia Content, Game Scenarios, Personalized Ads, Character Interaction Simulation</td></tr><tr><td>Scientific Research</td><td>Reasoning,Tool Use (Expert Tools), Knowledge Integration</td><td>Organic Synthesis, Drug Discovery, Material Design, Healthcare Diagnostics, Educational Content ID</td></tr><tr><td>Collaborative Games</td><td>Cooperation, Strategic Planning, Interaction</td><td>Game Simulation, NPC Development, Agents in Imperfect Information Games</td></tr><tr><td>Engineering Design</td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Collaboration, Problem Solving, Knowledge Integration</td><td>Complex Multidisciplinary Projects, Design Optimization</td></tr><tr><td>Manufacturing</td><td>Real-time Decision, Prediction, Management</td><td>Dynamic Scheduling, Quality Detection, Supply Chain, Predictive Maintenance</td></tr><tr><td>Web Navigation</td><td>Perception, Planning, Acting, Tool Use</td><td>Interacting with Real Websites, Information Retrieval</td></tr><tr><td>Autonomous Driving</td><td>Perception, Decision- Making, Training Data Generation</td><td>Training Simulators, Assisting Driving Functions</td></tr><tr><td>Smart Operations</td><td>Fault Analysis, Reasoning, Knowledge Integration (Multi-modal)</td><td>Intelligent O&M, Root Cause Localization, Automated Repair</td></tr><tr><td>Collaborative Problem Solving</td><td>Debate, Negotiation, Reasoning, Tool Use</td><td>Scientific Debates, Math Problem Solving, Decision Support (Finance, Legal)</td></tr><tr><td>Information Retrieval (RAG)</td><td>Knowledge Integration, Query Answering</td><td>Multi-Agent RAG Systems, Integrating Diverse Knowledge Sources</td></tr><tr><td>Smart Cities</td><td>Optimization, Management</td><td>Traffic Management, Energy Distribution, Public Safety, Urban Planning</td></tr><tr><td>Business & Finance</td><td>Task Automation, Decision Support, Prediction</td><td>Risk Assessment, Investment, Retail Recommendations, Logistics</td></tr></table></body></html>  

Specific application domains highlight both the potential and current limitations of LLM-based MAS. In software development, LLM-based agents are being explored for programming assistance and solving real-world problems like those on GitHub [2,12,21,24]. Projects simulate software companies where agents write code, conduct meetings, and coordinate [21]. While agents perform well in simpler coding tasks, complex challenges, such as cross-file modifications or prolonged projects, still pose difficulties, with reported low success rates for top AI agents on real-world problems [21]. Autonomous GUI testing agents for mobile applications also demonstrate potential in this area [30].  

Customer service and intelligent interaction systems represent a significant application area [7,8,24,28,33]. LLM-based MAS are used in customer support chatbots, virtual assistants, and conversational banking, providing personalized and efficient responses and handling inquiries and requests automatically [7,8,33]. Simulations like τ-Bench assess agent compliance and database interaction in aviation customer service scenarios, often using user simulators to test adaptability to "tricky users" [21].​  

For content creation, multi-modal agents can collaborate to generate multimedia content, game scenarios, and personalized advertisements [3]. Applications include marketing content creation [24] and simulating character interactions [34].​  

In scientific research, LLM agents assist in specific field tasks like organic synthesis, drug discovery, and material design using expert tools [12,23]. While effective for single tasks, agents may fail when switching between different types of tasks, such as simultaneously coding and checking documents [21]. Applications extend to healthcare for diagnostics, treatment planning, and drug discovery [8,10,27,28], and education for personalized learning paths and automated assessment of teacher content knowledge [3,8,10,32]. Specialized models like OceanGPT demonstrate potential for specific scientific domains [9].​  

Collaborative games serve as simplified environments to study cooperation, strategic planning, and mutual assistance in LLM-based MAS [18]. Systems are applied in game simulation [2,12] and for building agents in imperfect information games [9]. The development of AI-driven open-world NPCs also leverages generative AI for creating complex, realistic environments [29].​  

In engineering design, LLM-based MAS frameworks are designed to support complex, multidisciplinary projects with conflicting objectives, such as senior design projects [5]. Manufacturing applications include dynamic scheduling, real-time quality detection, supply chain management, and predictive maintenance [3,8,10,27,28].​  

Web navigation is evolving from tests in simplified environments to interacting with real dynamic websites based on natural language instructions [9,21]. However, challenges remain in evaluating security aspects, such as avoiding misclicking advertisements and ensuring policy compliance [21].  

Autonomous driving benefits from generative AI for training purposes, including generating simulated events and driving scene videos for precise vehicle control training [13]. AI agents also assist in autonomous driving functions [10].  

For smart operations, LLM-based MAS are applied in intelligent operation and maintenance for tasks like fault analysis, root cause localization, and automated repair recommendations [25]. These systems process multi-modal data to provide comprehensive system views and improve efficiency [25]. Predictive maintenance is a key area within smart operations [28].  

Collaborative problem solving is enhanced through LLM-based MAS, such as scientific debate scenarios where agents exchange views to improve collective reasoning across tasks like math problems and strategic reasoning [12,20,30]. Toolintegrated reasoning agents are specifically designed for solving complex mathematical problems [9]. MAS also contribute to general complex problem-solving and decision support in fields like financial analysis and legal consulting [7]. Interdisciplinary research explores social science, psychology, economics, and policy-making through world simulation [2,12,23]. While not explicitly named "automated negotiation," game theory and economics applications are related research directions [23].​  

Information retrieval, particularly via Retrieval-Augmented Generation (RAG) systems, is viewed as an application where MAS excel, integrating knowledge from various sources to answer user queries [15,26]. An example involves a Multi-Agent RAG system for searching Wikipedia [26].  

Finally, smart cities leverage MAS for optimizing traffic management, energy distribution, public safety strategies [3], and urban planning [28]. Federated learning integrated with LLMs can support urban safety operation management [34]. Beyond these, LLM-based agents find applications in finance for risk assessment and investment [8,28], retail for personalized recommendations [8], transportation and logistics [10], agriculture [10], telecommunications [28], insurance [28], and the public sector [28]. The application landscape is broad, impacting internal business processes, product innovation, and R&D through complex task handling, real-time decision support, and continuous learning capabilities [33].  

# 8. Challenges and Future Directions  

<html><body><table><tr><td>Challenge Area</td><td>Description of Challenge</td><td>Mitigation Strategies / Future Research Directions</td></tr><tr><td>Scalability & Efficiency</td><td>High computational cost of LLMs & multi-agent interaction overhead; managing complexity with more agents.</td><td>Distributed Computing, Model Optimization/Compression, Efficient Communication Protocols (e.g., Pruning), Balancing Cost/Performance Trade-offs, Federated Learning.</td></tr><tr><td>Robustness & Reliability</td><td>Agent unpredictability ("prompt fragility"), hallucinations,incorrect reasoning,inconsistent performance under varying conditions.</td><td>Prompt Engineering, Collaborative Mechanisms (e.g., Debate),Defenses Against Adversarial Inputs, Self-Adaptive Architectures, Rigorous Testing.</td></tr></table></body></html>  

<html><body><table><tr><td>Explainability & Interpretability</td><td>Difficulty understanding agent decisions & reasoning processes; opacity of LLMs and complex interactions.</td><td>Documenting "Chain of Thought", Specialized Tracking Tools (e.g., LangSmith), Research into Inherently Explainable LLMs/Architectures.</td></tr><tr><td>Safety & Security</td><td>System errors,algorithmic bias,toxic content, data leakage,unintended operations,adversarial attacks (e.g., prompt injection), code execution risks.</td><td>Human Oversight, Data Protection Compliance, Standard Security Practices (Encryption, Access Control), Input Sanitization, Whitelisting for Code Execution, Bias Detection/Mitigation.</td></tr><tr><td>Ethical & Societal Concerns</td><td>Bias, fairness,transparency, accountability, copyright, responsible development guidelines.</td><td>Integrating Ethical Principles in Design/Deployment, Bias Detection/Mitigation, "Societal and Ethical Agents", Guidelines for Responsible Al Development, Accountability Mechanisms.</td></tr><tr><td>Future Research</td><td>Enhancing capabilities, improving system design, better collaboration, addressing real-world deployment, self- improvement.</td><td>Refining Coordination/Collaboration Strategies, Communication Optimization, Advancing Reasoning/Learning (Memory Mgmt, "Learning to Reason"), Multi-Agent Ecosystems, Automated Workflows (Feedback, Self- repair),Al Self-Evolution, Interdisciplinary Research, FMware/Infrastructure Development, Domain- Specific Optimizations.</td></tr></table></body></html>  

The advancement of Large Language Model (LLM)-based multi-agent systems, while promising, encounters significant limitations and challenges that necessitate focused research and development efforts [2]. A primary concern revolves around the inherent scalability and efficiency of these systems, driven by the substantial computational resources required by LLMs themselves and the complexities introduced by interactions among multiple agents [12,17,23]. Specific multi-agent coordination techniques, such as debate, further amplify computational overhead [20]. Addressing these issues requires optimizing computational costs, enhancing communication efficiency, and exploring solutions like distributed computing, model optimization, and efficient communication protocols [6,13,17,34].​  

Ensuring the robustness and reliability of LLM-based multi-agent systems is critical for their trustworthy deployment [28]. Challenges include the unpredictability and "prompt fragility" of individual LLM agents, as well as issues like hallucinations and incorrect reasoning within the multi-agent context [17,31]. Mitigation strategies encompass prompt engineering, leveraging collaborative mechanisms like multi-agent debate to improve factual accuracy, developing defenses against adversarial inputs, and implementing self-adaptive architectures [6,17,33,35].  

Transparency is another crucial challenge, specifically concerning the explainability and interpretability of agent decisionmaking processes [14,17,28]. The opacity of underlying LLMs and the complexity of multi-agent interactions make it difficult to understand the rationale behind system outputs [1]. Approaches like documenting the "chain of thought" and utilizing specialized tracking tools are being explored to provide greater insight into agent operations [14,17].​  

Safety and security concerns are paramount for responsible deployment [17]. This involves mitigating risks such as system errors, algorithmic biases, generation of toxic content, potential data leakage, unintended operations, and vulnerabilities to adversarial attacks like prompt injection [3,8,14,17,21,26,28,34]. Prevention measures include human oversight, stringent data protection compliance, standard security practices (encryption, access control), and techniques like input sanitization and whitelisting for code execution [8,14,17,26,28,33].  

Ethical and societal considerations, including bias, fairness, transparency, and accountability, are critical for building trust and ensuring responsible development [14,17,19,28]. Guidelines for responsible development are essential, advocating for the integration of ethical principles from design through deployment [14]. Specific issues like copyright infringement and algorithmic bias necessitate dedicated attention, including bias detection and mitigation strategies [3,34]. The concept of incorporating "societal and ethical agents" has been proposed to address broader concerns [5].  

Looking ahead, promising future research directions aim to overcome these challenges and unlock the full potential of LLMbased multi-agent systems [2,24]. Key areas include enhancing agent coordination and collaboration strategies, improving communication efficiency through novel protocols and optimization techniques, and advancing agent reasoning and learning capabilities, including memory management [3,4,6,18,22,25,29,32,33,35]. Research into system design advancements, such as multi-agent ecosystems and automated workflows with feedback loops and self-repair mechanisms, is crucial [4,15,17,33]. The potential for system self-improvement through automated data generation and dynamic updates represents an exciting frontier, requiring sophisticated evaluation benchmarks [19,20,21]. Interdisciplinary collaboration, drawing inspiration from human management, biological systems, and integrating with fields like cyber-physical systems and blockchain, offers rich avenues for innovation [3,25,27]. Furthermore, addressing practical deployment challenges, including software engineering aspects for FMware and specialized domain applications like connected autonomous driving, is vital for realizing the widespread impact of LLM-based multi-agent systems [1,13,34]. Ultimately, the goal is to leverage these systems to automate complex tasks and advance towards more intelligent and efficient automation [19,33].  

# 8.1 Scalability and Efficiency  

The scalability and efficiency of Large Language Model (LLM)-based multi-agent systems present significant challenges, primarily stemming from the substantial computational demands inherent in LLMs themselves and the complexities introduced by inter-agent interactions. Both single-agent systems and multi-agent frameworks require significant computing resources during training and deployment phases [23]. In production environments, the computational demands of LLMs can become unmanageable [17]. Furthermore, certain multi-agent procedures, such as multi-agent debate, introduce additional computational overhead due to the requirement for multiple language generations and the execution of the debate protocol, making them more computationally expensive than simpler prompting techniques [20]. Beyond raw computation, managing the increasing complexity of diverse agent types and ensuring effective agent synergy becomes critical as LLM-MA systems expand and the number of agents grows [12].​  

Addressing these challenges necessitates a careful analysis of the trade-offs between cost and performance [6,17]. High computational demands directly translate into operational costs. Optimizing these systems aims to reduce resource consumption while maintaining or enhancing performance metrics such as throughput, latency, and quality of output. For instance, multi-agent systems can sometimes reduce the computational demands compared to fine-tuning single large LLMs, potentially offering a more scalable approach in specific contexts [35]. Conversely, communication efficiency emerges as a key technical challenge in large-scale agent systems, leading to potential information overload and latency issues [3]. Economic efficiency frameworks like AgentPrune directly address this by focusing on communication optimization to reduce token consumption and integrate with existing multi-agent systems to lower overall token counts, thereby mitigating scalability and efficiency concerns [6].​  

Potential solutions to enhance scalability and efficiency involve several key strategies. Distributed computing approaches are crucial for handling the workload. Companies are already balancing loads across multiple LLM providers and accounts to manage computational demands [17]. Federated learning, for example, addresses the challenges of scaling and efficiency in training large models by distributing the process across multiple entities [34]. Model optimization and compression techniques are also vital; focus is being placed on optimizing model architecture and training processes to reduce inherent resource consumption [17]. Finally, developing and implementing efficient protocols, particularly for communication, is paramount. Solutions like AgentPrune demonstrate the benefit of reducing communication overhead through token consumption reduction [6]. In related domains like the Internet of Vehicles, semantic communication technology can significantly reduce bandwidth occupancy, increasing network capacity and preventing congestion, an principle  

transferable to inter-agent communication in large-scale systems [13]. By implementing these solutions, businesses can leverage AI agents to handle higher interaction volumes, reduce response times, and scale operations effectively, ensuring consistent service quality even during peak loads [8].  

# 8.2 Robustness and Reliability  

Ensuring the robustness and reliability of Large Language Model (LLM)-based multi-agent systems is paramount, particularly regarding the accuracy and safety of decisions and interactions [3,28]. LLM agents inherently possess characteristics that challenge reliability, notably unpredictability, where minor perturbations in input can lead to significantly different outputs [17]. This phenomenon, often termed "prompt fragility," necessitates rigorous testing and meticulous prompt engineering to mitigate [17].  

Addressing these vulnerabilities requires a multifaceted approach encompassing various techniques and architectural considerations. Prompt-level interventions, such as careful prompt engineering, prompt integration, and the strategic use of few-shot learning, have been identified as methods to enhance the robustness of individual LLM agents [17]. Beyond individual agent capabilities, collaborative strategies within the multi-agent framework offer pathways to improved reliability. For instance, multi-agent debate mechanisms are employed to reduce common LLM issues like hallucinations and improve the factual accuracy of outputs, thereby enhancing overall system reliability [35].  

Defensive mechanisms against adversarial inputs also contribute significantly to robustness. The AgentPrune framework, for example, demonstrates enhanced robustness by defending against adversarial attacks, leading to improved system performance [6]. From an architectural perspective, implementing self-adaptive loops within agentic workflows allows agents to continuously collect feedback, evaluate their performance, and adjust behaviors accordingly [33]. This iterative self-optimization process fosters continuous improvement and contributes to the long-term reliability and consistency of the system [33]. In practical applications, the reliability manifests as consistent and accurate outputs, essential for building trust, such as ensuring error-free responses in customer interactions [8]. Evaluation contexts, such as testing robustness against background noise and speaker variation in speech-to-text tasks or evaluating translation safety concerning bias and toxicity, highlight the importance of empirical assessment in validating reliability across different domains [34]. While adversarial training and data augmentation are recognized techniques for improving model robustness, and fault-tolerant architectures are crucial for system resilience, the specific application and adaptation of these methods within the unique context of multi-agent LLM systems remain active areas of research, building upon the foundational techniques identified.  

# 8.3 Explainability and Interpretability  

A significant challenge in the development and deployment of Large Language Model (LLM) based multi-agent systems is achieving adequate explainability and interpretability. Understanding the rationale behind specific decisions made by LLM agents can be difficult [17]. This lack of transparency hinders debugging, safety assurance, and user trust [28]. Consequently, there is a critical need for systems that are not only capable but also understandable and explainable [14].  

Addressing this challenge necessitates approaches that illuminate the agent's internal processes. One recommended method involves documenting the agent's "chain of thought" [14], providing a trace of the steps and reasoning leading to an outcome. Furthermore, specialized tools are emerging to support this need. For instance, platforms like LangSmith offer tracking features that allow researchers and developers to examine agent thought processes, monitor tool calls, and pinpoint potential sources of error [17]. These tools serve as valuable aids in externalizing and analyzing the agent's operational flow.​  

Beyond these external tracking and documentation methods, the inherent characteristics of the underlying LLMs themselves pose challenges to interpretability. The opacity of many advanced, especially open-source, models can directly impede the ability to understand the behavior and outputs of the agents built upon them [1]. This highlights the need for further research not only into agent architectures and tracking methodologies but also potentially into developing more inherently explainable LLM architectures or training techniques that facilitate greater transparency. While the provided digests do not detail specific architectural modifications like attention mechanism analysis or advanced visualization tools, the underlying issue of model transparency [1] underscores the relevance of such future research directions in enhancing the overall explainability of LLM-based multi-agent systems. Achieving robust explainability is paramount for ensuring the reliability, safety, and widespread adoption of these complex systems [14,17,28].​  

# 8.4 Safety and Security  

The deployment of Large Language Model (LLM)-based multi-agent systems introduces significant safety and security considerations that must be addressed to ensure trustworthy and responsible operation. Addressing these concerns is crucial for the successful integration of these systems into various applications [17].  

Safety concerns encompass various risks associated with agent behavior and outputs. These include the potential for system errors, algorithmic biases (such as gender bias or biases in translation outputs), the generation of toxic content, issues related to copyright ownership, and the overall robustness of the system [3,14,34]. Furthermore, agents may perform unintended or dangerous operations and could inadvertently leak private information [21]. Mitigation strategies for these safety issues often involve implementing mechanisms like human oversight to identify and correct errors and biases that automated systems might perpetuate [14].  

From a security perspective, protecting data privacy and ensuring the security of sensitive information are paramount, particularly when these systems interact with customer data or other confidential assets [8,28]. Compliance with stringent data protection regulations, such as GDPR and CCPA, is crucial [8]. Specific security risks include the potential for agents to leak private data [21] and the inherent dangers associated with allowing agents, particularly code agents, to execute code [26]. A prevalent threat is prompt injection, where malicious actors craft adversarial prompts to manipulate agents into bypassing security protocols or disclosing sensitive information [17]. This type of attack highlights the vulnerability of agents to manipulated inputs [17].​  

Addressing these security risks necessitates the implementation of robust defense mechanisms. Standard security practices, such as encryption, access control, and audit tracking, are essential for protecting sensitive information handled by agentic workflows and systems [28,33]. To mitigate the risks of code execution, techniques like whitelisting can be employed to restrict agents to authorized commands and environments [26]. Countering prompt injection attacks requires rigorous input sanitization and validation processes to detect and neutralize malicious inputs before they can affect agent behavior [17]. Overall, a multi-layered approach integrating technological safeguards with procedural controls and human oversight is critical for developing secure and safe LLM-based multi-agent systems.​  

# 8.5 Ethical and Societal Concerns  

Addressing the ethical and societal implications of Large Language Model (LLM)-based multi-agent systems is paramount for their responsible development and deployment [17,19]. A fundamental recommendation across the field is that ethical principles should serve as guiding tenets throughout the system design and deployment lifecycle [14]. Prioritizing fairness, equity, and ethical AI practices is deemed essential for cultivating trust in these advanced systems [28].  

Key ethical considerations encompass issues inherent in the generated content and algorithmic processes. Specifically, concerns such as copyright infringement and algorithm bias have been identified in the context of AI-generated content systems [3]. Bias detection and mitigation are critical aspects requiring dedicated attention. For instance, evaluations of specific LLM applications, like the SeamlessM4T translation system, have focused on assessing gender bias and the potential for introducing toxicity during the translation process [34].  

Beyond specific technical biases, the broader spectrum of ethical, social, and environmental concerns necessitates consideration. Some proposed frameworks for multi-agent systems explicitly incorporate the concept of "societal and ethical agents" designed to actively address these multifaceted issues [5]. Developing comprehensive guidelines for responsible development and deployment is crucial to navigate these challenges and ensure accountability across the system lifecycle [17,19]. Establishing clear mechanisms for accountability is vital to manage the potential impacts of autonomous multi-agent behaviors.​  

# 8.6 Future Research Directions  

The advancement of large language model (LLM) based multi-agent systems presents significant opportunities while simultaneously introducing substantial research challenges [2]. Future research should focus on refining foundational capabilities, improving system design, enhancing collaboration mechanisms, and addressing real-world deployment complexities. A primary challenge lies in ensuring the accuracy and safety of decisions made by agentic automation systems [28]. Broader software engineering considerations also necessitate research into platform and software support for LLMpowered applications, deep integration of traditional software with LLMs, and the development of open-source and transparent code LLMs, alongside addressing challenges in creating and deploying high-quality foundation model software (FMware) [1].​  

Algorithmic and systemic improvements are crucial for unlocking the full potential of these systems. Research avenues include enhancing coordination and collaboration strategies among agents. This involves exploring approaches to utilize positive strategies and mitigate negative ones within multi-agent groups [22], studying the emergence of social behaviors [22], and improving agents' capacity to identify and respond to partner needs, potentially through sophisticated prompting or internal reasoning mechanisms [18]. Refining debate mechanisms has shown promise in enhancing language generation and overall system performance [35], suggesting this remains a valuable research direction. Exploring multi-agent collaboration frameworks inspired by enterprise organizational management methods [25] or bio-inspired self-organizing agent swarms [3] represents interdisciplinary approaches to coordination.​  

Communication among agents is another critical area. Future work should investigate a broader spectrum of communication strategies [32] and optimize communication efficiency through techniques like adaptive adjustment for varying agent numbers and complex scenarios [6]. This could involve combining pruning operations with token reduction strategies and integrating cache sharing for multi-round interactions [6]. The design of cross-lingual and cross-cultural agent interaction protocols also warrants attention [3].​  

Advancements in agent reasoning and learning capabilities are essential. Developing "learning to reason" methods and exploring the unification of "inference scaling" and "learning to reason" approaches are key future directions [4]. Adaptive learning is integral to the design patterns for robust, autonomous agentic workflows [33]. Furthermore, challenges related to LLM memory management, such as autonomous parameter management and applying dynamic weights based on task types, need to be addressed [29].  

From a system design perspective, multi-agent ecosystems are recognized for offering greater control, flexibility, and scalability compared to monolithic agent designs [17]. Future research could explore the potential for AI applications to run on dedicated AI computer virtual instances with specialized planning agents acting as processing units [15]. Designing automated workflows guided by agentic patterns like feedback loops, tool use, planning, and self-repair is a significant area for development [4,33].​  

The self-improvement of agents and the overall system presents intriguing possibilities. Utilizing multi-agent systems to generate additional data that can be extracted to self-improve the base model is a promising direction [20]. Research could aim towards achieving an "AI self-evolution cycle" through automated test question generation and dynamic system updates that simulate real-world failures [21]. This highlights the need for sophisticated benchmarks capable of evaluating agent performance under dynamic conditions [21].  

Interdisciplinary research is vital for realizing the broader potential of LLM-based multi-agent systems. Investigating humanmachine collaboration models [4] and drawing inspiration from human management methods to treat agents as collaborative equals [25] are examples. Convergence with other fields, such as biologically inspired approaches, cyberphysical systems, and blockchain technologies, offers rich avenues for exploration [27].  

Finally, focusing on application and deployment challenges is critical. Integrating industrial, academic, and research resources to develop large models for specific vertical fields and promote large-scale industrial applications, including multimodal federated learning, is a key goal [34]. Specific application domains, such as connected autonomous driving, necessitate further research into model compression, pruning, hardware optimization, and privacy protection for domainspecific models [13]. Ultimately, leveraging LLMs as a driving force to automate complex tasks and achieve the goal of automating human efforts through advanced multi-agent systems remains a central aspiration [19].  

# 9. Conclusion  

This survey has explored the rapidly evolving field of Large Language Model (LLM) based Multi-Agent Systems (MAS), highlighting their potential to address complex challenges and revolutionize collaboration [2,7]. The core theme throughout the literature is the synergy between the advanced cognitive capabilities of LLMs and the distributed problem-solving inherent in MAS architectures, enabling systems that are smarter, more flexible, and powerful than their single-agent counterparts [7,15].  

Key achievements summarized from the research include the demonstrated potential of LLM-based MAS across diverse application domains. These range from connected autonomous driving, where generative AI and agent-based models can enhance perception, decision-making, and execution [13], to intelligent operation and maintenance (O&M), leveraging domain knowledge and multi-modal data for improved fault analysis and repair [25]. Furthermore, LLM-MAS are being explored for complex engineering projects, facilitating teamwork, problem-solving, and integrating diverse knowledge  

domains [5], and within the AIGC field, enabling intelligent collaboration and unlocking the potential of distributed intelligence [3]. In business, AI agents and MAS are reshaping strategies by automating tasks, improving customer service, and driving productivity, promising enhanced efficiency and scalability [8,14]. Educational applications are also emerging, such as using multi-agent LLMs for content knowledge identification without manual annotation [32]. The evolution of LLM reasoning itself is shifting from standalone models to multi-agent systems, emphasizing learning to reason and the importance of designing key components like refiners and verifiers [4]. Frameworks are being developed to simplify the development and deployment of AI agents [24], facilitate multi-agent collaboration, and explore emergent behaviors [11,22]. Techniques like multi-agent debate have shown effectiveness in enhancing the reasoning and factual accuracy of LLMs [20,35]. The concept of Multi-Agent as a Service (MAaaS), built on cloud-native principles, promises scalable and production-ready AI applications [31].​  

Despite these advancements, significant challenges remain. LLM agents are not yet fully mature, facing issues such as system instability, knowledge hallucinations, understanding biases, and the critical risk of blindly executing dangerous instructions [17]. Communication optimization is a practical challenge, addressed by frameworks like AgentPrune which prunes redundant information to improve efficiency and robustness, particularly in resource-constrained environments [6]. The evaluation of AI Agents and MAS is an ongoing research area, requiring methods that better understand human intent and evaluate performance beyond basic capabilities [21]. While LLMs show potential in multi-agent collaboration, particularly in understanding partner intentions and reasoning contextually, areas for improvement exist, such as enhancing agents' ability to proactively offer assistance and adapt to partners [18]. Most current agents tend to use limited tools, often as a single sub-module within the LLM [11].​  

The potential of LLM-based MAS to revolutionize complex problem-solving and collaboration is substantial [2]. They represent the next evolution of automation, promising to reshape work, production, and lifestyle [28,33]. However, realizing this potential necessitates a strong emphasis on responsible development [17]. Addressing challenges related to safety, reliability, and ethical considerations, including data privacy and training data quality, is paramount [5,8,17]. Human oversight remains crucial during deployment [17].​  

Looking ahead, continued research is vital [2]. This includes refining communication strategies and leveraging advanced techniques [32], exploring brain-inspired architectures, modularity, self-evolution, and collaboration mechanisms [19], and developing robust infrastructure and engineering practices to support these complex systems [27,31]. The field is dynamic, with ongoing efforts to compile and share resources to support further investigation [36]. By systematically addressing the identified challenges and continuing to innovate, researchers can pave the way for the responsible and effective deployment of LLM-based multi-agent systems, unlocking their transformative capabilities across various sectors.  

# References  

[1] AIware论坛：大模型时代下软件工程的机遇与挑战 https://issi.xidian.edu.cn/info/1019/1792.htm   
[2] LLM多智能体系统：进展、挑战与展望 https://blog.csdn.net/qq_38423732/article/details/144731497   
[3] AIGC 领域多智能体系统：核心技术、应用范式与未来展望 https://blog.csdn.net/2501_91473346/article/details/147577606   
[4] 大型语言模型推理前沿综述：推理扩展、学习推理与自主智能系统 https://mp.weixin.qq.com/s?   
__biz $: =$ MjM5ODIwNjEzNQ $\scriptstyle = =$ &mid=2649904596&idx $\varXi$ 2&sn=cbbbd84872beb4ed5fd6e6b98c4ea3fe&chksm=bfe43496ab012217   
bafbd50746d6fa8476c9bfcc8c65c71f61cc2cd2d8bfa37cc2647473f14d&scene=27   
[5] Multi-Agent LLMs for Engineering Senior Design Pro http://www.paperreading.club/page?id $\begin{array} { r } { { \bf \Pi } = \frac { \ d } { \ d t } } \end{array}$ 276241   
[6] AgentPrune：经济高效的LLM多智能体系统通信优化框架 https://blog.csdn.net/qq_27590277/article/details/144756937   
[7] LLM赋能的多智能体系统：架构设计与实践 https://blog.csdn.net/universsky2015/article/details/144164277   
[8] AI Agents: Reshaping Business Strategies in 2025 https://www.future-processing.com/blog/ai-agents-in-modern  
business-strategies/   
[9] ICLR'24：大语言模型智能体研究进展综述 https://mp.weixin.qq.com/s?   
__biz=Mzg4ODg5MDc1NA $\scriptstyle 1 = =$ &mid=2247486435&idx $\underline { { \underline { { \mathbf { \Pi } } } } }$ 1&sn=99f4b647df642c1a292c25d6b8ca05cc&chksm=cff57f0ef882f6185f   
6564682f30c34d9c506fd6f001bbf8c062792bf0e7288069a3bf3ca9f5&scene=27   
[10] 全球60个AI Agent大盘点：大语言模型创业参考 http://healthnews.sohu.com/a/723819787_104421   
[11] Multi-Agent：多角色协同完成复杂任务的框架分析与比较 https://developer.aliyun.com/article/1343139  

[12] LLM多智能体综述：进展、挑战与应用 https://baijiahao.baidu.com/s?id $\ c =$ 1798175049419538709&wfr=spider&for=pc  

[13] 基于生成式人工智能的网联自动驾驶通感融合决策技术 https://mp.weixin.qq.com/s?  
_biz=MjM5NzM1OTA1Mw $\scriptstyle 1 = =$ &mid=2651178109&idx $\tan ( 3 . 5 ) =$ 2&sn=834ff080a2d8986cd977b745bffbf665&chksm $\mid =$ bd2a58df8a5dd1c9  
80c05a921b3ed09e2734d4b7bc869fe34faa858c652bcc9eaa050cbbcec6&scene=27  

[14] AI Agents: The Cognitive Leap in Business Transfor https://www2.deloitte.com/us/en/pages/consulting/articles/ai-agen architecture-and-multiagent-systems.html  

[15] 多智能体探索：协同、架构与应用 http://qiankunli.github.io/2024/06/10/multi_agent.html [16] Agent评估方法与工具：核心能力与实战指南 https://news.qq.com/rain/a/20250430A04ZXL00  

7] LLM Agent：技术架构、现实挑战与未来展望 https://www.cnblogs.com/quincyqiang/p/18648920 [18] LLM-Co：评估大型语言模型的多智能体协作能力 https://blog.csdn.net/qq_43207982/article/details/135738068​ [19] 264页最强Agents综述：微软、谷歌等机构联合探索发展与挑战 https://mp.weixin.qq.com/s? _biz=MzI5OTEyNzkxMg==&mid=2247602531&idx=1&sn=b243c4e6ec1efcddaff96f9f148b6f58&chksm=ed7323103fb7276de1c f3cce2a884ee9565d0f2fdb9036fed8ee587ba24bccff1ac374892ba8&scene $^ { - 2 7 }$  

[20] 多Agent辩论提升语言模型真实性和推理能力 https://blog.csdn.net/wangning0714/article/details/135229888 [21] LLM Agent 评估综述：从基础能力到实战场景，构建AI“考场规则” https://blog.csdn.net/qq_27590277/article/details/146449491  

[22] AgentVerse：基于LLM的多智体协同框架及其涌现行为探索 https://blog.csdn.net/yorkhunter/article/details/139051973  

3] LLM-Based Agent 详尽解读：原理、架构与应用 https://cloud.tencent.com/developer/article/2493368 [24] 2025年AI Agent五大框架精选：快速入门指南 https://blog.csdn.net/2401_85325397/article/details/144783653 [25] 大模型与 Multi-Agent 赋能智能运维：实践与探索 https://weibo.com/ttarticle/p/show?id $\ c =$ 2309405079986246647918 [26] 基于Qwen-2.5-7B的本地开源Multi-Agent RAG系统：Code Agent实践 https://blog.csdn.net/2401_85325557/article/details/146174336  

[27] Emerging Techniques in Engineering Intelligent Age http://www.wikicfp.com/cfp/servlet/event.showcfp? eventid $=$ 182923&copyownerid=167352  

[28] Agentic Automation: The Next Evolution of Automati https://www.uipath.com/automation/agentic-automation? utm_content=apjautomationinfobrief  

[29] 生成式AI：记忆流还原与人类行为交互模拟初探 https://mp.weixin.qq.com/s? biz=MzI3MTQzOTY3OQ $\scriptstyle = =$ &mid=2247532915&idx=1&sn=6fab9582d09f969a7cc94690eb69a91b&chksm $\mid =$ eac3c323ddb44a35   
fdfe9ddf1f687d0e7f7aa2a7a4b4b9dd69f45d0a83429bedc1aaa189234a&scene=27  

[30] LLM多代理系统：实现与应用 https://wenku.csdn.net/answer/1ypp8cxrs9 [31] MAaaS：从工程师视角看多智能体即服务 https://juejin.cn/post/7407701827175432204 [32] 基于多智能体LLM的内容知识识别框架LLMAgentCK https://download.csdn.net/blog/column/12596440/140498428 [33] Agentic AI与Agentic Workflow：下一代智能自动化 https://blog.csdn.net/kunpengtingting/article/details/140100403 [34] 大模型(LLM)优秀论文合集：Meta AI、浙大、清华等机构前沿研究 https://mp.weixin.qq.com/s? biz=MzkwMTM0NzU1MQ $\scriptstyle = =$ &mid=2247504150&idx $\mathbf { \Psi } : = \mathbf { \Psi }$ 1&sn=16b3869fc1e5f8f9e8d13327e198a3c2&chksm=c0b4b0a6f7c339b 0728df36e8543c8b7d170f24b1449b490864f8b57bd1c858abd3a2888f5a1&scene=27  

[35] LLM-Multi Agent：多智能体提升大模型推理能力新策略 https://caip.org.cn/news/detail?id $\mathbf { \tau } = \mathbf { \dot { \tau } }$ 24025 [36] ICLR 2024 LLM-Based Agent 论文集锦 https://www.aminer.cn/topic/65409b681512231370cbf681 [37] 陈蓉蓉：应用心理学助理教授 https://staff.uic.edu.cn/rainerrchen/en  