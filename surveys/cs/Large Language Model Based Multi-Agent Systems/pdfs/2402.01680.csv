sentence,references
"This ability exactly aligns with the expectations of humans for autonomous agents that can perceive the surroundings, make decisions, and take actions in response [Xi et al., 2023; Wooldridge and Jennings, 1995; Russell and Norvig, 2009; Guo et al., 2023; Liang et al., 2023]","[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023.；[Wooldridge and Jennings, 1995] Michael Wooldridge and Nicholas R. Jennings. Intelligent agents: theory and practice. The Knowledge Engineering Review, 10:115 – 152, 1995.；[Russell and Norvig, 2009] Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall Press, USA, 3rd edition, 2009.；[Guo et al., 2023] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, Xiangliang Zhang, et al. What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023.；[Liang et al., 2023] Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang Zhang, and Ashwin Kaylan. Let gpt be a math tutor: Teaching math word problem solvers with customized exercise generation. arXiv preprint arXiv:2305.14386, 2023."
"Hence, LLM-based agent has been studied and rapidly developed to understand and generate humanlike instructions, facilitating sophisticated interactions and decision-making in a wide range of contexts [Yao et al., 2023; Shinn et al., 2023; Li et al., 2023d]","[Yao et al., 2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.；[Shinn et al., 2023] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.；[Li et al., 2023d] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms, 2023."
"Timely survey papers systematically summarize the progress of LLM-based agents, as seen in works [Xi et al., 2023; Wang et al., 2023b]","[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023.；[Wang et al., 2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023."
"Recent research has demonstrated promising results in utilizing LLM-based multi-agents for solving various tasks, such as software development [Hong et al., 2023; Qian et al., 2023], multi-robot systems [Mandi et al., 2023; Zhang et al., 2023c], society simulation [Park et al., 2023; Park et al., 2022], policy simulation [Xiao et al., 2023; Hua et al., 2023], and game simulation [Xu et al., 2023c; Wang et al., 2023c]","[Hong et al., 2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.；[Qian et al., 2023] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development, 2023.；[Mandi et al., 2023] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023.；[Zhang et al., 2023c] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023.；[Park et al., 2023] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.；[Park et al., 2022] Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 1–18, 2022.；[Xiao et al., 2023] Bushi Xiao, Ziyuan Yin, and Zixuan Shan. Simulating public administration crisis: A novel generative agent-based simulation system to lower technology barriers in social science research. arXiv preprint arXiv:2311.06957, 2023.；[Hua et al., 2023] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars, 2023.；[Xu et al., 2023c] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023.；[Wang et al., 2023c] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023."
"The volume of research papers is rapidly increasing, as shown in Fig. 1 (inspired by the design in [Gao et al., 2023b]), thus broadening the impact of LLM-based Multi-Agent research","[Gao et al., 2023b] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023."
"We introduce the background by first outlining the capabilities of a single-agent system based on LLMs, following the discussion presented in [Weng, 2023]","[Weng, 2023] Lilian Weng. Llm powered autonomous agents. https://lilianweng.github.io/posts/ 2023-06-23-agent/, 2023."
"Decision-making Thought: This term denotes the capability of LLM-based agents, guided by prompts, to break down complex tasks into smaller subgoals [Khot et al., 2023], think through each part methodically (sometimes exploring multiple paths) [Yao et al., 2023], and learn from past experiences [Shinn et al., 2023] to perform better decision-making on complex tasks","[Khot et al., 2023] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks, 2023.；[Yao et al., 2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.；[Shinn et al., 2023] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023."
"Tool-use: LLM-based agents’ tool-use capability allows them to leverage external tools and resources to accomplish tasks, enhancing their functional capabilities and operate more effectively in diverse and dynamic environments [Li et al., 2023d; Ruan et al., 2023; Gao et al., 2023b]","[Li et al., 2023d] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms, 2023.；[Ruan et al., 2023] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu: Large language model-based ai agents for task planning and tool usage, 2023.；[Gao et al., 2023b] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023."
"Memory: This ability refers to the capability of LLMbased agent for conducting in-context learning [Dong et al., 2023a] as short memory or external vector database [Lewis et al., 2021] as long memory to preserve and retrieve information over prolonged periods [Wang et al., 2023b]","[Dong et al., 2023a] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.；[Lewis et al., 2021] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨uttler, Mike Lewis, Wen tau Yih, Tim Rockta¨schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.；[Wang et al., 2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023."
"Single-Agent systems empowered by LLMs have shown inspiring cognitive abilities [Sumers et al., 2023]","[Sumers et al., 2023] Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023."
"For example, these environments can be like software development [Hong et al., 2023], gaming [Mao et al., 2023], and various other domains such as financial markets [Li et al., $2 0 2 3 \mathrm { g } ]$ or even social behavior modeling [Park et al., 2023]","[Hong et al., 2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.；[Mao et al., 2023] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, and Furu Wei. Alympics: Language agents meet game theory. arXiv preprint arXiv:2311.03220, 2023.；[Park et al., 2023] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023."
"This kind of interface is widely used in software development (code interpreter as simulated environment) [Hong et al., 2023], gaming (using game rules as simulated environment) [Mao et al., 2023], etc","[Hong et al., 2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.；[Mao et al., 2023] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, and Furu Wei. Alympics: Language agents meet game theory. arXiv preprint arXiv:2311.03220, 2023."
"For example, in tasks such as sweeping the floor, making sandwiches, packing groceries, and arranging cabinets, robotic agents are required to perform actions iteratively, observe the physical environment, and continuously refine their actions [Mandi et al., 2023]","[Mandi et al., 2023] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023."
"For example, many applications [Du et al., 2023; Xiong et al., 2023; Chan et al., 2023] utilize multiple agents to debate a question to reach a consensus","[Du et al., 2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023.；[Xiong et al., 2023] Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate, 2023.；[Chan et al., 2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate, 2023."
"Layered communication is structured hierarchically, with agents at each level having distinct roles and primarily interacting within their layer or with adjacent layers. [Liu et al., 2023] introduces a framework called Dynamic LLM-Agent Network (DyLAN), which organizes agents in a multi-layered feed-forward network","[Liu et al., 2023] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llmagent collaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170, 2023."
"Shared Message Pool is proposed by MetaGPT [Hong et al., 2023] to improve the communication efficiency","[Hong et al., 2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023."
"Based on the sources from which agents receive this feedback, it can be categorized into four types. 1) Feedback from Environment, e.g., from either real world environments or virtual environments [Wang et al., 2023b]","[Wang et al., 2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023."
"This kind of feedback is widely used in most “Human-in-the-loop” applications [Wang et al., 2021]","[Wang et al., 2021] Zijie J. Wang, Dongjin Choi, Shenyu Xu, and Diyi Yang. Putting humans in the natural language processing loop: A survey, 2021."
"When performing actions, they can retrieve relevant, valuable memories, particularly those containing successful actions for similar past goals, as highlighted in [Wang et al., 2023b]","[Wang et al., 2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023."
"Instead of only relying on the historical records to decide subsequent actions as seen in Memory-based solutions, agents can dynamically self-evolve by modifying themselves such as altering their initial goals and planning strategies, and training themselves based on feedback or communication logs. [Nascimento et al., 2023] proposes a selfcontrol loop process to allow each agent in the multi-agents systems to be self-managed and self-adaptive to dynamic environments, thereby improving the cooperation efficiency of multiple agents. [Zhang et al., 2023b] introduces ProAgent which anticipates teammates’ decisions and dynamically adjusts each agent’s strategies based on the communication logs between agents, facilitating mutual understanding and improving collaborative planning capability. [Wang et al., 2023a] discusses a Learning through Communication (LTC) paradigm, using the communication logs of multiagents to generate datasets to train or fine-tune LLMs","[Nascimento et al., 2023] Nathalia Nascimento, Paulo Alencar, and Donald Cowan. Self-adaptive large language model (llm)-based multiagent systems. In 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C), pages 104–109. IEEE, 2023.；[Zhang et al., 2023b] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, et al. Proagent: Building proactive cooperative ai with large language models. arXiv preprint arXiv:2308.11339, 2023.；[Wang et al., 2023a] Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, and Yelong Shen. Adapting llm agents through communication, 2023."
"In some scenarios, the system can generate new agents on-the-fly during its operation [Chen et al., 2023a; Chen et al., 2023c]","[Chen et al., 2023a] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Bo¨rje F Karlsson, Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023.；[Chen et al., 2023c] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023."
"Agents Orchestration emerged as a pivotal challenge and began to gain attention in [Moura, 2023; Dibia, 2023]","[Moura, 2023] Joa˜o Moura. Crewai. https://github.com/ joaomdmoura/crewAI, 2023.；[Dibia, 2023] Victor Dibia. Multi-agent llm applications a review of current research, tools, and challenges. https://newsletter.victordibia.com/p/ multi-agent-llm-applications-a-review, 2023."
"Agents generally interact with the code interpreter, other agents or human to iteratively refine the generated code. [Li et al., 2023b] first proposes a simple role-play agent framework, which utilizes the interplay of two roles to realize autonomous programming based on one-sentence user instruction","[Li et al., 2023b] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for” mind” exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023."
"It provides insights into the “cognitive” processes of communicative agents. [Dong et al., 2023b] makes LLMs work as distinct “experts” for sub-tasks in software development, autonomously collaborating to generate code","[Dong et al., 2023b] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt, 2023."
"Moreover, [Qian et al., 2023] presents an end-toend framework for software development, utilizing multiple agents for software development without incorporating advanced human teamwork experience. [Hong et al., 2023] first incorporates human workflow insights for more controlled and validated performance","[Qian et al., 2023] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development, 2023.；[Hong et al., 2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023."
"It encodes SOPs into prompts to enhance structured coordination. [Huang et al., 2023a] delves deeper into multi-agent based programming by solving the problem of balancing code snippet generation with effective test case generation, execution, and optimization","[Huang et al., 2023a] Dong Huang, Qingwen Bu, Jie M. Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation, 2023."
"Hence, LLMMA can be used to model robots with different capabilities and cooperate with each other to solve real-world physical tasks. [Dasgupta et al., 2023] first explores the potential to use LLM as an action planner for embedded agents. [Mandi et al., 2023] introduces RoCo, a novel approach for multirobot collaboration that uses LLMs for high-level communication and low-level path planning","[Dasgupta et al., 2023] Ishita Dasgupta, Christine KaeserChen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. arXiv preprint arXiv:2302.00763, 2023.；[Mandi et al., 2023] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023."
"Experimental results demonstrate the adaptability and success of RoCo in collaborative tasks. [Zhang et al., 2023c] presents CoELA, a Cooperative Embodied Language Agent, managing discussions and task planning in an LLM-MA setting","[Zhang et al., 2023c] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023."
"This challenging setting is featured with decentralized control, complex partial observation, costly communication, and multi-objective long-horizon tasks. [Chen et al., 2023d] investigates communication challenges in scenarios involving a large number of robots, as assigning each robot an LLM will be costly and unpractical due to the long context","[Chen et al., 2023d] Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and Chuchu Fan. Scalable multirobot collaboration with large language models: Centralized or decentralized systems? arXiv preprint arXiv:2309.15943, 2023."
"The study compares four communication frameworks, centralized, decentralized, and two hybrid models, to evaluate their effectiveness in coordinating complex multi-agent tasks. [Yu et al., 2023] proposes CoNavGPT for multi-robot cooperative visual target navigation, integrating LLM as a global planner to assign frontier goals to each robot. [Chen et al., 2023b] proposes an LLM-based consensus-seeking framework, which can be applied as a cooperative planner to a multi-robot aggregation task","[Yu et al., 2023] Bangguo Yu, Hamidreza Kasaei, and Ming Cao. Co-navgpt: Multi-robot cooperative visual semantic navigation using large language models, 2023.；[Chen et al., 2023b] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. Multi-agent consensus seeking via large language models. arXiv preprint arXiv:2310.20151, 2023."
"Human experts are at the center of these agents to process the information of agents and give feedback to the agents. [Zheng et al., 2023] utilizes multiple LLM-based agents, each focusing on specific tasks for the science experiments including strategy planning, literature search, coding, robotic operations, and labware design","[Zheng et al., 2023] Zhiling Zheng, Oufan Zhang, Ha L. Nguyen, Nakul Rampal, Ali H. Alawadhi, Zichao Rong, Teresa Head-Gordon, Christian Borgs, Jennifer T. Chayes, and Omar M. Yaghi. Chatgpt research group for optimizing the crystallinity of mofs and cofs. ACS Central Science, 9(11):2161–2170, 2023."
"LLM-MA can be set for science debating scenarios, where agents debate with each other to enhance the collective reasoning capabilities in tasks such as Massive Multitask Language Understanding (MMLU) [Hendrycks et al., 2020], Math problems [Cobbe et al., 2021], and StrategyQA [Geva et al., 2021]","[Hendrycks et al., 2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.；[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.；[Geva et al., 2021] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies, 2021."
"Through multiple rounds of debate, the agents converge on a single, consensus answer. [Du et al., 2023] leverages the multi-agents debate process on a set of six different reasoning and factual accuracy tasks and demonstrates that LLM-MA debating can improve factuality. [Xiong et al., 2023] focuses on the commonsense reasoning tasks and formulates a three-stage debate to align with real-world scenarios including fair debate, mismatched debate, and roundtable debate","[Du et al., 2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023.；[Xiong et al., 2023] Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate, 2023."
"The paper also analyzes the inter-consistency between different LLMs and claims that debating can improve the inter-consistency. [Tang et al., 2023] also utilizes multiple LLM-based agents as distinct domain experts to do the collaborative discussion on a medical report to reach a consensus for medical diagnosis","[Tang et al., 2023] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning, 2023."
"In societal simulation, LLM-MA models are used to simulate social behaviors, aiming to explore the potential social dynamics and propagation, test social science theories, and populate virtual spaces and communities with realistic social phenomena [Park et al., 2023]","[Park et al., 2023] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023."
"Initial work by [Park et al., 2023] introduces generative agents within an interactive sandbox environment reminiscent of the sims, allowing end users to engage with a modest community of 25 agents through natural language","[Park et al., 2023] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023."
"At the same time, [Park et al., 2022] develops Social Simulacra, which constructs a simulated community of 1,000 personas","[Park et al., 2022] Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 1–18, 2022."
"Building on this, [Gao et al., 2023a] takes the concept further by constructing vast networks comprising 8,563 and 17,945 agents, respectively, designed to simulate social networks focused on the topics of Gender Discrimination and Nuclear Energy","[Gao et al., 2023a] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong Li. $S ^ { \tilde { 3 } }$ : Social-network simulation system with large language model-empowered agents. arXiv preprint arXiv:2307.14984, 2023."
"Recent studies such as [Chen et al., 2023b;  
Kaiya et al., 2023; Li et al., 2023a; Li et al., 2023f; Ziems et al., 2023] highlight the evolving complexity in multi-agent systems, LLM impacts on social networks, and their integration into social science research","[Chen et al., 2023b] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. Multi-agent consensus seeking via large language models. arXiv preprint arXiv:2310.20151, 2023.；[Kaiya et al., 2023] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time social interactions. arXiv preprint arXiv:2310.02172, 2023.；[Li et al., 2023a] Chao Li, Xing Su, Chao Fan, Haoying Han, Cong Xue, and Chunmo Zheng. Quantifying the impact of large language models on collective opinion dynamics. arXiv preprint arXiv:2308.03313, 2023.；[Li et al., 2023f] Siyu Li, Jin Yang, and Kui Zhao. Are you in a masquerade? exploring the behavior and impact of large language model driven social bots in online social networks. arXiv preprint arXiv:2307.10337, 2023.   
[Li et al., $2 0 2 3 \mathrm { g l }$ Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. Tradinggpt: Multi-agent system with layered memory and distinct characters for enhanced financial trading performance, 2023.；[Ziems et al., 2023] Caleb Ziems, Omar Shaikh, Zhehao Zhang, William Held, Jiaao Chen, and Diyi Yang. Can large language models transform computational social sci  

ence? Computational Linguistics, pages 1–53, 2023."
"This technology enables the development of controlled, scalable, and dynamic settings that closely mimic human interactions, making it ideal for testing a range of game theory hypotheses [Mao et al., 2023; Xu et al., 2023b; Gong et al., 2023]","[Mao et al., 2023] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, and Furu Wei. Alympics: Language agents meet game theory. arXiv preprint arXiv:2311.03220, 2023.；[Xu et al., 2023b] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023.；[Gong et al., 2023] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023."
"Most games simulated by LLM-MA rely heavily on natural language communication, offering a sandbox environment within different game settings for exploring or testing game theory hypotheses including reasoning, cooperation, persuasion, deception, leadership, etc.  
[Akata et al., 2023] leverages behavioral game theory to examine LLMs’ behavior in interactive social settings, particularly their performance in games like the iterated Prisoner’s Dilemma and Battle of the Sexes","[Akata et al., 2023] Elif Akata, Lion Schulz, Julian CodaForno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models. arXiv preprint arXiv:2305.16867, 2023."
"Furthermore, [Xu et al., 2023b] proposes a framework using ChatArena library [Wu et al., 2023b] for engaging LLMs in communication games like Werewolf, using retrieval and reflection on past communications for improvement, as well as the Chain-of-Thought mechanism [Wei et al., 2022]. [Light et al., 2023b] explores the potential of LLM agents in playing Resistance Avalon, introducing AVALONBENCH, a comprehensive game environment and benchmark for further developing advanced LLMs and multi-agent frameworks. [Wang et al., 2023c] also focuses on the capabilities of LLM Agents in dealing with misinformation in the Avalon game, proposing the Recursive Contemplation (ReCon) framework to enhance LLMs’ ability to discern and counteract deceptive information. [Xu et al., $2 0 2 3 \mathrm { c } ]$ introduces a framework combining LLMs with reinforcement learning (RL) to develop strategic language agents for the Werewolf game","[Xu et al., 2023b] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023.；[Wu et al., 2023b] Yuxiang Wu, Zhengyao Jiang, Akbir Khan, Yao Fu, Laura Ruis, Edward Grefenstette, and Tim Rockta¨schel. Chatarena: Multi-agent language game environments for large language models. GitHub repository, 2023.；[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.；[Light et al., 2023b] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. From text to tactic: Evaluating llms playing the game of avalon. arXiv preprint arXiv:2310.05036, 2023.；[Wang et al., 2023c] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023."
"It introduces a new approach to use RL policy in the case that the action and state sets are not predefined but in the natural language setting. [Mukobi et al., 2023] designs the “Welfare Diplomacy”, a general-sum variant of the zero-sum board game Diplomacy, where players must balance military conquest and domestic welfare","[Mukobi et al., 2023] Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton. Welfare diplomacy: Benchmarking language model cooperation. arXiv preprint arXiv:2310.08901, 2023."
"On top of that, there is a work [Li et al., 2023c] in a multi-agent cooperative text game testing the agents’ Theory of Mind (ToM), the ability to reason about the concealed mental states of others and is fundamental to human social interactions, collaborations, and communications. [Fan et al., 2023] comprehensively assesses the capability of LLMs as rational players, and identifies the weaknesses of LLM-based Agents that even in the explicit game process, agents may still overlook or modify refined beliefs when taking actions","[Li et al., 2023c] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration via large language models, 2023.；[Fan et al., 2023] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational players in game theory? a systematic analysis. arXiv preprint arXiv:2312.05488, 2023."
"This method facilitates the study of interpersonal dynamics and group behaviors, providing insights into how individual psychological traits influence collective actions. [Ma et al., 2023] explores the psychological implications and outcomes of employing LLM-based conversational agents for mental well-being support","[Ma et al., 2023] Zilin Ma, Yiyang Mei, and Zhaoyuan Su. Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. arXiv preprint arXiv:2307.15810, 2023."
"It emphasizes the need for carefully evaluating the use of LLM-based agents in mental health applications from a psychological perspective. [Kovacˇ et al., 2023] introduces a tool named SocialAI school for creating interactive environments simulating social interactions","[Kovacˇ et al., 2023] Grgur Kovacˇ, Re´my Portelas, Peter Ford Dominey, and Pierre-Yves Oudeyer. The socialai school: Insights from developmental psychology towards artificial socio-cultural agents. arXiv preprint arXiv:2307.07871, 2023."
"It draws from developmental psychology to understand how agents can acquire, demonstrate, and evolve social skills such as joint attention, communication, and cultural learning. [Zhang et al., 2023d] explores how LLM agents, with distinct traits and thinking patterns, emulate human-like social behaviors such as conformity and majority rule","[Zhang et al., 2023d] Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A social psychology view, 2023."
"This integration of psychology into the understanding of agent collaboration offers a novel lens for examining and enhancing the mechanisms behind LLMbased multi-agents systems. [Aher et al., 2023] introduces Turing Experiments to evaluate the extent to which large language models can simulate different aspects of human behaviors","[Aher et al., 2023] Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies, 2023."
"This is similar to the way economists model ’homo economicus’, the characterization of man in some economic theories as a rational person who pursues wealth for his own self-interest [Horton, 2023]","[Horton, 2023] John J Horton. Large language models as simulated economic agents: What can we learn from homo silicus? Technical report, National Bureau of Economic Research, 2023."
"Agents interact in cooperative or debate, decentralized environments. [Li et al., 2023e] employs LLMs for macroeconomic simulation, featuring prompt-engineering-driven agents that emulate human-like decision-making, thereby enhancing the realism of economic simulations compared to rule-based or other AI agents. [Anonymous, 2023] explores the buyer’s inspection paradox in an information marketplace, reveals improved decision-making and answer quality when agents temporarily access information before purchase. [Li et al., 2023g] presents an LLM-MA framework for financial trading, emphasizing a layered memory system, debate mechanisms, and individualized trading characters, thereby fortifying decisionmaking robustness. [Zhao et al., 2023] utilizes LLM-based Agents to simulate a virtual town with restaurant and customer agents, yielding insights aligned with sociological and economic theories","[Li et al., 2023e] Nian Li, Chen Gao, Yong Li, and Qingmin Liao. Large language model-empowered agents for simulating macroeconomic activities, 2023.；[Anonymous, 2023] Anonymous. Rethinking the buyer’s inspection paradox in information markets with language agents. In Submitted to The Twelfth International Conference on Learning Representations, 2023. under review.；[Zhao et al., 2023] Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition behaviors in large language model-based agents, 2023."
"The use of the LLM-MA in recommender systems is similar to that in psychology since studies in both fields involve the consideration of extrinsic and intrinsic human factors such as cognitive processes and personality [Lex and Schedl, 2022]","[Lex and Schedl, 2022] Elisabeth Lex and Markus Schedl. Psychology-informed recommender systems: A humancentric perspective on recommender systems. In Proceedings of the 2022 Conference on Human Information Interaction and Retrieval, CHIIR ’22, page 367–368, New York, NY, USA, 2022. Association for Computing Machinery."
"To bridge the gap between offline metrics and real-world performance in recommendation systems, Agent4Rec [Zhang et al., 2023a] introduces a simulation platform based on LLM-MA. 1000 generative agents are initialized with the MovieLens-1M dataset to simulate complex user interactions in a recommendation environment","[Zhang et al., 2023a] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On generative agents in recommendation, 2023."
"Different from Agent4Rec work, [Zhang et al., 2023e] treats both users and items as agents, optimizing them collectively to reflect and adjust to realworld interaction disparities","[Zhang et al., 2023e] Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian McAuley, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. Agentcf: Collaborative learning with autonomous language agents for recommender systems, 2023."
"These simulations provide valuable insights into how policies are formulated and their potential effects, aiding policymakers in understanding and anticipating the consequences of their decisions [Farmer and Axtell, 2022]","[Farmer and Axtell, 2022] J. Doyne Farmer and Robert L. Axtell. Agent-Based Modeling in Economics and Finance: Past, Present, and Future. INET Oxford Working Papers 2022-10, Institute for New Economic Thinking at the Oxford Martin School, University of Oxford, June 2022."
"The research outlined in [Xiao et al., 2023] is centered on simulating a township water pollution crisis","[Xiao et al., 2023] Bushi Xiao, Ziyuan Yin, and Zixuan Shan. Simulating public administration crisis: A novel generative agent-based simulation system to lower technology barriers in social science research. arXiv preprint arXiv:2311.06957, 2023."
"Within the water pollution crisis simulation, this work provides an in-depth analysis of how a virtual government entity might respond to such a public administration challenge and how information transfer in the social network in this crisis. [Hua et al., 2023] introduces WarAgent to simulate key historical conflicts and provides insights for conflict resolution and understanding, with potential applications in preventing future international conflicts","[Hua et al., 2023] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars, 2023."
"The most recent study in [Williams et al., 2023] delves into the use of LLM-MA in simulating disease spread","[Williams et al., 2023] Ross Williams, Niyousha Hosseinichimeh, Aritra Majumdar, and Navid Ghaffarzadegan. Epidemic modeling with generative agents. arXiv preprint arXiv:2307.04986, 2023."
"Impressively, their actions contribute to the attenuation of the epidemic curve. [Ghaffarzadegan et al., 2023] also discusses the epidemic propagation simulation and decomposes the simulation into two parts: the Mechanistic Model which represents the information or propagation of the virus and the Decision-Making Model which represents the agents’ decision-making process when facing the virus","[Ghaffarzadegan et al., 2023] Navid Ghaffarzadegan, Aritra Majumdar, Ross Williams, and Niyousha Hosseinichimeh. Generative agent-based modeling: Unveiling social system dynamics through coupling mechanistic models with generative artificial intelligence. arXiv preprint arXiv:2309.11456, 2023."
"We provide a detailed introduction to three open-source multi-agent frameworks: MetaGPT [Hong et al., 2023], CAMEL [Li et al., 2023b], and Autogen [Wu et al., 2023a]","[Hong et al., 2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.；[Li et al., 2023b] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for” mind” exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023.；[Wu et al., 2023a] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023."
"More recently, [Chen et al., 2023c; Chen et al., 2023a] introduce frameworks for dynamic multi-agent collaboration, while [Zhou et al., 2023a; Li et al., 2023h; Xie et al., 2023] present platforms and libraries for building autonomous agents, emphasizing their adaptability in tasksolving and social simulations","[Chen et al., 2023c] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023.；[Chen et al., 2023a] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Bo¨rje F Karlsson, Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023.；[Zhou et al., 2023a] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al. Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870, 2023.；[Li et al., 2023h] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv preprint arXiv:2310.06500, 2023.；[Xie et al., 2023] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023."
"It refers to the phenomenon where the model generates text that is factually incorrect [Huang et al., 2023b]","[Huang et al., 2023b] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023."
"As highlighted in [Dibia, 2023], designing advanced Agents Orchestration methodologies is increasingly important","[Dibia, 2023] Victor Dibia. Multi-agent llm applications a review of current research, tools, and challenges. https://newsletter.victordibia.com/p/ multi-agent-llm-applications-a-review, 2023."
"Firstly, as discussed in [Xu et al., 2023a], much of the existing research focuses on evaluating individual agents’ understanding and reasoning within narrowly defined scenarios","[Xu et al., 2023a] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng. Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration, 2023."
"Furthermore, there are opportunities to explore LLMMA systems from various theoretical perspectives, such as Cognitive Science [Sumers et al., 2023], Symbolic Artificial Intelligence, Cybernetics, Complex Systems, and Collective Intelligence","[Sumers et al., 2023] Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023."
