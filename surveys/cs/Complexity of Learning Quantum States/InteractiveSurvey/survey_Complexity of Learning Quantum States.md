# A Survey of Complexity of Learning Quantum States

# 1 Abstract


The study of quantum information processing has emerged as a frontier of modern physics and computer science, driven by the potential of quantum mechanics to revolutionize computation, communication, and cryptography. This survey paper delves into the complexity of learning quantum states, a topic that lies at the intersection of quantum computing, quantum information theory, and machine learning, with the aim of providing a comprehensive overview of the current state of research. The paper highlights key concepts, methods, and applications, including Quantum Markov Decision Processes, sparse quantum simulation techniques, and the integration of quantum mechanics into decision-making processes. It also explores the application of quantum states in machine learning, optimization, and the characterization and measurement of quantum states, such as quantum state tomography and the measurement of toroidal moments in molecules. The main findings of the survey include advancements in quantum amplitude encoding, variational quantum algorithms for entangled states, and threshold quantum state tomography, which collectively address the fundamental challenges of state preparation, manipulation, and characterization. The paper synthesizes recent advancements, identifies open research questions, and underscores the importance of addressing the complexity of quantum state manipulation, which is crucial for the development of practical quantum technologies and the realization of quantum advantages in various applications.

# 2 Introduction
The study of quantum information processing has emerged as a frontier of modern physics and computer science, driven by the potential of quantum mechanics to revolutionize computation, communication, and cryptography. Quantum systems, characterized by their ability to exist in superpositions and entangled states, offer a fundamentally different approach to information processing compared to classical systems. The manipulation and control of quantum states are at the heart of this revolution, with significant implications for the development of quantum algorithms, quantum error correction, and quantum-enhanced machine learning [1]. As the field progresses, the complexity of learning and manipulating quantum states has become a critical research focus, encompassing both theoretical and practical challenges.

This survey paper delves into the complexity of learning quantum states, a topic that lies at the intersection of quantum computing, quantum information theory, and machine learning [1]. The primary objective is to provide a comprehensive overview of the current state of research, highlighting the key concepts, methods, and applications. The paper aims to address the fundamental questions of how quantum states can be efficiently prepared, characterized, and utilized in practical quantum algorithms and protocols [2]. By synthesizing recent advancements and identifying open research questions, this survey seeks to guide future research and foster interdisciplinary collaboration.

The paper begins by exploring the theoretical frameworks and algorithms that underpin the manipulation of quantum states. This includes an in-depth discussion of Quantum Markov Decision Processes (QMDPs), which extend classical decision-making models to the quantum domain, enabling more efficient exploration of decision paths and better handling of uncertainty [3]. The integration of quantum mechanics into decision-making processes is further enhanced by the development of pre-trajectory sampling with batched execution (PTSBE) for quantum simulations, which significantly reduces computational bottlenecks and improves the scalability of quantum algorithms [4]. Additionally, the paper examines sparse quantum simulation techniques, which address the exponential growth of quantum state spaces by efficiently managing sparse state representations, thus enabling the simulation of larger and more complex quantum systems [5].

The survey then transitions to the application of quantum states in machine learning and optimization [6]. Quantum Complex-Valued Self-Attention Models (QCSAM) are introduced as a significant advancement in the integration of quantum mechanics with attention mechanisms, providing a more nuanced representation of quantum similarity and enhancing the expressivity of quantum machine learning models [7]. The paper also discusses Quantum Circuit Search (QCS) for ADME tasks in drug discovery, where automated search algorithms are used to optimize quantum circuits for specific biochemical data, leading to more accurate predictions of drug properties. Furthermore, Quantum Tensor Hypernetwork Adaptation (QTHA) is explored as a method for enhancing the capabilities of Large Language Models (LLMs) through the integration of quantum computing principles, achieving parameter-efficient fine-tuning and improved performance in high-dimensional data processing [8].

The paper also covers the challenges and solutions related to quantum state preparation and control. Quantum amplitude encoding and sparse state preparation techniques are discussed for their efficiency in representing large datasets in quantum computers, while quantum error correction in atomic ions is examined for its role in achieving fault-tolerant quantum computing [2]. Variational Quantum Algorithms (VQAs) for entangled states are highlighted for their ability to adapt to the noise characteristics of near-term quantum devices, making them a practical tool for generating and manipulating entangled states in quantum computing applications [9].

Finally, the survey paper addresses the characterization and measurement of quantum states, including quantum state tomography and the measurement of toroidal moments in molecules. Techniques such as threshold quantum state tomography (tQST) and selective and efficient quantum state tomography (SE-QST) are discussed for their ability to reduce the number of required measurements while maintaining the accuracy of state reconstruction [10]. The paper also explores the use of geometric invariant theory and non-free tensors in understanding the structure of tensor spaces and the implications for entanglement and complexity.

The contributions of this survey paper are multifaceted. It provides a comprehensive overview of the theoretical and practical aspects of learning quantum states, synthesizing recent advancements and identifying key research directions [10]. By highlighting the interdisciplinary nature of the field, the paper aims to serve as a valuable resource for researchers and practitioners in quantum computing, quantum information theory, and machine learning. The paper also underscores the importance of addressing the complexity of quantum state manipulation, which is crucial for the development of practical quantum technologies and the realization of quantum advantages in various applications.

# 3 Quantum Error Correction and State Preparation

## 3.1 Theoretical Frameworks and Algorithms

### 3.1.1 Quantum Markov Decision Processes and Decision Making
Quantum Markov Decision Processes (QMDPs) extend the classical Markov Decision Process (MDP) framework by incorporating principles from quantum mechanics, such as superposition and entanglement. In a QMDP, the state of the system is represented by a quantum state vector in a Hilbert space, and actions are modeled as quantum operations or unitary transformations. This extension allows for a more nuanced representation of uncertainty and decision-making in environments where classical probabilistic models may fall short. The quantum state vector evolves according to the Schrödinger equation, and the transition probabilities are determined by the Born rule, which provides a natural way to handle probabilistic outcomes.

The integration of quantum mechanics into decision-making processes offers several advantages. For instance, the superposition principle allows the system to exist in multiple states simultaneously, enabling parallel exploration of different decision paths. This can lead to more efficient search algorithms and faster convergence to optimal policies. Entanglement, another key quantum feature, can be used to model correlations between different states, which is particularly useful in multi-agent systems or scenarios where the actions of one agent affect the state of another [11]. Moreover, the use of quantum states and operations can provide a more compact and efficient representation of complex decision-making problems, reducing the computational resources required for policy evaluation and optimization [3].

Despite these potential benefits, the development and application of QMDPs face several challenges. One major challenge is the design of quantum algorithms that can efficiently simulate the dynamics of QMDPs on near-term quantum hardware [5]. This involves developing methods to encode classical decision-making problems into quantum states and to implement the necessary quantum operations with high fidelity. Additionally, the interpretation of quantum states and the extraction of meaningful decisions from quantum measurements remain open research questions. Nevertheless, the theoretical foundation of QMDPs and their potential to revolutionize decision-making in uncertain environments make them an exciting area of ongoing research.

### 3.1.2 Pre-Trajectory Sampling with Batched Execution for Quantum Simulations
Pre-Trajectory Sampling (PTS) with Batched Execution (BE), or PTSBE, represents a significant advancement in the simulation of quantum algorithms, particularly in addressing the computational bottlenecks associated with noise sampling and state evolution [4]. Traditional quantum trajectory methods typically interleave the application of quantum gates with per-step noise sampling, which necessitates serial execution and often leads to inefficiencies [4]. By decoupling these processes, PTSBE allows for the pre-sampling of noise trajectories, which can then be executed in batches, thereby leveraging parallel computing resources more effectively. This approach not only reduces the overall simulation time but also improves the scalability of quantum simulations, making it feasible to handle larger and more complex quantum circuits [5].

The PTSBE methodology is built on the foundation of advanced simulation techniques that exploit the sparse nature of quantum states and the parallel processing capabilities of modern GPUs. For instance, CUDA-Q’s GPU-accelerated simulator facilitates the efficient execution of quantum trajectory methods by optimizing batched trajectory execution, detecting unitary channels for probability caching, and supporting distributed multi-GPU architectures [4]. These optimizations are crucial for overcoming the limitations of conventional trajectory methods, which often suffer from unoptimized sampling and inefficient memory usage. By pre-sampling noise trajectories, PTSBE transforms the simulation process from a statistical black box into a programmable data collection pipeline, enabling researchers to generate large datasets for training and validating quantum algorithms.

Moreover, PTSBE extends the utility of quantum simulators by integrating them with machine learning workflows. This integration is particularly valuable in the context of variational quantum algorithms (VQAs), where the ability to efficiently simulate and optimize quantum circuits is essential for achieving quantum advantage [9]. By supplementing traditional data modalities with large amounts of data from universal quantum simulators, PTSBE simplifies and expands the training paradigm, making it easier to explore the parameter landscape of VQAs and identify optimal configurations. This approach not only accelerates the development of quantum algorithms but also enhances their robustness and reliability, paving the way for more practical applications in fields such as materials science, drug discovery, and optimization problems [12].

### 3.1.3 Sparse Quantum Simulation and Memory Efficiency
Sparse quantum simulation is a critical technique for managing the exponential growth of quantum state spaces, which poses significant challenges for classical simulation. Traditional methods, such as the Schrödinger equation-based approach, require storing the entire quantum state vector, leading to memory usage that scales as \(O(2^n)\) for \(n\) qubits. This exponential growth quickly becomes infeasible, even for relatively small quantum systems. Sparse quantum simulators, however, address this issue by storing only the nonzero components of the quantum state, significantly reducing memory requirements. This approach is particularly useful for simulating quantum algorithms that involve sparse state vectors, where most amplitudes are zero or negligible.

The Sparse Quantum Simulator (SparQSim) is a notable example of such a tool, designed to operate at the register level and efficiently manage sparse state representations. SparQSim employs quantum arithmetic operations and leverages the sparsity of quantum states to perform simulations with lower memory overhead [5]. By focusing on nonzero components, SparQSim can handle larger quantum systems than traditional simulators, making it a valuable tool for both algorithm development and hardware benchmarking [5]. Numerical experiments have shown that SparQSim outperforms Schrödinger-based simulators in both time and memory usage, particularly for algorithms like Quantum Linear System Solvers (QLSS) that benefit from sparse state representations [5].

Moreover, the integration of Quantum Random Access Memory (QRAM) in SparQSim further enhances its efficiency by allowing rapid access to the nonzero components of the quantum state. This feature is crucial for simulating complex quantum algorithms that require frequent state updates and manipulations. The ability to efficiently manage and manipulate sparse quantum states not only extends the capabilities of classical simulators but also provides insights into the behavior of quantum algorithms, facilitating the development of more advanced quantum computing applications [2]. Overall, sparse quantum simulation techniques, exemplified by SparQSim, represent a significant step forward in the efficient and scalable simulation of quantum systems.

## 3.2 Quantum Machine Learning and Optimization

### 3.2.1 Quantum Complex-Valued Self-Attention Models
Quantum Complex-Valued Self-Attention Models (QCSAM) represent a significant advancement in the integration of quantum mechanics with attention mechanisms in machine learning [7]. Traditional quantum self-attention models have primarily relied on real-valued overlaps or simple fusion methods for attention weights, which inadequately capture the rich, complex-valued nature of quantum states [7]. QCSAM addresses this limitation by extending the concept of Linear Combination of Unitaries (LCUs) to Complex Linear Combination of Unitaries (CLCUs), thereby allowing for the use of complex coefficients in the attention mechanism [7]. This enhancement ensures that the attention weights are inherently complex, aligning more closely with the underlying principles of quantum mechanics.

The core innovation of QCSAM lies in its ability to derive complex-valued attention weights directly from the real and imaginary parts of the inner product ⟨K|Q⟩, where K and Q are the key and query vectors, respectively. By capturing both the amplitude and phase relationships between quantum states, QCSAM provides a more nuanced and precise representation of quantum similarity. This approach not only enhances the model's expressivity but also improves its ability to handle complex interactions and dependencies within the data. The use of complex-valued weights in QCSAM allows for a more faithful representation of quantum states, which is crucial for tasks that require high-fidelity quantum state manipulation and analysis.

Furthermore, the introduction of CLCUs in QCSAM facilitates the development of more sophisticated and flexible quantum self-attention mechanisms. This framework supports the creation of compact and efficient quantum circuits that can be executed on near-term quantum hardware, making it a practical solution for quantum machine learning applications [8]. The enhanced expressivity and robustness of QCSAM make it a promising tool for advancing the field of quantum information processing, particularly in areas such as quantum chemistry, quantum simulation, and quantum-enhanced machine learning. By leveraging the full potential of complex-valued quantum states, QCSAM opens new avenues for exploring the intersection of quantum mechanics and artificial intelligence.

### 3.2.2 Quantum Circuit Search for ADME Tasks
Quantum Circuit Search (QCS) has emerged as a powerful technique for optimizing quantum circuits tailored for specific tasks, including Absorption, Distribution, Metabolism, and Excretion (ADME) predictions in drug discovery. This approach draws inspiration from classical Neural Architecture Search (NAS) methodologies, which have been successful in automating the design of neural network architectures. In the context of ADME tasks, QCS aims to discover high-performance quantum circuits that can efficiently process and analyze complex biochemical data [13]. By leveraging automated search algorithms, QCS can explore a vast space of potential circuit designs, identifying those that exhibit superior performance in terms of accuracy and robustness against noise [13].

State-of-the-art QCS methods employ a variety of algorithmic designs, ranging from evolutionary algorithms to reinforcement learning, to generate and evaluate circuit candidates [13]. These methods typically involve defining a search space of quantum gates and connectivity patterns, followed by an iterative process of circuit synthesis and evaluation. Performance metrics, such as gate fidelity, circuit depth, and computational efficiency, are used to guide the search towards optimal solutions. Moreover, QCS approaches often incorporate noise models to ensure that the generated circuits are resilient to the imperfections inherent in near-term quantum devices. This is particularly important for ADME tasks, where the quality of the predictions can significantly impact the success of drug development pipelines.

The application of QCS to ADME tasks has shown promising results, with quantum circuits achieving competitive performance compared to classical models in specific scenarios. For instance, in regression tasks, QCS-generated circuits have demonstrated lower mean squared error, indicating their potential to provide more accurate predictions of drug properties. Additionally, the ability to tailor quantum circuits to the specific characteristics of ADME data can lead to more efficient and targeted computational workflows. Future work in this area will focus on further refining QCS algorithms to enhance the robustness and scalability of quantum circuits, ultimately paving the way for more widespread adoption of quantum computing in pharmaceutical research.

### 3.2.3 Quantum Tensor Hypernetwork Adaptation for LLMs
Quantum Tensor Hypernetwork Adaptation (QTHA) represents a pioneering approach to enhancing the capabilities of Large Language Models (LLMs) through the integration of quantum computing principles [8]. This method leverages the hybrid quantum-classical paradigm to reparameterize pre-trained layers into quantum tensor hybrid architectures, effectively merging the expressive power of Quantum Neural Networks (QNNs) with the efficiency of tensor networks [8]. By doing so, QTHA aims to achieve Parameter-Efficient Fine-Tuning (PEFT) of LLMs, a critical requirement for scaling these models to larger datasets and more complex tasks.

The core innovation of QTHA lies in its ability to capture complex transformations using QNNs while maintaining computational efficiency through tensor-based methods. This is achieved by representing pre-trained layers as tensor networks, which are then adapted to quantum circuits. The tensor networks facilitate the decomposition of high-dimensional parameters into more manageable components, allowing for efficient optimization and training on quantum hardware [8]. This approach not only reduces the computational overhead but also enhances the model's ability to generalize from limited data, a common challenge in the fine-tuning of LLMs.

Moreover, QTHA addresses the limitations of existing quantum-inspired frameworks, which often remain theoretical and lack practical validation on physical quantum hardware. By providing a concrete implementation, QTHA bridges the gap between theoretical advancements and real-world applications. The framework's modular design enables seamless integration with existing LLM architectures, making it a versatile tool for researchers and practitioners. Through extensive experimentation, QTHA has demonstrated significant improvements in model performance, particularly in tasks requiring high-dimensional data processing and complex pattern recognition, thus paving the way for the next generation of quantum-enhanced language models.

## 3.3 Quantum State Preparation and Control

### 3.3.1 Quantum Amplitude Encoding and Sparse State Preparation
Quantum amplitude encoding is a method for encoding classical data into the amplitudes of a quantum state, enabling the representation of high-dimensional vectors using a logarithmic number of qubits [14]. This technique is particularly useful for quantum algorithms that require the manipulation of large datasets, such as those in machine learning and optimization. In amplitude encoding, a vector \( \mathbf{x} \in \mathbb{R}^d \) is encoded into a quantum state \( |\psi(\mathbf{x})\rangle \) such that the amplitudes of the state correspond to the components of \( \mathbf{x} \). The number of qubits required for this encoding scales logarithmically with the dimension \( d \), making it an efficient way to represent large datasets in a quantum computer.

Sparse state preparation, on the other hand, focuses on the efficient preparation of quantum states that have a small number of non-zero amplitudes [2]. This is particularly relevant in scenarios where the classical data is sparse, meaning that most of the components of the vector are zero. Sparse state preparation algorithms aim to reduce the computational resources required to prepare such states, which is crucial for practical applications on near-term quantum devices [2]. One such algorithm has a runtime of \( O(m^2 \log(m) n) \) and requires \( O(m n) \) qubits, where \( m \) is the number of non-zero entries and \( n \) is the number of qubits. This efficiency is achieved by leveraging the sparsity of the input data, which allows for the use of simpler and more efficient quantum circuits.

The practical implications of these techniques are significant, especially in the context of quantum machine learning and quantum algorithms for combinatorial optimization [15]. By encoding data efficiently, quantum amplitude encoding and sparse state preparation can help mitigate the computational bottlenecks associated with state preparation, thereby enabling the realization of quantum advantages in these domains [2]. However, the challenge of efficiently preparing and manipulating these states remains an active area of research, with ongoing efforts to develop more robust and scalable algorithms.

### 3.3.2 Quantum Error Correction in Atomic Ions
Quantum error correction (QEC) in atomic ions represents a critical advancement in the pursuit of scalable quantum computing. Atomic ions, confined in electromagnetic traps, serve as ideal qubits due to their long coherence times and high-fidelity operations. The primary challenge in QEC for atomic ions lies in the precise control and manipulation of individual ions, which are susceptible to various sources of noise, including motional heating, laser phase fluctuations, and environmental perturbations. To address these issues, researchers have developed sophisticated error correction codes tailored to the unique properties of trapped ions. One notable approach involves the use of surface codes, which have been successfully implemented to increase the code distance from \(d = 3\) to \(d = 7\), demonstrating significant improvements in logical error suppression.

The implementation of QEC in atomic ions often leverages the natural resilience of these systems to certain types of errors. For instance, the strong insensitivity of trapped ions to atomic position uncertainty and dephasing errors has been exploited to enhance the robustness of quantum operations. This robustness is further bolstered by the use of advanced error correction protocols, such as the Steane code and the Bacon-Shor code, which are designed to correct both bit-flip and phase-flip errors. These codes are particularly effective in atomic ion systems due to the high-fidelity state preparation and measurement capabilities, which are essential for the successful execution of error correction cycles.

Moreover, the development of reconfigurable atom arrays has enabled the construction of logical quantum processors where neutral atoms serve as physical qubits [16]. These arrays allow for dynamic reconfiguration, facilitating the implementation of complex error correction schemes and the realization of scalable quantum processors. The ability to precisely control and manipulate individual ions within these arrays is crucial for achieving the high levels of coherence and gate fidelity required for effective QEC. As a result, the integration of advanced error correction techniques with reconfigurable atom arrays represents a significant step forward in the realization of fault-tolerant quantum computing with atomic ions [16].

### 3.3.3 Variational Quantum Algorithms for Entangled States
Variational Quantum Algorithms (VQAs) have emerged as a promising approach for generating multi-qubit entangled states on noisy intermediate-scale quantum (NISQ) devices [9]. These algorithms leverage the hybrid quantum-classical paradigm, where a parameterized quantum circuit (PQC) is iteratively optimized to minimize a cost function defined by the overlap with a target entangled state [9]. The key advantage of VQAs lies in their ability to adapt to the specific noise characteristics of the quantum hardware, making them particularly suitable for NISQ systems where decoherence and gate errors are significant.

In the context of entangled state preparation, VQAs typically start with an initial state, often a product state, and apply a sequence of parameterized gates to evolve this state towards the desired entangled state. The parameters of these gates are optimized using classical optimization algorithms, such as gradient descent or Bayesian optimization, based on the measurement outcomes of the quantum circuit [9]. This iterative process allows VQAs to explore the high-dimensional Hilbert space efficiently, even in the presence of noise, and to converge to states with high fidelity.

Recent advancements in VQAs for entangled states have focused on improving the efficiency and robustness of these algorithms. Techniques such as symmetry-protected training, where the PQC is designed to respect the symmetries of the target state, have been shown to enhance the convergence and stability of the optimization process. Additionally, the use of adaptive measurement strategies, which dynamically adjust the measurement basis based on the current state of the system, has been demonstrated to further improve the performance of VQAs. These developments highlight the potential of VQAs to serve as a practical tool for generating and manipulating entangled states in quantum computing applications.

# 4 Quantum State Tomography and Characterization

## 4.1 Theoretical and Computational Models

### 4.1.1 Quantum State Erasure and Thermodynamics
Quantum state erasure, a process where quantum information is deliberately removed from a system, plays a crucial role in the interplay between quantum mechanics and thermodynamics. This section explores the fundamental principles and implications of quantum state erasure, particularly in the context of thermodynamic processes. The erasure of a quantum state typically involves a measurement followed by a reset operation, which can be modeled using quantum channels. These channels describe the transformation of the quantum state, often leading to a mixed state with reduced coherence and entanglement. The process of erasing quantum information is not only a fundamental aspect of quantum information theory but also has profound implications for the second law of thermodynamics, as it involves the dissipation of energy and the increase of entropy.

The thermodynamic cost of quantum state erasure is a topic of significant interest, as it provides insights into the fundamental limits of information processing in quantum systems. Landauer's principle, which states that the erasure of one bit of information requires a minimum amount of energy dissipation, has been extended to the quantum domain. In quantum systems, the energy cost of erasure is not only dependent on the amount of information being erased but also on the initial state of the system and the specific erasure protocol used. For example, the erasure of a pure state generally requires less energy than the erasure of a mixed state, due to the higher entropy of the latter. This relationship between state purity and the energy cost of erasure highlights the non-trivial nature of quantum thermodynamics and the importance of considering the quantum state's properties in thermodynamic analyses.

Moreover, the study of quantum state erasure and its thermodynamic implications has practical applications in quantum computing and quantum communication. In these fields, efficient state preparation and reset operations are essential for the operation of quantum algorithms and protocols [2]. The ability to erase quantum states with minimal energy cost and without introducing significant errors is crucial for the scalability and reliability of quantum technologies. Recent experimental and theoretical work has focused on developing protocols that optimize the erasure process, balancing the trade-offs between energy efficiency, speed, and fidelity. These advancements not only contribute to the foundational understanding of quantum thermodynamics but also pave the way for more practical and efficient quantum information processing systems.

### 4.1.2 Quantum Circuits for Radical-Pair Reactions
Quantum circuits for radical-pair reactions have emerged as a promising approach to simulate and understand the quantum mechanics underlying these biochemical processes. Radical-pair reactions, which are crucial in various biological phenomena such as avian navigation and photosynthesis, involve the creation and evolution of entangled electron spin states. Quantum circuits designed for these reactions typically consist of a series of gates that simulate the Hamiltonian dynamics of the radical-pair system, including the hyperfine interactions between electron and nuclear spins, and the influence of external magnetic fields. These circuits are designed to capture the coherent and incoherent dynamics of the radical-pair system, thereby providing a detailed model of the reaction kinetics.

The implementation of quantum circuits for radical-pair reactions requires careful consideration of the physical qubits used to represent the electron and nuclear spins. Superconducting qubits, trapped ions, and nitrogen-vacancy (NV) centers in diamond are among the leading candidates due to their long coherence times and high-fidelity gate operations. Each platform offers unique advantages and challenges, such as the ability to control and measure individual qubits with high precision, the scalability of the system, and the integration with classical control electronics. For instance, superconducting qubits can be rapidly initialized and measured, making them suitable for fast simulations, while NV centers provide a robust platform for long-term coherence and potential integration with solid-state devices.

To enhance the accuracy and efficiency of these simulations, various techniques have been developed to optimize the quantum circuits [5]. These include the use of variational algorithms to find the optimal parameters for the gates, the application of error mitigation strategies to reduce the impact of noise and decoherence, and the implementation of hybrid quantum-classical approaches that leverage the strengths of both classical and quantum computing [6]. Additionally, the simulation of radical-pair reactions often involves the use of ancilla qubits to perform measurements and to implement more complex operations, such as the simulation of non-unitary processes. These advances in quantum circuit design and optimization are crucial for advancing our understanding of the quantum nature of radical-pair reactions and their potential applications in biophysics and quantum biology.

### 4.1.3 Spin Momentum Flow Measurement in Quantum Dots
In the realm of quantum dot (QD) technology, the precise measurement of spin momentum flow has emerged as a critical tool for understanding and manipulating few-electron states [17]. This section delves into the methodologies and implications of measuring spin momentum flow in QDs, focusing on the innovative use of nuclear spin sensing [17]. By leveraging the spins of crystal atom nuclei as a sensor, researchers have developed a technique capable of detecting the spatial flow of spin momentum, a phenomenon distinct from the conventional charge carrier flow [17]. This method not only provides a non-invasive means of probing the internal dynamics of QDs but also opens avenues for exploring the intricate interplay between spin and orbital degrees of freedom.

The application of nuclear spin sensing in QDs has led to several groundbreaking observations. For instance, the technique has enabled the detection of phase transitions between spinless and spin-polarized four-electron ground states, offering insights into the complex interactions within multi-electron systems. Additionally, the observation of strong spin-orbit coupling in a five-electron configuration highlights the potential for engineering QDs with tailored spin properties. Furthermore, the ability to initialize, read out, and maintain long spin lifetimes—exceeding approximately 50 milliseconds for unpaired electrons in the s and p shells—demonstrates the practical utility of this approach in quantum information processing. These findings underscore the importance of nuclear spin sensing in advancing the control and manipulation of spin states in QDs.

Beyond the fundamental science, the reduction in electron-nuclear spin interaction observed when transitioning from a single-electron to a three-electron configuration suggests a nuanced dependence of spin dynamics on the electronic environment. This insight is crucial for optimizing the design of QD-based devices, particularly in contexts where minimizing decoherence is paramount. The robustness of nuclear spin sensing, combined with its sensitivity to subtle changes in the electronic structure, positions this technique as a powerful diagnostic tool for the development of next-generation quantum technologies. As research in this area continues to evolve, the integration of advanced nuclear spin sensing methods with other quantum characterization techniques will likely lead to further breakthroughs in the field of quantum dots.

## 4.2 Experimental Techniques and Validation

### 4.2.1 Anomalous Hall Effect in Antiferromagnetic Materials
The Anomalous Hall Effect (AHE) in antiferromagnetic materials is a phenomenon characterized by a transverse voltage generated in the absence of an external magnetic field, driven by the intrinsic spin structure of the material. Unlike in ferromagnets, where the AHE is primarily attributed to the Berry curvature in momentum space induced by the magnetic order, antiferromagnets exhibit a more complex interplay between spin and orbital degrees of freedom. In these materials, the AHE can arise from several mechanisms, including the skew scattering, side-jump, and intrinsic contributions, each of which is influenced by the specific arrangement of magnetic moments and the underlying crystal symmetry.

In antiferromagnetic materials, the skew scattering mechanism involves the deflection of charge carriers due to the spin-dependent scattering off magnetic impurities or defects. This process is sensitive to the relative orientation of the antiferromagnetic sublattices and can lead to a significant AHE even in the absence of a net magnetization. The side-jump mechanism, on the other hand, arises from the abrupt change in the carrier's wavefunction upon scattering, leading to a transverse displacement and contributing to the AHE. Both of these extrinsic mechanisms are strongly dependent on the disorder and impurity concentration within the material, making them challenging to control and predict.

The intrinsic contribution to the AHE in antiferromagnets is particularly intriguing, as it originates from the Berry phase acquired by the Bloch electrons as they move through the Brillouin zone. This contribution is closely tied to the topological properties of the band structure and can be enhanced by the presence of strong spin-orbit coupling. Recent experimental and theoretical studies have highlighted the role of topological band structures in driving the AHE in antiferromagnetic materials, suggesting that these systems could serve as a platform for exploring novel quantum phenomena and potential applications in spintronics.

### 4.2.2 Nonlinear Optical Metasurfaces and Bound-States-in-the-Continuum
Nonlinear optical metasurfaces have emerged as a powerful platform for enhancing and controlling light-matter interactions, particularly through the excitation of bound-states-in-the-continuum (BICs) [18]. BICs are unique optical modes that exist within the continuous spectrum but remain spatially localized and decoupled from the radiation continuum. This property endows them with extremely high quality factors and strong field confinement, making them ideal for nonlinear optical processes. Recent advancements in the design and fabrication of dielectric metasurfaces have enabled the realization of BICs in various geometries, including arrays of subwavelength resonators and periodic gratings, which can support multiple BICs simultaneously.

The integration of BICs into nonlinear optical metasurfaces has opened new avenues for manipulating light at the nanoscale [18]. By leveraging the high field intensities and long lifetimes associated with BICs, researchers have demonstrated significant enhancements in second-harmonic generation (SHG), sum-frequency generation (SFG), and four-wave mixing (FWM) processes. These nonlinear effects, typically observed in bulk materials, can now be achieved in thin, planar metasurfaces, offering a compact and tunable alternative for nonlinear optics. Moreover, the ability to engineer the dispersion and symmetry of BICs allows for precise control over the phase matching conditions, further optimizing the efficiency of nonlinear optical interactions.

Despite the promising potential of BICs in nonlinear optical metasurfaces, several challenges remain [18]. One key issue is the sensitivity of BICs to structural imperfections and environmental perturbations, which can lead to mode broadening and reduced performance. Advanced fabrication techniques, such as electron-beam lithography and focused ion beam milling, are being developed to mitigate these issues and achieve high-fidelity BICs. Additionally, theoretical models and numerical simulations are crucial for predicting and optimizing the behavior of BICs in complex metasurface designs. As the field continues to evolve, the combination of experimental advancements and theoretical insights will likely lead to the development of novel nonlinear optical devices with unprecedented capabilities.

### 4.2.3 Threshold Quantum State Tomography in Photonic Systems
Threshold Quantum State Tomography (tQST) represents a significant advancement in the characterization of quantum states, particularly in photonic systems where the complexity of state reconstruction can be overwhelming [10]. Unlike traditional Quantum State Tomography (QST), which requires an exponentially increasing number of measurements with the system size, tQST leverages the inherent structure of the density matrix to reduce this requirement [10]. By focusing on the most significant measurements, tQST can efficiently reconstruct the state with fewer resources, making it a practical solution for large-scale quantum systems [10].

The core principle of tQST is the recognition that the off-diagonal elements of a density matrix are not independent but are constrained by the diagonal elements. This interdependence allows for a systematic reduction in the number of measurements needed to accurately reconstruct the state. In photonic systems, where the state space can be vast due to the continuous nature of the variables, this reduction is particularly beneficial. The method can be applied to both discrete-variable (DV) and continuous-variable (CV) systems, making it a versatile tool for a wide range of quantum technologies. For DV systems, tQST can be used to reconstruct multi-qubit states, while for CV systems, it can handle the reconstruction of Gaussian and non-Gaussian states with high fidelity.

Moreover, tQST is robust against noise and incomplete data, which are common issues in experimental settings. By employing a rank-controlled ansatz, tQST can effectively handle noisy datasets and recover the original quantum state with higher fidelity compared to other methods [19]. This robustness is crucial for practical applications, as it allows for reliable state reconstruction even in the presence of experimental imperfections. The ability to prioritize significant measurements also enhances the efficiency of the tomographic process, making tQST a valuable tool for the characterization and optimization of photonic quantum systems.

## 4.3 Quantum State Tomography and Characterization Methods

### 4.3.1 Quantum Memristors and One-Photon Qubits
Quantum memristors represent a novel class of quantum devices that exhibit memory effects at the quantum level, extending the classical concept of memristors to the quantum domain. These devices are characterized by their ability to store and process quantum information in a non-volatile manner, which is crucial for the development of quantum neuromorphic computing and other advanced quantum technologies. The integration of quantum memristors with one-photon qubits, which are quantum bits encoded in the states of single photons, opens up new possibilities for scalable and robust quantum information processing. One-photon qubits are particularly attractive due to their long coherence times and ease of manipulation, making them ideal for long-distance quantum communication and distributed quantum computing.

The interaction between quantum memristors and one-photon qubits can be realized through various physical platforms, such as integrated photonic circuits and quantum dots. In integrated photonic circuits, the quantum memristor can be designed to control the phase and amplitude of the one-photon qubit, thereby enabling the implementation of complex quantum logic gates and memory operations. Quantum dots, on the other hand, provide a natural platform for the generation and manipulation of one-photon qubits, and their integration with quantum memristors can lead to the creation of highly tunable and controllable quantum systems. This combination not only enhances the functionality of individual quantum components but also facilitates the development of hybrid quantum systems that can leverage the strengths of both technologies.

Moreover, the study of quantum memristors and one-photon qubits has significant implications for the broader field of quantum information science. The non-volatility and memory effects of quantum memristors can be exploited to create quantum memory elements that are essential for quantum error correction and fault-tolerant quantum computing. Additionally, the use of one-photon qubits in these systems can help mitigate the effects of decoherence and other noise sources, which are major challenges in the practical implementation of quantum technologies. As research in this area continues to advance, the integration of quantum memristors and one-photon qubits is expected to play a pivotal role in the realization of next-generation quantum devices and systems.

### 4.3.2 Selective and Efficient Quantum State Tomography
Selective and Efficient Quantum State Tomography (SE-QST) addresses the computational and resource challenges associated with traditional Quantum State Tomography (QST) by focusing on specific aspects of the quantum state that are most relevant to the system under study [10]. Traditional QST requires an exponential number of measurements with respect to the number of qubits, making it impractical for large-scale quantum systems [19]. SE-QST mitigates this issue by employing strategies that reduce the number of required measurements while maintaining the accuracy and reliability of the state reconstruction. One such approach is the use of a rank-controlled ansatz, which assumes that the quantum state can be approximated by a low-rank matrix. This assumption not only speeds up the computation but also enhances the robustness of the state reconstruction against noise and incomplete data.

Another key technique in SE-QST is threshold Quantum State Tomography (tQST), which exploits the inherent structure of the density matrix to prioritize measurements [10]. By focusing on the most significant elements of the density matrix, tQST can significantly reduce the number of required measurements without compromising the fidelity of the reconstructed state [10]. This method is particularly useful for systems where the state is known to have a sparse representation, such as pure states or states with low entanglement. Additionally, tQST can be adapted to handle noisy and incomplete datasets, making it a versatile tool for practical quantum state characterization [4]. The effectiveness of tQST has been demonstrated in both discrete variable (DV) systems, such as multi-qubit states, and continuous variable (CV) systems, where the measurement operators are often projectors.

In the context of large-scale quantum systems, SE-QST techniques have shown significant promise in improving the efficiency and accuracy of state reconstruction [19]. For instance, in systems with up to seven qubits, SE-QST methods have achieved full-rank state tomography with high fidelity, outperforming traditional QST approaches. These techniques are particularly advantageous in scenarios where the quantum state is expected to have a specific structure, such as being close to a pure state or having a low rank. Moreover, the ability to handle noisy and incomplete data makes SE-QST a valuable tool for real-world applications, where experimental imperfections are common. The combination of rank-controlled ansatz and tQST represents a powerful approach to advancing the field of quantum state tomography, enabling more efficient and reliable characterization of quantum systems.

### 4.3.3 Toroidal Moments in Molecules and Spectroscopic Techniques
Toroidal moments in molecules arise from the circulation of charge currents within molecular structures, leading to unique electromagnetic properties that are distinct from conventional electric and magnetic dipole moments. These moments are particularly significant in chiral molecules, where the handedness of the molecule can influence the direction and magnitude of the toroidal moment. The presence of toroidal moments can lead to novel physical phenomena, such as enhanced light-matter interactions and unique optical responses, which are of great interest in both fundamental and applied research.

Spectroscopic techniques have played a crucial role in the detection and characterization of toroidal moments in molecules. Techniques such as Raman spectroscopy, electron paramagnetic resonance (EPR), and nuclear magnetic resonance (NMR) have been employed to probe the toroidal moments by measuring the response of molecular systems to external electromagnetic fields. For instance, Raman spectroscopy can detect the anisotropic scattering of light due to the toroidal moment, providing insights into the molecular structure and dynamics. Similarly, EPR and NMR spectroscopies can reveal the interaction between the toroidal moment and nuclear spins, offering a deeper understanding of the electronic and magnetic properties of the molecule.

Recent advances in spectroscopic techniques have enabled more precise and sensitive measurements of toroidal moments, facilitating the study of these phenomena in complex molecular systems. For example, the use of ultrafast laser pulses in time-resolved spectroscopy has allowed researchers to observe the transient behavior of toroidal moments, providing a temporal dimension to the analysis. Additionally, the integration of computational methods with experimental data has enhanced the ability to model and predict the presence and impact of toroidal moments in various molecular configurations. These combined efforts are crucial for advancing the field and exploring potential applications in areas such as molecular electronics and quantum information processing.

# 5 Quantum Complexity and Entanglement Dynamics

## 5.1 Localization and Dynamics in Quantum Systems

### 5.1.1 Krylov Complexity in Quantum Kicked Rotor Models
Krylov complexity (K-complexity) has emerged as a crucial tool for understanding the dynamics and complexity of quantum systems, particularly in the context of the quantum kicked rotor (QKR) model [20]. The QKR model, a paradigmatic example of a quantum system that exhibits both integrable and chaotic behavior, provides a rich setting for investigating the interplay between quantum chaos and dynamical localization [21]. In this model, the system's Hamiltonian is periodically kicked, leading to a discrete-time evolution that can be studied using Krylov complexity. K-complexity quantifies the growth of operators over time by analyzing their spread over a Krylov basis, which is a set of vectors generated by repeated applications of the Hamiltonian on an initial state [21]. This approach has been particularly useful in identifying the transition from integrable to chaotic dynamics, as the growth of K-complexity can reveal the onset of quantum chaos and the breakdown of integrability.

In the QKR model, the Krylov basis is constructed by repeatedly applying the time-evolution operator to an initial state, generating a sequence of vectors that span a subspace known as the Krylov subspace. The K-complexity of an operator is then defined as the number of Krylov basis vectors required to approximate the operator to a given precision [21]. This measure provides a quantitative way to track the operator growth and the spread of quantum information within the system. Recent studies have shown that in the integrable regime, K-complexity grows linearly with time, reflecting the regular and predictable behavior of the system. However, as the system transitions into the chaotic regime, the growth of K-complexity becomes exponential, indicating the rapid spread of quantum information and the onset of quantum chaos. This transition is further characterized by the behavior of the Lanczos coefficients, which are the coefficients that appear in the Lanczos algorithm used to construct the Krylov basis [20].

The application of K-complexity to the QKR model has also shed light on the phenomenon of dynamical localization, where the quantum system exhibits a suppression of energy diffusion despite the presence of classical chaos. In this regime, the K-complexity growth is significantly slowed down, and the system's behavior deviates from the exponential growth expected in fully chaotic systems. This deviation is attributed to the formation of a quasi-energy spectrum with a discrete set of eigenstates, leading to a localization of the wave function in momentum space. The study of K-complexity in the QKR model thus provides a comprehensive framework for understanding the intricate dynamics of quantum chaos and localization, highlighting the role of operator growth and the spread of quantum information in these phenomena.

### 5.1.2 Many-Body Localization and Emergent Phenomena
Many-Body Localization (MBL) represents a fascinating deviation from the conventional wisdom that isolated quantum systems thermalize over time. In disordered systems, MBL arises when interactions between particles are insufficient to delocalize the system, leading to a breakdown of ergodicity and the emergence of localized eigenstates. These localized states are characterized by an area-law entanglement entropy, in stark contrast to the volume-law entanglement typical of thermalizing systems. The persistence of memory in the system's initial conditions, a hallmark of MBL, has profound implications for the dynamics of quantum information and the relaxation of observables.

The study of MBL has revealed a rich landscape of emergent phenomena, including the formation of localized excitations that can be thought of as quasiparticles with non-trivial properties. These quasiparticles do not thermalize and can exhibit unusual transport behaviors, such as power-law decay of correlations and logarithmic growth of entanglement entropy. The emergence of such phenomena is not only of theoretical interest but also has practical implications for the design of quantum information processing systems, where the ability to control and manipulate localized states could be leveraged for error correction and information storage.

Moreover, the interplay between MBL and other quantum phenomena, such as topological order and symmetry-protected topological phases, has opened new avenues for research. For example, the presence of MBL can stabilize topological order in the presence of strong disorder, leading to the possibility of realizing robust topological qubits. The study of these emergent phenomena in MBL systems is crucial for understanding the fundamental limits of quantum dynamics and the potential for novel quantum technologies.

### 5.1.3 Geometric Invariant Theory and Non-Free Tensors
Geometric Invariant Theory (GIT) provides a framework for understanding the structure of orbits under group actions, particularly in the context of tensors. In the study of tensor spaces \( \mathbb{C}^n \otimes \mathbb{C}^n \otimes \mathbb{C}^n \), GIT helps to classify tensors based on their invariance properties under the action of the general linear group \( GL(n, \mathbb{C}) \). A tensor is considered free if its orbit under this action is closed; otherwise, it is non-free. The distinction between free and non-free tensors is crucial for understanding the geometric and algebraic properties of tensor spaces, especially in the context of entanglement and complexity.

In Section 5.1.3, we delve into the criteria for determining whether a tensor is non-free [22]. The moment map, a central tool in GIT, plays a pivotal role in this classification. The moment map \( \mu: \mathbb{C}^n \otimes \mathbb{C}^n \otimes \mathbb{C}^n \setminus \{0\} \to \text{Herm}(n, n, n) \) maps a non-zero tensor to a triple of Hermitian matrices. These matrices capture the symmetries and invariants of the tensor under the action of \( GL(n, \mathbb{C}) \). By analyzing the image of the moment map, one can determine the stability and freeness of a tensor [22]. Specifically, a tensor is non-free if its image under the moment map lies on the boundary of the moment polytope, indicating that the tensor's orbit is not closed.

To illustrate the application of these concepts, we present two examples of concise non-free tensors in \( \mathbb{C}^3 \otimes \mathbb{C}^3 \otimes \mathbb{C}^3 \), denoted as \( T_2 \) and \( T_5 \) [22]. These examples highlight the complexity and subtlety involved in identifying non-free tensors. We then generalize the example \( T_2 \) to construct an explicit family of non-free tensors in \( \mathbb{C}^n \otimes \mathbb{C}^n \otimes \mathbb{C}^n \) for arbitrary \( n \). This generalization underscores the broader implications of non-freeness in tensor spaces and its relevance to the study of entanglement and quantum complexity.

## 5.2 Entanglement and Complexity Measures

### 5.2.1 Tensor Cross Interpolation for Entanglement Feature Computation
Tensor Cross Interpolation (TCI) emerges as a pivotal technique for the efficient computation of entanglement features in complex quantum systems [11]. By leveraging the tensor network formalism, TCI allows for the systematic approximation of high-dimensional entanglement structures, which are otherwise computationally intractable. Specifically, TCI constructs an interpolative approximation of the entanglement feature by iteratively sampling a subset of purity values, thereby reducing the computational complexity from exponential to polynomial in the system size [11]. This approach is particularly advantageous in scenarios where the full density matrix is not readily accessible, such as in large-scale quantum simulations or experimental settings with limited measurement capabilities.

The core of the TCI algorithm lies in its ability to accurately estimate the entanglement spectrum using a minimal set of input data. The algorithm begins by selecting a small number of representative purity values, which are then used to construct an initial approximation of the entanglement feature. Through a series of iterative refinements, the algorithm progressively improves the accuracy of the approximation, ensuring that the final result captures the essential entanglement properties of the system. This iterative process is guided by a carefully chosen set of interpolation points, which are selected to maximize the information gain at each step. The efficiency of TCI is further enhanced by exploiting the tensor structure of the problem, allowing for parallelization and distributed computing strategies.

Two primary applications of the TCI-based entanglement feature computation are highlighted in this section. Firstly, the interpolated entanglement feature provides a robust metric for comparing the entanglement properties of different quantum states, facilitating the identification of universal entanglement patterns across various physical systems [11]. Secondly, the TCI approach enables the efficient reordering of spatial indices in tensor networks, optimizing the representation of entangled states and improving the performance of numerical algorithms. These applications underscore the versatility and practical utility of TCI in advancing our understanding of quantum entanglement and its role in emergent phenomena.

### 5.2.2 Variational Multipartite Geometric Entanglement Algorithm
The Variational Multipartite Geometric Entanglement Algorithm (VMGEA) is a sophisticated approach designed to quantify and analyze multipartite entanglement in complex quantum systems. This algorithm leverages geometric principles to provide a robust framework for understanding the intricate entanglement structures that emerge in many-body quantum states. At its core, the VMGEA operates by constructing a variational ansatz that approximates the entanglement structure of a given quantum state. This ansatz is typically formulated using a tensor network representation, such as matrix product states (MPS) or projected entangled pair states (PEPS), which are well-suited for capturing the long-range correlations characteristic of multipartite entanglement.

The algorithm begins by defining a set of variational parameters that describe the entanglement between different subsystems. These parameters are optimized to minimize a cost function that quantifies the distance between the ansatz and the true entanglement structure of the quantum state. The cost function often takes the form of a geometric distance, such as the Bures distance or the Sjoqvist metric, which are sensitive to the entanglement properties of the state. By iteratively refining the variational parameters, the VMGEA can efficiently converge to a highly accurate approximation of the multipartite entanglement, even for systems with a large number of qubits.

One of the key advantages of the VMGEA is its ability to handle both pure and mixed states, making it a versatile tool for a wide range of quantum systems. For pure states, the algorithm can be used to compute entanglement measures such as the von Neumann entropy and purity, while for mixed states, it can provide insights into the structure of the density matrix and the distribution of entanglement across different subsystems. The VMGEA has been successfully applied to study the entanglement properties of topological phases, quantum critical systems, and complex many-body Hamiltonians, offering a powerful method for characterizing the entanglement patterns that underlie these phenomena.

### 5.2.3 Geometric Approaches to Quantify Complexity in Mixed States
To capture complexity in mixed states, three geometric approaches are primarily employed: the Bures metric, the Sjoqvist metric, and Nielsen’s circuit complexity [23]. The Bures metric, derived from the Uhlmann fidelity, measures the distance between density matrices by considering the shortest path on the manifold of quantum states. This metric is particularly useful for mixed states as it provides a natural extension of the Fubini-Study metric used for pure states. The Bures metric captures the intrinsic geometry of the space of density matrices, making it a robust tool for quantifying the complexity of mixed states [23].

The Sjoqvist metric, on the other hand, reflects distances between sets of rays from the spectral decomposition of the density matrix. This approach focuses on the geometric structure of the eigenstates and their corresponding eigenvalues, providing a detailed picture of the state's internal correlations. By analyzing the distances between these rays, the Sjoqvist metric offers a complementary perspective to the Bures metric, emphasizing the role of the state's spectral properties in defining its complexity.

Nielsen’s circuit complexity, formulated through purification, extends the concept of quantum complexity to mixed states by considering the minimum number of quantum gates required to transform a reference state into the target mixed state. This approach involves purifying the mixed state into a larger, entangled pure state and then applying the standard circuit complexity measures [23]. The purification step introduces an additional layer of complexity, as multiple purifications can exist for a given mixed state. Despite this, Nielsen’s circuit complexity provides a powerful framework for understanding the computational resources needed to prepare and manipulate mixed states, bridging the gap between theoretical and practical aspects of quantum complexity.

## 5.3 Analytical and Computational Methods

### 5.3.1 Krylov Complexity and Operator Dynamics
Krylov complexity (K-complexity) has emerged as a robust framework for understanding the dynamics of operators in quantum systems, particularly in the context of chaotic and integrable systems [20]. By quantifying the growth of operators through nested commutators with the Hamiltonian, K-complexity provides a measure of how quickly an initially localized operator spreads out in the Hilbert space. This spreading is captured by the Krylov basis, a set of vectors generated by repeatedly applying the Hamiltonian to an initial operator. The K-complexity function, which tracks the norm of the operator in the Krylov basis, thus serves as a diagnostic tool for identifying different dynamical regimes, such as ballistic, diffusive, or subdiffusive growth [21].

The operator dynamics in the Krylov basis are intimately linked to the structure of the Hamiltonian and the initial state. For integrable systems, the K-complexity typically grows linearly with time, reflecting the regular and predictable nature of the dynamics. In contrast, chaotic systems exhibit exponential growth of K-complexity, indicative of the rapid scrambling of quantum information [21]. This distinction is crucial for understanding the transition between integrable and chaotic behavior in quantum systems, as well as for probing the onset of thermalization and the breakdown of ergodicity [20]. The K-complexity formalism also allows for the study of weak ergodicity breaking, where the dynamics are neither fully integrable nor chaotic, leading to intermediate power-law growth of complexity.

Moreover, the K-complexity framework has been extended to study the dynamics of mixed states and the entanglement properties of quantum systems. By generalizing the concept of K-complexity to mixed states using geometric approaches such as the Bures metric and the Sjoqvist metric, researchers can quantify the complexity of density matrices and their evolution under unitary dynamics [23]. This extension is particularly useful for analyzing the complexity of states in topological quantum computing, where the Hilbert space lacks a natural factorization into subregions [24]. The interplay between K-complexity and entanglement measures, such as entanglement entropy and purity, provides a comprehensive picture of the dynamical behavior of quantum systems, bridging the gap between operator dynamics and state complexity [21].

### 5.3.2 Quantum Many-Body Scars and Non-Ergodic Dynamics
Quantum many-body scars (QMBS) represent a unique class of non-thermal eigenstates that coexist within a thermal spectrum, leading to persistent dynamics that violate the ergodic hypothesis [20]. Unlike typical eigenstates, which are expected to thermalize over time, scar states exhibit periodic revivals and long-lived oscillations, indicating a form of weak ergodicity breaking. This phenomenon is particularly intriguing in the context of spin-1 quantum systems, where local SU(2) algebra constraints can give rise to nematic Néel states that exhibit periodic revivals in Krylov complexity [20]. These revivals suggest that the system retains memory of its initial state, even in the presence of strong interactions, which is a hallmark of non-ergodic dynamics.

The emergence of QMBS is sensitive to the initial state and the specific interactions within the system. In spin-1 XY and XXZ magnets, for instance, the interplay between local constraints and the Hamiltonian's symmetry can lead to the formation of scars. These scars are characterized by a reduced entanglement entropy compared to typical thermal states, indicating that the system's entanglement structure is highly non-trivial. The presence of scars can also be detected through the behavior of observables, such as the Krylov complexity, which exhibits periodic oscillations rather than the expected exponential growth observed in ergodic systems. This behavior is indicative of a breakdown in the eigenstate thermalization hypothesis (ETH), which posits that local observables in generic many-body systems should thermalize to a thermal ensemble.

Understanding the mechanisms that give rise to QMBS and their implications for non-ergodic dynamics is crucial for advancing our knowledge of quantum many-body systems [20]. The study of these scars not only provides insights into the fundamental limits of thermalization but also has practical implications for the design of quantum simulators and the optimization of resources in cold-atom experiments [23]. By exploring the conditions under which scars emerge and the properties they exhibit, researchers can develop new theoretical frameworks to describe non-equilibrium quantum dynamics and potentially harness these unique states for quantum information processing tasks.

### 5.3.3 Differential Route for Entanglement Dynamics
In the differential route for entanglement dynamics, the focus shifts from static entanglement measures to the evolution of entanglement under continuous transformations, particularly in the context of topological quantum computing. This approach leverages the concept of complexity, which quantifies the minimal number of quantum gates required to transform one state into another. By applying the complexity parameter formulation, we derive a set of differential equations that describe the evolution of entanglement measures over time [25]. These equations are essential for understanding how entanglement changes in response to varying system parameters, such as coupling strengths or external fields, without the need for explicit knowledge of the underlying Hamiltonian [25].

The differential route is particularly useful for analyzing the entanglement dynamics of link states, which are defined by the Chern-Simons path integral on three-manifolds with multiple boundary tori [24]. Unlike traditional methods that require the density matrix, this approach allows for a more direct analysis of entanglement by considering the state tensor and its evolution under continuous transformations. The use of tensor cross interpolation algorithms (TCI) further enhances this method by enabling the efficient computation of entanglement features, even for complex systems with large Hilbert spaces [11]. This technique constructs the interpolation of entanglement measures iteratively, requiring only a limited number of purity values, making it computationally feasible for practical applications [11].

By integrating the differential equations derived from the complexity parameter formulation, we can track the entanglement dynamics of link states over time, providing insights into the behavior of entanglement in topologically non-trivial systems. This approach not only bridges the gap between theoretical models and experimental observations but also opens new avenues for exploring the entanglement structure of generic fibered links, including hyperbolic links, which are considered "typical" in the moduli space of fibered links [24]. The differential route thus offers a powerful tool for advancing our understanding of entanglement in topological quantum computing and related fields.

# 6 Future Directions


The current landscape of research on the complexity of learning quantum states, while rich and diverse, still faces several limitations and gaps that need to be addressed. One of the primary challenges is the computational complexity associated with simulating and manipulating large quantum systems. While significant progress has been made in developing efficient algorithms and techniques, such as sparse quantum simulation and pre-trajectory sampling with batched execution, these methods often struggle to scale to the sizes required for practical applications. Additionally, the integration of quantum mechanics with decision-making models, such as Quantum Markov Decision Processes (QMDPs), remains in its early stages, with many open questions regarding the practical implementation and optimization of these models on near-term quantum hardware. Furthermore, the fidelity and robustness of quantum state preparation and control, particularly in the presence of noise and decoherence, continue to pose significant hurdles. The development of more advanced error correction techniques and the exploration of novel encoding schemes are essential for achieving fault-tolerant quantum computing. Lastly, the characterization and measurement of quantum states, including the efficient and accurate reconstruction of complex entangled states, remain areas of active research, with a need for more scalable and noise-resilient tomography methods.

To address these limitations, several directions for future research are proposed. First, the development of more efficient and scalable algorithms for simulating large quantum systems is crucial. This includes advancing sparse quantum simulation techniques and optimizing pre-trajectory sampling methods to better leverage parallel computing resources. Additionally, the exploration of hybrid classical-quantum approaches, where classical algorithms are used to preprocess and reduce the complexity of quantum simulations, could provide a practical pathway to handling larger systems. Second, the integration of quantum mechanics with decision-making models, particularly QMDPs, should be further explored. This involves developing more sophisticated algorithms for encoding classical decision-making problems into quantum states and optimizing the implementation of quantum operations to handle the inherent uncertainty and superposition in these models. Third, the robustness and efficiency of quantum state preparation and control need to be enhanced. Research into novel error correction codes, such as topological codes, and the development of adaptive measurement strategies for variational quantum algorithms can significantly improve the fidelity and reliability of quantum state manipulation. Fourth, the field of quantum state tomography and characterization should focus on developing more efficient and noise-tolerant methods, such as selective and threshold quantum state tomography, to enable the accurate reconstruction of complex quantum states in practical settings.

The potential impact of the proposed future work is substantial. More efficient and scalable simulation techniques will facilitate the development of practical quantum algorithms and protocols, paving the way for real-world applications in fields such as materials science, drug discovery, and optimization. The integration of quantum mechanics with decision-making models could lead to revolutionary advancements in areas like autonomous systems and artificial intelligence, where the ability to handle uncertainty and make optimal decisions is critical. Enhanced quantum state preparation and control will be essential for achieving fault-tolerant quantum computing, a key milestone in the realization of quantum advantages. Finally, improved methods for quantum state tomography and characterization will enable more precise and reliable measurements, supporting the advancement of quantum technologies and the discovery of new quantum phenomena. Overall, these research directions have the potential to significantly advance the field of quantum information processing and open new avenues for scientific and technological innovation.

# 7 Conclusion



The survey paper provides a comprehensive overview of the complexity of learning quantum states, a critical area at the intersection of quantum computing, quantum information theory, and machine learning. Key findings include the development of Quantum Markov Decision Processes (QMDPs) for efficient decision-making in quantum systems, the introduction of pre-trajectory sampling with batched execution (PTSBE) for scalable quantum simulations, and the advancement of sparse quantum simulation techniques for managing large quantum state spaces. The paper also highlights the integration of quantum mechanics with machine learning, showcasing innovations such as Quantum Complex-Valued Self-Attention Models (QCSAM) and Quantum Circuit Search (QCS) for drug discovery. Additionally, the survey delves into the challenges and solutions for quantum state preparation and control, including quantum amplitude encoding, quantum error correction, and variational quantum algorithms for entangled states. The characterization and measurement of quantum states are addressed through techniques like threshold quantum state tomography and the measurement of toroidal moments in molecules.

The significance of this survey lies in its thorough examination of the theoretical and practical aspects of learning quantum states, synthesizing recent advancements and identifying key research directions. By providing a detailed overview of the field, the paper serves as a valuable resource for researchers and practitioners in quantum computing, quantum information theory, and machine learning. The interdisciplinary nature of the topics covered underscores the importance of collaboration across these fields to address the complex challenges and realize the potential of quantum technologies. The survey also highlights the practical implications of these advancements, such as the development of more efficient quantum algorithms, the improvement of quantum error correction techniques, and the enhancement of quantum machine learning models.

In conclusion, the field of learning quantum states is poised for significant advancements, driven by the ongoing research and innovation highlighted in this survey. Future work should focus on developing more robust and scalable methods for quantum state manipulation, enhancing the integration of quantum computing with machine learning, and addressing the practical challenges of implementing these techniques on near-term quantum hardware. Researchers and practitioners are encouraged to build upon the foundational knowledge presented in this survey to drive the next wave of discoveries and applications in quantum information processing. The realization of practical quantum technologies depends on continued interdisciplinary collaboration and the exploration of new theoretical and experimental approaches.

# References
[1] Machine Learning for Estimation and Control of Quantum Systems  
[2] Fast Quantum Amplitude Encoding of Typical Classical Data  
[3] Quantum-Inspired Reinforcement Learning in the Presence of Epistemic  Ambivalence  
[4] Augmenting Simulated Noisy Quantum Data Collection by Orders of  Magnitude Using Pre-Trajectory Samp  
[5] SparQSim  Simulating Scalable Quantum Algorithms via Sparse Quantum  State Representations  
[6] Variational preparation of entangled states in a system of transmon  qubits  
[7] Quantum Complex-Valued Self-Attention Model  
[8] Quantum-Enhanced LLM Efficient Fine Tuning  
[9] Benign Overfitting with Quantum Kernels  
[10] Experimental verification of Threshold Quantum State Tomography on a  fully-reconfigurable photonic  
[11] Tensor Cross Interpolation of Purities in Quantum Many-Body Systems  
[12] Quantum Circuit Design for Decoded Quantum Interferometry  
[13] QCS-ADME  Quantum Circuit Search for Drug Property Prediction with  Imbalanced Data and Regression A  
[14] On the Generalization of Adversarially Trained Quantum Classifiers  
[15] Warm-Starting QAOA with XY Mixers  A Novel Approach for Quantum-Enhanced  Vehicle Routing Optimizati  
[16] Quantum Error Correction in Quaternionic Hilbert Spaces  
[17] Few-electron spin qubits in optically active GaAs quantum dots  
[18] Nonlinear optical metasurfaces empowered by bound-states in the  continuum  
[19] Gradient-descent methods for fast quantum state tomography  
[20] Krylov complexity in quantum many-body scars of spin-1 models  
[21] Probing the localization effects in Krylov basis  
[22] Explicit non-free tensors  
[23] Complexity of Bose-Einstein condensates at finite temperature  
[24] Entanglement in Typical States of Chern-Simons Theory  
[25] Entanglement dynamics of many-body quantum states with evolving system  conditions  