# 5/1/2025, 6:15:15 PM_Complexity of Learning Quantum States  

# 0. Complexity of Learning Quantum States  

1. Introduction  

![](images/9f1956aa898c55e56713d51313fcb3ef2cacd3efb9c4382b6b41a5b44a8088df.jpg)  

Quantum Information Processing (QIP) represents a transformative paradigm built upon the principles of quantum mechanics to enable computational and information processing capabilities that fundamentally surpass those of classical systems [6,14]. This interdisciplinary field, drawing from quantum mechanics, information theory, and computer science [9], centers on the manipulation of quantum bits (qubits) and quantum gates [6]. Unlike classical bits confined to states 0 or 1, qubits leverage superposition to exist in multiple states simultaneously, and quantum gates enable complex operations on these qubits to execute quantum algorithms [6]. Key areas within QIP include quantum computing, communication, cryptography, and sensing, with quantum computing specifically focused on achieving superior computational power for complex problems [6,15].​  

A fundamental challenge within this domain is the problem of learning unknown quantum states. Accurately characterizing or reconstructing a quantum state from experimental data is crucial for validating quantum hardware, verifying quantum simulations, and implementing quantum algorithms. This task is profoundly different from learning classical probability distributions due to inherent quantum mechanical phenomena. Specifically, the properties of superposition and entanglement mean that a quantum state describing a system of $n$ qubits resides in a Hilbert space whose dimension grows exponentially with $\scriptstyle n$ . Entanglement, a non-classical correlation between quantum systems, ensures that the state of the composite system cannot be described by the states of its individual components, leading to complex, multi-partite correlations that must be captured during the learning process [18]. Furthermore, measurement in quantum mechanics is probabilistic and generally disturbs the state, meaning information about the state can only be gathered indirectly and state preparation and measurement resources are critical. This contrasts sharply with classical state learning, where independent samples of a probability distribution can be acquired and combined straightforwardly. These challenges manifest in difficulties when simulating complex quantum-classical hybrid dynamics, particularly concerning the exponential growth of Hilbert space and the intricate coupling between classical and quantum degrees of freedom [5].​  

To address the problem of learning quantum states and related tasks, various approaches have emerged. Quantum Machine Learning (QML), a rapidly developing field at the intersection of AI and quantum computing, seeks to leverage quantum computational advantages for machine learning tasks [1,2,18]. QML offers potential benefits in handling large-scale and high-dimensional data, implementing complex learning models, and tackling problems intractable for classical methods [18]. Frameworks like TensorCircuit support the development of relevant algorithms, including hybrid quantum-neural network paradigms [16]. Beyond QML, theoretical analysis plays a critical role, focusing on developing provably efficient algorithms and understanding fundamental limitations [5]. This includes work on algorithms that can achieve quantum speedups even with classical access to data [20] and techniques that bridge classical machine learning with quantum systems, such as using neural networks to estimate parameters of quantum states or Hamiltonians [4].​  

Central to the study of learning quantum states is the concept of complexity. This involves quantifying the resources required for the learning process, including computational complexity (the computational time or operations needed), sample complexity (the number of copies of the quantum state required), and query complexity (the number of times specific measurements or operations need to be applied). Understanding the fundamental limits and potential advantages in these complexity measures is paramount. For instance, challenges exist in predicting complex dynamics regarding error bounds and sample complexity [5], and a key goal in QML is demonstrating rigorous quantum speedups in complexity [20]. Research into complexity theory within quantum information processing, including quantum simulation and quantum networks, is an active area [11].  

The significance of understanding the complexity of learning quantum states cannot be overstated. As quantum technologies progress, marked by milestones like the demonstration of "quantum supremacy" [8], the ability to efficiently characterize and learn quantum states is crucial for scaling up quantum computers, developing robust quantum algorithms, and realizing the potential of quantum computing for practical applications [11,14]. Difficulties such as improving qubit stability and optimizing algorithms are recognized challenges [6,10], directly tying into the practical implications of complexity bounds.​  

This survey provides a comprehensive overview of the complexity landscape associated with learning quantum states. We will delve into established theoretical bounds and recent algorithmic advancements, analyzing different facets of complexity and the resources required under various assumptions. By synthesizing findings from key research efforts, this work aims to elucidate the current state of the field, highlight significant challenges, and identify promising directions for future research in the pursuit of efficient quantum state learning.  

# 2. Foundational Concepts and Background  

Understanding the complexity of learning quantum states necessitates a firm grasp of the foundational concepts   
underpinning quantum computation and information processing. At the core of this field lies the qubit, which serves as the   
fundamental unit of quantum information, fundamentally different from the classical bit [6,14,15,18]. While a classical bit is   
constrained to represent either 0 or 1, a qubit can exist in a superposition of these states, embodying a linear combination of   
$| 0 \rangle$ and $| 1 \rangle$ simultaneously [6,15,18]. A single-qubit pure state is mathematically described by a state vector in a two  
dimensional complex vector space, given by   
where $\alpha$ and $\beta$ are complex amplitudes satisfying the normalization condition   
\​  

[6,13]. This capability for superposition allows quantum systems to explore multiple computational paths concurrently, offering a potential avenue for parallel processing beyond classical limits [14,15,18].  

Beyond individual qubits, entanglement represents a critical quantum resource, establishing strong, non-classical correlations between multiple qubits such that their fates are intrinsically linked, irrespective of spatial separation [13,15,18]. Both superposition and entanglement are indispensable for unlocking the computational power promised by quantum algorithms [1]. The state space for a system of $n$ qubits grows exponentially, reaching a dimensionality of $2 ^ { n }$ [6,8], highlighting the inherent complexity of describing and manipulating multi-qubit states [11].  

The manipulation of quantum states is achieved through quantum gates, which are operations acting on qubits, analogous to logic gates in classical computing [1,2,14,18]. Sequences of quantum gates form quantum circuits, which represent quantum algorithms [8]. These operations, particularly for reversible processes, are formally described by unitary transformations acting on the state vectors or density matrices in a complex vector space known as a Hilbert space [1,11]. The mathematical description of quantum states and operations relies heavily on linear algebra, including concepts such as vectors, matrices, and tensor products [1,2,11]. While pure states are represented by state vectors, mixed states, which arise from classical uncertainty or entanglement with an environment, are described by density matrices. The evolution of a density matrix under a unitary operation $U ( t )$ is given by  

[11]. This mathematical framework is fundamental for tasks like quantum state tomography and state preparation, which are directly relevant to learning quantum states.  

Realizing quantum computation and state learning in practice depends critically on the underlying quantum computing hardware architectures. Several distinct platforms are under development, each with unique characteristics affecting scalability, coherence, gate fidelity, and qubit connectivity [1,2,14]. Prominent architectures include superconducting qubits (e.g., Google's Sycamore and Willow processors) [8,13,14], trapped ions (e.g., Quantinuum's H-Series) [1,2,13,14], neutral  

atoms (e.g., QuEra's chip) [1,2], and photonic systems [1,2,19]. Other approaches include topological qubits [13,14], electron spins, NMR, and spin chains [9,11]. Specific programmable processors are also being explored for tasks like simulating quantum channels [3]. The current state of these architectures varies, with challenges remaining in scaling up qubit numbers while maintaining high performance and resilience to noise [17]. The capabilities of these platforms—specifically the number of qubits, their coherence times, gate fidelities, and connectivity—directly influence the complexity and size of quantum states that can be prepared and learned.​  

The interdisciplinary field of Quantum Machine Learning (QML) emerges from the intersection of quantum physics and artificial intelligence, seeking to harness quantum computation to enhance machine learning capabilities [1,2,15,18]. QML algorithms can potentially offer advantages over classical approaches for certain tasks, such as processing highdimensional quantum data, learning complex data patterns, accelerating training, and improving efficiency for classification or optimization problems [1,2,15,18,20]. A crucial aspect of QML is the representation of classical data in a quantum format, often involving encoding techniques to map classical information onto quantum states for processing by quantum algorithms [1]. This quantum encoding can facilitate techniques like quantum feature mapping into high-dimensional Hilbert spaces, potentially benefiting generalization, particularly in scenarios with limited data [1]. Relevant QML paradigms include Quantum Support Vector Machines (QSVM) [13,15], Quantum Neural Networks (QNN) [15], and hybrid quantumclassical approaches like Variational Quantum Algorithms (VQAs) [16,22]. Machine learning techniques are also applied to problems within quantum mechanics, such as estimating Hamiltonian parameters or simulating quantum channels [3,4,5]. These various QML approaches provide the algorithmic context within which the problem of learning quantum states is situated.​  

# 2.1 Quantum Information Basics  

![](images/08817d3b925a7163e9048ce72aa77e5a239806f7764f59a517c0ffcc94e6c652.jpg)  

The fundamental unit of information in quantum computing is the qubit, which contrasts sharply with the classical bit [6,15,18]. Unlike a classical bit confined to states 0 or 1, a qubit can exist in a superposition of these states simultaneously [6,15]. This superposition allows a qubit to be in state $| 0 \rangle$ , $| 1 \rangle$ , or any linear combination of the two [18]. Mathematically, a single-qubit pure state can be represented as a vector in a two-dimensional complex vector space, denoted as  

$$
| \psi \rangle = \alpha | 0 \rangle + \beta | 1 \rangle ,
$$  

where $\alpha$ and $\beta$ are complex amplitudes satisfying the normalization condition  

$$
| \alpha | ^ { 2 } + | \beta | ^ { 2 } = 1
$$  

[6,13]. This representation is also known as the Bloch representation for a single qubit [6]. While a pure state is described by a single state vector, quantum systems can also exist in mixed states, which are probabilistic ensembles of different quantum states [6]. The ability of qubits to exist in superposition enables quantum computers to store and process multiple states concurrently, facilitating a form of parallel computation that is not possible classically [14,15,18].​  

Another crucial quantum phenomenon is entanglement, which describes strong correlations between quantum systems [18]. When qubits are entangled, the state of one is inextricably linked to the state of others, such that measuring the state of one instantaneously influences the states of the others, regardless of their physical separation [13,15]. Entanglement is a unique resource in quantum information processing, enabling enhanced information transfer and synchronization, which is essential for achieving powerful computational capabilities [14,15,18].  

Together, superposition and entanglement form the bedrock of quantum computation and information, providing the nonclassical resources necessary for quantum algorithms to potentially outperform classical algorithms for certain problems [1]. The state space of a system of $n$ qubits grows exponentially with $n$ , possessing a dimensionality of $2 ^ { n }$ [6,8]. For example, a system of just 50 qubits corresponds to a $2 ^ { 5 0 }$ -dimensional state space [8]. This exponential growth underscores the immense power but also the inherent complexity associated with quantum states, particularly as the number of qubits increases, as seen in processors like the 54-qubit Sycamore system (with 53 usable qubits) [8]. Entanglement, in particular, is not only a resource but also a significant source of this complexity, playing a key role in theoretical studies concerning the complexity of quantum networks and computations [11]. Understanding these fundamental concepts—qubits, superposition, and entanglement—and the resulting high-dimensional state spaces is prerequisite to analyzing the complexity of learning quantum states.​  

# 2.2 Mathematical Formalism of Quantum Mechanics  

The description and manipulation of quantum states are fundamentally rooted in a rigorous mathematical formalism, primarily relying on linear algebra and the concept of Hilbert spaces. A quantum system's state is typically represented as a vector in a complex vector space, known as a Hilbert space. For pure states, this representation is a vector of unit norm [1]. Operations on these quantum states, which represent quantum gates or the natural evolution of the system, are described by linear transformations within this space. Specifically, reversible quantum operations correspond to unitary transformations, which preserve the norm of the state vector. The study of these operations necessitates an understanding of concepts such as basic matrix operations and quantum linear algebra, as these are the tools used to describe the transformations between different states, including the transformation of classical data into quantum states within quantum machine learning contexts [1,11].​  

While pure states are represented by vectors, mixed states, which are statistical ensembles of pure states, require a different representation: the density matrix. A density matrix is a positive semi-definite, Hermitian operator with trace equal to one. This formalism is essential for describing systems that are entangled with other systems or are subject to noise. The evolution of a density matrix under unitary operations is given by  

where $\backslash ( \mathsf { U } ( \mathsf { t } ) \backslash )$ is the unitary operator describing the time evolution [11].  

This comprehensive mathematical framework, encompassing state representation via vectors and density matrices and state evolution via unitary and more general quantum operations, is indispensable for analyzing the complexity of learning quantum states. Problems such as quantum state tomography, which aims to reconstruct an unknown quantum state from experimental measurements, or quantum state preparation, which focuses on creating specific quantum states, are formulated and analyzed within this mathematical language. Understanding the properties of Hilbert spaces, the structure of density matrices, and the nature of unitary transformations provides the necessary theoretical foundation to characterize the information content of quantum states and the resources required to learn them effectively [11].  

# 2.3 Quantum Computing Architectures  

Developing practical quantum state learning experiments is fundamentally dependent on the underlying hardware platforms.  

<html><body><table><tr><td>Architecture</td><td>Qubit Type/Mechanism</td><td>Examples</td><td>Key Characteristics/Chall enges</td></tr><tr><td>Superconducting</td><td>Superconducting circuits</td><td>Google Sycamore, Willow</td><td>Scalable potential, microfabrication;</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td></td><td>Noise-sensitive, Coherence</td></tr><tr><td>Trapped lons</td><td>Individual atomic ions trapped by fields</td><td>Quantinuum H- Series, lonQ</td><td>Long coherence, high fidelity, connectivity; Scaling challenges</td></tr><tr><td>Neutral Atoms</td><td>Neutral atoms in optical lattices/tweezers</td><td>QuEra</td><td>High qubit count potential; Control complexity</td></tr><tr><td>Photonic</td><td>Photons</td><td>PsiQuantum</td><td>Room temperature potential, integration; Qubit interaction</td></tr><tr><td>Topological</td><td>Encoding in topological properties</td><td>Microsoft</td><td>Theoretical fault tolerance; Early stage of development</td></tr></table></body></html>  

A variety of quantum computing architectures are currently being pursued, each presenting distinct characteristics in terms of qubit implementation, manipulation, and connectivity, which subsequently impact their scalability, noise resilience, and performance for complex tasks like state learning [1,2,14].  

Among the most prominent architectures are superconducting qubits, trapped ions, neutral atoms, and photonic systems, alongside emerging approaches like topological qubits. Superconducting quantum computers, exemplified by Google's Sycamore and Willow processors, utilize superconducting circuits—often involving Josephson junctions—operated at extremely low temperatures to define and manipulate qubits [8,13,14]. Google's Willow platform boasts 105 qubits [1,2], while Sycamore employs a 2D qubit array with tunable couplers to manage interactions and is particularly suited for tasks like running random quantum circuits [8]. Superconducting circuits benefit from microfabrication techniques, offering potential for scalability in terms of qubit numbers [17]. However, maintaining qubit coherence and achieving high-fidelity gate operations remain significant challenges due to sensitivity to environmental noise [17].  

Trapped ion architectures, utilized by companies like Quantinuum and IonQ, confine individual atomic ions using electric fields and manipulate their quantum states with lasers [13,14]. Quantinuum's H-Series processors, including the H2 chip with 56 fully connected qubits, are noted for achieving longer coherence times and higher gate fidelity compared to some other platforms [1,2]. The ability to achieve high gate fidelity and, in some configurations, full connectivity between a significant number of qubits, provides an advantage for implementing complex quantum algorithms and preparing highly entangled states required for state learning.  

Neutral atom systems, such as the 156-qubit chip developed by QuEra, trap and manipulate neutral atoms, often using optical lattices or tweezers [1,2]. These platforms are rapidly advancing in qubit count, demonstrating significant scalability potential.​  

Photonic quantum computing employs photons as qubits. PsiQuantum has made notable progress in integrating photonic chips for potentially large-scale optical quantum computing [1,2]. Photonic systems are also explored in experimental settings related to quantum information and thermodynamics [8,19].  

Topological quantum computing, researched by entities like Microsoft, encodes quantum information in topological properties of matter using anyons, theoretically offering enhanced stability and inherent resistance to certain types of noise [13,14]. While offering promise for fault tolerance, this architecture is generally considered to be in an earlier stage of development compared to superconducting or trapped ion systems in terms of practical, large-scale implementations. Other physical implementations mentioned include systems based on electron spins, NMR, and spin chains [9,11]. Specific programmable processors, such as the generalized teleportation processor (GTP), are also being developed to simulate quantum channels, which could be relevant for understanding noise effects in learning algorithms [3].  

The characteristics of these architectures directly impact the feasibility and performance of quantum state learning experiments. The scalability, measured by the number of available qubits, dictates the complexity and size of quantum states that can be prepared, manipulated, and learned. Learning states of larger Hilbert spaces requires platforms capable of hosting a greater number of qubits. Noise resilience, encompassing qubit coherence times and gate fidelities, is critical because errors accumulate during the gate sequences needed for state preparation and the measurements required for tomography or learning algorithms. Higher fidelity and longer coherence allow for deeper circuits and more accurate state representations. Connectivity between qubits affects the efficiency and feasibility of implementing the multi-qubit gates necessary to create entangled states. Architectures with high connectivity (like fully connected trapped ions for smaller systems) can simplify circuit design and reduce the overhead associated with qubit rearrangement, potentially improving performance for state learning tasks that rely on complex entangling operations. Conversely, architectures with limited connectivity might require additional gates (e.g., SWAP gates), increasing circuit depth and susceptibility to noise. Therefore, the choice and capabilities of the quantum hardware architecture are paramount considerations for designing, executing, and interpreting the results of quantum state learning experiments.​  

# 2.4 Introduction to Quantum Machine Learning  

Quantum Machine Learning (QML) represents an emerging interdisciplinary field situated at the confluence of quantum physics and artificial intelligence [1,2,15,18]. This field seeks to enhance machine learning capabilities by leveraging the unique properties of quantum computation, while also exploring the application of machine learning techniques to problems within quantum mechanics [4,5,16].​  

The potential benefits of employing quantum computers for machine learning tasks are significant. Quantum computing offers the prospect of overcoming fundamental limitations inherent in classical computing for certain problems [2]. By utilizing quantum phenomena such as entanglement and superposition, QML algorithms can process quantum data directly and potentially gain complexity advantages, particularly when dealing with high-dimensional data spaces and complex patterns [1,2]. Specifically, QML may facilitate learning complex data patterns in high dimensions that are challenging for classical methods [1,2]. Potential advantages include handling larger datasets and accelerating model training [15], improving the efficiency and accuracy of traditional machine learning algorithms [18], and achieving quantum speed-ups over classical counterparts for tasks like supervised classification [20]. Furthermore, techniques like quantum feature mapping can embed data into high-dimensional Hilbert spaces, potentially reducing required training data and improving generalization, especially for small sample learning [1]. Quantum algorithms may also reduce the number of queries needed for data retrieval and optimization problems, enhancing overall efficiency [1].  

The landscape of QML relevant to quantum state learning encompasses several paradigms and approaches. Significant research areas include Quantum Support Vector Machines (QSVM) [13,15] and Quantum Neural Networks (QNN) [15], which aim to adapt classical machine learning models for quantum execution. Hybrid quantum-classical algorithms, particularly relevant for current Noisy Intermediate-Scale Quantum (NISQ) devices [16], are also a major focus. These approaches often involve Variational Quantum Algorithms (VQAs), which are promising candidates for utilizing limited quantum resources despite potential error sensitivity [22]. The generalization ability and expressivity of VQAs are subjects of ongoing investigation through the lens of statistical learning theory [22]. Machine learning techniques are also applied to address challenges within quantum systems, such as estimating physical parameters of quantum states using convolutional neural networks [4], simulating time-varying quantum channels using algorithms like Matrix Exponentiated Gradient Descent within online convex optimization frameworks [3], and studying adiabatic learning for quantum-classical dynamics [5]. Other relevant areas include quantum kernel methods [13] and provably efficient algorithms for tasks like online learning of quantum states [17]. These diverse approaches highlight the multifaceted nature of QML and its potential applications in analyzing, simulating, and interacting with quantum systems and data.  

# 3. Theoretical Analysis of Learning Complexity  

Analyzing the complexity of learning quantum states requires a rigorous theoretical foundation, drawing insights from quantum information theory, statistical learning theory, and quantum computational complexity theory [19,22]. This section synthesizes these perspectives to understand the fundamental limits, data requirements, computational resources, and inherent hardness associated with quantum state learning tasks. A prerequisite for this analysis is the understanding of how quantum states are represented and how their intrinsic complexity is quantified.  

Quantum states can be represented in various forms, including state vectors for pure states and density matrices for mixed states. While state vectors and density matrices contain complete information, their dimensionality grows exponentially with the number of qubits [8], rendering full state representation and learning computationally intractable for large systems [4,8]. More tractable representations, such as reduced density matrices, are often used when learning specific properties or subsystems [4]. Quantifying the intrinsic complexity of these states is crucial as it correlates with the resources needed for learning. Metrics include quantum statistical complexity [19], entanglement [10], and complexity derived from network models based on mutual information [11]. The optimal circuit depth required for state preparation can also serve as a measure of state complexity [7,8], directly linking state complexity to the computational effort involved in its creation and potentially its learning. The physical resources, including those linked to thermodynamics, are also relevant when dealing with complex states [19].​  

Quantum information theory provides fundamental principles that govern the limits of information extraction from quantum systems [22]. Concepts such as state distinguishability, measurement postulates, and the No-Cloning Theorem [13] establish inherent bounds on what can be learned about an unknown quantum state from a finite number of measurements or copies. This framework, utilizing tools like entropies (Shannon, von Neumann, Rényi) and the concept of majorisation [19], quantifies the information content and the fundamental precision limits achievable in learning tasks, thereby influencing sample complexity and error bounds [5].​  

Building upon these information-theoretic limits, quantum statistical learning theory focuses on the sample complexity and generalization capabilities of quantum learning models [7]. It addresses key questions regarding the number of copies of a quantum state or the amount of data required to learn its properties to a desired accuracy and how well the learned model generalizes to unseen instances [5]. The quantum analogue of statistical complexity also plays a role in understanding the resources required for processing quantum information within this framework [19]. Research in this area aims to establish bounds on the sample complexity and ensure the generalizability of quantum learning algorithms [5,7].​  

Quantum computational complexity theory complements these perspectives by classifying the inherent hardness of quantum learning tasks and identifying potential quantum speedups compared to classical computation [9,13]. It analyzes the computational resources, such as time and number of qubits, required by quantum algorithms [9]. This framework explores complexity classes, such as BQP (Bounded-Error Quantum Polynomial time) [13], to distinguish problems efficiently solvable on a quantum computer from potentially intractable ones. Leveraging computational hardness assumptions, such as the discrete logarithm problem, can provide theoretical evidence for quantum advantage in specific learning scenarios by demonstrating limitations of classical learners [20]. Research also focuses on proving lower bounds on the resources required for quantum computation [7,10,17,21]. These theoretical investigations into quantum algorithms and complexity theory are actively pursued in various research groups [12].​  

These three theoretical frameworks are deeply interconnected. Quantum information theory establishes the fundamental limits on extractable information, which directly constrains the sample complexity and achievable accuracy analyzed by quantum statistical learning theory. Quantum computational complexity theory then evaluates the feasibility and efficiency of quantum algorithms designed to perform learning tasks within these information and statistical bounds, assessing whether quantum resources offer a speedup in terms of computational effort.  

Evaluating the efficiency of quantum learning algorithms relies on specific complexity measures: computational complexity (time or operation count), sample complexity (number of state copies or measurements), and query complexity (number of oracle accesses) [1,2,9]. Quantum algorithms have demonstrated potential speedups across these measures [1,2,7]. For computational complexity, techniques like Quantum Singular Value Transformation (QSVT) can potentially reduce complexity from exponential to polynomial for certain problems [1,2], and the difficulty of classically simulating random quantum circuits highlights a computational separation [8]. In sample complexity, Quantum Feature Mapping can reduce data requirements and improve generalization [1,2], with specific algorithms like PEAL achieving logarithmic system size sampling complexity [5]. Query complexity speedups are exemplified by Grover's algorithm for search tasks [1,2,9,13]. However, proving fundamental lower bounds, such as the $\Omega ( d ^ { 2 } )$ bound for randomized polynomial transformations [17,21], remains a critical area within quantum complexity theory [12]. Challenges remain in rigorously demonstrating quantum advantages for practical learning tasks and developing algorithms robust to noise on current hardware. Future research directions include identifying specific problem classes where quantum speedups are provable and bridging theoretical complexity analysis with experimental realizations.  

# 3.1 Quantum State Representations and Complexity Metrics  

Understanding the complexity of learning quantum states necessitates a clear definition of how these states are represented and quantified. Quantum states can be represented in various forms, each offering different trade-offs in terms of information content, computational tractability, and relevance to specific learning tasks. A fundamental representation for a pure state of $n$ qubits is a state vector in a Hilbert space of dimension $2 ^ { n }$ . This representation, particularly the coefficients of the basis states [4], contains the full information about the state. However, the exponential growth of this dimensionality with the number of qubits, e.g., $2 ^ { 5 0 }$ for 50 qubits [8], highlights the computational intractability of explicitly representing or learning large-scale pure states. Mixed states are typically represented by density matrices, which are positive semi-definite operators with trace one. While also living in a high-dimensional space (specifically, the space of $2 ^ { n } \times 2 ^ { n }$ matrices for $n$ qubits), density matrices are essential for describing states that are not pure or that result from partial measurements or interactions with an environment.  

Alternatively, instead of learning the full state, one might focus on learning specific properties or partial information, which often involves representations like reduced density matrices [4]. Reduced density matrices describe the state of a subsystem and can be significantly lower dimensional than the full state representation, offering better tractability for learning specific properties. For instance, reduced density matrices were used as input features for a Convolutional Neural Network (CNN) after being visualized using the Qubism map [4], demonstrating their utility in learning scenarios where full state knowledge is not required. These property-based representations are particularly relevant for learning scenarios focused on observables or correlations within a system. Techniques like Singular Value Decomposition (SVD) can be applied to state representations (e.g., density matrices) to examine their information content, which relates to the state's complexity.  

Beyond representations, various metrics have been proposed to quantify the inherent complexity of quantum states, capturing different facets of "complexity" and correlating with the difficulty of learning or simulating the state [19]. One such concept is quantum statistical complexity, which draws parallels to its classical counterpart but reveals unique quantum phenomena, potentially showing processes as less complex or exhibiting qualitatively different complexity behaviors [19]. This perspective suggests that the quantum lens fundamentally alters our understanding of complexity [19].​  

Specific quantitative metrics exist. Entanglement is a crucial complexity measure; highly entangled states are generally considered more complex and are harder to simulate classically. While not detailed in the provided digests how entanglement is calculated from a representation, one digest mentions a hard limit on entanglement was discovered during algorithm design [10], hinting at its role in complexity bounds. Another approach quantifies complexity using network models based on mutual information, where correlations within the state structure are represented as a complex network. This method calculates complexity based on the structural properties of this network derived from the state.​  

The complexity of a quantum state, as measured by these metrics, strongly correlates with the resources required for learning or simulation. States with higher complexity, such as highly entangled states or those requiring deep circuits for preparation, generally demand more computational resources. For instance, the complexity of state preparation can be measured by the optimal circuit depth required [7], and studies involving random quantum circuits implicitly relate circuit depth (e.g., 20 layers) to the complexity of the generated states [8]. This relationship suggests that learning a complex state might require resources proportional to its preparation complexity or other intrinsic complexity measures like entanglement or statistical complexity [19]. Furthermore, the physical resources required, potentially linked to thermodynamics and minimizing dissipation [19], also play a role in dealing with complex states.​  

# 3.2 Theoretical Learning Frameworks  

![](images/360fccec2c874d6d948032b60072171923b1c0c5117c3227629444c05f78dc6f.jpg)  

Theoretical frameworks drawn from quantum statistical learning theory, quantum information theory, and quantum computational complexity theory provide essential tools for analyzing the learnability of quantum states. These frameworks offer distinct but complementary perspectives on the resources required, the fundamental limits, and the inherent difficulty of quantum learning tasks.  

Quantum statistical learning theory provides a framework for analyzing the sample complexity and generalization performance of learning algorithms for quantum data. Key questions in this domain revolve around how many copies of a quantum state or how many interactions with a quantum system are needed to learn its properties to a desired accuracy, and how well a model trained on finite data will perform on unseen data. Research contributing to understanding sample complexity and generalization bounds in quantum contexts is crucial [7]. For instance, some work explicitly aims to manage sample complexity and ensure generalizability, particularly in the context of learning quantum‐classical dynamics, often leveraging principles from quantum information theory [5]. The concept of statistical complexity within quantum information processing also plays a significant role in understanding the resources required for processing quantum information [19].​  

Quantum information theory sets fundamental bounds on the amount of information that can be extracted from quantum states. The principles governing measurement, state distinguishability, and the manipulation of quantum information inherently influence the limits of what can be learned about an unknown quantum state. By quantifying the information content of quantum states and processes, quantum information theory helps establish the minimum resources or measurements necessary for learning tasks. This includes understanding error bounds associated with learning algorithms, which are ultimately constrained by the amount of information accessible from the quantum system [5]. Core concepts such as entropies and the relationship between information and physical resources are fundamental to this analysis [19].  

Quantum computational complexity theory classifies the difficulty of quantum learning problems and identifies tasks where classical computers may face fundamental limitations [13]. This framework analyzes the computational resources, such as time and number of qubits, required to perform quantum computations, including those involved in learning algorithms [9]. By placing quantum learning problems into complexity classes, it helps distinguish problems that are efficiently solvable on a quantum computer from those that are likely intractable, even with quantum resources. A significant aspect is the identification of potential quantum speedups for specific learning tasks by leveraging computational hardness assumptions. For example, the widely believed hardness of the discrete logarithm problem can be used to demonstrate limitations of classical learners in certain classification tasks, thereby providing a theoretical basis for quantum acceleration [20]. Research in this area explores quantum algorithms and lower bounds for various computational problems, including those related to optimization and finding stationary points, contributing to a broader understanding of complexity classes and the potential for quantum advantage [7]. Analysis sometimes involves considering hypothetical devices to understand the bounds of computation [10].​  

These three theoretical frameworks provide distinct yet interconnected insights into the learnability of quantum states. Quantum statistical learning theory focuses on the data requirements and generalization performance from a finite number of observations. Quantum information theory provides the foundational understanding of the information content of  

quantum systems and the limits on extracting that information, which directly impacts the sample complexity and error bounds considered in statistical learning. Quantum computational complexity theory assesses the computational feasibility and inherent difficulty of implementing quantum learning algorithms, determining whether a task is efficiently solvable in principle and whether quantum computers offer an advantage over classical ones. Together, these frameworks form a comprehensive theoretical foundation for understanding the possibilities and limitations in learning quantum states. Research groups actively engaged in areas like theoretical computer science, including quantum information theory and quantum complexity theory, contribute significantly to advancing these foundational aspects, even if specific applications to quantum state learning are not always explicitly detailed in their high-level descriptions [12].​  

# 3.3 Specific Complexity Measures  

<html><body><table><tr><td>Complexity Measure</td><td>Definition</td><td>Examples of Potential Speedups/Bounds</td></tr><tr><td>Computational Complexity</td><td>Number of elementary operations or time</td><td>QSVT (Exponential to Polynomial), Random Circuit Sampling (Classical intractability), Classification speedups</td></tr><tr><td>Sample Complexity</td><td>Number of state copies or measurements required</td><td>Quantum Feature Mapping (Reduced data), PEAL (Logarithmic scaling with system size)</td></tr><tr><td>Query Complexity</td><td>Number of accesses to an oracle/system</td><td>Grover's algorithm(O(√N) Vs O(N)), Matrix-vector products</td></tr></table></body></html>  

Evaluating the efficiency of quantum algorithms for learning quantum states necessitates defining specific complexity measures. Key among these are computational complexity, sample complexity, and query complexity [1,2,9]. Computational complexity typically quantifies the number of elementary operations or the time required by an algorithm. Sample complexity pertains to the number of copies of the quantum state or measurements needed to achieve a certain learning accuracy. Query complexity measures the number of accesses or queries made to an oracle, which might represent the state itself or a function related to it.​  

Quantum computation offers potential speedups across these measures compared to classical methods [1,2,7]. In terms of computational complexity, certain quantum algorithms like Quantum Singular Value Transformation (QSVT) have been shown to reduce complexity from exponential to polynomial for specific problems, indicating a significant potential advantage [1,2]. The concept of quantum speedup in computational tasks is broadly illustrated by comparisons such as the time taken by Google's Sycamore processor for random circuit sampling versus the estimated time for a classical supercomputer [8]. Furthermore, research has demonstrated a computational complexity separation for a family of datasets where a quantum classifier achieves high accuracy under the assumption of discrete logarithm problem hardness, while classical learners perform poorly [20]. Analyses of computational complexity are also crucial in areas like quantum simulation [17], optimization [17], and the design of algorithms with near-optimal complexity using limited resources like single ancilla qubits [21]. Algorithms like MEGD show favorable time scaling with sublinear regret [3], and PEAL exhibits favorable time scaling properties [5].​  

Sample complexity is another domain where quantum approaches show promise. Techniques such as Quantum Feature Mapping can reduce the amount of training data required and improve the generalization ability of learning models [1,2]. Specific algorithms like PEAL have been shown to achieve logarithmic system size sampling complexity [5], indicating a potential advantage in scaling with system size. Quantum algorithms designed for tasks like multi-armed bandits also involve analysis of sample complexity [7].​  

For query complexity, a well-known example of quantum speedup is Grover's search algorithm, which reduces the queries needed to find an item in an unstructured database from $O ( N )$ classically to $O ( \sqrt { N } )$ quantum mechanically [1,2,9]. While learning quantum states is different from searching a database, the principle of achieving a polynomial speedup in the number of queries is relevant. Research explores quantum query complexity in various contexts, including matrix-vector products and matrix games [7].  

Despite these potential advantages, achieving and proving these complexity benefits for practical quantum state learning tasks presents significant challenges. Proving lower bounds on complexity for quantum algorithms is a rigorous area of research, providing fundamental limits on performance, such as the $\Omega ( d ^ { 2 } )$ lower bound shown for randomized methods implementing polynomial transformations [17,21]. The field of quantum complexity theory broadly investigates these questions [12]. Opportunities lie in identifying specific classes of quantum states or learning problems where quantum algorithms can provably outperform classical ones, developing robust algorithms resilient to noise on near-term quantum hardware, and establishing clearer connections between theoretical complexity measures and experimental feasibility. The development of efficient quantum algorithms requires careful consideration of these specific complexity measures to demonstrate a clear advantage over classical computing.​  

# 4. Methods and Algorithms for Learning Quantum States  

Effectively learning quantum states and their properties is a crucial task in quantum information science, enabling tasks ranging from verifying quantum device performance to utilizing states as resources for quantum algorithms and simulations. The landscape of methods addressing this challenge is diverse, spanning foundational techniques, specialized discrimination algorithms, quantum and hybrid machine learning approaches, and classical methods adapted for quantum data. This section categorizes and analyzes these diverse methods, highlighting their principles, capabilities, limitations, and trade-offs in terms of accuracy, computational cost, sample requirements, and hardware demands.  

Quantum State Tomography (QST) serves as a foundational method for fully characterizing an unknown quantum state by reconstructing its density matrix, $\rho$ . This typically involves performing measurements on multiple identical copies of the state and employing statistical inference. While providing a complete description, full QST faces significant scalability challenges. The number of parameters required to describe the density matrix of an $n$ -qubit system grows exponentially as $O ( 4 ^ { n } )$ , leading to an exponential scaling in the number of measurements and computational resources needed for reconstruction. To address this, alternative approaches such as compressed sensing QST and shadow tomography have emerged. Shadow tomography, in particular, focuses on efficiently estimating a collection of linear properties or predicting expectation values of specific observables rather than reconstructing the full state [7,17]. This is achieved by measuring in randomly chosen bases and constructing a classical shadow. The key advantage is a sample complexity that scales logarithmically with the number of properties $M$ $\vert O ( \log M )$ ) and polynomially with system size $n$ , a significant improvement over full tomography for property estimation [17]. Despite these advancements, the accuracy of quantum state estimation is fundamentally limited by noise in the measurement process and statistical uncertainties from a finite number of measurements.​  

Distinct from full state reconstruction is the task of Quantum State Discrimination, which involves distinguishing between a set of known possible quantum states. Due to the principles of quantum mechanics, distinguishing non-orthogonal states with certainty is generally impossible, leading to inherent probabilistic outcomes. Theoretical limits, such as the Helstrom bound for discriminating two states, define the maximum achievable success probability. Optimal measurement strategies, often expressed as positive operator-valued measures (POVMs), exist but can be computationally challenging to determine, especially for multiple states or high-dimensional systems. The error probability is influenced by state distinguishability, the chosen measurement strategy, and the number of state copies. The computational cost depends on the system dimension, the number of states to distinguish, and the complexity of the measurement or its simulation.​  

The burgeoning field of Quantum Machine Learning (QML) offers powerful paradigms for learning quantum states and properties [1,2,7,13,16,18]. Key approaches include Quantum Kernel Methods, Quantum Neural Networks (QNNs), and Variational Quantum Algorithms (VQAs) [2]. Quantum Kernel Methods map data into a quantum feature space and compute similarity using a quantum kernel function, estimable on a quantum computer [1,2,18,20]. The Quantum Support Vector Machine (QSVM) is a prominent example [1,13,18,20]. Quantum Neural Networks utilize quantum circuits structured similarly to classical neural networks for tasks like regression, generation, or parameter estimation, including using convolutional neural networks (CNNs) on quantum data representations for Hamiltonian parameter estimation [1,2,4]. Training QNNs involves challenges like barren plateaus, spurring research into optimization techniques such as OICD and MEGD [3,21]. Variational Quantum Algorithms (VQAs), including the Variational Quantum Eigensolver (VQE) and the  

Quantum Approximate Optimization Algorithm (QAOA), are central to near-term quantum computing, employing parameterized quantum circuits optimized classically to minimize a cost function [13]. They are particularly effective for finding ground states or optimizing state preparation/characterization parameters [13,16,17]. Other QML algorithms like Quantum Gradient Descent (QGD) and Quantum Principal Component Analysis (QPCA) also contribute to the field [18].​  

Hybrid Quantum-Classical methods represent a practical approach for leveraging current Noisy Intermediate-Scale Quantum (NISQ) hardware by distributing computational tasks between quantum processors and classical computers [22]. VQAs are a prime example of this architecture, utilizing classical optimizers to adjust quantum circuit parameters [7,17,22]. This framework is applied to learning tasks such as estimating ground state energies with improved accuracy and noise resistance [16], estimating quantum kernels [20], learning parameters in quantum systems [3], and estimating Hamiltonian parameters from quantum data via classical CNNs processing quantum-derived images [4]. Methods like OICD also exemplify hybrid optimization for parameterized quantum circuits [21]. Hybrid methods offer a pathway to use NISQ devices for complex problems, mitigating quantum hardware limitations, but face challenges in classical optimization complexity, quantum overhead, and noise sensitivity [17,21]. The PEAL algorithm is another example addressing quantum-classical dynamics learning [5,7].  

Finally, Classical Methods are employed for tasks related to quantum states, often by applying classical machine learning techniques to data obtained from quantum experiments. This includes approaches like QubismNet, which transforms quantum state information into classical image representations processed by CNNs to estimate physical parameters [4]. While benefiting from mature classical infrastructure, these methods are fundamentally limited by the exponential resources required for classical representation and processing of general, highly entangled quantum states. This contrasts with the potential advantages of quantum algorithms for handling such states and highlights the ongoing competition between classical simulation capabilities and quantum computational power [8]. Classical methods may struggle to fully capture and utilize complex quantum correlations compared to approaches operating directly in the quantum domain.  

In summary, the landscape of methods for learning quantum states presents a spectrum of capabilities and trade-offs. Full QST offers complete state information but is not scalable. Shadow tomography provides scalable estimation of state properties. Quantum state discrimination addresses the specific task of distinguishing known states with inherent probabilistic limits. QML approaches leverage quantum computation for tasks like classification, regression, and optimization, with various paradigms like Quantum Kernels, QNNs, and VQAs offering distinct strengths and facing challenges like trainability and hardware requirements. Hybrid quantum-classical methods are essential for near-term hardware, combining the strengths of both paradigms but contending with optimization and noise issues. Classical methods can process certain forms of quantum data using established machine learning techniques but are ultimately limited by the complexity of quantum states. The choice of method depends critically on the specific learning task, the desired accuracy, the available computational resources (quantum and classical), sample constraints, and the nature of the quantum state itself, particularly its size and entanglement properties.​  

# 4.1 Quantum State Tomography and Estimation  

<html><body><table><tr><td>Method Category</td><td>Goal</td><td>Key Examples/Variat ions</td><td>Scalability (General)</td><td></td></tr><tr><td>QST& Estimation</td><td>Full state or property characterization</td><td>Full QST, Compressed Sensing QST, Shadow Tomography</td><td>Full QST: Exponential; Shadow: Logarithmic/Pol ynomial property</td><td>High fidelity needed; Shadow for NISQ</td></tr><tr><td>Quantum State Discrimination</td><td>Distinguish between known states</td><td>Optimal POVMs</td><td>Moderate dimension</td><td>Varies; depends on measurement</td></tr><tr><td>Quantum ML Approaches</td><td>Learn patterns/proper</td><td>Quantum Kernels (QSVM),</td><td>Varies; often Exponential</td><td>Varies; QNN/Kernel</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>ties, Classification/R egression</td><td>QNNs, VQAS (VQE, QAOA)</td><td>data for classical data</td><td>fault-tolerant?, VQA/Hybrid NISQ</td></tr><tr><td>Hybrid Quantum- Classical</td><td>Combine quantum& classical</td><td>VQAs, Hybrid QNNs, PEAL, QubismNet</td><td>Better than full quantum on NISQ</td><td>NISQ devices</td></tr><tr><td>Classical Methods</td><td>Process QubismNet quantum data classically</td><td>(CNNs on quantum images)</td><td>Limited by classical simulation/repr</td><td>Classical hardware</td></tr></table></body></html>  

Quantum state tomography (QST) is a fundamental process in quantum information science aimed at completely characterizing an unknown quantum state. The goal is to reconstruct the density matrix $\rho$ of a quantum system, which fully describes its state. This typically involves performing measurements on multiple identical copies of the state and statistically inferring $\rho$ from the measurement outcomes. However, a major challenge of full QST is its inherent scalability issue. The number of parameters required to describe the density matrix of an $n$ -qubit system grows exponentially with $\scriptstyle n$ , specifically as $O ( 4 ^ { n } )$ . Consequently, the number of measurements and the computational resources needed for reconstruction also scale exponentially, making full QST infeasible for systems with more than a few qubits.  

Traditional QST techniques often rely on performing a complete set of linearly independent measurements. For an $\scriptstyle n$ -qubit system, this might involve measuring in different bases for each qubit, leading to $3 ^ { n }$ potential product bases in a standard approach, or using more efficient methods that still face exponential scaling in terms of necessary measurements and computational complexity for reconstructing the full density matrix. The sample complexity, defined as the number of copies of the state required, and the computational complexity for data processing become prohibitive for large systems.  

To circumvent the exponential scaling of full tomography, alternative approaches focus on estimating specific properties of the quantum state rather than reconstructing the entire density matrix. Techniques like compressed sensing QST and shadow tomography are designed to achieve this with significantly reduced resources. Compressed sensing methods leverage the sparsity or low-rank structure often present in density matrices of physically relevant states, requiring fewer measurements than full QST, although they still aim for state reconstruction under specific assumptions.  

Shadow tomography, in particular, offers a powerful paradigm for predicting the outcomes of many different measurements on a quantum state by performing relatively few measurements chosen randomly from a specific set [17]. Unlike full tomography, which aims to learn the entire state for arbitrary future predictions, shadow tomography is designed to estimate a collection of linear properties or predict expectation values of specific observables. The core idea involves measuring the state in a few random bases, processing the results to construct a "classical shadow" of the quantum state, which is a classical representation from which estimates of many properties can be computed. The key advantage is that the number of state copies (sample complexity) required to predict the expectation values of $M$ different observables to a certain accuracy scales logarithmically with the number of observables $M$ , i.e., ${ \cal O } ( \log M )$ , and polynomially with the system size ​n and the desired accuracy, rather than exponentially with $n$ . This makes shadow tomography significantly more scalable for predicting a limited number of properties compared to full state reconstruction. Research such as that by Jiyu Jiang, Zongqi Wan, Tongyang Li, and others has contributed to the development and application of shadow tomography techniques, specifically exploring aspects like predicting future outcomes from measurement data [17].​  

Despite these advancements, the accuracy of quantum state estimation, whether full tomography or shadow tomography, is fundamentally limited by the presence of noise in the measurement process and the finite number of measurements that can be performed in practice. Noise can stem from imperfections in quantum gates, state preparation, or the measurement apparatus itself, leading to deviations between the observed outcomes and the ideal theoretical probabilities. A finite number of measurements introduces statistical uncertainties. While increasing the number of measurements generally improves accuracy, it does so with diminishing returns and cannot overcome systematic biases introduced by noise. Advanced techniques for noise mitigation and error correction are often necessary to improve the reliability of state estimation results.​  

# 4.2 Quantum State Discrimination  

Quantum state discrimination is a fundamental task in quantum information theory, concerning the problem of distinguishing between different quantum states. Given a quantum system known to be in one of a set of possible states, the goal is to perform a measurement that identifies which state the system is in. This process is inherently probabilistic due to the nature of quantum mechanics, and it is often impossible to distinguish between non-orthogonal states with certainty. The theoretical limits on the distinguishability of quantum states are a cornerstone of the field. For discriminating between two known states, $\rho _ { 0 }$ ​ and $\rho _ { 1 }$ , the maximum probability of correctly identifying the state is given by the Helstrom bound. This bound is achieved by the optimal Helstrom measurement, which is a specific positive operator-valued measure (POVM) defined in terms of the difference between the two states, $\rho _ { 1 } - \rho _ { 0 }$ ​ . For discriminating among multiple states, the problem becomes more complex, and while general optimal strategies exist (e.g., using POVMs satisfying certain optimality conditions), finding the explicit form of these optimal measurements can be challenging. Various algorithms have been developed to approximate or find these optimal strategies, particularly for specific sets of states or under certain constraints. The error probability in quantum state discrimination is influenced by several factors, including the intrinsic distinguishability of the states themselves (quantified by measures like trace distance for two states), the chosen measurement strategy, and the number of copies of the state available for measurement. Distinguishing non-orthogonal states requires accepting a non-zero error probability, and the minimal error is bounded by the theoretical limits. The computational cost of discrimination algorithms depends significantly on the dimension of the Hilbert space the states inhabit, the number of possible states being discriminated, and the complexity of implementing the required measurement apparatus or simulating it classically. Computing the optimal measurement operators can be computationally intensive, particularly for high-dimensional systems or large sets of states.​  

# 4.3 Quantum Machine Learning Approaches  

The field of quantum machine learning (QML) explores the potential of quantum computing to enhance machine learning algorithms, including those focused on the intricate task of learning quantum states. Several prominent QML approaches have emerged, including Quantum Kernel Methods, Quantum Neural Networks (QNNs), Variational Quantum Algorithms (VQAs), and Quantum Transformers [1,2,15]. Other related methods include quantum algorithms for optimization like Quantum Gradient Descent (QGD) and dimensionality reduction such as Quantum Principal Component Analysis (QPCA) [18], as well as more specialized techniques like Quantum Wasserstein Generative Adversarial Networks and methods for learning quantum-classical dynamics such as the PEAL algorithm [5,7]. Research also investigates theoretical aspects like the expressivity and generalization of QML models, along with techniques to improve performance such as coreset selection [17,22].​  

Quantum Kernel Methods leverage the power of quantum computation to potentially achieve a quantum advantage in classification tasks [1,2,20]. The core idea involves mapping input data—including potentially quantum states themselves— into a high-dimensional quantum feature space via a quantum feature map [1,2]. The similarity between these mapped states is then computed using a quantum kernel function, which can be estimated on a fault-tolerant quantum computer or implemented through specific quantum operations [18,20]. A notable example is the Quantum Support Vector Machine (QSVM), which adapts the classical SVM by employing a quantum kernel to perform binary classification on datasets (such as the MNIST dataset) [1,13,18,20]. The strength of quantum kernels lies in the potential for the quantum feature space to be intractable for classical computers, although realizing a true quantum advantage depends on the choice of feature map and the specific problem.​  

Quantum Neural Networks (QNNs) are another significant paradigm, drawing inspiration from classical neural networks but implemented or enhanced using quantum circuits [2,15]. Architectures range from quantum perceptrons to deep quantum neural networks [2]. QNNs can be designed for various tasks, including regression and generative modeling [1,2]. For instance, convolutional neural networks (CNNs) have been employed in a QML context to estimate parameters of a Hamiltonian from observed quantum states by training the network on a dataset of states and corresponding parameters [4]. While fault-tolerant QNNs and perceptrons are theoretically explored, near-term QNNs are being developed in line with current hardware capabilities [2]. A promising approach for the NISQ (Noisy Intermediate-Scale Quantum) era is the quantum-neural network hybrid paradigm, which combines quantum processing with classical optimization and computation [16]. Training QNNs presents challenges, particularly regarding parameter optimization. Methods like Optimal Interpolation-based Coordinate Descent (OICD) and Matrix Exponentiated Gradient Descent (MEGD) are relevant for  

optimizing parameters in parameterized quantum circuits and quantum programs, respectively [3,21]. The potential strength of QNNs lies in their capacity for universal function approximation in the quantum domain, although trainability issues such as barren plateaus pose significant challenges.  

Variational Quantum Algorithms (VQAs), including the Variational Quantum Eigensolver (VQE) and the Quantum Approximate Optimization Algorithm (QAOA), are central to NISQ computing and have applications in learning quantum states. VQAs employ a parameterized quantum circuit whose parameters are optimized using a classical algorithm to minimize a cost function [13]. This framework is particularly relevant for approximating the ground states of quantum systems, representing a form of learning or preparation of specific quantum states. VQAs and QAOA can be used to optimize parameters related to state preparation or characterization tasks, such as determining the optimal circuit parameters to prepare a desired state or to perform state tomography more efficiently [13,16]. Studies also investigate the theoretical underpinnings of VQAs, analyzing their expressivity and generalization ability using tools from statistical learning theory [22]. The strength of VQAs lies in their adaptability to NISQ hardware through the use of shallow circuits, whereas their weaknesses often relate to the challenges of classical optimization in high-dimensional parameter spaces and sensitivity to noise.​  

In comparison, Quantum Kernel methods are primarily geared towards classification by leveraging potentially hard-tosimulate kernel functions. QNNs offer a more general framework for learning complex patterns, applicable to regression, generation, and estimation tasks. VQAs are highly effective for optimization problems involving quantum states, particularly ground state approximation and state preparation optimization, making them well-suited for NISQ devices. Quantum Transformers, an emerging area, aim to achieve quantum speedups in specific architectures relevant to tasks such as natural language processing [1,2]. While Quantum Kernels and certain QNN architectures might require fault-tolerant resources for their full potential, VQAs and hybrid QNN approaches are designed with near-term hardware in mind. Each approach presents unique capabilities and challenges, collectively contributing to the broader goal of learning and understanding quantum states through machine learning paradigms.  

# 4.4 Hybrid Quantum-Classical Methods  

Hybrid quantum-classical algorithms represent a significant paradigm for leveraging the capabilities of near-term quantum hardware, which is characterized by limitations such as fragility, limited resources, and susceptibility to errors [22]. These methods are designed to offload computationally intensive tasks to classical computers, while utilizing quantum processors for tasks where they offer potential advantages, thereby mitigating the defects of current quantum machines [22].  

Variational Quantum Algorithms (VQAs) are leading candidates within this hybrid framework [22]. The core architecture of VQAs typically involves a parameterized quantum circuit, whose output is measured to compute an objective function evaluated on a classical computer. A classical optimizer then updates the parameters of the quantum circuit in an iterative feedback loop, aiming to minimize or maximize the objective function. The Variational Quantum Eigensolver (VQE) is a prime example of a VQA [7,17].  

Hybrid quantum-classical methods are applied in various tasks related to learning quantum states and properties. A hybrid quantum-neural network paradigm has been developed, demonstrating efficient construction and inherent noise resistance. This approach has achieved exponential speedup in the non-unitary post-processing of quantum states and significantly improved the accuracy of ground state energy estimation, illustrating its utility in learning properties of quantum systems [16]. Another application is found in quantum machine learning classifiers, where a fault-tolerant quantum computer may be used to estimate kernel functions, while the classification algorithm itself is likely executed classically, forming a hybrid approach for learning patterns in quantum data [20]. The MEGD algorithm, operating within a hybrid framework, employs classical optimization techniques to adjust parameters in a quantum system (GTP), relevant for tasks involving parameter learning in quantum processes [3]. Furthermore, the QubismNet approach for Hamiltonian estimation from quantum states utilizes a hybrid structure where quantum states (ground states or reduced density matrices) are obtained, mapped classically using the Qubism map, and then analyzed by a classical Convolutional Neural Network (CNN) to estimate Hamiltonian parameters [4]. Methods like OICD also exemplify hybrid optimization, utilizing classical interpolation to approximate the objective function of parameterized quantum circuits and performing classical parameter updates [21].  

Beyond VQAs, other hybrid systems include quantum-classical co-processing involving tensor networks and algorithms addressing quantum-classical dynamics, such as the PEAL algorithm [5,13].  

The potential advantages of these hybrid methods lie in their ability to leverage the strengths of both computing paradigms, offering a pathway to utilize currently available NISQ hardware for computational problems, including those related to learning quantum states and properties. They aim to mitigate the limitations of purely quantum approaches on noisy hardware and can offer benefits like potential speedups, improved estimation accuracy, and noise resistance. However, these methods also face limitations. Challenges include the complexity of classical optimization landscapes for highdimensional quantum parameter spaces, the overhead associated with repeated quantum circuit executions and measurements, and sensitivity to noise in the quantum computation. The development of efficient classical optimization techniques and robust quantum circuit architectures, as seen in methods like OICD and SpacePulse [17,21], is crucial for overcoming these challenges. Efficient simulation frameworks provided by platforms like NVIDIA also play a vital role in accelerating the development and testing of hybrid QML algorithms on classical hardware [1].​  

# 4.5 Classical Methods for Quantum Systems  

The application of classical machine learning techniques to tasks involving quantum states represents a significant area of research, seeking to leverage the capabilities of established classical computing resources for problems originating in the quantum domain. Such tasks include the estimation of parameters of quantum systems from experimental data and the search for efficient classical descriptions of specific quantum states. One approach in this domain involves transforming quantum state information into a format amenable to classical algorithms. For instance, QubismNet utilizes classical Convolutional Neural Networks (CNNs) to learn and estimate physical parameters directly from quantum states, which are represented as images through a process generating a "Qubism map" [4]. This method demonstrates how classical neural networks can be adapted to process visual representations derived from quantum information, enabling tasks like parameter estimation [4].​  

Despite these advancements, classical methods face inherent challenges when dealing with complex quantum states, particularly those exhibiting high entanglement. The fundamental difficulty stems from the exponential scaling of resources required to represent and process general quantum states classically. This challenge is evident in the broader context of classical simulation of quantum circuits, where significant computational power, often from classical supercomputers, is necessary to replicate quantum experiments [8]. Claims regarding the capability to classically simulate complex quantum phenomena, such as simulating a specific quantum experiment within a feasible timeframe using extensive memory and storage [8], underscore the ongoing competition between classical and quantum computational capabilities. While classical simulation techniques are continually being optimized [8], the resource requirements for simulating or processing general, highly entangled quantum states remain a significant barrier.  

Within this landscape, specific classical approaches like QubismNet attempt to circumvent the full complexity of quantum state representation by focusing on extracting relevant information from tailored representations [4]. However, the extent to which such methods effectively capture and utilize subtle quantum correlations, especially in highly entangled states, is a critical question. The reliance on classical models trained on transformed data means their effectiveness in tasks sensitive to complex quantum correlations may be inherently limited compared to methods that operate directly on quantum information.​  

Comparing the performance and limitations of classical methods like QubismNet or classical generative models against quantum or hybrid quantum-classical methods for similar tasks highlights the trade-offs. While classical methods benefit from mature infrastructure and algorithmic development, they are ultimately constrained by the classical difficulty of handling quantum superpositions and entanglement. Quantum and hybrid methods, conversely, leverage quantum principles to potentially achieve computational advantages for specific quantum problems, though they face challenges related to hardware limitations, noise, and scalability. The competitive aspect observed in simulating quantum circuits [8] suggests that for tasks where quantum effects provide a significant speedup or capability advantage, classical methods, despite optimizations, may encounter fundamental limits in scalability and efficiency compared to their quantum counterparts.  

# 5. Applications of Learning Quantum States  

The ability to learn and characterize quantum states and their properties underpins a diverse range of applications across multiple domains, significantly impacting the development and utilization of quantum technologies. These techniques offer pathways to enhance quantum computing infrastructure, bolster quantum communication capabilities, and deepen our understanding of complex quantum systems.  

<html><body><table><tr><td>Application Domain</td><td>Examples/Specific Tasks</td></tr><tr><td>Quantum Computing</td><td>Hardware verification/calibration, Noise mitigation/modeling, Algorithm development, Quantum simulation (ground states, Hamiltonians)</td></tr><tr><td>Quantum Communication</td><td>Quantum Key Distribution (QKD), Channel characterization, Entanglement purification</td></tr><tr><td>Characterizing Quantum Systems</td><td>Materials science (entanglement, properties), Condensed matter simulation (Ising, XXZ, Holstein), Molecular electronic structure</td></tr><tr><td>Other Fields</td><td>QML (CV, NLP, Finance), Optimization (Logistics, Finance, MaxCut), Biosensing, Clinical Medicine, Fluid Dynamics, Economics (mechanism design)</td></tr></table></body></html>  

Within quantum computing, learning quantum states contributes fundamentally to both hardware and software advancements. For hardware, quantum state tomography—often employing state learning principles—is crucial for device verification and calibration, ensuring the reliability and performance of quantum processors [4,22]. Learned models of quantum channels, particularly time-varying ones, are vital for understanding and mitigating noise, which has direct implications for building fault-tolerant quantum computers and developing robust error correction schemes [3,22]. On the software side, state learning is integral to the development and simulation of algorithms for noisy intermediate-scale quantum (NISQ) devices [16]. Parameterized quantum circuits, often trained using techniques akin to state learning, form the basis for many variational quantum algorithms used in diverse applications [21]. Learning methods can also enhance quantum simulation by aiding in the estimation of ground state energies or inferring Hamiltonians that give rise to desired quantum states [4].​  

In quantum communication, learning techniques play a crucial role in enhancing both security and efficiency. Quantum Key Distribution (QKD), a prominent application of quantum communication, relies on the properties of quantum states for secure encryption [9,10,15]. Understanding and controlling the dynamics of quantum channels through learning and simulation is essential for developing reliable long-distance quantum communication protocols and techniques like entanglement purification [3,9].  

A significant area of application lies in characterizing and understanding complex quantum materials and physical system By learning the quantum states of these systems, researchers can probe their intricate properties, such as entanglement and correlations [10]. This is particularly valuable for quantum simulation, enabling the investigation of molecular electronic structures for chemical reaction mechanisms, the design of new materials with tailored properties, and drug discovery [13,14,15,17]. Specific examples include simulating high-Tc superconductors using methods like Variational Quantum Eigensolver (VQE), studying condensed matter models such as transverse field Ising and XXZ models, and analyzing dynamics in systems like the Holstein model [5,13,21]. Techniques that learn to predict quantum dynamics or infer Hamiltonians are also beneficial for Hamiltonian-based quantum simulations [4,5].​  

Beyond these core quantum technology areas, learned quantum states and properties find utility in numerous other fields. Quantum machine learning (QML), leveraging quantum states for data processing and model design, is being explored for accelerating classical machine learning algorithms, handling larger datasets, optimizing neural networks, and specific applications in computer vision, natural language processing, and financial modeling [2,7,11,13,14,15]. Quantum computing's capability to handle complex optimization problems, often involving implicit learning of problem structures, is applicable to diverse areas such as transportation scheduling, resource allocation, logistics, financial analysis, and solving problems like MaxCut and low-rank semidefinite programming, with techniques like Quantum Annealing being notable examples [7,13,14,15,21]. Other mentioned applications span biosensing, clinical medicine, fluid dynamics simulations (e.g., via QSVT), and even foundational studies exploring the role of information in thermodynamics or potential applications in economic theory like mechanism design and auctions [11,19,21,23].​  

Note: There were no formulas involving macros or parentheses issues detected in the text, so no modifications in that regard were necessary.  

# 6. Experimental Challenges and Implementations  

Transitioning quantum state learning algorithms from theoretical frameworks to practical implementations on physical quantum hardware presents significant challenges [6,14]. The primary hurdles stem from the inherent limitations of current quantum hardware, particularly in the Noisy Intermediate-Scale Quantum (NISQ) era [2,22]. These limitations severely impact the accuracy and scalability of quantum state learning protocols.  

A major challenge is the ubiquitous presence of noise and decoherence in quantum systems [13,15,22]. Noise, arising from environmental interactions, imperfect control, and gate errors, leads to the degradation of quantum states, manifesting as decoherence (loss of quantum information) and dephasing. The cumulative effect of these errors can drastically reduce computational fidelity. For instance, the Sycamore processor exhibited substantial fidelity loss after executing relatively shallow circuits [8,13]. Errors also affect measurement outcomes, which are critical for state estimation, and can introduce additive errors in intermediate quantities like kernel entries in quantum machine learning algorithms [20]. Furthermore, maintaining entanglement, a crucial resource for many learning tasks, is highly susceptible to noise [11]. Addressing these issues requires strategies that make learning algorithms more robust or incorporate error mitigation and correction techniques [22]. Approaches include developing algorithms with intrinsic noise resistance, such as hybrid quantum–neural networks [16] or robust quantum classifiers [20], and employing error mitigation techniques like zero-noise extrapolation [13]. While error mitigation offers near-term improvements for NISQ devices, the ultimate solution for fault-tolerant and high-fidelity quantum state learning lies in robust quantum error correction (QEC) [9]. However, implementing QEC demands a significantly larger number of physical qubits and much lower physical error rates than currently available [13].  

Beyond noise, scalability remains a fundamental bottleneck for many quantum state learning methods [3]. Standard techniques like quantum state tomography require resources that scale exponentially with the number of qubits, making them infeasible for large systems. Current quantum hardware is characterized by limitations in the number of high-quality qubits, short coherence times, and imperfect quantum gates [13,15,17]. Different hardware architectures, including ion traps, Josephson junctions, and optical lattices [9,11], face distinct engineering challenges related to scaling and maintaining performance. Overcoming these limitations necessitates both continuous hardware improvements—increasing qubit counts [8,10,17], improving coherence times [15], and enhancing gate fidelities [8]—and the development of scalable and fault-tolerant algorithms and software [8,13,16,20].  

Despite these challenges, significant experimental progress has been made, with numerous demonstrations highlighting current capabilities and paving the way for more complex learning tasks. Foundational experiments like quantum state teleportation, observations of multi-particle entanglement [9], and realizations of quantum phenomena like Maxwell's demon in photonic systems [19] demonstrate increasing control over quantum states. Early implementations of quantum algorithms on various platforms, including NMR computers [9] and quantum annealing systems [15], explored the preparation and manipulation of specific state types. More recently, experiments leveraging quantum machine learning paradigms on NISQ devices have explored tasks such as energy estimation and learning in dynamic, noisy environments [3,16]. Case studies include simulations of time-varying channels using algorithms like GTP and MEGD, demonstrating the potential for tracking optimal states in changing conditions [3], and numerical tests of optimization algorithms on complex quantum models [21]. Experimental demonstrations also extend to challenging domains like quantum computational chemistry on superconducting qubits [17] and fluid dynamics simulation [21], illustrating the practical application of state preparation and manipulation techniques. These experiments prioritize demonstrating empirical advantages on existing hardware for specific problems [1,2].​  

The concept of quantum supremacy serves as a prominent experimental milestone that starkly illustrates both current quantum capabilities and the limits of classical simulation [8,13]. The Sycamore experiment, for instance, demonstrated the ability of a 53-qubit processor to perform a random circuit sampling task in minutes that would take classical supercomputers millennia [8,13]. This result highlights the difficulty of classically simulating or characterizing the probability distributions associated with complex quantum states generated by even moderately sized noisy quantum circuits. Related experiments, such as multi-photon Boson sampling [8], further probe this boundary. While not directly state tomography, these demonstrations underscore the increasing complexity of states achievable on quantum hardware and the challenges inherent in their classical description or simulation, which is intimately related to the problem of  

quantum state learning. Evaluating these results against theoretical hardness conjectures reveals the significant progress in building processors capable of classically hard tasks, yet also underscores that achieving fault tolerance and a broad quantum advantage remains a goal for future development. The current state of experimental progress implies that while learning simple states or specific properties of more complex states may be feasible on NISQ devices with appropriate algorithms and error mitigation, full characterization or learning of arbitrary, highly complex, large-scale quantum states remains a significant challenge awaiting fault-tolerant quantum computation.  

# 6.1 Learning with Noisy Devices  

![](images/e23f8c8e48ce680f4d84cdceea1eef34839958d0fe67a647cfd2fc6b74d6aaf5.jpg)  

The reliable learning of quantum states constitutes a foundational task in quantum information science; however, its implementation on contemporary quantum hardware faces significant limitations due to the unavoidable presence of noise and imperfections [17].  

Near-term quantum machines, particularly those operating in the Noisy Intermediate-Scale Quantum (NISQ) era, are inherently fragile and error-prone [22]. These limitations fundamentally constrain the achievable performance of quantum state learning protocols.  

Noise manifests in various forms, including decoherence, dephasing, and gate errors, which lead to deviations of quantum states from their ideal evolution. The cumulative effect of these errors can drastically reduce the fidelity of quantum computations. For instance, experimental results on the Sycamore processor demonstrated a fidelity of merely $0 . 2 \%$ after executing 20 layers of quantum circuits, starkly illustrating the detrimental impact of noise and decoherence even with relatively high-precision control [8]. These noise processes corrupt the quantum state being learned and directly affect the outcomes of measurement procedures essential for state estimation. The presence of noise requires careful consideration, especially in complex scenarios such as learning states in time-varying channels, where the dynamic nature of the noise adds further complexity to its management and mitigation [3]. Furthermore, maintaining entanglement—a critical resource —becomes challenging in the presence of environmental interactions, contributing to noise-induced errors [11]. Noise can introduce additive errors in intermediate computational steps, such as kernel entries in quantum machine learning algorithms, which can impact the final learning accuracy [20].  

To address these challenges, various techniques are being explored to improve robustness and mitigate errors in the quantum state learning process. Some approaches focus on developing learning algorithms that exhibit inherent resilience to noise. For example, the hybrid quantum–neural network paradigm is discussed as possessing intrinsic noise resistance, offering an advantage on noisy devices [16]. Similarly, certain quantum classifiers have shown robustness against specific noise types, such as additive errors in kernel entries [20]. Beyond algorithmic robustness, direct error mitigation techniques are crucial for NISQ devices. These techniques aim to lessen the impact of noise on measurement outcomes and are explicitly needed in the NISQ era [13]. An example of such a technique is zero-noise extrapolation [13].  

While error mitigation provides a near-term solution, the long-term goal for robust quantum computation relies on quantum error correction (QEC) [9]. QEC codes encode logical quantum information redundantly across multiple physical qubits, enabling the detection and correction of errors provided that physical error rates are below a certain threshold. When available, fault-tolerant QEC would fundamentally change the landscape of quantum state learning by enabling arbitrarily long and complex quantum computations with high fidelity, effectively shielding the quantum state from environmental noise and imperfect operations [9]. Techniques like entanglement purification, discussed in the context of quantum information theory, are also relevant as they address the degradation of entanglement—a key resource susceptible to noise [9].  

However, implementing QEC requires a large number of physical qubits and extremely low physical error rates—capabilities that are not yet widely available on current quantum hardware. Therefore, current efforts in learning quantum states on  

noisy devices primarily focus on developing noise-resilient algorithms and employing error mitigation strategies, while acknowledging QEC as the ultimate solution for fault-tolerant learning in the future.  

# 6.2 Scalability and Hardware Limitations  

A significant bottleneck affecting many quantum state learning methods, particularly standard quantum state tomography, is scalability as the number of qubits increases. The experimental resources required for comprehensive state reconstruction grow exponentially with system size, rendering tomography impractical for all but the smallest systems [3]. This fundamental challenge is exacerbated by the inherent limitations of current quantum hardware.  

Near-term quantum machines are characterized by limited quantum resources [22]. Key hardware constraints include the restricted number of available qubits, short coherence times—which limit the duration of computations—and imperfect quantum gates prone to errors. These limitations collectively restrict the complexity of quantum states that can be reliably prepared, manipulated, and subsequently learned. For instance, maintaining coherence becomes increasingly challenging in systems exceeding 100 qubits [13]. Gate imperfections pose a substantial hurdle; the need for fault tolerance is highlighted by estimates suggesting that approximately ${ 1 0 } ^ { 4 }$ physical qubits may be required per logical qubit for robust error correction schemes like the surface code [13]. Furthermore, error rates in entangling gates can scale adversely with system size, as observed in platforms like large ion crystals [17]. Different hardware architectures, such as ion traps, Josephson junctions, and optical lattices, face distinct engineering challenges that impact their scalability and performance [9].​  

Significant research efforts are directed towards overcoming these limitations through both hardware improvements and algorithmic advancements. On the hardware front, there is continuous progress in increasing the number of high-quality qubits and in improving coherence times and gate fidelities across various platforms. For example, efforts are underway to build superconducting quantum computers with increasing qubit counts, including the development of 50-qubit technology [8,17]. Concurrently, algorithmic and software developments aim to mitigate hardware imperfections and improve scalability. The concept of fault tolerance is central to enabling complex quantum computations on larger systems, and algorithms—such as those for kernel estimation—are being designed with this future capability in mind [20]. The necessity of effective quantum error correction for supporting large-scale quantum computation is widely acknowledged [8,13]. Moreover, software frameworks capable of simulating larger systems, such as TensorCircuit—which can handle hundreds of qubits under specific circuit structures—provide valuable tools for exploring and developing algorithms suitable for overcoming existing hardware limitations and addressing scalability in the noisy intermediate-scale quantum (NISQ) era [16].​  

# 6.3 Case Studies and Experimental Demonstrations  

Experimental demonstrations and case studies on real quantum hardware are crucial for validating theoretical progress and identifying practical challenges in learning and manipulating quantum states. Early demonstrations of fundamental quantum information concepts, such as teleportation of quantum states and the observation of Greenberger–Horne– Zeilinger entanglement, along with implementations of quantum algorithms on platforms like NMR quantum computers, laid the groundwork for more complex state learning tasks [9]. The experimental realization of concepts like Maxwell’s demon in photonic setups further illustrates the ability to control and extract information from quantum systems [19]. Quantum annealing systems, such as those developed by D-Wave, have demonstrated success in solving specific combinatorial optimization problems, highlighting their potential for preparing and exploring complex states relevant to optimization landscapes [15].​  

More recent work focuses on leveraging quantum machine learning (QML) paradigms to address challenges inherent in noisy intermediate-scale quantum (NISQ) devices. Case studies involving hybrid quantum–neural network paradigms demonstrate improved performance, such as enhanced ground state energy estimation accuracy and increased resistance to noise, through careful co-design of quantum and classical components [16]. Specific algorithmic advancements, like the Optimization by Iterative and Correlative Descent (OICD) method, have been numerically tested on various complex quantum models including MaxCut, transverse field Ising models, and XXZ models, using simulation platforms like Qiskit Aer [21]. Another example is the application of the PEAL algorithm to the Holstein model, benchmarking its effectiveness for predicting dynamics and transfer learning in quantum systems. Furthermore, researchers have explored scalable quantum computational chemistry using superconducting qubits, presenting experimental results that detail specific system setups, algorithms, and performance metrics, offering insights into the practicalities of applying quantum computation to complex molecular state problems [17]. End-to-end digital simulation of unsteady fluid dynamics on superconducting hardware represents another step towards tackling complex scientific computation problems requiring intricate state preparation and manipulation [21].  

Characterizing and learning properties of quantum systems operating in dynamic and noisy environments presents a significant challenge. Online learning techniques offer a promising approach. A case study simulating time-varying dephasing channels using algorithms like Gradient Tracking Proximal (GTP) and Mirror Descent with Gradient Tracking (MEGD) demonstrates the potential of such methods [3]. The results, exhibiting sublinear regret, indicate the algorithms effectiveness in tracking the optimal program state as the channel parameters change over time, illustrating progress in learning about evolving quantum states and channels [3]. While these experimental and simulation results demonstrate significant progress, gaps remain in terms of scalability, robustness against correlated noise, and the ability to characterize and learn arbitrary complex quantum states efficiently on current hardware.​  

The debate surrounding quantum supremacy serves as a prominent case study illustrating the boundaries of both quantum hardware capabilities and classical simulation techniques when tasked with sampling from highly complex quantum state distributions. The Google Sycamore experiment, presented as a demonstration of quantum supremacy, involved a 53-qubit processor solving a specific random circuit sampling problem [8,13]. This task was completed in approximately 200 seconds on the quantum device, a time estimated to require around 10,000 years for even the most powerful classical supercomputers at the time [13]. This result highlighted the potential for quantum devices to perform tasks intractable for classical computers, effectively sampling from probability distributions derived from complex quantum states that are difficult to simulate. The experiment spurred further efforts to push classical simulation limits, with research groups demonstrating simulations of chain-type circuits with up to 1000 qubits and 42 layers, and 2D circuits with 72 qubits and 32 layers, on supercomputers like the Sunway TaihuLight [8]. Other related experimental milestones include multi-photon Boson sampling experiments, such as the 20-photon demonstration by a Chinese team, which also explore the boundary between quantum and classical computational power for specific sampling problems involving multi-particle quantum states [8]. These experiments, while focused on sampling rather than full state tomography, underscore the difficulty of classically characterizing or even simulating the output distributions of complex quantum circuits, which is intrinsically linked to the complexity of the underlying quantum states. Evaluating these experimental results against theoretical hardness conjectures reveals that while significant progress has been made in building quantum processors capable of executing classically challenging tasks, achieving fault tolerance and demonstrating a clear, unambiguous, and sustained quantum advantage for problems beyond specific sampling tasks remains an active area of research.  

# 7. Open Problems and Future Research Directions  

![](images/9c95ce744ba5bd9408b54f720933fde4d8196f22213951d7b6217138eb281d49.jpg)  

The complexity of learning quantum states remains a vibrant research area, characterized by significant open problems and burgeoning future directions. A central challenge lies in the transition from noisy intermediate-scale quantum (NISQ) devices to fault-tolerant quantum computers [6,8,13,18]. Building practical, fault-tolerant machines necessitates overcoming hurdles in qubit control and readout accuracy, alongside the development of effective quantum error correction techniques [6,8]. Enhancing qubit stability and reliability is paramount for enabling large-scale quantum computing [6]. The exploration of diverse quantum computing architectures, including superconducting, trapped ion, and photonic systems, is crucial for identifying optimal paths forward, with application-specific quantum simulation systems serving as an intermediate step towards universal computation [8].​  

Theoretical understanding and algorithmic development present another set of significant challenges. Open questions persist regarding the trainability and vanishing gradient issues in quantum neural networks (QNNs), as well as difficulties in parameter optimization [1,2]. Theoretical gaps remain in various quantum machine learning (QML) algorithms, impacting their efficiency and reliability [18]. A fundamental challenge is identifying problems where quantum computers can demonstrably outperform classical machines [10]. Furthermore, foundational issues such as generalizing thermodynamic laws to the finite, quantum, non-equilibrium domain and understanding the relationship between information and work in this context represent significant theoretical frontiers [19].  

Future research is directed towards developing more robust, scalable, and efficient quantum algorithms. This includes optimizing QML algorithms to leverage the capabilities of the latest quantum hardware and improving their training stability and efficiency [1,2]. Research into data-driven methods for inferring Hamiltonians from designed ground states, exploring more complex quantum systems and different machine learning architectures, also presents a promising avenue, though work is needed to relax training set constraints for broader applicability [4]. Developing quantum algorithms that offer provable speedups, even with classical data access, is a key direction, alongside exploring the limitations and practical implementations on near-term devices [20]. Specific algorithms, such as the PEAL algorithm for learning quantum-classical dynamics or the MEGD algorithm for simulating time-varying channels, require further refinement and extension to broader ranges of systems, dynamics, and higher dimensions [3,5].​  

The exploration of novel applications is critical for demonstrating the practical utility and quantum advantages of learned quantum states and related QML techniques. This spans diverse fields such as computer vision, natural language processing, financial modeling, high-precision sensing, and control [1,2,6,18]. The integration of QML with traditional machine learning and deep learning techniques is expected to lead to enhanced performance and wider applications [18]. Furthermore, novel properties of quantum mechanics applied to economic systems, such as quantum games, open questions about the effect of entanglement on outcomes, the design of manipulation-resistant quantum mechanisms, and the ethical implications like fairness and privacy [23].​  

Potential synergies between different approaches are a vital component of future progress. This involves integrating theoretical insights with experimental design, combining QML methods with NISQ hardware to maximize efficiency on current devices, and exploring the performance of algorithms across different quantum computing architectures [1,2,3,8,18]. The inherently interdisciplinary nature of quantum computing, requiring collaboration across physics, computer science, and mathematics, underscores the importance of fostering such synergistic efforts [14,15].  

Beyond the technical aspects, broader implications shape the research landscape. Increased investment from both companies and venture capital is fueling research, development, and commercialization efforts, contributing to technological progress [14,15]. However, ethical considerations also arise, particularly concerning the risks of quantum decryption and the consequent necessity for standardization in post-quantum cryptography [13]. The ethical implications of quantum technologies extend to new economic systems, requiring careful consideration of fairness, privacy, and security [23]. The field is characterized by numerous open questions, actively stimulating new lines of research [11].  

# References  

[1] AI从业者首个QML教程发布：助力迈向量子计算时代 https://mp.weixin.qq.com/s?   
_biz=MzIzMjQyNzQ5MA $\scriptstyle { \underline { { = } } } =$ &mid=2247708446&idx=1&sn=7814a54ade0d740ba4c3ec45d26d320f&chksm=e898f593dfef7c85d   
2cf412306fba4adfaeee7e4ef7b0ce0ea0a883c2aaa6af87f2f2f437067&scene=27  

[2] 首个AI从业者量子机器学习教程发布 https://hub.baai.ac.cn/view/43339  

[3] Quantum Machine Learning for Time-Varying Channels https://blogs.kcl.ac.uk/kclip/category/quantum-machinelearning/  

[4] Deep Learning for Hamiltonian Estimation from Quan https://cpl.iphy.ac.cn/article/10.1088/0256-307X/38/11/110301  

[5] 量子-经典动力学绝热学习的理论与算法 https://www.math.pku.edu.cn/kxyj/xsbg/tlb/computationaandappliedmath/162733.htm  

[6] 量子信息处理导论 https://blog.csdn.net/universsky2015/article/details/137311702  

[7] 李彤阳 - 前沿计算研究中心助理教授 - 量子算法与理论 https://cs.pku.edu.cn/info/1071/1687.htm  

[8] 谷歌“量子优越性”：里程碑还是新起点？ http://finance.sina.com.cn/stock/relnews/us/2019-10-28/dociicezuev5367291.shtml  

[9] Quantum Computation and Quantum Information Theory https://www.worldscientific.com/worldscibooks/10.1142/426 [10] Quantum Computing: Progress and Challenges https://www.quantamagazine.org/tag/quantum-computing/  

[11] 浙江大学第五届研究生国际暑期学校：量子模拟与量子网络复杂性理论http://www.math.zju.edu.cn/2019/1226/c38073a1876448/page.htm  

[12] 南京大学理论计算机科学研究组 https://tcs.nju.edu.cn/  

[13] Quantum Computing: Principles, Progress, and Prosp https://blog.csdn.net/SmartTony/article/details/146719633  

[14] 量子计算：突破边界的奇妙旅程 https://news.sohu.com/a/813812635_120991886  

[15] 量子计算：复杂问题求解的新范式 http://mt.sohu.com/a/814708665_267471  

[16] TensorCircuit：腾讯量子软件框架与NISQ算法应用 https://mp.weixin.qq.com/s?   
_biz=MzU0MjU5NjQ3NA $\scriptstyle 1 = =$ &mid=2247496329&idx $\mathop { : = }$ 1&sn=d40524a01c865dc02befebc72a8c9b12&chksm=fb1afe9ccc6d778a   
de043b299a4a2689c186ca87c8a1f1fb1c0ae1b388edb9d31c20c0587ebf&scene=27  

[17] Publications and Research https://cfcs.pku.edu.cn/publications/index15.htm  

18] 量子机器学习：新算法与应用前沿 https://blog.csdn.net/universsky2015/article/details/137310095  

[19] Quantum Information, Foundations & Thermodynamics: https://cqi.tsinghua.edu.cn/en/show-5662-1.html  

[20] 量子机器学习讲座：一种严格且鲁棒的监督学习量子加速算法 https://mp.weixin.qq.com/s? biz $: =$ MzU0MjU5NjQ3NA $\scriptstyle = =$ &mid=2247491828&idx=1&sn=450683ccd25aba6bfc948aba7e306adb&chksm $\scriptstyle 1 = { \frac { } { } }$ fb1aece1cc6d65f7 a3f008a4d3ca97931f28a573b955d9938491d9720986ba0e73be6f43fa6f&scene=27  

[21] Quantum Scientific Computation and Quantum Artific https://ymsc.tsinghua.edu.cn/info/1053/4023.htm  

[22] 近期物理讲座：吴健雄诞辰纪念、光学探针、超冷费米气体、超构材料等 https://mp.weixin.qq.com/s? _biz $: =$ MjM5NjYwNzM1Ng==&mid=2651699721&idx=1&sn=4801a8e3ac3a57caa77437dff1ec18ff&chksm=bd1fbe1a8a68370c4 5e94b4ca0e8437d65814e4b6f474c3085786595c1235c408c3b7e93d80a&scene=27  

[23] Quantum Games and Economic Behavior: Quantum Money https://link.springer.com/article/10.1007/s11128-021-0337 5​  

[24] 计算机科学2区易中SCI期刊推荐 http://www.sciqk.com/lwgl/scilw/15091.html  