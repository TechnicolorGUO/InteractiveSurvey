[
    "[1] Attention that does not Explain Away",
    "[2] Augmenting Self-attention with Persistent Memory",
    "[3] Why  classic  Transformers are shallow and how to make them go deep",
    "[4] One Wide Feedforward is All You Need",
    "[5] Investigating the Role of Feed-Forward Networks in Transformers Using  Parallel Attention and Feed-Forward Net Design",
    "[6] Mansformer  Efficient Transformer of Mixed Attention for Image  Deblurring and Beyond",
    "[7] DAE-Former  Dual Attention-guided Efficient Transformer for Medical  Image Segmentation",
    "[8] Hybrid Focal and Full-Range Attention Based Graph Transformers",
    "[9] A Multiscale Visualization of Attention in the Transformer Model",
    "[10] Combiner  Full Attention Transformer with Sparse Computation Cost",
    "[11] The I O Complexity of Attention, or How Optimal is Flash Attention",
    "[12] Softermax  Hardware Software Co-Design of an Efficient Softmax for  Transformers",
    "[13] Predicting Attention Sparsity in Transformers",
    "[14] Superiority of Softmax  Unveiling the Performance Edge Over Linear  Attention",
    "[15] KDEformer  Accelerating Transformers via Kernel Density Estimation",
    "[16] Stable, Fast and Accurate  Kernelized Attention with Relative Positional  Encoding",
    "[17] TaylorShift  Shifting the Complexity of Self-Attention from Squared to  Linear (and Back) using Taylor-Softmax",
    "[18] Softmax Acceleration with Adaptive Numeric Format for both Training and  Inference",
    "[19] SALO  An Efficient Spatial Accelerator Enabling Hybrid Sparse Attention  Mechanisms for Long Sequences",
    "[20] MELTing point  Mobile Evaluation of Language Transformers",
    "[21] Efficiently Scaling Transformer Inference",
    "[22] STI  Turbocharge NLP Inference at the Edge via Elastic Pipelining",
    "[23] Optimizing the Deployment of Tiny Transformers on Low-Power MCUs",
    "[24] A Heterogeneous Chiplet Architecture for Accelerating End-to-End  Transformer Models",
    "[25] LiteTransformerSearch  Training-free Neural Architecture Search for  Efficient Language Models",
    "[26] Shared Mobile-Cloud Inference for Collaborative Intelligence",
    "[27] Edge AI  On-Demand Accelerating Deep Neural Network Inference via Edge  Computing",
    "[28] Delay-aware and Energy-Efficient Computation Offloading in Mobile Edge  Computing Using Deep Reinforcement Learning",
    "[29] Towards Greener LLMs  Bringing Energy-Efficiency to the Forefront of LLM  Inference",
    "[30] A Cost-Efficient FPGA Implementation of Tiny Transformer Model using  Neural ODE",
    "[31] Generating Long Sequences with Sparse Transformers",
    "[32] Flowformer  Linearizing Transformers with Conservation Flows",
    "[33] H-Transformer-1D  Fast One-Dimensional Hierarchical Attention for  Sequences",
    "[34] Ring Attention with Blockwise Transformers for Near-Infinite Context",
    "[35] A Unified View of Long-Sequence Models towards Modeling Million-Scale  Dependencies",
    "[36] DeViT  Decomposing Vision Transformers for Collaborative Inference in  Edge Devices",
    "[37] Quantized Variational Inference",
    "[38] LLM As DBA",
    "[39] Post-Training Sparsity-Aware Quantization",
    "[40] Value-aware Quantization for Training and Inference of Neural Networks",
    "[41] Partial Quantifier Elimination",
    "[42] Magic for the Age of Quantized DNNs",
    "[43] Mitigating the Impact of Outlier Channels for Language Model  Quantization with Activation Regularization",
    "[44] Softmax Bias Correction for Quantized Generative Models",
    "[45] Adaptive Precision Training for Resource Constrained Devices",
    "[46] Post-training 4-bit quantization of convolution networks for  rapid-deployment",
    "[47] Learning to Quantize Deep Networks by Optimizing Quantization Intervals  with Task Loss",
    "[48] Fixed-point Quantization of Convolutional Neural Networks for Quantized  Inference on Embedded Platforms",
    "[49] Alternate Model Growth and Pruning for Efficient Training of  Recommendation Systems",
    "[50] Enhanced Sparsification via Stimulative Training",
    "[51] Exploiting Channel Similarity for Accelerating Deep Convolutional Neural  Networks",
    "[52] Rethinking the Value of Network Pruning",
    "[53] Transfer Learning for Structured Pruning under Limited Task Data",
    "[54] AutoDistil  Few-shot Task-agnostic Neural Architecture Search for  Distilling Large Language Models",
    "[55] Born Again Neural Networks",
    "[56] Residual Knowledge Distillation",
    "[57] Structural Knowledge Distillation  Tractably Distilling Information for  Structured Predictor",
    "[58] Multi Resolution Analysis (MRA) for Approximate Self-Attention",
    "[59] Adaptive Multi-Resolution Attention with Linear Complexity",
    "[60] Ripple Attention for Visual Perception with Sub-quadratic Complexity",
    "[61] Fast Transformers with Clustered Attention",
    "[62] TensorFlow Lite Micro  Embedded Machine Learning on TinyML Systems",
    "[63] On-Device Neural Net Inference with Mobile GPUs",
    "[64] Accelerating Framework of Transformer by Hardware Design and Model  Compression Co-Optimization",
    "[65] Multi-user Co-inference with Batch Processing Capable Edge Server",
    "[66] Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer  IoT Devices",
    "[67] The Case for Hierarchical Deep Learning Inference at the Network Edge",
    "[68] LLM-QAT  Data-Free Quantization Aware Training for Large Language Models",
    "[69] Adaptive Precision Training (AdaPT)  A dynamic fixed point quantized  training approach for DNNs",
    "[70] QLLM  Accurate and Efficient Low-Bitwidth Quantization for Large  Language Models",
    "[71] Mixed Precision Post Training Quantization of Neural Networks with  Sensitivity Guided Search",
    "[72] Performance optimizations on deep noise suppression models",
    "[73] Block Pruning For Faster Transformers",
    "[74] Stochastic Subnetwork Annealing  A Regularization Technique for Fine  Tuning Pruned Subnetworks",
    "[75] Structured Pattern Pruning Using Regularization",
    "[76] Extreme compression of sentence-transformer ranker models  faster  inference, longer battery life, and less storage on edge devices",
    "[77] Understanding and Improving Knowledge Distillation for  Quantization-Aware Training of Large Transformer Encoders",
    "[78] Transformer Acceleration with Dynamic Sparse Attention",
    "[79] AttMEMO   Accelerating Transformers with Memoization on Big Memory  Systems",
    "[80] QuadTree Attention for Vision Transformers",
    "[81] Fast Monte-Carlo Approximation of the Attention Mechanism",
    "[82] ALISA  Accelerating Large Language Model Inference via Sparsity-Aware KV  Caching",
    "[83] Fixed Encoder Self-Attention Patterns in Transformer-Based Machine  Translation",
    "[84] SparseBERT  Rethinking the Importance Analysis in Self-attention",
    "[85] PartialFormer  Modeling Part Instead of Whole",
    "[86] Improved Transformer for High-Resolution GANs",
    "[87] Cross-Architecture Transfer Learning for Linear-Cost Inference  Transformers",
    "[88] Random Feature Attention",
    "[89] Easy and Efficient Transformer   Scalable Inference Solution For large  NLP model",
    "[90] Confidant  Customizing Transformer-based LLMs via Collaborative Edge  Training",
    "[91] Improving Post Training Neural Quantization  Layer-wise Calibration and  Integer Programming",
    "[92] LQF  Linear Quadratic Fine-Tuning",
    "[93] Distance-aware Quantization",
    "[94] Post-Training Quantization with Low-precision Minifloats and Integers on  FPGAs",
    "[95] A2Q+  Improving Accumulator-Aware Weight Quantization",
    "[96] Neural Language Model Pruning for Automatic Speech Recognition",
    "[97] Pruning's Effect on Generalization Through the Lens of Training and  Regularization",
    "[98] Dissecting Pruned Neural Networks",
    "[99] HomoDistil  Homotopic Task-Agnostic Distillation of Pre-trained  Transformers",
    "[100] Knowledge Distillation Beyond Model Compression",
    "[101] Improved knowledge distillation by utilizing backward pass knowledge in  neural networks",
    "[102] Armour  Generalizable Compact Self-Attention for Vision Transformers",
    "[103] Mask Attention Networks  Rethinking and Strengthen Transformer",
    "[104] Improving Transformers with Probabilistic Attention Keys",
    "[105] Open RAN  Evolution of Architecture, Deployment Aspects, and Future  Directions",
    "[106] Towards Comparing Performance of Algorithms in Hardware and Software",
    "[107] Investigating the Impact of Quantization on Adversarial Robustness",
    "[108] LLM-Enhanced Data Management",
    "[109] Can pruning make Large Language Models more efficient",
    "[110] Quantifying lottery tickets under label noise  accuracy, calibration,  and complexity",
    "[111] Magnitude Attention-based Dynamic Pruning",
    "[112] PERP  Rethinking the Prune-Retrain Paradigm in the Era of LLMs",
    "[113] DistiLLM  Towards Streamlined Distillation for Large Language Models",
    "[114] On Good Practices for Task-Specific Distillation of Large Pretrained  Models",
    "[115] Model Distillation with Knowledge Transfer from Face Classification to  Alignment and Verification",
    "[116] MixKD  Towards Efficient Distillation of Large-scale Language Models",
    "[117] Learning Interpretation with Explainable Knowledge Distillation",
    "[118] Can a student Large Language Model perform as well as it's teacher",
    "[119] You Only Sample (Almost) Once  Linear Cost Self-Attention Via Bernoulli  Sampling",
    "[120] Reformer  The Efficient Transformer",
    "[121] Efficient Content-Based Sparse Attention with Routing Transformers",
    "[122] Sparsifying Transformer Models with Trainable Representation Pooling",
    "[123] Faster Causal Attention Over Large Sequences Through Sparse Flash  Attention",
    "[124] Linear Self-Attention Approximation via Trainable Feedforward Kernel",
    "[125] PolySketchFormer  Fast Transformers via Sketching Polynomial Kernels",
    "[126] SPT  Fine-Tuning Transformer-based Language Models Efficiently with  Sparsification",
    "[127] Context Compression for Auto-regressive Transformers with Sentinel  Tokens",
    "[128] Gated Linear Attention Transformers with Hardware-Efficient Training",
    "[129] Hyena Hierarchy  Towards Larger Convolutional Language Models"
]