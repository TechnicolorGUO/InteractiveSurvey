[
    "[1] Comparative Study of Parameter Selection for Enhanced Edge Inference for  a Multi-Output Regression model for Head Pose Estimation",
    "[2] Magnitude Attention-based Dynamic Pruning",
    "[3] Hessian-Aware Pruning and Optimal Neural Implant",
    "[4] Layer-wise Model Pruning based on Mutual Information",
    "[5] Beyond Size  How Gradients Shape Pruning Decisions in Large Language  Models",
    "[6] Structured Pattern Pruning Using Regularization",
    "[7] Effective Sparsification of Neural Networks with Global Sparsity  Constraint",
    "[8] Cyclical Pruning for Sparse Neural Networks",
    "[9] LEAP  Learnable Pruning for Transformer-based Models",
    "[10] Is Complexity Required for Neural Network Pruning  A Case Study on  Global Magnitude Pruning",
    "[11] Advancing Model Pruning via Bi-level Optimization",
    "[12] Integer or Floating Point  New Outlooks for Low-Bit Quantization on  Large Language Models",
    "[13] Value-aware Quantization for Training and Inference of Neural Networks",
    "[14] HAQ  Hardware-Aware Automated Quantization with Mixed Precision",
    "[15] Understanding INT4 Quantization for Transformer Models  Latency Speedup,  Composability, and Failure Cases",
    "[16] SQuAT  Sharpness- and Quantization-Aware Training for BERT",
    "[17] Post-Training Sparsity-Aware Quantization",
    "[18] The case for 4-bit precision  k-bit Inference Scaling Laws",
    "[19] Deja Vu  Contextual Sparsity for Efficient LLMs at Inference Time",
    "[20] Outlier Weighed Layerwise Sparsity (OWL)  A Missing Secret Sauce for  Pruning LLMs to High Sparsity",
    "[21] Dynamic Context Pruning for Efficient and Interpretable Autoregressive  Transformers",
    "[22] Learn To be Efficient  Build Structured Sparsity in Large Language  Models",
    "[23] ProSparse  Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models",
    "[24] Block-Sparse Recurrent Neural Networks",
    "[25] GASL  Guided Attention for Sparsity Learning in Deep Neural Networks",
    "[26] CATS  Contextually-Aware Thresholding for Sparsity in Large Language  Models",
    "[27] Pre-gated MoE  An Algorithm-System Co-Design for Fast and Scalable  Mixture-of-Expert Inference",
    "[28] SEER-MoE  Sparse Expert Efficiency through Regularization for  Mixture-of-Experts",
    "[29] SiDA  Sparsity-Inspired Data-Aware Serving for Efficient and Scalable  Large Mixture-of-Experts Models",
    "[30] Pushing Mixture of Experts to the Limit  Extremely Parameter Efficient  MoE for Instruction Tuning",
    "[31] DSelect-k  Differentiable Selection in the Mixture of Experts with  Applications to Multi-Task Learning",
    "[32] SwapMoE  Efficient Memory-Constrained Serving of Large Sparse MoE Models  via Dynamic Expert Pruning and Swapping",
    "[33] Efficient Deweather Mixture-of-Experts with Uncertainty-aware  Feature-wise Linear Modulation",
    "[34] Pruning Neural Networks at Initialization  Why are We Missing the Mark",
    "[35] An Operator Theoretic View on Pruning Deep Neural Networks",
    "[36] Enhanced Sparsification via Stimulative Training",
    "[37] Win the Lottery Ticket via Fourier Analysis  Frequencies Guided Network  Pruning",
    "[38] Integer-Only Neural Network Quantization Scheme Based on  Shift-Batch-Normalization",
    "[39] HAWQ-V2  Hessian Aware trace-Weighted Quantization of Neural Networks",
    "[40] F8Net  Fixed-Point 8-bit Only Multiplication for Network Quantization",
    "[41] Low-bit Quantization of Neural Networks for Efficient Inference",
    "[42] Dual Grained Quantization  Efficient Fine-Grained Quantization for LLM",
    "[43] Spartus  A 9.4 TOp s FPGA-based LSTM Accelerator Exploiting  Spatio-Temporal Sparsity",
    "[44] Attention is Naturally Sparse with Gaussian Distributed Input",
    "[45] Architectural Approaches to Overcome Challenges in the Development of  Data-Intensive Systems",
    "[46] Constructing cost-effective infrastructure networks",
    "[47] Dynamic Memory Based Adaptive Optimization",
    "[48] Homogenous and Heterogenous Parallel Clustering  An Overview",
    "[49] Theoretical Model of Computation and Algorithms for FPGA-based Hardware  Accelerators",
    "[50] Quantization-Aware Phase Retrieval",
    "[51] Collaborative Uploading in Heterogeneous Networks  Optimal and Adaptive  Strategies",
    "[52] Knowledge Distillation Beyond Model Compression",
    "[53] Ping-Pong Swaps",
    "[54] HOL Light QE",
    "[55] Sparse Gaussian ICA",
    "[56] SE-MoE  A Scalable and Efficient Mixture-of-Experts Distributed Training  and Inference System",
    "[57] HetuMoE  An Efficient Trillion-scale Mixture-of-Expert Distributed  Training System",
    "[58] A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize  Mixture-of-Experts Training",
    "[59] Pipeline MoE  A Flexible MoE Implementation with Pipeline Parallelism",
    "[60] Mixture of Quantized Experts (MoQE)  Complementary Effect of Low-bit  Quantization and Robustness",
    "[61] Bifurcated Attention for Single-Context Large-Batch Sampling",
    "[62] KIVI  A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
    "[63] GEAR  An Efficient KV Cache Compression Recipe for Near-Lossless  Generative Inference of LLM",
    "[64] SnapKV  LLM Knows What You are Looking for Before Generation",
    "[65] Scissorhands  Exploiting the Persistence of Importance Hypothesis for  LLM KV Cache Compression at Test Time",
    "[66] ALISA  Accelerating Large Language Model Inference via Sparsity-Aware KV  Caching",
    "[67] Efficient Memory Management for Large Language Model Serving with  PagedAttention",
    "[68] ChunkAttention  Efficient Self-Attention with Prefix-Aware KV Cache and  Two-Phase Partition",
    "[69] A Simple and Effective Pruning Approach for Large Language Models",
    "[70] Rethinking Weight Decay For Efficient Neural Network Pruning",
    "[71] Protective Self-Adaptive Pruning to Better Compress DNNs",
    "[72] Can pruning make Large Language Models more efficient",
    "[73] What Makes Quantization for Large Language Models Hard  An Empirical  Study from the Lens of Perturbation",
    "[74] HAWQV3  Dyadic Neural Network Quantization",
    "[75] Hardware-Centric AutoML for Mixed-Precision Quantization",
    "[76] Radial Networks  Dynamic Layer Routing for High-Performance Large  Language Models",
    "[77] SparQ Attention  Bandwidth-Efficient LLM Inference",
    "[78] Fast Inference of Mixture-of-Experts Language Models with Offloading",
    "[79] Shortcut-connected Expert Parallelism for Accelerating  Mixture-of-Experts",
    "[80] Scalable and Efficient MoE Training for Multitask Multilingual Models",
    "[81] Task-Specific Expert Pruning for Sparse Mixture-of-Experts",
    "[82] EvoMoE  An Evolutional Mixture-of-Experts Training Framework via  Dense-To-Sparse Gate",
    "[83] One Student Knows All Experts Know  From Sparse to Dense",
    "[84] CPSAA  Accelerating Sparse Attention using Crossbar-based  Processing-In-Memory Architecture",
    "[85] Leveraging Speculative Sampling and KV-Cache Optimizations Together for  Generative AI using OpenVINO",
    "[86] Infinite-LLM  Efficient LLM Service for Long Context with DistAttention  and Distributed KVCache",
    "[87] Model Tells You What to Discard  Adaptive KV Cache Compression for LLMs",
    "[88] Visualization and Optimization Techniques for High Dimensional Parameter  Spaces",
    "[89] A Survey on Hardware Accelerators for Large Language Models",
    "[90] Understanding Training Efficiency of Deep Learning Recommendation Models  at Scale",
    "[91] Understanding the Potential of FPGA-Based Spatial Acceleration for Large  Language Model Inference",
    "[92] Flexible Communication Avoiding Matrix Multiplication on FPGA with  High-Level Synthesis",
    "[93] Modeling and Leveraging Prerequisite Context in Recommendation",
    "[94] Optimized Partitioning and Priority Assignment of Real-Time Applications  on Heterogeneous Platforms with Hardware Acceleration",
    "[95] Device-Edge Cooperative Fine-Tuning of Foundation Models as a 6G Service",
    "[96] Accelerator Codesign as Non-Linear Optimization",
    "[97] Statistical Hardware Design With Multi-model Active Learning"
]