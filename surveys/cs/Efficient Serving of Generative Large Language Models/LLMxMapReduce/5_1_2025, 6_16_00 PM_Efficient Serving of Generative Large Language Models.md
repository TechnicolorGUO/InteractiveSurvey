# 5/1/2025, 6:16:00 PM_Efficient Serving of Generative Large Language Models  

0. Efficient Serving of Generative Large Language Models  

# 1. Introduction  

The rapid advancements in Large Language Models (LLMs) have paved the way for sophisticated generative AI applications, such as chatbots, virtual assistants, and content generation systems, which are increasingly integrated into various sectors and daily life [7,29,38]. This widespread adoption necessitates efficient and scalable LLM serving, especially as these models grow in size and complexity [6,11,20]. The demand for efficient serving is particularly pronounced in resource-constrained environments and for specific workloads, such as offline batch processing requiring high throughput [4] or deployment on edge devices [12,34]. The inference phase of LLMs, distinct from training, focuses on efficient parameter utilization for prediction [8], making its optimization critical for practical viability.​  

However, deploying and serving these massive models presents significant technical and economic challenges. The primary hurdles identified across the literature include high computational cost, substantial memory footprint, increased inference latency, limitations in scalability, and the need to maintain accuracy [6,19]. Computational demands are immense; for instance, generating a single token with a 20B parameter model can require over 40 GFLOPs [15], escalating to TFLOPs for longer sequences, often exceeding the practical capabilities of even powerful hardware like the NVIDIA A100 due to bottlenecks beyond raw FLOPS [10,15]. The large number of parameters translates directly to a significant memory footprint, often exceeding available GPU memory and complicating deployment on standard or constrained hardware [17,20,23,34]. Memory bandwidth is increasingly becoming the primary bottleneck, outpacing improvements in computational throughput and limiting performance, especially during serving [10]. Autoregressive decoding inherently introduces latency, posing challenges for real-time applications and leading to excessive inference times [22,25]. Furthermore, inefficient management of the KV cache, which stores intermediate attention results, can lead to memory fragmentation and limit batch size, thereby restricting throughput and increasing the cost per request [1,2,7]. Scalability is challenged by the need to handle complex real-world conditions involving multiple models, heterogeneous hardware, diverse traffic patterns, and varying Service Level Objectives (SLOs) [13,14]. Optimizing these factors while preserving the model's generalizability and accuracy after applying efficiency techniques remains a core concern [34].​  

![](images/b45e258e9abdd9e70218ca19aac3dab3f5d3df3256f00dfec70e105ae396f8a5.jpg)  

Addressing these multifaceted challenges requires a systematic approach encompassing various optimization strategies. Core ideas proposed in the literature include novel memory management techniques, improved scheduling, model compression, and system-level optimizations [5,7,27]. For instance, inspired by virtual memory, approaches like PagedAttention improve KV cache utilization [1]. Algorithmic methods like model compression (e.g., quantization, pruning) aim to reduce model size and computation, while system-level optimizations like kernel fusion and efficient parallelization enhance hardware utilization [11,23,24,34]. The efficient serving of LLMs often involves a complex interplay between these techniques, requiring holistic solutions.​  

This survey provides a comprehensive overview of the techniques for efficient LLM serving. Relative to existing works, which may focus on specific aspects such as on-device deployment [12], KV cache management [2], or particular optimization categories [5], this survey aims to offer a broader perspective [8]. It synthesizes research across key themes in LLM serving efficiency. The scope covers various types of LLMs and discusses optimization strategies applicable across different serving environments, from cloud-based servers to edge devices. By structuring the discussion around the core challenges and the corresponding algorithmic and system-level solutions, this survey provides researchers and practitioners with a systematic understanding of the field and potential directions for future research and deployment [5,6].  

# 2. Background on LLM Inference  

![](images/79f3ff69024e611283a10a8655fb3c31eeb8b6d9c24b1ed2f9584009be617c56.jpg)  

<html><body><table><tr><td>Aspect</td><td>Prefill Stage (Prompt)</td><td>Decode Stage (Autoregressive Generation)</td></tr><tr><td>Input</td><td>Entire prompt sequence</td><td>Current token + previous tokens</td></tr><tr><td>Output</td><td>Initial token, initial KV cache</td><td>Next token</td></tr><tr><td>Computation</td><td>Processes sequence in parallel</td><td>Processes one token at a time</td></tr><tr><td>Key Ops</td><td>Matrix Multiplication (GEMM)</td><td>Matrix-Vector Multiplication (GEMV)</td></tr><tr><td>Hardware Use</td><td>Leverages Tensor Cores</td><td>Poor Tensor Core utilization</td></tr><tr><td>KV Cache</td><td>Builds initial cache for prompt tokens</td><td>Uses/Updates cache with current token</td></tr><tr><td>Bottleneck</td><td>Generally Compute-Bound</td><td>Predominantly Memory Bandwidth-Bound</td></tr><tr><td>Efficiency</td><td>Well-suited for parallel hardware</td><td>Bottleneck for throughput</td></tr></table></body></html>  

Large Language Models (LLMs) represent a class of powerful deep learning models, typically based on neural network architectures, trained on extensive text data to acquire linguistic understanding and generation capabilities [38]. A foundational architecture for modern generative LLMs is the Transformer model [1,5,7,8,14]. While the original Transformer is a sequence-to-sequence model comprising an encoder and a decoder [40], most generative LLMs, such as the GPT series, utilize a decoder-only structure, processing input and generating output tokens autoregressively [4,10].​  

The Transformer architecture consists of multiple identical blocks stacked together [5,40]. Each decoder block primarily includes a Multi-Head Self-Attention (MHSA) module and a position-wise Feed Forward Network (FFN) [2,5,40]. Residual connections and Layer Normalization (or RMSNorm) are employed around these modules to facilitate training deeper networks [3,40]. Positional encoding is incorporated to provide the model with information about the sequence order, compensating for the inherent lack of temporal awareness in the attention mechanism [3,8].  

The self-attention layer is a critical component of Transformer-based models [7,10]. For an input sequence of hidden states \ $( ( \mathsf { x \_ { 1 } } , \mathsf { \backslash d o t s } , \mathsf { x \_ n } ) \backslash ) .$ , linear transformations using learnable weight matrices \( W_q, W_k, W_v \in \mathbb{R}^{d \times d} \) are applied at each position $\backslash ( \mathfrak { i } \backslash )$ to derive query $( \backslash ( { \mathsf { q } } _ { - } { \mathsf { i } } \backslash ) ) , { \mathsf { k e y } } ( \backslash ( { \mathsf { k } } _ { - } { \mathsf { i } } \backslash ) )$ , and value $( \backslash ( \vee _ { - } \mathfrak { i } \backslash ) )$ vectors [1,3,7]:  

$$
q _ { i } = x _ { i } W _ { q } , \quad k _ { i } = x _ { i } W _ { k } , \quad v _ { i } = x _ { i } W _ { v }
$$  

Attention scores are then computed by taking the dot product of the query vector at position $\backslash ( \mathfrak { i } \backslash )$ with all preceding key vectors (i.e. $\backslash ( { \mathsf { q } } _ { - } { \mathsf { i } } ^ { \wedge } \{ \backslash { \mathsf { t o p } } \} { \mathsf { k } } _ { - } { \mathsf { j } } ^ { \backslash } \backslash ) \rceil$ . These scores are typically scaled, passed through a softmax function to obtain attention weights $\backslash ( \mathsf { a \_ { i j j } } \backslash )$ , and used to compute a weighted sum of value vectors to produce the output vector $\backslash ( \circ _ { - } \mathrm { i } \backslash )$ for position \( $\mathfrak { i } \backslash \backslash$ [1,7]:​  

$$
a _ { i j } = \frac { \exp ( \boldsymbol { q } _ { i } ^ { \top } \boldsymbol { k } _ { j } ) } { \sum _ { j ^ { \prime } } \exp ( \boldsymbol { q } _ { i } ^ { \top } \boldsymbol { k } _ { j ^ { \prime } } ) } , \quad o _ { i } = \sum _ { j } a _ { i j } v _ { j }
$$  

Or, more generally including scaling by $\big \backslash \big ( \mathrm { \backslash \mathtt { s q r t } \{ d \} \backslash } \big )$ :  

$$
a _ { i j } = { \frac { \exp \left( { \frac { q _ { i } ^ { \top } k _ { j } } { \sqrt { d } } } \right) } { \sum _ { t = 1 } ^ { i } \exp \left( { \frac { q _ { i } ^ { \top } k _ { t } } { \sqrt { d } } } \right) } } , \quad o _ { i } = \sum _ { j = 1 } ^ { i } a _ { i j } v _ { j }
$$  

For generative models, causal attention is used, applying a triangular mask to prevent positions from attending to future positions [3,40]. The self-attention computation, particularly the $\langle ( { \mathsf { Q } } { \mathsf { K } } { \mathsf { \Large \wedge } } \{ \backslash  { \mathsf { t o p } } \} \backslash )$ operation, has a time complexity of \( ${ \mathsf { O } } ( { \mathsf { S } } ^ { \wedge } 2 { \mathsf { D } } ) \backslash )$ , where $\mathsf { \backslash } ( \mathsf { S \backslash } )$ is the sequence length, making it computationally intensive for long sequences [3].  

LLMs generate text in an autoregressive manner [1,2,5,7,8]. Given an input prompt, the model generates one token at a time using the prompt and all previously generated tokens as context for the next prediction [7,8]. This process repeats until a termination token is generated [7,8]. To optimize this sequential process, the Key and Value vectors computed for preceding tokens are stored and reused in a Key-Value (KV) cache [1,2,3,5,7,14,17]. This avoids redundant computations during each token generation step, significantly accelerating the process [2,5]. The size of the KV cache is a significant contributor to GPU memory consumption, growing with sequence length and the number of concurrent requests [3,17], posing challenges for memory management [2]. Some architectural variants, such as multiquery attention used in Falcon, aim to improve efficiency by sharing key and value tensors across attention heads [11].​  

The autoregressive generation process involves determining the next token based on the model's output probability distribution [8]. Various decoding algorithms are employed for this purpose, including deterministic methods like greedy search (selecting the token with the highest probability) and beam search (exploring multiple likely sequences), as well as probabilistic methods like sampling (e.g., Top-k, Top-p, temperature scaling) which introduce variability [8]. More advanced techniques like speculative decoding aim to accelerate this single-token-at-a-time process by predicting and verifying multiple tokens in parallel [9].​  

LLM inference for a given request is typically divided into two main stages: Prefill (or Prompt stage) and Decode (or Autoregressive generation stage) [1,3,4,5,6,7,8,25]. The Prefill stage processes the entire input prompt in parallel to compute the initial representation, generate the first output token, and build the initial KV cache for the prompt tokens [3,4,5]. This stage is well-suited for matrix multiplication (GEMM) operations, particularly leveraging Tensor Cores on GPUs, and is generally considered compute-bound [3,25].  

In contrast, the Decode stage involves the step-by-step generation of subsequent tokens [3,4,5,6]. For each new token, the model computes attention using the query vector for the current position and the accumulated Key and Value vectors stored in the KV cache [3]. This involves matrix-vector multiplication (GEMV) rather than GEMM, which has significantly lower arithmetic intensity and consequently poor Tensor Core utilization [3]. The Decode stage heavily relies on accessing the KV cache, and memory access operations scale linearly with the number of generated tokens [3]. Due to the repetitive loading and storing of KV cache data, this stage is predominantly memory bandwidth-bound [1,3,7,25], often becoming the primary bottleneck in LLM inference efficiency, particularly limiting serving throughput [1,6,7]. The distinct computational and memory access patterns of the Prefill and Decode stages contribute to difficulties in fully utilizing hardware resources [6].  

To improve throughput in serving scenarios, batching techniques are widely used, processing multiple independent requests concurrently [1,7]. While simple static batching helps keep GPUs busy, challenges arise from the variable input and output lengths and asynchronous arrival times of requests, necessitating more dynamic and fine-grained batching mechanisms like cellular batching or iterative-level scheduling [1]. The overall inference process also includes initial steps like tokenization and embedding, and final steps like output post-processing, though model inference itself is the most timeconsuming part [8,19]. Besides model parameters and KV cache, GPU memory is also consumed by activations and temporary buffers [17]. The size of model parameters directly correlates with the model's scale (e.g., a 34.5 billion parameter model requiring 69 GB at FP16 precision) [17]. Techniques like quantization can reduce model size, but computations may still require dequantization to floating-point for minimal precision loss, involving additional overhead [15].  

# 3. Challenges in Efficient LLM Serving  

<html><body><table><tr><td>Challenge Category</td><td>Key Issues</td><td>Examples/Impact</td></tr><tr><td>Computational Cost</td><td>Immense scale, Autoregressive overhead</td><td>TFLOPs per token, High energy consumption, Slow inference speed</td></tr><tr><td>Memory Footprint</td><td>Model size exceeds GPU memory, Large/dynamic KV Cache</td><td>Deployment limitations, Increased processing time (swapping), Memory "wall"</td></tr><tr><td>Memory Management</td><td>KV Cache fragmentation (internal/external), Lack of sharing</td><td>Limits batch size, Reduces throughput, Memory waste</td></tr><tr><td>Communication</td><td>Inter-device data transfer overhead</td><td>Bottleneck in distributed inference (TP/PP)</td></tr></table></body></html>  

<html><body><table><tr><td>Concurrency / Real-time</td><td>High QPS,Variable request lengths, Batching trade-offs</td><td>Difficult to achieve high throughput& low latency simultaneously</td></tr><tr><td>Generation Quality</td><td>inconsistencies, Safety issues</td><td>Requires robust quality control, Trade-off with</td></tr><tr><td>Multimodal Complexity</td><td>Higher dimensionality, Diverse data types</td><td>Increased costs for multi- modal inference</td></tr><tr><td>System-Level</td><td>Multi-model support, Heterogeneous hardware, Traffic patterns, SLOs, Orchestration complexity</td><td>Challenging global optimization, Complex deployment scenarios</td></tr><tr><td>Edge Deployment</td><td>Severe resource constraints (compute,memory,energy)</td><td>Requires extreme optimization, Limits model</td></tr></table></body></html>  

The efficient serving of generative Large Language Models (LLMs) is paramount for their widespread deployment, yet it is hampered by a complex interplay of technical challenges spanning computational, memory, and communication domains [6,25]. These challenges significantly impact deployment feasibility and cost, manifesting as performance bottlenecks that limit throughput and increase latency [5,16,23,25]. Performance bottlenecks are commonly categorized into computebound, memory bandwidth-bound, communications-bound, and overhead-bound scenarios, each requiring distinct optimization strategies [25]. Identifying the dominant bottleneck in a given scenario is crucial for effective optimization [25].  

A primary challenge stems from the immense scale of LLMs, which necessitates substantial computational resources and energy consumption [6,11,12,15,28,29,32]. For instance, inference with models like LLaMA-2-70B can require multiple highend GPUs such as six RTX 3090Ti or two NVIDIA A100 GPUs for efficient operation [6]. The autoregressive nature of LLM inference, generating one token at a time, leads to considerable computational overhead and exacerbates latency, particularly with large model parameter sizes [8,22,23]. Generating a single token with OPT-66B, for example, can incur a latency of 500ms to 800ms [23]. While the prefill stage involves significant computation, the subsequent decode stage often becomes memory bandwidth-limited, as the computational intensity decreases and processing units frequently await data [3]. This phenomenon contributes to the "memory wall" problem, where the pace of computation outstrips memory access speeds [10,26,40].  

Memory-related challenges are particularly acute. The sheer size of model parameters can exceed the available memory on a single GPU, even with optimization techniques like quantization or pruning [20,29]. GPT-3, with 175 billion parameters, requires approximately 700GB of memory for float32 precision [29]. The growth in model size significantly outpaces the increase in hardware memory capacity, intensifying memory bottlenecks [6,10,40]. Beyond model weights, the Key-Value (KV) cache, storing intermediate attention states, poses a substantial memory challenge. The KV cache grows linearly with sequence length and the number of layers [2,3,5,23], dynamically growing and shrinking with unpredictable lifespan and length [7]. For instance, the KV cache for LLaMA2-70B can exceed 60GB per user session [23], and for a 13B model with a sequence length of 2048 tokens, it can reach 1.6 GB per request [1].  

Efficient management of the KV cache is hindered by several factors. Existing systems often mandate contiguous memory allocation for the KV cache, leading to significant memory fragmentation [1,2,7,17]. Static pre-allocation based on maximum sequence length results in substantial internal fragmentation, as actual sequence lengths are often much shorter; analysis shows that only $2 0 . 4 \%$ to $3 8 . 2 \%$ of the pre-allocated KV cache memory is actually utilized in existing systems [1,7]. External fragmentation also arises from varying request sizes [7]. Furthermore, existing systems struggle to leverage memory sharing opportunities, particularly with advanced decoding algorithms like parallel sampling and beam search, where multiple sequences within a single request can share parts of their KV cache [1,7,17]. Insufficient GPU memory can necessitate transferring data to slower CPU memory or disk, further increasing processing time [17].  

Communication bottlenecks become prominent when distributing LLM inference across multiple accelerators to overcome single-chip memory limitations [10,20,40]. Data transfer between GPUs can be slower and less efficient than transfers withi a chip, posing a significant challenge in optimizing both computing and communication resources [21,40].  

Concurrency and achieving real-time performance under high load are critical challenges [8,17,19]. High-concurrency scenarios, such as those experienced by popular LLM services, demand extremely high system throughput and efficient scheduling of potentially small batch requests [8,23]. While dynamic batching can improve GPU utilization, it may concurrently increase latency for individual requests [8]. Challenges also arise in specific scenarios like large-batch offline inference, where suboptimal KV cache reuse and inefficient scheduling that doesn't prioritize decoding-heavy requests can lead to hardware underutilization [4].  

Other notable challenges include the overhead associated with model loading times [19] and the difficulties in maintaining high generation quality. LLMs can sometimes produce repetitive or off-topic content, factual inconsistencies, or sensitive material, requiring robust quality control mechanisms [8]. Furthermore, balancing prediction accuracy and generation efficiency, particularly in techniques like speculative decoding, remains an area requiring enhancement [9]. The expansion to multi-modal inference introduces increased costs due to the higher dimensionality and complexity of processing diverse data types like text and images [6,18].​  

Serving LLMs in complex real-world environments presents additional system-level challenges. These include supporting multiple LLM models of varying sizes and applications, managing heterogeneous hardware environments with diverse GPU types and network topologies, optimizing parallel orchestration schemes (e.g., Tensor Parallel and Pipeline Parallel) considering their impact on performance, handling complex traffic distributions with fluctuating queries per second (QPS) and sequence lengths, and meeting diverse Service Level Objectives (SLOs) for online versus offline services [13,14]. These complexities make achieving globally optimal inference performance a challenging undertaking [14]. Finally, deploying LLMs on resource-constrained edge devices introduces unique challenges related to balancing performance with severe limitations on computational cost, memory footprint, and energy consumption [12,34]. Ultimately, efficiently serving LLMs involves navigating inherent trade-offs between conflicting performance metrics such as latency, throughput, memory usage, computational cost, and generation quality [5,6,16,23,25].​  

# 4. Optimization Techniques  

![](images/1b3f41e019d9ccd3d8d492909b88c5b37cefdbca7ffbb730fa87609911fd15c7.jpg)  

The efficient serving of generative Large Language Models (LLMs) presents significant technical challenges, primarily stemming from their massive scale, computational demands, and memory requirements. Addressing these challenges necessitates a wide array of optimization strategies applied across different levels of the serving pipeline [5,6,19,25]. These techniques aim to improve throughput, reduce latency, and minimize resource consumption while maintaining acceptable levels of model accuracy and output quality [8,27,34].  

Optimization strategies for LLM serving can be broadly categorized based on their primary focus: data, model, algorithm, and hardware [5,6,19,25]. Data-level optimizations focus on manipulating the input data or organizing the output process to enhance efficiency without altering the model itself [5]. Model-level techniques, often referred to as model compression, aim to reduce the size and computational complexity of the model itself, frequently requiring modifications to the model architecture or parameters [5,8,26,27,34]. Algorithmic and low-level system optimizations involve refining the inference algorithms, data flow, and system infrastructure to improve computational efficiency and resource utilization during runtime [6,10,26,27,34]. Hardware acceleration leverages specialized computing platforms designed for deep learning workloads to provide the necessary computational power and memory bandwidth [12,26,27]. Furthermore, hybrid approaches that judiciously combine techniques from multiple categories are increasingly employed to achieve greater synergistic benefits and overcome limitations inherent in individual methods [27].​  

Each optimization technique involves inherent trade-offs between potentially conflicting objectives, primarily accuracy, efficiency, and implementation complexity. For instance, model compression techniques like quantization and pruning can significantly improve efficiency by reducing model size and computation, but often at the risk of degrading model accuracy unless carefully applied and potentially followed by fine-tuning [8,34]. Algorithmic optimizations such as dynamic batching can boost system throughput but may increase latency for individual requests [8,26]. Hardware acceleration offers substantial performance gains but requires specialized infrastructure and expertise for effective utilization [27]. The choice and configuration of optimization techniques are therefore critical and depend heavily on the specific requirements of the LLM application and the constraints of the deployment environment. This section provides a structured overview of these diverse strategies, detailing their principles and highlighting the associated trade-offs.​  

# 4.1 Data-Level Optimization  

Data-level optimization techniques represent a class of methods aimed at improving the efficiency of large language model (LLM) serving by manipulating the data handled during inference [5]. These techniques are broadly categorized into input compression and output organization [5].​  

<html><body><table><tr><td>llms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllm</td></tr><tr><td>s.txtllms.txtlms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.tx</td></tr><tr><td>tllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllm</td></tr><tr><td>s.txtllms.txtllms.txtlms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.tx</td></tr><tr><td>tllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllm</td></tr><tr><td>s.txtllms.txtllms.txtlms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.tx</td></tr><tr><td>tllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtllms.txtlms.txtllms.txtllm</td></tr><tr><td>s .txtllms.txt .txt` file to guide content access [41].</td></tr></table></body></html>  

Retrieval-Augmented Generation (RAG) stands out as a prominent input compression and enhancement technique [5,16,29]. RAG improves the factual accuracy of LLMs by providing them with high-quality external knowledge retrieved from databases or documents [16,18]. This external data enhances the model's context understanding and enables it to access real-time or domain-specific information, thereby reducing hallucinations and grounding responses [16,29]. The effectiveness of RAG is significantly impacted by the accuracy of the retrieval process [5]; precise retrieval ensures that the provided context is relevant and reliable, leading to more accurate and helpful model outputs [18]. The concept has been extended to Multimodal Retrieval-Augmented Generation (MRAMG), which integrates non-textual modalities such as images into the retrieval process [18]. Evaluating the performance of MRAMG systems necessitates specialized benchmarks designed to assess multimodal retrieval and generation capabilities [18]. In scenarios where LLMs lack direct image processing capabilities, textual descriptions or titles associated with images can serve as inputs for retrieval within an MRAMG framework [18].  

Output organization focuses on structuring the generation process to enable parallel decoding and thus reduce end-to-end inference latency [5]. By planning the structure or content of the output in advance, techniques in this category can facilitate generating different parts of the response concurrently, contrasting with the typical token-by-token sequential generation process [5].​  

# 4.2 Model Compression Techniques  

Model compression techniques are fundamental strategies for enabling the efficient serving of large language models (LLMs) by addressing their significant size and computational requirements [5,8,26,27,34]. These techniques aim to reduce the model's memory footprint and computational overhead while striving to maintain acceptable levels of accuracy and performance [11,19,29,32].  

Key approaches in model compression include Quantization, which reduces the numerical precision of model parameters and activations to decrease memory usage and accelerate computation [6,8,25,36], and Pruning, which eliminates redundant or less important model components like weights or neurons to reduce model complexity and computational load [8,19,26,28]. Knowledge Distillation transfers the capabilities of a large "teacher" model to a smaller "student" model [27,28,34], thereby achieving efficiency through architectural reduction while preserving performance. Finally, Compact Architecture Design focuses on creating inherently more efficient network structures from the ground up, or by modifying existing ones, through methods such as parameter sharing, low-rank factorization, efficient attention mechanisms, or the use of alternative model paradigms [5,12,26,34].​  

Each of these techniques presents unique trade-offs between the degree of compression achieved and the resulting impact on model accuracy and downstream task performance [8,10,34]. Effectively applying model compression often requires careful calibration, fine-tuning, or retraining the model after compression to recover potential accuracy losses [28,34]. Collectively, these model compression techniques are indispensable for deploying and serving LLMs efficiently on diverse hardware environments, particularly those with limited resources [5,25].​  

# 4.2.1 Quantization  

<html><body><table><tr><td>Feature</td><td>Post-Training Quantization (PTQ)</td><td>Quantization-Aware Training (QAT)</td></tr><tr><td>Process</td><td>Quantize a pre-trained model</td><td>Incorporate quantization into training</td></tr><tr><td>Training Data</td><td>Not required (small calibration set optional)</td><td>Requires original or representative data</td></tr><tr><td>Computational Cost</td><td>Low overhead</td><td>High (full training run)</td></tr><tr><td>Accuracy</td><td>Can incur accuracy loss (mitigated by tech)</td><td>Typically higher accuracy</td></tr><tr><td>Implementation</td><td>Easier, faster</td><td>Morecomplex,requires modifying training</td></tr><tr><td>Use Case</td><td>Common for LLMs due to low overhead</td><td>Used when higher accuracy is critical</td></tr></table></body></html>  

Quantization is a critical technique for optimizing the serving efficiency of large language models (LLMs) by reducing their memory footprint and computational overhead [8,19,28,29,34]. It achieves this by converting model parameters, typically weights and activations, from high-precision floating-point formats (e.g., 32-bit or 16-bit) to lower-precision integer or floating-point formats (e.g., 8-bit or 4-bit) [8,19,29]. This conversion enables the use of specialized hardware like 8-bit Tenso Cores for faster computation [25].  

The core principle of quantization involves mapping a range of real-valued numbers to a smaller set of discrete, lowerprecision values. Affine quantization is a common formulation, defined by the equation  

$$
q = \operatorname { r o u n d } \left( { \frac { r } { s } } + Z \right)
$$  

where $r$ is the original real value, $q$ is the quantized value, ​s is the scale factor, and $Z$ is the zero point [36]. The scale and zero point are determined during a calibration process, which selects the numerical range for quantization based on statistical properties of weights and activations [36]. Various calibration methods exist, including max, entropy (KL divergence), and percentile, chosen based on the data distribution (e.g., entropy for activations, max or percentile for  

weights) [36]. Quantization can be applied per-tensor (using one scale and zero point for an entire tensor) or per-channel (using distinct parameters for each channel), with per-channel generally yielding higher accuracy [36].  

Quantization methods can be broadly categorized into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) [5,11,15,26,28,32,36]. PTQ quantizes a pre-trained model without requiring additional training data or significant computational resources, often using a small calibration dataset [5,28]. It is widely adopted for LLMs due to its low overhead [5]. QAT, conversely, incorporates pseudo-quantization operators into the training process, allowing the model to adapt to the precision loss during training [11,28,32]. While QAT typically achieves higher accuracy than PTQ, it demands substantial data and computational resources for retraining [5,32].  

Further distinctions exist between uniform and non-uniform quantization [36], and dynamic and static quantization [28]. Dynamic quantization determines the scaling factors and zero points at runtime based on the actual data range, particularly useful for activations whose distributions vary significantly. Static quantization pre-calculates these parameters based on a calibration dataset and fixes them during inference, which can offer better performance on hardware [28].  

The choice of bit-width significantly impacts the trade-off between efficiency and accuracy. Converting weights and activations to INT8 is a common approach, effectively reducing memory usage and computational load [8,11,26]. Lowering precision further to INT4 dramatically decreases memory footprint and memory access costs, proving particularly effective for deployment on memory-constrained devices [10,11,15]. Other formats include FP8 [16,24], and mixed-precision schemes like W4A16 (INT4 weights, FP16 activations) [15]. Specific use cases also benefit from quantization, such as KV8 quantization, which quantizes the Key and Value caches in the Transformer architecture to INT8, reducing memory occupation during token generation [15]. While INT4 quantization is relatively straightforward with minimal accuracy impact, achieving efficient and accurate inference at sub-INT4 precision remains an active research challenge [10]. Extreme examples like "1-bit LLMs" explore pushing quantization to its theoretical limits [12,35].  

Several advanced PTQ techniques have been developed to mitigate accuracy degradation, a primary challenge caused by the inherent rounding errors during quantization [11,20,28]. Techniques include scaling weights or using dynamic range quantization [28], adjusting kernel parameter distributions, or compensating for quantization errors [27]. Prominent methods include GPTQ [12,28,35], AWQ (Activation-aware Weight Quantization) [12,16,26,28,35], and SmoothQuant [6,28]. AWQ, for instance, focuses on quantizing weights while accounting for activation distributions [26]. SmoothQuant addresses the issue of non-uniform weight and activation distributions by adjusting them to be more quantization-friendly before conversion, thereby minimizing quantization error and improving post-quantization accuracy [6]. Quantization can target different parts of the model; Weight-Activation Quantization is often used during the prefilling stage, while Weight-only Quantization (e.g., AWQ) is common in the decoding stage to reduce memory access costs [5,26]. Implementing quantization effectively often requires specific software libraries and hardware support, such as efficient CUDA kernels for 4-bit inference provided by systems like TurboMind [15] or frameworks like HuggingFace and bitsandbytes which facilitate loading and executing quantized models [27,29].  

# 4.2.2 Pruning  

Pruning is a fundamental model compression technique employed to reduce the computational and storage overhead associated with large language models (LLMs) by decreasing their complexity [19,29]. The core principle involves identifying and removing redundant or less important components of the neural network while striving to maintain model capacity and performance [11,26,28,34]. These components can manifest as individual weights, entire neurons, or connections between neurons [8,26,27]. By eliminating these elements, pruning effectively reduces the number of parameters and operations required during inference, leading to improved efficiency, particularly in compute-bound scenarios [8,25].​  

Pruning techniques can be broadly categorized based on their granularity: unstructured and structured pruning [5,26,27,28]. Unstructured pruning involves the removal of individual weights based on importance criteria, which can achieve high sparsity levels and potentially greater precision [27,28]. However, this often results in irregular, sparse weight matrices that may not align well with standard hardware or require specialized libraries and hardware support, such as the sparse acceleration units in NVIDIA's Ampere architecture [26,27]. Techniques like SparseGPT and Wanda are examples of unstructured pruning methods, with Wanda pruning weights based on the multiplication of weights and corresponding input activation values without retraining [11,26,28].  

In contrast, structured pruning removes entire structures such as neurons, filters, layers, or blocks [27,28]. While potentially yielding lower sparsity compared to unstructured methods, structured pruning results in smaller, dense matrices,  

preserving the original network architecture and enabling direct implementation on mature deep learning frameworks without requiring specialized hardware or libraries [26,27]. This makes the pruned models more compatible with existing hardware and software stacks. LLM-Pruner is an example of a structured pruning technique that selectively removes noncritical coupled structures based on gradient information [11,26,28]. Sparse attention can also be considered a form of structured sparsification in the attention mechanism, reducing redundant calculations to optimize prefilling computation and KV cache memory [5].​  

The impact of pruning ratios (the proportion of elements removed) and granularity (structured vs. unstructured) on model accuracy and resulting sparsity is significant [27]. Higher pruning ratios lead to greater sparsity and model size reduction but increase the risk of accuracy degradation [10]. Research indicates that achieving high sparsity without significant accuracy loss is challenging. For instance, up to $3 0 \%$ of neurons can potentially be pruned with structural sparsity, and up to $8 0 \%$ with unstructured sparsity, without drastically impacting accuracy; however, pushing beyond these thresholds often results in noticeable performance degradation [10].  

A primary challenge in applying pruning to LLMs is maintaining high accuracy despite significant model reduction [12,35,40]. Model pruning inherently introduces some loss of training accuracy, and balancing the desired level of compression with performance preservation remains an active research area [40]. Effective pruning strategies often involve iterating through steps like initial training, evaluating parameter importance, pruning, and subsequent fine-tuning to recover performance loss [28]. The difficulty in preserving functionality while removing a large fraction of parameters is a key challenge in the deployment of efficient LLMs [10,12,35].  

# 4.2.3 Knowledge Distillation  

Knowledge distillation (KD) is a model compression technique widely applied to large language models (LLMs) to enhance inference efficiency and reduce computational requirements [25,34]. The core principle involves transferring the knowledge embedded within a large, complex "teacher" model to a smaller, more efficient "student" model [27,28,32]. This process allows the student model to approximate the performance of the teacher model while utilizing fewer parameters and requiring fewer operations, thereby improving inference speed and real-time performance [19,25]. While KD generally aims to maintain good performance with reduced model size, some approaches might involve a slight sacrifice in accuracy for efficiency gains . A key aspect of successful KD is maintaining the model's generalization capabilities after compression [34].  

The knowledge distillation process typically involves selecting a pre-trained large model as the teacher and a smaller model with a simpler architecture as the student [29]. The student model is then trained to mimic the behavior and predictions of the teacher model [26,28]. This is often achieved by training the student to replicate the teacher's output probabilities or logits, which serve as "soft targets," in addition to being trained on the true labels [26,28,29]. A common loss function used in KD combines a distillation loss, which minimizes the difference between the teacher's and student's softened outputs, and a standard cross-entropy loss based on the true labels [26]. A typical formulation for the loss function is given by:​  

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { K D } } = \mathcal { L } _ { \mathrm { d i s t i l l } } \Big ( \operatorname { s o f t m a x } \big ( \mathbf { z } _ { t } , T \big ) , \operatorname { s o f t m a x } \big ( \mathbf { z } _ { s } , T \big ) \Big ) + \lambda \mathcal { L } _ { \mathrm { C E } } \big ( \mathbf { y } , \mathbf { z } _ { s } \big ) } \end{array}
$$  

Here, ${ \bf z } _ { t }$ and ${ \bf z } _ { s }$ ​ represent the logits from the teacher and student models, respectively, $T$ is the temperature parameter used to soften the probability distributions, $\mathcal { L } _ { \mathrm { d i s t i l l } }$ is the distillation loss (often Kullback-Leibler divergence), $\mathcal { L } _ { \mathrm { C E } }$ is the cross-entropy loss with respect to the true labels $\mathbf { y }$ , and $\lambda$ is a weighting factor balancing the two loss components [26].  

Knowledge distillation approaches can be broadly categorized. One common distinction is between White-box and Blackbox methods, differing in whether internal layers or only the output of the teacher model is used for distillation [5]. For LLMs, approaches specifically address the transfer of complex capabilities, such as distinguishing between standard KD and EA-based KD, which focuses on distilling the emergent abilities of LLMs [28]. Furthermore, sequence-level knowledge distillation (Seq-KD) has been explored, where the student model is trained using sentences generated by the teacher LLM, proving effective in aligning the student's output and enhancing generation quality [9].  

The effectiveness of knowledge distillation is influenced by various factors, including the size and architecture of both the teacher and student models [34]. Examples of knowledge distillation applied to language models include DistilBERT, which is a well-known instance where a smaller model is trained via distillation to mimic a larger BERT model [29]. MiniLLM is another referenced knowledge distillation approach specifically tailored for large language models [12,35].​  

# 4.2.4 Compact Architecture Design  

Compact architecture design is a fundamental approach for compressing and optimizing Large Language Models (LLMs) [34], aiming to achieve a better balance between model accuracy and inference efficiency by modifying the network structure [5]. This strategy is also broadly applied in general deep learning models to reduce computational cost and memory footprint [27]. Key techniques in this domain include parameter sharing, efficient attention mechanisms, and alternative architectures.​  

One method for achieving efficient architectures is through Parameter Sharing [12], where parameters are reused across different parts of the model, thereby reducing the total number of unique parameters. Low Rank Factorization is another structure optimization technique identified as representative in this field [5], often used to decompose weight matrices into lower-rank approximations to reduce parameter count and computational cost.​  

Mixture-of-Experts (MoE) is a distinct architectural paradigm used to balance computational cost and model scale [26]. Instead of activating all parameters for every input, an MoE layer divides a large model into multiple "expert" sub-models and employs a gating function to select and activate only a subset of these experts based on the input sample [26]. This approach significantly reduces the computational resources required for each inference step [26]. The calculation of an MoE layer can be represented by the formula:​  

$$
\begin{array} { c l l } { \displaystyle \operatorname { M o E } ( x ) = \displaystyle \sum _ { i = 1 } ^ { n } \bigl ( G ( x ) _ { i } E _ { i } ( x ) \bigr ) , } \\ { G ( x ) = \mathrm { T o p K } \Bigl ( \operatorname { s o f t m a x } \bigl ( W _ { g } ( x ) + \epsilon \bigr ) \Bigr ) . } \end{array}
$$  

Here, $E _ { i } ( x )$ denotes the output of the $\mathbf { \chi } _ { i }$ -th expert, $G ( x )$ is the gating function, and $W _ { g }$ is the weight matrix for the gating network [26]. Notable implementations of MoE include GShard, Switch-Transformer, and FastMoE [26]. While increasing the total parameter count, MoE reduces the activeparameters and computation per token, allowing for larger models that are computationally tractable during inference.  

Efficient attention mechanisms are crucial for optimizing the Transformer architecture. Multi-Query Attention is one such technique, exemplified by architectures like Falcon [11]. In Multi-Query Attention, query heads are separate, but all heads share the same key and value tensors [11]. This sharing significantly improves efficiency, particularly by reducing the memory footprint associated with storing key and value caches during sequence generation [11].​  

Beyond modifications to the standard Transformer, exploring alternative architectures such as State Space Models (SSMs) or designing inherently Smaller Language Models (small LMs) represents another direction in compact architecture design [5]. Designing smaller LMs that can fit entirely on a single chip can yield significant speedups and energy savings [10], representing a direct path to reduced model size, memory footprint, and computational complexity.  

Comparing these techniques, Multi-Query Attention primarily impacts computational efficiency and memory footprint by sharing key/value tensors. Parameter sharing directly reduces the total model size and memory footprint. MoE increases the total parameter count but reduces the computational cost and memory access per token by activating only a subset of experts, effectively balancing scale and cost. Low-rank factorization typically reduces both parameter count, model size, and computational complexity by approximating dense matrices. Designing small LMs offers a holistic reduction across model size, computation, and memory by building smaller models from the ground up. The ability of these specific techniques to handle long sequences is not explicitly detailed in the available digests [5].​  

# 4.3 Algorithmic & Low-Level System Optimizations  

Efficient serving of generative large language models necessitates a combination of algorithmic innovations and optimizations at the low-level system infrastructure to mitigate computational and memory overheads, thereby improving throughput and reducing latency [6,10,26,27,34]. This section analyzes various techniques falling under this category, focusing on critical areas such as Key and Value (KV) cache management, operator fusion and graph optimization, the utilization of optimized kernels and libraries, accelerated decoding strategies like speculative decoding, and dynamic batching approaches.​  

A primary area of focus is the efficient KV Cache Management. During autoregressive decoding, caching key and value vectors from previous tokens significantly reduces redundant computation [8]. However, the KV cache constitutes a substantial memory overhead, scaling with model and sequence size [2,15,17]. Optimization strategies are explored at token-level (e.g., selective caching, budget allocation, merging, quantization), model-level (modifications to attention), and system-level [2]. A key system-level technique is PagedAttention, which adapts operating system memory paging principles to manage the KV cache in non-contiguous blocks, enhancing memory utilization and enabling efficient batching and dynamic allocation [1,7,17,24]. Techniques like prefix sharing further optimize KV cache usage across batch requests with common prefixes [4]. Managing the trade-off between memory usage and performance is central to these optimizations [2,15].​  

Operator Fusion and Graph Optimization aim to enhance computational efficiency by reducing overhead from frequent kernel launches and memory accesses [6,25,26]. Operator fusion merges multiple operations into a single kernel, improving data locality, particularly for common patterns in Transformers like attention and normalization layers [6,23]. Graph optimization, a broader concept, applies various transformations to the computation graph, including fusion, constant folding, and batch normalization folding, to simplify and optimize the execution flow [27]. These techniques are often implemented via automated compilers or manual custom kernels [25].​  

Complementing graph-level optimizations, the use of Optimized Kernels and Libraries is crucial for leveraging hardware capabilities [6,23]. High-performance acceleration libraries such as ONNX Runtime, TVM, cuBLAS, TensorRT, and frameworkspecific libraries like TensorRT-LLM, FasterTransformer, and DeepSpeed Inference provide highly optimized implementations of standard and Transformer-specific operators, including attention mechanisms like FlashAttention [6,11,16,23,24,25,26,27]. Custom CUDA kernels are also developed for critical operations to exploit hardware parallelism and memory hierarchies effectively [4,7,11,14,15].  

Decoding Strategy Optimizations, such as speculative decoding, accelerate the generation process by enabling parallel verification of tokens [5,8,9]. This method uses a smaller, faster draft model to propose token sequences, which the larger target model verifies in parallel, reducing sequential steps by the expensive model [5,6,9,26]. Variations like Medusa and AtSpeed demonstrate adaptations for different prediction and verification strategies [6,22,26].  

Finally, Dynamic Batching improves GPU utilization and overall throughput by dynamically adjusting the batch size at runtime, allowing for continuous processing of requests as they arrive and complete [11,14,26]. Techniques like contiguous or iteration batching minimize idle GPU time compared to static batching [6,11,14,16]. While increasing throughput, dynamic batching presents a trade-off with latency, which can be mitigated by integrated scheduling mechanisms [8,26]. Some dynamic batching approaches also consider memory constraints and token types for optimization [4].  

Collectively, these algorithmic and low-level system optimizations address key performance bottlenecks in LLM serving, from memory inefficiencies in KV caching to computational overheads in graph execution and kernel calls, and inefficiencies in decoding and batch processing. Their combined application, often within sophisticated serving systems and libraries, is essential for achieving high-throughput and low-latency inference, enabling the deployment of large models at scale.​  

# 4.3.1 KV Cache Management  

<html><body><table><tr><td>Category</td><td>Focus</td><td>Techniques / Examples</td><td>Goal</td></tr><tr><td>Token-Level</td><td>Individual tokens in sequence</td><td>Selection/Pruning (StreamingLLM), Budget Allocation (PyramidKV), Merging (CCM), Quantization (KVQuant)</td><td>Reduce memory, Improve speed</td></tr><tr><td>Model-Level</td><td>Attention mechanism mods</td><td>Attention grouping, Sharing K/V (Multi- Query Attention)</td><td>Reduce computation/memor y</td></tr><tr><td>System-Level</td><td>System/Infrastructur e level</td><td>PagedAttention, Prefix Sharing (BatchLLM), Swapping/Recomput ation, Distributed Management,</td><td>Improve utilization, Enable scale</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>Memory Configuration</td><td></td></tr></table></body></html>  

Efficient management of the Key and Value (KV) cache is paramount for optimizing the serving of large language models (LLMs), particularly during the autoregressive decoding phase where computed key-value vectors are cached to avoid redundant calculations [8]. This caching mechanism, however, introduces significant memory overhead, as the KV cache size scales with sequence length, batch size, model size, and the number of layers and heads [2]. Consequently, a primary focus of research in efficient LLM serving is on KV cache optimization techniques aimed at reducing memory consumption and enhancing inference speed and system throughput [2].  

KV cache optimization techniques can be broadly categorized into token-level, model-level, and system-level approaches [2].​  

Token-Level Optimizations operate on the granularity of individual tokens within the sequence to manage the KV cache.   
These include KV cache selection, budget allocation, merging, and quantization [2].  

• KV Cache Selection, or selective caching/pruning, aims to reduce memory utilization and inference latency by storing only the most relevant tokens, thereby improving overall throughput [2]. Methods range from static filtering during the prefill phase (e.g., FastGen, SnapKV, Attention-Gate) to dynamic approaches that continuously update the cache during decoding, potentially with permanent eviction (e.g., StreamingLLM, LMInfinite, H2O, BUZZ, NACL, Scissorhands, Keyformer, SepLLM) or using multi-layered cache systems and advanced data structures for retrieval efficiency (e.g., InfLLM, Quest, PQCache, SqueezedAttention, RetrievalAttention, EM-LLM, SparQ, InfiniGen, RecycledAttention, MagicPIG) [2].​  

• KV Cache Budget Allocation addresses the inefficiency of uniform compression across all layers, recognizing that different layers contribute varying importance to prediction accuracy [2]. Techniques intelligently allocate memory resources based on component importance, employing strategies that can be layer-wise (e.g., PyramidKV, PyramidInfer, DynamicKV, PrefixKV, CAKE, SimLayerKV) or head-wise (e.g., AdaKV, CriticalKV, LeanKV, RazorAttention, HeadKV, DuoAttention) [2].  

• KV Cache Merging seeks to compress or consolidate the KV cache by leveraging intra-layer and inter-layer redundancies, thereby optimizing memory utilization without significant accuracy loss [2]. Strategies are classified into intra-layer merging (e.g., CCM, LoMA, DMC, CaM, KVMerger, D2O, AIM, Look-M, CHAI) and cross-layer merging (e.g., MiniCache, KVSharer) [2].  

• KV Cache Quantization reduces the precision of the numerical representations within the KV cache, compressing its size and enabling deployment on devices with limited resources [2]. This can involve fixed-precision quantization (e.g., ZeroQuant, FlexGen, QJL, PQCache), mixed-precision approaches (e.g., KVQuant, IntactKV, SKVQ, KIVI), or outlier management techniques [2].  

Model-Level Optimizations, as indicated by the survey structure, typically involve modifications related to attention mechanisms, such as attention grouping or sharing keys/values, though specific implementations were not detailed in the provided digests for this section [2].  

System-Level Optimizations encompass strategies applied at the system or infrastructure level, including memory management, scheduling, and hardware-aware designs [2]. A prominent technique in this category is PagedAttention [1,6,7,17,24]. PagedAttention manages the KV cache by dividing it into fixed-size blocks, analogous to page management in operating systems [1,7]. A KV Cache Manager is employed to handle the dynamic allocation and deallocation of these blocks, mapping logical KV blocks to physical ones [1,7]. This approach enables non-contiguous storage, reduces memory fragmentation, facilitates memory sharing across requests, and allows the KV cache memory to grow dynamically without requiring pre-allocation for maximum possible sequences, thereby eliminating memory waste [1,7]. The attention calculation can then be framed as a block-wise operation [1]:​  

$$
A _ { i j } = \frac { \exp \left( \frac { q _ { i } ^ { \top } K _ { j } } { \sqrt { d } } \right) } { \sum _ { t = 1 } ^ { \lceil i / B \rceil } \exp \left( \frac { q _ { i } ^ { \top } K _ { t } } { \sqrt { d } } \right) } , \quad o _ { i } = \sum _ { j = 1 } ^ { \lceil i / B \rceil } V _ { j } A _ { i j } ^ { \top }
$$  

where $A _ { i j }$ ​ represents the attention scores row vector computed on the j-th KV block [1].  

--cache-max-entry-count--cache-max-entry-count--cache-max-entry-count--cache-max-entrycount--cache-max-entry-count--cache-max-entry-count--cache-max-entry-count--cache-max-entrycount--cache-max-entry-count--cache-max-entry-count--cache-max-entry-count--cache-max-entrycount--cache-max-entry-count--cache-max-entry-count--cache-max-entry-count--cache-max-entrycount--cache-max-entry-count--cache-max-entry-count--cache-max-entry-count--cache-max-entrycount--cache-max-entry-count -max-entry-count\`, also represent a system-level knob influencing memory usage and inference speed, presenting a clear trade-off between the two [15]. Furthermore, for supporting extremely long contexts, distributed KV cache management across multiple nodes or data centers, as explored in systems like Infinite-LLM, becomes necessary [14].  

Analyzing the trade-offs between memory usage and performance is crucial [2]. Techniques that aggressively reduce KV cache size, such as selective caching or quantization, can free up significant memory and potentially reduce latency by decreasing memory traffic [2]. However, they must be carefully designed to avoid degrading model accuracy. PagedAttention improves memory utilization efficiency, which directly translates to higher throughput by allowing larger batch sizes or longer sequences within the same hardware constraints [1,7]. Prefix sharing techniques also boost throughput by reducing redundant computation and memory usage for common prefixes [4]. Ultimately, the choice and combination of KV cache optimization techniques depend on the specific constraints of the deployment environment and the desired balance between memory efficiency, throughput, and model accuracy.  

# 4.3.2 Operator Fusion & Graph Optimization  

Efficient serving of generative large language models necessitates optimization techniques to mitigate computational and memory overheads. Operator fusion and graph optimization are key strategies employed for this purpose [6,27].  

Operator fusion operates by merging multiple individual computational kernels or operators into a single, more complex kernel [6,25,26]. The primary principle behind this technique is the reduction of overhead associated with frequent kernel launches and memory accesses between successive operations [6,25,26]. By consolidating operations, operator fusion reduces the number of times data must be read from and written to memory, thereby improving data locality and accelerating computation speed [6]. This is particularly critical in deep learning models, especially large Transformers, where numerous small operations, such as matrix multiplications (GEMM) and normalization layers (LayerNorm, RMSNorm), can create throughput bottlenecks [6,23]. Common candidates for fusion in LLM serving include operations within attention mechanisms (e.g., FlashAttention) and key-value caching (KVCache), as well as standard layers like LayerNorm and RMSNorm [6,23].​  

Graph optimization is a broader category that encompasses operator fusion as one of its techniques [27]. It involves applying various transformations to the computation graph of the neural network model to improve efficiency before execution. Besides operator fusion (Op fusion), graph-level optimizations include transformations such as Batch Normalization folding (BN fold), Constant folding, and other computation graph equivalence transformations [27]. These transformations aim to simplify the graph, reduce redundant computations, and optimize the execution flow.​  

Strategies for implementing operator fusion and graph optimization involve both automated and manual approaches [25]. Compilers and deep learning frameworks can automatically identify patterns of compatible operators in the computation graph and fuse them [25]. Alternatively, performance engineers can manually write custom kernels that combine the logic of multiple operations for specific hardware architectures or critical model components [25]. Prominent serving systems leverage these techniques; for instance, NVIDIA's FasterTransformer employs graph fusion to merge multiple layers into single kernels, and DeepSpeed Inference also utilizes kernel fusion to accelerate parts of Transformer layers [23,26]. Transformer Kernel Fusion, specifically, is applied to alleviate bottlenecks arising from numerous small GEMM and LayerNorm operations within Transformer architectures [23].  

# 4.3.3 Optimized Kernels & Libraries  

A fundamental approach to accelerating large language model inference involves leveraging highly optimized kernels and libraries tailored for efficient computation on specific hardware. These libraries and kernels aim to improve performance by optimizing common neural network operators as well as the specific operations dominant in Transformer architectures, such as attention [6,23].​  

Several high-performance acceleration libraries are utilized to optimize the computational throughput of neural network operations. Examples include ONNX Runtime, TVM (Tensor Virtual Machine), and cuBLAS [6]. TVM, for instance, provides a framework for optimizing and deploying models across diverse hardware platforms [6,27]. Libraries like MLC LLM, built upon Apache TVM Unity, offer solutions specifically targeting deployment on mobile and web platforms [26]. Other notable libraries include MNN (Alibaba) and OpenVINO (Intel), catering to various hardware ecosystems [27].​  

For Transformer-specific optimizations, libraries like FasterTransformer are designed using CUDA and highly optimized operator libraries to significantly improve inference speed [6]. The DeepSpeed Inference module also employs optimized kernels to accelerate inference, particularly for Transformer networks [11,23,26]. NVIDIA's TensorRT is another prominent library for optimization [27], with TensorRT-LLM specifically targeting LLMs by utilizing optimized kernels based on techniques like FlashAttention and leveraging NVIDIA TensorRT primitives [24,25]. The Friendli DNN Library also contributes optimized GPU kernels for generative AI workloads [16].  

Kernel-level optimizations, often implemented as custom CUDA kernels, play a crucial role in enhancing efficiency. These kernels are meticulously crafted to exploit hardware parallelism and memory hierarchies. Examples include optimized CUDA kernels used in libraries like DeepSpeed, vLLM, and Text Generation Inference [11]. Specific operations such as FlashAttention and PagedAttention, critical for managing attention computation and memory in large contexts, rely on highly optimized CUDA kernel implementations, which are integrated into systems like Lorax [7,14,25]. Furthermore, enginespecific kernels, such as the efficient 4-bit inference CUDA kernels provided by the TurboMind engine, contribute to performance gains for quantized models [15].​  

Kernel fusion is a key optimization technique where multiple operations are combined into a single kernel launch, reducing kernel launch overhead and improving data locality. BatchLLM, for instance, implements a fused attention kernel using OpenAI Triton to mitigate long-tail effects and reduce overhead compared to separate kernel calls [4]. This fusion is essential for optimizing the sequence of operations within Transformer blocks.  

While these libraries and kernels offer significant performance improvements by optimizing both common neural network operators and specific Transformer operations, the extent of their performance gains can vary across different hardware platforms. Libraries like TVM and OpenVINO are designed for portability and performance on diverse hardware, including non-NVIDIA platforms [26,27], whereas CUDA-based libraries like FasterTransformer, cuBLAS, cuSPARSELt, and TensorRT are primarily optimized for NVIDIA GPUs [6,27].  

Note: No mathematical formulas were present in the original content. If any formula were to be rendered by KaTeX, ensure that its syntax is valid, with correct nesting of parentheses, brackets, and other delimiters, and replace non-KaTeXsupported macros with their KaTeX-compatible counterparts.  

![](images/54b938350b72d14cde9dae57f154dfec00be17a5da9ffc346552343b25e8bc2f.jpg)  
4.3.4 Decoding Strategy Optimizations (Speculative Decoding)  

Decoding strategies significantly influence the efficiency of large language model (LLM) inference [8]. Among these, speculative decoding has emerged as a prominent technique for accelerating the decoding process, primarily through enabling parallel processing [5].  

Speculative decoding operates on a generate-then-verify paradigm [9]. It employs a smaller, faster draft model to predict a sequence of potential future tokens. These predicted tokens are then evaluated in parallel by the larger target LLM [5,9,26]. The core principle involves two main stages: speculation (or drafting) and verification [5,9]. In the speculation stage, the draft model generates a number of tokens ahead of the current position. In the verification stage, the target LLM checks the validity of these proposed tokens simultaneously. Accepted tokens are appended to the output sequence, and the process continues from the last accepted token [8]. This approach effectively reduces the number of sequential autoregressive steps performed by the computationally expensive large model, as multiple tokens can be validated in a single forward pass [8], thereby increasing inference speed [6,9] and reducing overall computational complexity [6].  

Several methods build upon the speculative decoding framework to further enhance performance. Techniques like EAGLE [6] and Medusa [6,26] leverage autoregressive prediction to accelerate inference. Specifically, Medusa utilizes multiple decoding heads to predict tokens at different offsets, incorporating a tree attention mechanism to select optimal candidates [26]. Speculative decoding is versatile and can support various standard decoding algorithms, including parallel sampling, beam search, shared prefixes, and other hybrid methods [7]. Optimizations such as AtSpeed-S and AtSpeed-R have been proposed to tailor speculative decoding for different verification strategies, aligning sequences under strict top-K verification or relaxed sampling verification respectively [22].  

Despite its advantages, speculative decoding presents challenges, particularly concerning the verification stage. Applying speculative decoding to tasks requiring stringent quality control, such as generative recommendation, necessitates careful consideration of the verification process to ensure the fidelity of the generated output [22].  

# 4.3.5 Dynamic Batching  

Dynamic batching is a fundamental technique employed in efficient LLM serving systems to enhance throughput and GPU utilization [26]. Instead of processing requests in fixed-size batches, it allows for flexible adjustment of the batch size during runtime, typically at each iteration of the generation process [26]. This dynamic adjustment enables systems to keep the GPU occupied with computation, adapting to the varying progress of different requests and the completion of sequences within the batch [11,26]. Systems designed with efficient memory management, such as block-level allocation, and preemptive scheduling mechanisms inherently support and benefit from dynamic batching [7].  

A key trade-off associated with dynamic batching is the potential conflict between maximizing throughput and minimizing latency for individual requests [8,26]. While larger dynamic batches increase overall system throughput by improving GPU utilization, they can lead to increased waiting times for individual requests before they are processed or before their turn within a batch, thereby increasing latency [8]. Techniques like preemptive scheduling are often integrated to mitigate this latency increase by prioritizing certain requests or allowing urgent ones to interrupt ongoing processing [26].  

A prominent strategy within dynamic batching is known as contiguous batching [6,16]. This technique is also referred to as iteration batching or continuous batching [16]. Contiguous batching aims to maintain a continuous flow of processing by dynamically determining the batch size at each iteration, allowing new sequences to be seamlessly integrated as others complete [11]. This iteration-level flexibility maximizes GPU utilization compared to static batching approaches [11,14]. By sustaining continuous batch processing, contiguous batching significantly reduces the overhead associated with context switching and memory scheduling, leading to improved inference efficiency [6]. FriendliAI is credited with developing iteration batching [16], and systems like Orca, Lorax, and Splitwise utilize continuous batching to enhance performance [11,14]. Furthermore, some approaches like BatchLLM implement throughput-oriented token batching strategies that consider KV memory constraints and manage different types of tokens (common prefix, private prompt, decoding) via dedicated queues to optimize the mixing of prefill and decoding steps and increase the overall token batch size, demonstrating variations in dynamic batching focused on token processing efficiency [4].​  

# 4.4 Hardware Acceleration  

<html><body><table><tr><td>Hardware Type</td><td>Description</td><td>Pros</td><td>Cons</td><td>Primary Use Case for LLMs</td></tr><tr><td></td><td>purpose</td><td>adoption, Rich ecosystem</td><td>performance/w att vs</td><td>deployment, Research</td></tr><tr><td></td><td>designed for deep learning</td><td>tensor ops, High perf/watt</td><td>availability (primarily GCP)</td><td>cloud inference</td></tr><tr><td></td><td>hardware logic</td><td>Customizable for specific tasks</td><td>development, Lower peak perf</td><td>applications, Edge with</td></tr><tr><td></td><td>for specific tasks</td><td>performance/po wer efficiency</td><td>High development</td><td>High-volume, Fixed-function tasks, Edge</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td></td><td>cost, No flexibility</td><td></td></tr><tr><td>Tensor Cores</td><td>Specialized units within GPUs</td><td>Accelerate mixed- precision matrix ops</td><td>Specific to certain GPUs, Precision dependent</td><td>Accelerating GEMM in Prefill/Decode</td></tr><tr><td>PIM</td><td>Processing-in- Memory</td><td>Brings computation closer to memory</td><td>Emerging technology, Limited support</td><td>Addressing bandwidth bottlenecks</td></tr></table></body></html>  

Efficient serving of large language models (LLMs) heavily relies on specialized hardware acceleration. This involves leveraging heterogeneous computing architectures beyond general-purpose CPUs to meet the demanding computational and memory requirements of LLM inference [27]. Key hardware platforms utilized for accelerating deep learning models and LLMs include Graphics Processing Units (GPUs), Tensor Processing Units (TPUs), Field-Programmable Gate Arrays (FPGAs), and Application-Specific Integrated Circuits (ASICs) [26,27]. These specialized units are crucial for accelerating matrix operations, which are fundamental to transformer models [8,32].  

Comparing these hardware types, GPUs are widely adopted due to their versatility and strong support ecosystem, exemplified by the frequent use of NVIDIA GPUs and CUDA kernels in frameworks like TensorRT-LLM [24]. Specific NVIDIA architectures such as Turing, Ampere, and Ada Lovelace are supported by various optimization techniques like W4A16 quantization [15]. However, achieving optimal performance is highly sensitive to the underlying GPU architecture [30]. TPUs, developed by Google, are designed specifically for deep learning workloads and are often necessary for large model inference due to their optimized architecture for tensor computations [8,32]. FPGAs offer flexibility, allowing customizability of the hardware logic, while ASICs provide the highest potential performance and power efficiency for a specific task but lack flexibility and incur high development costs. The choice among these accelerators involves trade-offs between performance, power consumption, and cost [27]. While GPUs offer a balance of performance and flexibility, TPUs excel in performance per watt for specific workloads, and ASICs target maximum efficiency for fixed tasks.  

A significant advancement in accelerating matrix multiplications, particularly relevant for LLMs, is the use of specialized processing units like NVIDIA Tensor Cores [3,25]. These cores are designed to perform mixed-precision matrix operations efficiently, significantly accelerating compute-bound phases of LLM inference by leveraging lower precision formats like FP16 or INT8 [3].​  

Beyond the core processing units, memory solutions also play a critical role. Advanced memory technologies, such as Samsung HBM2-PIM (High Bandwidth Memory 2 with Processing-in-Memory), are being explored to improve performance on LLM inference by bringing computation closer to memory [12,35]. Designing AI accelerators should prioritize improving the computation/bandwidth trade-off, even if it means sacrificing peak compute performance, by considering alternative architectures with more efficient cache hierarchies and larger DRAM capacities [10].​  

Hardware acceleration is often integrated within broader system-level optimizations, which can encompass solutions utilizing single or multiple GPUs, I/O-based approaches, heterogeneous computing, and even SSD-based solutions to address data movement bottlenecks [2]. Despite the significant performance gains offered by hardware improvements [27], a key challenge remains in effectively programming and optimizing LLMs for the diverse landscape of available hardware platforms [27]. This requires sophisticated software frameworks and techniques tailored to extract maximum efficiency from each specific hardware architecture.  

# 4.5 Hybrid Approaches  

The pursuit of greater efficiency in serving large language models often necessitates the integration of multiple optimization techniques. Combining different model compression and acceleration strategies can yield significantly enhanced compression and acceleration ratios compared to employing single methods in isolation [27]. This hybrid approach leverages the synergistic potential of various techniques to address different bottlenecks in the serving pipeline. For instance, the combination of Network Slimming, a compression technique, with TensorRT INT8 optimization resulted in a substantial reduction in inference time with a minimal compression ratio on an older network [27].  

Hybrid approaches manifest in various forms. At the architectural and model layer level, techniques can be combined, such as integrating Mixture-of-Experts (MoE) architectures with low-rank decomposition, or pairing model quantization with weight sparsity or low-rank decomposition [5]. Similarly, weight pruning can be combined with low-rank decomposition [5]. These combinations demonstrate the potential for deeper model optimization through multi-faceted compression [5].  

Quantization, a common technique for reducing model size and computational cost, can also be effectively combined with others. Pre-training on a specific domain before applying quantization can help mitigate the quality degradation often associated with reduced precision [11]. Furthermore, performance can be optimized by combining FP8 quantization with specific compiler parameters, such as the \texttt{--use_fp8_context_fmha} flag in certain environments [24]. Beyond model parameters, hybrid approaches are crucial at the system and algorithmic level. For example, the vLLM system integrates the PagedAttention algorithm with block-level memory management and preemptive request scheduling to achieve efficient serving [7].​  

Combining different parallel execution techniques also represents a powerful hybrid strategy for building highly efficient and scalable LLM serving systems [20]. By judiciously blending tensor parallelism, pipeline parallelism, and data parallelism, significant performance gains can be achieved, as demonstrated by examples for LLaMA 3-8B and Mistral $8 ^ { \star } 7 \mathsf { B }$ models scaled across 64 GPUs [20].​  

However, integrating different techniques presents notable challenges [27]. Compatibility issues between methods, the increased complexity in implementation and tuning, and the potential for negative interactions that could negate individual benefits are significant concerns [27]. A key challenge, particularly when integrating quantization, is maintaining model accuracy [36]. Strategies to overcome this include partial quantization (sparing sensitive layers) and mixed-precision quantization (assigning different precision levels to different layers) [36]. Data-free techniques like weight equalization and bias correction can also aid in post-quantization accuracy recovery [36]. For models like BERT, where a single sensitive layer is not prominent, adapting approaches becomes necessary, potentially involving quantization-aware training or modifications to activation functions like GELU [36]. The complexity of selecting the optimal combination and configuration, especially for methods like mixed-precision quantization, can be addressed through automated search methods, including reinforcement learning, genetic algorithms, Neural Architecture Search (NAS), or second-order sensitivity analysis [36]. Overcoming integration challenges often requires careful empirical tuning and sophisticated optimization frameworks capable of managing the interplay between different techniques [27].  

# 5. System-Level Optimization & Architectures  

System-level optimization plays a pivotal role in enhancing the overall efficiency of serving generative large language models (LLMs) by focusing on the forward inference process and the overarching service infrastructure [5,19]. Given the immense scale and computational demands of modern LLMs, effectively deploying them for inference requires addressing challenges related to latency, throughput, memory constraints, and resource utilization. System-level optimizations encompass a suite of techniques aimed at improving how models are loaded, executed, managed, and scaled across hardware resources to deliver high performance and cost-effectiveness [29].  

![](images/cd56726f05088245b8e1db73c58fc4cabbeb0a640fa1bad266daf0e4d2a8e604.jpg)  

This section delves into key areas of system-level optimization and the architectural designs that facilitate efficient LLM serving. These areas, explored in detail in the following sub-sections, include the development and utilization of specialized inference frameworks and engines designed for accelerated execution; strategies for distributed computing and parallelism to handle models exceeding single-device capacity; advanced memory management techniques beyond standard KV cache handling; intelligent resource management and scheduling algorithms for optimal hardware utilization and request processing; methods for multi-tenancy and model sharing to improve resource efficiency in multi-user or multi-model scenarios; approaches for handling heterogeneous hardware environments and complex, dynamic workloads; and dedicated strategies for cost optimization. Collectively, these system-level efforts form the backbone of efficient LLM serving, enabling robust, scalable, and economical deployment solutions in diverse environments [13,14].  

# 5.1 Inference Frameworks & Engines  

Efficient serving of Large Language Models (LLMs) relies heavily on specialized inference frameworks and engines designed to optimize performance, reduce latency, and increase throughput. These tools provide the necessary infrastructure and algorithms to execute pre-trained models efficiently on various hardware [8,27].  

Several prominent inference frameworks and engines have emerged to address the unique challenges of LLM serving. NVIDIA's TensorRT is a widely used library focused on optimizing and accelerating deep learning inference specifically on NVIDIA GPUs [19,27]. TensorRT supports optimizing models from various deep learning frameworks and facilitates deployment on GPUs [27]. It includes support for model quantization, detailing various quantization schemes and calibrators [36]. An extension, TensorRT-LLM, is specifically tailored for LLMs, accelerating operations like Multi-Head Attention through hardware acceleration and operator optimization to reduce matrix computation costs [6]. TensorRT-LLM is also mentioned as facilitating the use of advanced inference techniques [20] and integrating with model optimizers and AOT compilers [25].​  

Serving frameworks like NVIDIA Triton Inference Server provide a flexible platform for deploying models. Triton supports multiple frameworks, including TensorFlow, ONNX Runtime, PyTorch, and TensorRT, making it adaptable for both CPU and GPU workloads [33]. Triton can utilize TensorRT-LLM as a backend inference engine for optimized LLM serving [24]. ONNX (Open Neural Network Exchange) serves as an open format enabling model interchange between frameworks, and models can be converted to ONNX for acceleration, including in TensorRT [19]. ONNX Runtime itself supports dynamic and static quantization, with acceleration on GPUs using Tensor Cores [36].  

Other specialized inference engines prioritize high throughput and efficient resource utilization. vLLM is highlighted as a high-throughput LLM service system and inference engine [7,17,41]. A key optimization employed by vLLM is PagedAttention, an efficient method for managing attention keys and values, which demonstrably provides higher throughput compared to standard implementations like HuggingFace Transformers without requiring model architectural changes [11]. vLLM supports various popular LLMs, including those exceeding the capacity of a single GPU [1], offers continuous batch inference, and features an extensible KV cache manager [15]. Systems like BatchLLM have been implemented on top of vLLM, utilizing its features like Prefix-Caching and Chunked-Prefill [4]. vLLM is also listed among tools facilitating advanced techniques [20].  

DeepSpeed provides solutions for accelerating transformer inference, referred to as DeepSpeed Inference [11,23,26]. Hugging Face's Text Generation Inference (TGI) is another prominent serving framework and inference engine, often used as a base for other systems like Lorax [8,11,13,14,20,26]. Friendli Inference is presented as a solution focusing on fast, efficient, and reliable generative AI inference [16].  

Beyond hardware-specific or high-throughput engines, frameworks exist targeting broader hardware support or specific compilation strategies. TVM utilizes Relay IR and a defined search space for automated compilation optimization, acting as an inference framework [27]. MNN supports various deep learning models with efficient operators, particularly optimized for mobile deployment and listed as an on-device framework [12,27,35].  

Operator optimization, including attention operators and linear operators, represents a critical class of methods implemented within these inference engines [5]. For instance, MindelIE-LLM enhances performance in multi-task parallel and long-sequence inference by leveraging attention variants like Grouped-Query Attention (GQA) and Multi-Query Attention (MQA) [6]. Other mentioned frameworks and engines include LMDeploy (which can use its engine TurboMind or PyTorch) [15], Accelerate, DeepSpeed MII, OpenLLM, Aviary, AlpaServe, Splitwise, Infinite-LLM, Mélange, MuxServe, Helix, FasterTransformer, Ollama, llama.cpp, PowerInfer, ExecuTorch, MediaPipe, MLC-LLM, and CUDA MPS [11,12,13,14,26,35,41]. These frameworks collectively employ a range of techniques, including graph optimization, operator optimization, kernel fusion, memory management like PagedAttention, compilation optimization, and hardware acceleration, to improve the efficiency of LLM inference across diverse hardware platforms [5,6,11,27].  

# 5.2 Distributed Computing & Parallelism  

![](images/500cdcc4f6595287fc33657063d9e8a61669c233cde8d0615b8584a4c2e59862.jpg)  

Efficiently serving Large Language Models necessitates the use of distributed computing and parallelism techniques, primarily due to the immense scale of these models which often exceed the memory and computational capabilities of a single device [6,17,19]. Distributed inference involves distributing the computation task across multiple machines or GPUs to enhance speed and manage concurrent requests [19].  

Model parallelism is a crucial strategy for distributing the model parameters themselves across multiple devices, particularly when the model does not fit into the memory of a single GPU [11,17,19]. Several distinct approaches exist within model parallelism:​  

• Tensor Model Parallelism (TP) --tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size--tp_size [24] or tptp \` in LMDeploy [15],and methodologies like "weight-gathered" partitioning [25].  

• Pipeline Model Parallelism (PP): This method distributes groups of model layers across different GPUs, forming stages in a pipeline [8,20,39]. Requests are processed sequentially through these stages, although multiple micro-batches can be processed concurrently across different stages to improve hardware utilization [26,39]. Key considerations for PP include achieving computational balance between stages and optimizing memory usage across devices [20]. PP is particularly effective for supporting models that are too large for tensor parallelism alone and can improve overall device utilization compared to pure tensor parallelism [26].  

• Expert Parallelism (EP): Primarily used in Mixture-of-Experts (MoE) models, EP divides the model into multiple expert sub-networks [20]. A gating network determines which specific experts are activated and utilized for each given input [20]. This selective activation means that only a subset of the total parameters is processed for any single input, significantly reducing the computational load per token compared to a dense model of equivalent capacity [20].  

Hybrid parallelism combines different model parallelism techniques, such as tensor and pipeline parallelism, to leverage the benefits of each approach [39].  

<html><body><table><tr><td>DataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataPar</td></tr><tr><td>allelDataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataParalelDat</td></tr><tr><td>aParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataParalle</td></tr><tr><td>lDataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataPar</td></tr><tr><td>allelDataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDataParallelDat</td></tr><tr><td>aParallelDataParallel andDistributedDataParallel` facitatedataparalelism[19].FullySharded Data Paralelism</td></tr><tr><td>(FSDP)isavariantthatshards modelparameters,gradients,andoptimizerstatesacrossdevices,enablingtheserving (or</td></tr><tr><td>training)ofmodels that wouldotherwisenotfitonasingleGPUbydistributingthememoryfootprint[1128,29].FSDPcan</td></tr></table></body></html>  

A critical challenge in distributed LLM serving is the communication overhead between devices [10,40]. As heterogeneous acceleration equipment performance increases and model sizes grow, the frequency and data scale of parameter communication rise significantly [40]. This makes parameter communication overhead a considerable performance bottleneck in large-scale parallel systems [40]. Efficient partitioning strategies, such as those used in tensor parallelism, and optimized collective communication patterns are essential to minimize the volume of data that needs to be exchanged between devices [25]. The challenge lies in overcoming technical limitations in scaling network bandwidth, as distributed work can sometimes face communication bottlenecks that are less efficient than on-chip data movement [10].  

Various distributed computing frameworks and specialized LLM serving systems have been developed to manage and optimize these parallelism techniques. Ray is one such general-purpose distributed computing framework that can be utilized for orchestrating distributed workloads [41]. Specialized systems like AlpaServe employ model parallelism, specifically tensor and pipeline parallelism, to achieve statistical multiplexing across multiple devices, often using a centralized controller to dispatch requests to groups of model replicas [13,14]. Splitwise leverages data parallelism by using separate node pools, potentially with different GPU types, for processing prompt and token phases of generation [13]. Infinite-LLM introduces custom distributed algorithms, such as DistAttention, for the distributed processing and storage of the KV cache, a key component in sequence generation [13,14]. Helix addresses the complexity of heterogeneous hardware environments by formulating the inference problem as a max-flow problem on a graph, optimizing model placement and request scheduling across the distributed system based on available GPU and network connections [13,14]. MindIE-LLM utilizes model sharding to distribute tasks across multiple devices, specifically employing Prefill-Decode (PD) separation to execute these distinct stages in parallel on different devices [6]. VLLM is designed as a distributed LLM service engine, utilizing a centralized scheduler to coordinate execution across distributed GPU worker nodes [1]. The choice and configuration of these parallelism strategies and distributed systems significantly impact the energy efficiency and overall performance of LLM inference serving [30].  

# 5.3 Memory Management (Beyond KV Cache)  

Efficient GPU memory management is critical for serving large language models, particularly regarding the KV cache, which grows with sequence length. Key challenges include memory fragmentation and over-allocation, which can significantly limit the number of concurrent requests or batch size [17]. The substantial size of KV caches—especially in scenarios involving long contexts—poses a significant challenge by potentially limiting concurrency and impacting throughput due to operations like context switching between high-bandwidth memory (HBM) and CPU memory [13].​  

To address these challenges, techniques such as PagedAttention have been introduced to reduce fragmentation and improve overall throughput [7,17]. PagedAttention manages the KV cache by organizing it into fixed-size blocks, analogous to paging in operating systems [1]. This block-based approach allows logical blocks representing the KV cache for a sequence to be mapped dynamically to physical blocks in GPU memory [1]. Such dynamic mapping enables the KV cache to grow without requiring pre-reservation of contiguous memory and facilitates sharing between different sequences, thereby improving memory utilization and reducing fragmentation [1,7].​  

In scenarios where GPU memory capacity is insufficient, strategies such as swapping and recomputation are employed [17]. Swapping involves temporarily moving KV cache data from GPU memory to CPU memory when the GPU memory is fully utilized [7,17], which frees up valuable GPU memory but incurs latency and overhead associated with data transfer [17]. Alternatively, recomputation involves recalculating the necessary KV cache data on demand instead of storing the entire history [7,17], reducing the memory footprint at the expense of increased computational load and latency [17]. Both swapping and recomputation are integrated into scheduling and preemption mechanisms to manage memory pressure. Additionally, memory-centric batching strategies—for example, those employed by BatchLLM—take KV memory usage into account to maximize the number of tokens that fit within GPU memory limits, thereby mitigating the need for excessive swapping and recomputation [4].  

kv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionk v_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_ cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_ca che_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cach e_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_ free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_fr ee_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free_gpu_mem_fractionkv_cache_free _gpu_mem_fractionkv_cache_free_gpu_mem_fraction tion\` [24]. Furthermore, guidance on calculating GPU memory requirements is available to assist with deployment decisions [17].  

# 5.4 Resource Management & Scheduling  

Efficient resource management and scheduling are paramount for optimizing the performance of generative large language models (LLMs) in serving scenarios. These techniques directly influence hardware utilization and request latency, which are critical metrics for cost-effectiveness and user experience [31]. Dynamic scheduling and effective load balancing are foundational approaches to improve resource utilization by adapting to fluctuating workloads and distributing requests optimally across available computational resources. By dynamically allocating resources and sequencing requests, inference systems can mitigate bottlenecks and reduce queuing delays, thereby lowering end-to-end latency. Research in this domain explores new directions for optimizing large model performance through refined computer system resource scheduling techniques [31].​  

Addressing the inherent challenges in LLM serving, such as variable request lengths, unpredictable arrival patterns, and significant memory requirements (particularly for KV cache), necessitates sophisticated methods. Various approaches have been proposed to overcome these hurdles. For instance, resource provisioning can leverage advanced techniques like Distributed Reinforcement Learning (DRPC) for scalable resource allocation in dynamic environments such as containerbased clusters [37]. Additionally, lightweight frameworks based on gradient descent, like LSRAM, have been developed for autoscaling and ensuring Service Level Objectives (SLOs) for microservices, principles applicable to modular LLM serving architectures [37]. Dedicated resource provisioning strategies, such as those offering exclusive access to high-performance GPU resources with auto-scaling, aim to ensure consistent performance and availability without resource contention [16].  

MAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONM AX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX _UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_U TILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTI LIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILIZATIONMAX_UTILI ZATIONMAX_UTILIZATIONMAX_UTILIZATION to maximize resource usage and GUARANTEED_NO_EVICT D_NO_EVICT\` to protect specific requests from being preempted [24].  

Several systems implement diverse scheduling and resource management strategies. BatchLLM utilizes a prefix-groupingbased scheduling strategy to consolidate requests sharing common initial tokens, reducing redundant computation. It also employs a request reordering scheme that prioritizes requests with longer decoding lengths based on the ratio of prompt length to decoding length [4]. DeepSpeed Inference integrates fine-grained scheduling, referred to as Low Latency Scheduling, to manage operations at a granular level and minimize execution delays [23]. Other systems utilize distinct architectural approaches: AlpaServe uses a centralized controller for dispatching requests across resources [13,14], while Punica employs a dedicated scheduler to distribute requests specifically to GPUs [13,14]. SpotServe incorporates a request manager responsible for dividing requests into batches before assigning them to inference instances [13]. Splitwise adopts a hierarchical two-level scheduling approach, involving cluster-level routing and machine-level queue and batch management [13,14]. MuxServe features a unified resource manager enabling flexible multiplexing and employs an adaptive batching scheduling strategy [13,14]. Helix utilizes a coordinator with a request scheduler to assign per-request pipelines and manage processing across compute nodes [13,14]. Furthermore, novel approaches like employing a matching game theory framework can be used for resource management and scheduling, allowing model layers and computational nodes to express preferences for allocation [21]. Collectively, these diverse scheduling and resource management techniques demonstrate ongoing efforts to optimize LLM serving performance by efficiently utilizing hardware resources and minimizing request latency under dynamic conditions.  

# 5.5 Multi-Tenancy & Model Sharing  

Multi-tenancy serves as a crucial strategy to enhance resource utilization when serving large language models, particularly when managing numerous fine-tuned variants or multiple concurrent users. This paradigm allows different requests or users to share underlying hardware and model resources, significantly improving efficiency compared to provisioning dedicated instances for each task or fine-tuned model. A prominent application of multi-tenancy in efficient LLM serving involves sharing a single pre-trained base model among multiple Low-Rank Adaptation (LoRA) models.  

Systems specifically designed for multi-tenant LoRA serving aim to minimize the overhead associated with loading and switching between different LoRA adapters while maximizing the utilization of the shared base model's computation. Punica and S-LoRA are notable examples engineered as multi-tenant systems capable of serving a multitude of distinct LoRA models atop a shared pre-trained base model [13,14]. This architecture leverages the fact that LoRA adapters constitute a small fraction of the total model parameters, allowing the computationally expensive base model to be loaded only once and shared across numerous simultaneous inference tasks, each potentially using a different LoRA adapter.  

Another system supporting multi-tenant LoRA scenarios is Lorax, which is based on Hugging Face TGI [14]. Lorax enhances efficiency in multi-tenant settings by incorporating features such as dynamic LoRA model loading and continuous batching [13]. Dynamic loading allows the system to load LoRA weights just in time based on incoming requests, avoiding the need to keep all potential LoRA adapters in memory simultaneously. Continuous batching further optimizes throughput by allowing new requests to be added to the processing batch as soon as the hardware becomes available, rather than waiting for the entire batch to complete. The concept of multi-LoRA serving is recognized more broadly as an efficient approach for deploying various fine-tuned models [16].  

Underpinning such multi-tenant systems are efficient memory management techniques. For instance, mechanisms like PagedAttention, employed in systems like vLLM, facilitate memory sharing at a fine granularity (block level) across different sequences and requests [7]. While not directly focused on sharing LoRA weights, this capability for efficient Key-Value (KV) cache management and sharing across concurrent requests within a multi-tenant environment significantly contributes to overall throughput and a reduced memory footprint [1,7]. By enabling effective resource pooling and sharing, multitenancy, supported by specialized systems and efficient underlying infrastructure, offers a powerful approach to scaling LLM serving economically.​  

# 5.6 Handling Heterogeneous Hardware & Complex Workloads  

Serving large language models efficiently presents significant challenges, particularly in environments characterized by heterogeneous hardware configurations and complex, variable workloads. Heterogeneous computing, involving diverse processing units such as different generations or types of GPUs, is recognized as a system-level hardware acceleration strategy for LLMs [2]. Furthermore, models exceeding the capacity of a single GPU necessitate multi-GPU deployments, which can involve heterogeneous hardware [7]. Addressing these complexities requires specialized system designs.  

Several systems have been developed to manage these challenges. Splitwise is specifically designed to utilize heterogeneous GPU clusters by assigning different GPU types to the distinct prefill and decoding stages of the LLM inference process [13,14]. This separation allows the system to leverage the strengths of different hardware units for the computationally distinct phases of request processing.​  

Another system, Mélange, focuses on minimizing LLM deployment costs by dynamically constructing the most cost-effective GPU collection for serving varying traffic types [13,14]. Mélange's strategy involves selecting hardware based on request characteristics such as size, request rate, and Service Level Objectives (SLOs) [14], effectively tailoring the heterogeneous hardware configuration to the specific demands of complex workloads.​  

Helix is designed to provide high-throughput and low-latency LLM inference within heterogeneous GPU clusters and networks [13,14]. While the digests do not detail its specific mechanisms, its stated goal indicates an approach focused on optimizing performance metrics across diverse hardware.  

Beyond cluster-level systems, heterogeneity can also be addressed at the edge, where LLM layers can be deployed across edge nodes with varying computational capacities and communication speeds, utilizing techniques like matching theory to optimize placement [21]. These different approaches—from phase-based assignment (Splitwise), cost-optimized selection (Mélange), and general performance optimization (Helix), to distributed layer deployment—demonstrate varied strategies for effectively utilizing heterogeneous hardware and accommodating the variability of complex workloads in LLM serving.  

# 5.7 Cost Optimization  

<html><body><table><tr><td>Strategy / System</td><td>Approach</td><td>Key Benefit / Example Impact</td></tr><tr><td>Cloud Instance Choice</td><td>Utilizing cost-effective instances (e.g., Spot Instances)</td><td>Reduced infrastructure cost (e.g., SpotServe saves 54%)</td></tr><tr><td>Hardware Selection</td><td>Intelligent use of heterogeneous, cost- effective GPUs</td><td>Lower deployment cost (e.g., Mélange reduces cost by 77%)</td></tr><tr><td>Processing Efficiency</td><td>Improving throughput (requests/token per time) on given hardware</td><td>Lower cost per request (e.g., PagedAttention improves throughput)</td></tr><tr><td>Specialized Solutions</td><td>Optimizing for heavy requests, requiring fewer GPUs</td><td>Efficient GPU utilization (e.g., Friendli Container)</td></tr><tr><td>Billing Model</td><td>Aligning cost with actual usage (e.g., per-token billing)</td><td>Improved cost alignment (e.g.,Friendli Serverless)</td></tr></table></body></html>  

Reducing the operational cost of serving large language models (LLMs) is a critical aspect of efficient deployment. Several strategies and systems have been developed to address this challenge. One approach involves leveraging cost-effective cloud infrastructure. For instance, SpotServe is designed to exploit the lower costs offered by preemptible instances in cloud environments [13,14]. By utilizing these ephemeral resources, SpotServe can achieve significant cost reductions, reportedly saving up to $5 4 \%$ compared to existing LLM service systems [13].​  

Another significant cost optimization vector lies in the efficient utilization and selection of heterogeneous hardware. Systems like Mélange aim to minimize LLM deployment costs by intelligently using a collection of GPUs that are costeffective for the specific demands of LLM service [13,14]. This strategy, by exploiting GPU heterogeneity, can lead to substantial savings, with Mélange demonstrating up to a $7 7 \%$ reduction in deployment costs [13].​  

Beyond hardware selection and instance types, optimizing processing efficiency directly impacts cost. Increasing throughput, or the number of requests processed per unit of time, directly translates to a lower cost per request [7]. Systems focused on improving fundamental aspects like memory management (e.g., via techniques like PagedAttention) contribute to cost efficiency by enabling higher throughput on given hardware resources [7]. Furthermore, specialized solutions like Friendli Container enhance efficiency by processing heavy requests effectively, potentially requiring fewer GPUs to handle a large scale of operations [16]. Complementary to processing efficiency, billing models can also be optimized. Friendli Serverless Endpoints, for example, adopt a per-token billing model, aligning costs more closely with actual usage [16].  

# 6. Deployment Strategies  

Deploying Large Language Models (LLMs) efficiently is crucial for making them accessible and cost-effective across various applications and workloads. The choice of deployment strategy significantly impacts factors such as latency, cost, security, and scalability [30,31]. Primarily, deployment strategies can be broadly categorized into cloud-based and edge-based approaches.​  

Cloud-based deployment involves running LLMs on remote servers managed by cloud providers. This approach is suitable for demanding workloads requiring significant computational resources and scalability. Infrastructure requirements typically include high-performance GPUs, large amounts of memory, and robust network connectivity [31]. Cloud platforms offer various deployment options, including standard API services [15] and containerized deployments using technologies like Docker [24,41] and Kubernetes [41]. These platforms facilitate managing dependencies, ensuring consistent environments, and scaling inference services. Specific cloud services provide managed endpoints, simplifying deployment with both no-code and full-code configurations [33]. Furthermore, strategies exist to optimize cloud resource utilization and cost, such as leveraging preemptible instances (SpotServe) [13,14] and utilizing frameworks like SkyPilot for cost-effective, multi-cloud deployments [13,14]. Techniques like dynamic adjustment of multi-GPU pools via platforms like TrueFoundry also help reduce costs [13,14]. Secure environment deployments, such as those offered by Friendli Container, provide additional control and security within the cloud infrastructure [16].  

In contrast, edge or on-device deployment focuses on running LLMs directly on user devices or edge infrastructure, such as mobile phones [5,12]. This strategy offers significant advantages, including reduced inference latency due to proximity to the user, enhanced data privacy as data remains on the device, and the ability to function offline [12,30,35]. However, edge deployment presents substantial challenges primarily related to the resource constraints of edge devices, including limited computational power, memory, and energy [12,35]. Overcoming these challenges involves developing techniques focused on deploying models with billions of parameters on mobile devices [5]. Two main approaches are pursued: designing smaller LLMs through techniques like direct training or model compression, and implementing system-level optimizations such as operator fusion and efficient memory management [5]. For heterogeneous edge networks, specialized deployment schemes like LLM layer deployment leverage matching theory to optimize computation and communication efficiency [21].  

<html><body><table><tr><td>Aspect</td><td>Cloud-Based Deployment</td><td>Edge/On-Device Deployment</td></tr><tr><td>Infrastructure</td><td>Remote servers (GPUs, memory, network)</td><td>Userdevices,Edge hardware</td></tr><tr><td>Resource Scale</td><td>High, Scalable</td><td>Limited, Constrained</td></tr><tr><td>Latency</td><td>Higher (network dependent)</td><td>Lower (proximity to user)</td></tr><tr><td>Data Privacy</td><td>Data often leaves device</td><td>Data stays on device</td></tr><tr><td>Offline Capable</td><td>Generally No</td><td>Yes</td></tr><tr><td>Model Size</td><td>Supports very large models</td><td>Requires smaller or compressed models</td></tr><tr><td>Complexity</td><td>Infrastructure management by provider</td><td>Optimizing for strict device limits</td></tr><tr><td>Cost</td><td>Variable (compute, traffic, storage)</td><td>Primarily hardware cost, Energy</td></tr><tr><td>Use Cases</td><td>Demanding workloads, Large-scale services</td><td>Offline apps,Privacy- sensitive tasks, Real-time device interaction</td></tr></table></body></html>  

Comparing cloud and edge deployments reveals distinct trade-offs. Cloud deployment excels in handling large models and high throughput with minimal infrastructure burden on the end-user, but can incur higher latency, costs, and potential data privacy concerns [31]. Edge deployment, while improving latency, privacy, and offline capabilities, is constrained by device resources, requiring significant model optimization and hardware-specific considerations [12,31,35]. Therefore, the suitability of each strategy depends heavily on the specific application's requirements for latency-sensitivity, data privacy, cost tolerance, and available computational resources [30,31]. Local deployment, essentially a form of edge deployment, where models are inferred directly on a single machine, and deployment as API services, typically cloud-based, represent specific implementations within these broader categories [15]. Infrastructure requirements for edge deployment are  

centered on optimizing model performance within strict hardware limitations, often necessitating specialized hardware accelerators and tailored system software [31].  

# 7. Performance Evaluation & Benchmarking  

<html><body><table><tr><td>Metric</td><td>Description</td><td>Importance for LLM Serving</td></tr><tr><td>Throughput</td><td>Requests or tokens processed per unit time</td><td>Overall capacity, Cost efficiency</td></tr><tr><td>Latency</td><td>Time from request to response (e.g., TTFT, TBT)</td><td>User experience,Real-time application viability</td></tr><tr><td>Tail Latency (P99)</td><td>Worst-case latency for 99% of requests</td><td>User experience consistency, SLO adherence</td></tr><tr><td>Cost Effectiveness</td><td>Performance per dollar spent</td><td>Economic viability of deployment</td></tr><tr><td>GPU Resource Util.</td><td>Percentage of GPU compute/memory used</td><td>Efficiency of hardware usage</td></tr><tr><td>Energy Consumption</td><td>Energy used per request or per token</td><td>Sustainability, Operational cost</td></tr><tr><td>Inference Speed</td><td>Tokens or words generated per second</td><td>Direct measure of generation rate</td></tr><tr><td>Arithmetic Intensity</td><td>FLOPs per byte of data transferred</td><td>Bottleneck analysis (Compute vs Memory)</td></tr><tr><td>Token Reuse Eff.</td><td>How effectively KV cache prefixesare reused</td><td>KV cache memory efficiency (e.g., BatchLLM)</td></tr></table></body></html>  

The efficient serving of generative large language models necessitates rigorous performance evaluation and benchmarking across various dimensions. Key metrics employed to quantify efficiency include throughput, latency (particularly tail latency such as P99), cost effectiveness, GPU resource utilization, energy consumption, token reuse efficiency, and inference speed measured in tokens or words per second [1,4,7,13,14,15,16,25,30]. Specialized metrics like end-to-end service time excluding cooldown (TECD) have been introduced for evaluating large-scale batch processing scenarios [4]. Performance can also be analyzed in terms of arithmetic intensity, defined as the number of floating-point operations (FLOPs) per byte of data transferred, which is instrumental in understanding computational bottlenecks and leveraging the Roofline model to distinguish between memory-bound and compute-bound performance limits [10,25,27]. Evaluating energy efficiency presents unique challenges, as naive estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption [30].​  

Benchmarking results across different frameworks and systems highlight significant variations and the impact of specific optimization techniques. For instance, vLLM, which utilizes PagedAttention for efficient memory management, has demonstrated substantial throughput improvements of 2–4 times compared to existing systems such as FasterTransformer and Orca in evaluations covering various models and workloads [1,7]. These gains were particularly pronounced for longer sequences, larger models, and more complex decoding algorithms [7]. Friendli Inference reports achieving cost savings of $5 0 \mathrm { - } 9 0 \%$ , requiring 6 times fewer GPUs, and delivering throughput up to 10.7 times higher with latency reduced by 6.2 times [16]. Specific implementations like LMDeploy have shown considerable speed advantages, achieving approximately 473.690 words/s, which is six times faster than the Transformer library’s 78.675 words/s in a quantitative comparison [15].  

Numerous advanced systems have reported significant performance enhancements through diverse architectural and algorithmic optimizations [13,14]. AlpaServe, for instance, can handle up to 10 times the request rate or 6 times the burst rate while adhering to latency constraints [13]. Punica and S-LoRA have achieved throughput improvements of up to 12 times and over 4 times, respectively [13]. SpotServe focuses on tail latency, reducing P99 latency by 2.4 to 9.1 times and offering $5 4 \%$ cost savings [13]. Splitwise demonstrates trade-offs between throughput and cost, achieving 1.4 times throughput at $2 0 \%$ lower cost or 2.35 times throughput within the same budget [13]. Infinite-LLM achieves throughput gains of 1.03 to 2.4 times and supports significantly longer context lengths, up to 1900K [13]. MuxServe increases throughput by up to 1.8 times or handles 2.9 times more requests while meeting Service Level Objectives (SLOs) [13]. Helix improves service throughput by 2.7 times and reduces prefill and decoding latency by 2.8 times and 1.3 times, respectively [13]. MindIE-LLM leverages techniques like Continuous Batching to improve throughput by 3 to 4 times and parallel communication/computation to reduce communication time by approximately $8 0 \%$ in cross-node scenarios [6]. DeepSpeed Inference also includes experimental comparisons demonstrating acceleration effects compared to standard inference [23]. For generative recommendation tasks, AtSpeed reports near 2 times speedup under strict verification and up to 2.5 times speedup under relaxed sampling verification [22].​  

The impact of various factors on performance has been analyzed. The choice of numerical precision significantly affects both inference speed and memory footprint [11]. Benchmarks comparing float32, float16, mixed precision, and bfloat16 settings on models like Falcon-7B show considerable differences in memory usage and implicit performance characteristics [11]. Similarly, the application of INT8 quantization on models like LLaMA-7B has been benchmarked [11]. Factors such as batch size, sequence length, and model size have been shown to influence performance outcomes, particularly affecting the effectiveness of optimizations like PagedAttention which demonstrates greater gains with longer sequences and larger models [7]. The structure of input data, such as varying common prefix and private prompt lengths, also impacts performance metrics like throughput and token reuse efficiency in systems like BatchLLM [4]. Performance evaluation also extends to specific deployment strategies, such as layer deployment in heterogeneous environments, where simulations and small-scale testbeds have been used to measure effectiveness compared to alternatives, showing performance improvements in autonomous driving contexts [21].​  

Existing benchmarks address various scenarios, including on-device LLMs using tools like MobileAIBench and MELTing Point [12,35]. Benchmarks like MRAMG-Bench have been developed for specific tasks such as multimodal retrieval-augmented generation, using comprehensive evaluation strategies combining statistical and LLM metrics to assess model performance and identify limitations on complex datasets [18]. While detailed discussions on limitations of existing general benchmarks and suggestions for improvements are less explicit in the provided digests, the development of specialized benchmarks and metrics (like TECD [4] or comprehensive multimodal evaluation strategies [18]) points towards the need for tailored evaluation approaches for different LLM use cases and emerging complexities. Analyzing the impact of specific design choices through ablation studies is a standard practice in performance evaluation, although detailed outcomes were not extensively presented in the provided digest content. However, the thorough experimental setups detailed in studies like the vLLM evaluation [7] provide a foundation for such detailed analysis.  

# 8. Case Studies & Applications  

<html><body><table><tr><td>Application Domain</td><td>Examples/Characteristics</td><td>Efficiency Requirements /</td></tr><tr><td>Conversational Al</td><td>Chatbots, Dialogue systems (e.g., ChatGPT)</td><td>Low latency, Content relevance</td></tr><tr><td>Content Generation</td><td>Code generation, Text summarization,Machine translation, Creative writing</td><td>Quality (syntax,faithfulness, style), Efficiency</td></tr><tr><td>Recommendation Systems</td><td>Personalized recommendations (e.g., generative rec)</td><td>Efficient processing of user/item data</td></tr><tr><td>On-Device LLMs</td><td>Messaging, Summarization, Healthcare support, Robotics, Disability assistance</td><td>Extreme efficiency (resource/energy constrained)</td></tr><tr><td>Multimodal Applications</td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>language), Image</td><td>data; Timeliness</td></tr><tr><td>LLMAgents/Multi-Model</td><td>Complex workflows</td><td>Managing multiple models, Increased input length</td></tr><tr><td>Batch Processing</td><td>Offline tasks (snippet High throughput, Efficient generation,ranking)</td><td>KV reuse</td></tr></table></body></html>  

Efficient serving of large language models (LLMs) is critical for enabling their deployment across a diverse range of realworld applications and domains. These deployments span various scales and complexities, from large-scale cloud-based services to resource-constrained on-device inference [8,12,35].​  

A primary application domain is conversational AI, exemplified by dialogue systems like ChatGPT and chatbot applications [7,8]. In these scenarios, efficient serving is necessary to balance rapid response speeds with content relevance, directly impacting user experience [8]. Other significant applications include code generation, which demands strict adherence to syntax and often involves handling specific programming language structures [8,30,41], text summarization requiring concise and faithful output [8,35], and machine translation, which necessitates handling language-specific nuances [1,8,35]. Creative content creation tasks also benefit from efficient LLM serving by allowing better control over style and creativity [8].  

Beyond these general categories, LLMs are being applied in more specialized domains. Generative recommendation systems leverage LLMs to produce personalized recommendations, highlighting the need for efficient processing of user data and item descriptions [22,41]. On-device LLMs are gaining traction, enabling applications such as text generation for messaging, meeting summarization, healthcare support, research assistance, and serving as companions for robots or disability support [35]. Major mobile manufacturers are exploring and implementing on-device LLMs like Gemini Nano, Octopus series, and OpenELM, demonstrating their potential in real-world mobile computing scenarios [12].​  

Multimodal applications represent another growing area, such as autonomous driving systems that utilize vision-language models [21], image description tasks using visual multi-modal models like Llava-v1.6-7b [15], and multimodal retrievalaugmented generation (MRAMG) which is increasingly aligned with practical user demands for multimodal answers [18]. Image identification tasks using models like densenet-onnx deployed via platforms like Triton on Azure Machine Learning also illustrate multimodal use cases for efficient serving infrastructure [33]. LLM agents and multi-model collaboration frameworks enhance application capabilities but introduce efficiency challenges due to the increased number of models and input length, necessitating optimized inference techniques [5,41]. Tools like Sim Studio are being developed to simplify the creation of AI agent workflows [41].  

Specific techniques and systems demonstrate quantifiable improvements in these applications. In web search scenarios, BatchLLM, an optimization for offline batch inference, achieved a $1 . 3 { \sf x }$ throughput improvement for snippet generation and a 1.47x throughput improvement for offline ranking compared to baseline methods [4]. Optimizations like FlashAttention have significantly improved performance for processing longer sequences by optimizing data loading during the decoding phase [25]. Efficient memory management techniques like PagedAttention support applications requiring parallel sampling, such as program assistants, and those using beam search, like machine translation [1].  

Case studies of deployment solutions highlight their effectiveness in addressing real-world challenges. Systems are designed for specific scenarios, including multi-tenant serving for LoRA models (e.g., Punica/S-LoRA), long-context inference (e.g., Infinite-LLM), and cost-optimized deployment on heterogeneous hardware or preemptible cloud instances (e.g., Mélange, SpotServe) [13,14]. Speculative decoding is noted as particularly suitable for tasks where input and output are similar, such as grammar correction and retrieval-augmented generation (RAG), with implementations like RaLMSpec accelerating RAG systems [9].​  

Real-world collaborations further illustrate the benefits of efficient serving platforms. The DeepSpeed Inference module has been adopted by prominent organizations including OpenAI, Meta, and Anthropic [23]. FriendliAI provides an inference platform that has been utilized by companies like SK Telecom and NextDay AI, demonstrating benefits in operating LLMs reliably and cost-efficiently while also saving GPU usage [16]. Practical deployment guides showcase how models like Qwen1.5 can be effectively served using combinations of tools like TensorRT-LLM and Triton [24].  

The successful deployment of LLMs in these diverse applications underscores the critical need for efficient serving. Quantified improvements in throughput and efficiency translate directly into better user experience, reduced operational costs, and expanded capabilities for businesses. Challenges remain, particularly in maintaining efficiency as model complexity, size, and the number of concurrent requests increase, especially in multi-model or agentic frameworks [5]. However, ongoing research and system development continue to address these issues, enabling broader and more impactful applications of generative AI.​  

# 9. Future Directions & Open Challenges  

Despite significant advancements in accelerating Large Language Model (LLM) inference, the field remains complex and is still in its early stages [11]. Numerous challenges persist, and emerging trends suggest promising avenues for future research [5,8,10,12,30].  

<html><body><table><tr><td>Area</td><td>Future Directions / Challenges</td></tr><tr><td>Deployment</td><td>Efficient Edge/On-Device deployment, Optimizing for long contexts (1M+ tokens)</td></tr><tr><td>Applications</td><td>Efficiency for Agent/Multi-model frameworks, Security-efficiency co- optimization</td></tr><tr><td>KV Cache</td><td>Task-specific/adaptive merging, Theoretical guarantees for information retention</td></tr><tr><td>Architecture</td><td>Improving arithmetic intensity (Decoder), Novel architectures (Sparse, MoE,SSM)</td></tr><tr><td>Compression</td><td>More efficient algorithms, Extreme low- precision (Sub-4-bit), Data-efficient models</td></tr><tr><td>Hardware</td><td>Hardware-aware frameworks, Dedicated hardware for Prefill/Decode, Heterogeneous acceleration</td></tr><tr><td>Systems</td><td>Unified systems for diverse constraints (models, hardware,traffic,SLOs), Virtual memory for GPU workloads</td></tr><tr><td>Adaptation</td><td>Dynamic adaptive inference, Adaptive optimization strategies</td></tr><tr><td>Sustainability</td><td> Improving energy efficiency for large models</td></tr><tr><td>Accessibility</td><td>Making training/deployment of large models more accessible</td></tr><tr><td>Multimodal</td><td>Advancing MRAMG efficiency, Handling complex datasets/image sequences</td></tr><tr><td>Holistic Opt.</td><td>Balancing software/architecture design, Interdisciplinary efforts</td></tr></table></body></html>  

Key research directions and open questions span across model, system, and hardware levels, highlighting the need for continued investigation.  

One prominent emerging trend is the increasing demand for deploying LLMs on resource-constrained edge devices [5,8,12], necessitating tailored optimization strategies. Furthermore, the efficient handling of long contexts remains a critical challenge, with the aspiration to reduce the service cost of 1M context lengths to that of 4K [13,14]. Research is also focusing on specific application scenarios, including enabling LLMs within agent and multi-model frameworks and optimizing for  

security-efficiency co-optimization, although the impact of current efficiency techniques on security is an area needing more attention [5]. Challenges related to KV cache management, such as developing task-specific or adaptive merging strategies and providing theoretical guarantees for information retention, are also key open questions. The arithmetic intensity of Transformer decoder blocks is identified as a critical area requiring further research, potentially addressed by techniques like increasing batch size [25]. The development of inference systems capable of simultaneously handling diverse constraints, including multiple LLMs, heterogeneous hardware, complex traffic patterns, and varied Service Level Objectives (SLOs), presents a significant system-level challenge [13,14]. Efficiently processing complex datasets and managing image sequences in multi-modal retrieval augmented generation (MRAMG) tasks also indicates limitations in current multi-modal models and the need for deeper research [18].​  

Central challenges that require further effort include significantly reducing the memory footprint of LLMs [8,10,27] and improving energy efficiency for sustainable deployment [8,30]. Efficient memory management in deep learning systems is fundamental to supporting increasingly large models [40]. Moreover, developing more robust optimization techniques is crucial [8,19]. This involves ensuring that acceleration methods do not compromise model quality or reliability [11], addressing limitations in existing tools like the lack of precision feedback to guide quantization and improving their generality and ease of use [36].  

Future research should explore several directions to tackle these challenges [5,8]. Novel architectures such as sparse attention and Mixture of Experts (MoE) hold promise for improving efficiency [8]. Simplifying Transformer module functions could further reduce model complexity and computational needs [6]. Developing more data-efficient AI models is also a key direction [10]. Continued research into compression algorithms is essential, focusing on more efficient methods that can maintain model performance [34], including exploring extreme low-precision quantization (e.g., 4-bit or lower) [6]. Hardware-aware frameworks and dedicated hardware designs are critical. This includes improving hardware accelerators to better handle memory constraints [10], designing specialized hardware modules for Prefill and Decode stages [6], and leveraging heterogeneous acceleration across different hardware platforms [6]. Applying virtual memory and paging techniques, initially explored for KV cache management, to other GPU workloads with LLM-specific optimizations is another potential avenue [7].  

Adaptive and green AI approaches are gaining importance, involving dynamic adaptive inference techniques [8], adaptive optimization strategies sensitive to workload, software, and hardware variations [30], and integrating dynamic load balancing and intelligent resource scheduling with AI to optimize performance [6]. Balancing software optimization and model architecture design is recognized as crucial for achieving efficient LLM inference acceleration [11]. Techniques like speculative decoding can be combined with other advanced methods and explored for multi-modal tasks [9]. Ultimately, making the training and deployment of large models more accessible to a wider research community is also a necessary goal [10]. Realizing the full potential of ubiquitous, intelligent computing through efficient LLMs necessitates interdisciplinary efforts [12], bringing together expertise from machine learning, systems, hardware design, and potentially other fields like energy research and security. Future advancements in technology and algorithms are expected to further enhance the performance of Transformer models [32].​  

# 10. Conclusion  

The efficient serving of generative Large Language Models (LLMs) stands as a critical challenge and a paramount necessity for their successful translation from theoretical models to widespread practical applications [8,19]. The efficiency and quality of the inference phase directly influence user experience and the viability of LLMs in real-world scenarios, ranging from cloud-based services to resource-constrained edge devices [8,12]. Key challenges identified in achieving efficient LLM serving include managing extensive memory requirements, particularly the "memory wall" related to on-chip and inter-chip data transfer [10], mitigating memory fragmentation [17], optimizing latency and throughput [25], handling diverse operational constraints such as multiple models, heterogeneous hardware, complex traffic patterns, and varied service level objectives (SLOs) [13,14], and balancing performance gains with resource utilization [2].​  

Throughout this survey, a diverse array of optimization strategies has been explored, demonstrating effectiveness across different levels of the LLM serving pipeline. At the model level, techniques such as model compression, precision reduction (e.g., quantization), sparse attention, and adapter-based fine-tuning significantly reduce model size and computational cost, thereby improving inference efficiency [11,32,34]. However, applying these techniques requires careful evaluation to ensure the maintenance of model quality and versatility [11,34]. System and framework-level optimizations, including the development of efficient inference frameworks and optimized libraries [11,19], play a crucial role. Techniques like Transformer Kernel Fusion, Low Latency Scheduling, and communication compression, as implemented in systems such as DeepSpeed Inference [23], enhance throughput and reduce latency. Advanced memory management techniques like PagedAttention and systems leveraging it, such as vLLM, have proven particularly effective in reducing memory fragmentation and enabling memory sharing, leading to state-of-the-art throughput [1,7,17]. Efficient KV cache management is also critical for balancing performance and resource use, especially with increasing sequence lengths [2]. Parallel and distributed techniques, including model, data, and pipeline parallelism [19,20], and distributed layer deployment [21], are essential for handling model scale and deploying on distributed infrastructure. Hardware acceleration, efficient GPU memory utilization, and heterogeneous computing are foundational, with hardware and software collaborative optimization being key to unlocking the full potential of underlying devices [6,17,27,32]. Algorithmic innovations in decoding, such as variations of speculative decoding [8,22], further contribute to reduced latency.​  

The application of these strategies often involves navigating significant trade-offs. A primary challenge is balancing conflicting objectives such as maximizing throughput versus minimizing latency, which necessitates careful system design [25]. Similarly, model compression techniques require a balance between achieving significant size reductions and maintaining the model's original versatility and generalization capabilities [34]. The selection of appropriate parallelization methods depends heavily on specific requirements and the deployment environment [20], and generally, strategies must be chosen based on the particular application scenario and identified performance bottlenecks [19,25]. Furthermore, optimizing for inference efficiency can have implications for energy consumption, highlighting the importance of considering energy efficiency in optimization strategies and evaluating them under real-world workloads [30].  

Achieving optimal LLM serving performance is widely recognized as a systematic project that demands a holistic approach [6,19]. Effective solutions require the coordination and optimization of capabilities across the entire stack, from operators and algorithms to frameworks, resource scheduling, and the underlying hardware [6,27]. Hardware resource utilization is central, and challenges are most effectively overcome through tight hardware and software collaboration [6].  

Despite significant progress, the field of efficient LLM serving presents ongoing areas for research and development. Continued research and innovation are needed to address emerging challenges [12]. Tackling the growing "memory wall" necessitates rethinking fundamental aspects of AI model design, training, deployment, and hardware architecture [10]. Furthermore, as LLMs are integrated into more complex applications, such as multimodal scenarios, further research is needed to advance the efficiency of specific methods like multimodal Retrieval Augmented Generation (RAG) technology, potentially leveraging new benchmarks like MRAMG-Bench for evaluation [18]. The continuous evolution of LLM architectures and the increasing diversity of deployment environments ensure that the pursuit of efficient and scalable LLM serving will remain a vibrant research area.​  

# References  

[1] PagedAttention：用于高效LLM服务的页式内存管理 https://blog.csdn.net/weixin_47936614/article/details/145523996   
[2] 大语言模型KV缓存管理加速技术综述：令牌、模型与系统级优化 https://baijiahao.baidu.com/s?   
id=1820360827309006560&wfr=spider&for=pc   
[3] 开普云大模型系统优化实战：系统视角下的推理理解 https://www.cls.cn/detail/xk/680b632a683615accaff05d2   
[4] BatchLLM：面向离线大批量 LLM 推理的前缀共享优化 https://blog.csdn.net/m0_59235945/article/details/144384923   
[5] 大模型高效推理技术综述：加速、优化与未来展望 https://news.sohu.com/a/790365299_121119001   
[6] 高效服务生成式大语言模型：技术挑战与MindIE-LLM实践 https://www.bilibili.com/read/cv40477434/   
[7] PagedAttention：LLM服务的高效内存管理 https://blog.csdn.net/qq_55728814/article/details/142991420   
[8] 大语言模型（LLM）推理阶段详解：定义、流程、挑战与优化 https://cloud.tencent.com.cn/developer/article/2508919?   
policyId=1004​   
[9] 推测解码：GPT-4或已采用，加速LLM推理综述 https://www.thepaper.cn/newsDetail_forward_26401660   
[10] 突破内存墙：高效服务生成式大语言模型 https://aijishu.com/a/1060000000461117   
[11] 大语言模型推理性能优化七策 https://segmentfault.com/a/1190000044086591   
[12] On-Device LLMs: A Comprehensive Survey https://github.com/NexaAI/Awesome-LLMs-on  
device/commit/8bb475f8e7aa3e61d63bad552696d6df62e42759​   
[13] LLM 推理系统盘点：10 种常见方案，应对多模型、异构硬件、复杂流量等挑战   
https://blog.csdn.net/datian1234/article/details/142696341​   
[14] LLM推理系统：10种常见架构总结与分析 https://blog.csdn.net/python12345678_/article/details/140039873​   
[15] LMDeploy 量化部署 LLM 实践：书生·浦语大模型实战营第5课笔记 https://cloud.tencent.com/developer/article/2433094​   
[16] FriendliAI: Accelerating Generative AI Inference f https://friendli.ai/​   
[17] LLM 部署：GPU 内存需求计算与优化指南 https://blog.itpub.net/70018536/viewspace-3035736/   
[18] MRAMG-Bench：北大华为云发布多模态检索增强-多模态生成评测基准 https://mp.weixin.qq.com/s?   
_biz=MzI4MDYzNzg4Mw $\scriptstyle 1 = =$ &mid=2247571232&idx=2&sn=86e463211033b024690d5e4b387038e6&chksm=ea8351a9df1a5c63   
a9bdc045a30215fc5a417498453bbee9abc2819130c6dfafe7ec6dc6ccc7&scene=27   
[19] Python加速：提升大语言模型实时性的策略 https://blog.csdn.net/liuweni/article/details/145065036​   
[20] 推理并行：定义、原理、方法与选择指南 https://baijiahao.baidu.com/s?id $\ c =$ 1820740739041474523&wfr=spider&for=pc​   
[21] LLM Layer Deployment via Matching Theory in Hetero https://ieeexplore.ieee.org/document/10966456/   
[22] AtSpeed: Accelerating LLM-based Generative Recomme   
https://zhuanzhi.ai/paper/256289902c3cc110b025d78d1dfa9b9d​   
[23] DeepSpeed Inference 加速：Transformer Kernel Fusion 与   
https://blog.csdn.net/sinat_28461591/article/details/147553411​   
[24] TensorRT-LLM & Triton：LLM 模型服务部署保姆级教程 https://juejin.cn/post/7398122968200593419   
[25] LLM推理优化：模型性能瓶颈分类及优化策略 https://cloud.baidu.com/qianfandev/topic/269762​   
[26] LLM推理优化技术详解 https://blog.csdn.net/stephen147/article/details/140332300   
[27] 深度学习模型压缩与优化加速概览 https://cloud.tencent.com/developer/article/2158165   
[28] 大模型计算与内存优化：混合精度/量化/剪枝/知识蒸馏 https://blog.csdn.net/a2875254060/article/details/145910501​   
[29] 优化LLM性能与可扩展性的策略 https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability​   
[30] Energy Efficiency of Large Language Model Inferenc http://www.paperreading.club/page?id=301689​   
[31] 徐敏贤博士：云计算系统资源管理专家，招收硕博研究生 https://people.ucas.edu.cn/\~minxian​   
[32] 大型Transformer模型效率优化策略深度剖析 https://developer.baidu.com/article/details/3323722​   
[33] High-Performance Inference with Triton Server on A https://docs.microsoft.com/en-us/azure/machine-learning/how-to  
deploy-with-triton​   
[34] 大型语言模型压缩与高效推理综述 https://blog.csdn.net/c_cpp_csharp/article/details/137399299​   
[35] On-Device LLMs: A Comprehensive Hub of Models, Arc https://gitee.com/bingosxs/Awesome-LLMs-on-device​   
[36] 深度学习模型量化基础与最新进展 https://blog.csdn.net/u013701860/article/details/121627946​   
[37] 徐敏贤博士：云计算资源管理专家，招收硕博研究生 https://people.ucas.ac.cn/\~minxian​   
[38] LLM赋能：聊天机器人系统架构 https://blog.csdn.net/m0_62554628/article/details/145489591   
[39] Megatron-LM：张量与流水线模型并行原理速览 https://blog.csdn.net/u013250861/article/details/134623129   
[40] 深度学习 GPU 内存优化综述：单卡训练显存优化技术 https://blog.csdn.net/weixin_43254181/article/details/146274042​   
[41] LLM 相关技术与应用概览 https://cloud.tencent.com.cn/developer/tag/17917​  