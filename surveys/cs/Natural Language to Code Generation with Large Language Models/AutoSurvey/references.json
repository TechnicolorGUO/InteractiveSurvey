[
    "[1] GraphCodeBERT  Pre-training Code Representations with Data Flow",
    "[2] What do pre-trained code models know about code",
    "[3] CSA-Trans  Code Structure Aware Transformer for AST",
    "[4] SparseCoder  Identifier-Aware Sparse Transformer for File-Level Code  Summarization",
    "[5] Model-Agnostic Syntactical Information for Pre-Trained Programming  Language Models",
    "[6] Diet Code Is Healthy  Simplifying Programs for Pre-trained Models of  Code",
    "[7] Understanding Long Programming Languages with Structure-Aware Sparse  Attention",
    "[8] Tree-Planted Transformers  Large Language Models with Implicit Syntactic  Supervision",
    "[9] Dynamic Context Pruning for Efficient and Interpretable Autoregressive  Transformers",
    "[10] What Does BERT Look At  An Analysis of BERT's Attention",
    "[11] Is Model Attention Aligned with Human Attention  An Empirical Study on  Large Language Models for Code Generation",
    "[12] The Expressibility of Polynomial based Attention Scheme",
    "[13] Attention is Naturally Sparse with Gaussian Distributed Input",
    "[14] Better Language Models of Code through Self-Improvement",
    "[15] CodeShell Technical Report",
    "[16] SynCoBERT  Syntax-Guided Multi-Modal Contrastive Pre-Training for Code  Representation",
    "[17] PALM  Pre-training an Autoencoding&Autoregressive Language Model for  Context-conditioned Generation",
    "[18] SPT-Code  Sequence-to-Sequence Pre-Training for Learning Source Code  Representations",
    "[19] Importance Guided Data Augmentation for Neural-Based Code Understanding",
    "[20] TRACED  Execution-aware Pre-training for Source Code",
    "[21] Text-to-Code Generation with Modality-relative Pre-training",
    "[22] Exploring and Evaluating Personalized Models for Code Generation",
    "[23] Prefix-Tuning  Optimizing Continuous Prompts for Generation",
    "[24] Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs",
    "[25] Delving into Parameter-Efficient Fine-Tuning in Code Change Learning  An  Empirical Study",
    "[26] MFTCoder  Boosting Code LLMs with Multitask Fine-Tuning",
    "[27] On Robust Prefix-Tuning for Text Classification",
    "[28] Towards Green AI in Fine-tuning Large Language Models via Adaptive  Backpropagation",
    "[29] AST-T5  Structure-Aware Pretraining for Code Generation and  Understanding",
    "[30] StructCoder  Structure-Aware Transformer for Code Generation",
    "[31] Structured Code Representations Enable Data-Efficient Adaptation of Code  Language Models",
    "[32] Graph Conditioned Sparse-Attention for Improved Source Code  Understanding",
    "[33] Language-Agnostic Representation Learning of Source Code from Structure  and Context",
    "[34] Toward Textual Transform Coding",
    "[35] Trees in transformers  a theoretical analysis of the Transformer's  ability to represent trees",
    "[36] Mamba  Linear-Time Sequence Modeling with Selective State Spaces",
    "[37] Zebra  Extending Context Window with Layerwise Grouped Local-Global  Attention",
    "[38] BlackMamba  Mixture of Experts for State-Space Models",
    "[39] LocalMamba  Visual State Space Model with Windowed Selective Scan",
    "[40] Hungry Hungry Hippos  Towards Language Modeling with State Space Models",
    "[41] Learn To be Efficient  Build Structured Sparsity in Large Language  Models",
    "[42] Deja Vu  Contextual Sparsity for Efficient LLMs at Inference Time",
    "[43] AWQ  Activation-aware Weight Quantization for LLM Compression and  Acceleration",
    "[44] Gradient-Free Adaptive Global Pruning for Pre-trained Language Models",
    "[45] LLM in a flash  Efficient Large Language Model Inference with Limited  Memory",
    "[46] Self-Selected Attention Span for Accelerating Large Language Model  Inference",
    "[47] An Exploratory Study on Code Attention in BERT",
    "[48] Looped Transformers as Programmable Computers",
    "[49] CodeArt  Better Code Models by Attention Regularization When Symbols Are  Lacking",
    "[50] Parameter-Efficient Finetuning of Transformers for Source Code",
    "[51] EchoPrompt  Instructing the Model to Rephrase Queries for Improved  In-context Learning",
    "[52] Identifying Semantic Induction Heads to Understand In-Context Learning",
    "[53] Improving BERT with Syntax-aware Local Attention",
    "[54] Two-stage LLM Fine-tuning with Less Specialization and More  Generalization",
    "[55] Instruction Position Matters in Sequence Generation with Large Language  Models",
    "[56] Large Language Models Can be Lazy Learners  Analyze Shortcuts in  In-Context Learning",
    "[57] How Abilities in Large Language Models are Affected by Supervised  Fine-tuning Data Composition",
    "[58] Pretrained Generative Language Models as General Learning Frameworks for  Sequence-Based Tasks",
    "[59] Enhancing Code Intelligence Tasks with ChatGPT",
    "[60] What Do They Capture  -- A Structural Analysis of Pre-Trained Language  Models for Source Code",
    "[61] Incorporating Domain Knowledge through Task Augmentation for Front-End  JavaScript Code Generation",
    "[62] CodeRL  Mastering Code Generation through Pretrained Models and Deep  Reinforcement Learning",
    "[63] CoderEval  A Benchmark of Pragmatic Code Generation with Generative  Pre-trained Models",
    "[64] UniXcoder  Unified Cross-Modal Pre-training for Code Representation",
    "[65] Data Fine-tuning",
    "[66] LeTI  Learning to Generate from Textual Interactions",
    "[67] On the Evaluation Metrics for Paraphrase Generation",
    "[68] I Learn Better If You Speak My Language  Enhancing Large Language Model  Fine-Tuning with Style-Aligned Response Adjustments",
    "[69] Automated Data Curation for Robust Language Model Fine-Tuning",
    "[70] Fine Tuning LLM for Enterprise  Practical Guidelines and Recommendations",
    "[71] Overcoming a Theoretical Limitation of Self-Attention",
    "[72] Syntax-augmented Multilingual BERT for Cross-lingual Transfer",
    "[73] SOT for MOT",
    "[74] A Multi-Modal Transformer-based Code Summarization Approach for Smart  Contracts",
    "[75] A Closer Look into Transformer-Based Code Intelligence Through Code  Transformation  Challenges and Opportunities",
    "[76] Exploring Software Naturalness through Neural Language Models",
    "[77] Using Transfer Learning for Code-Related Tasks",
    "[78] Sparse Coding and Autoencoders",
    "[79] Alien Coding",
    "[80] BERTQA -- Attention on Steroids",
    "[81] Transformer with Tree-order Encoding for Neural Program Generation",
    "[82] Horizontal and Vertical Attention in Transformers",
    "[83] Beyond Self-learned Attention  Mitigating Attention Bias in  Transformer-based Models Using Attention Guidance",
    "[84] Error Correction Code Transformer",
    "[85] Similarity",
    "[86] Explainable AI for Pre-Trained Code Models  What Do They Learn  When  They Do Not Work",
    "[87] Bias Testing and Mitigation in LLM-based Code Generation",
    "[88] Automatic Semantic Augmentation of Language Model Prompts (for Code  Summarization)",
    "[89] Roles of Scaling and Instruction Tuning in Language Perception  Model  vs. Human Attention",
    "[90] Tree-Based Hard Attention with Self-Motivation for Large Language Models",
    "[91] CodeT5  Identifier-aware Unified Pre-trained Encoder-Decoder Models for  Code Understanding and Generation",
    "[92] Span Fine-tuning for Pre-trained Language Models",
    "[93] Probing Pretrained Models of Source Code",
    "[94] Using LLM such as ChatGPT for Designing and Implementing a RISC  Processor  Execution,Challenges and Limitations",
    "[95] Automatically Generating CS Learning Materials with Large Language  Models",
    "[96] Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation  with Large Language Models",
    "[97] No More Fine-Tuning  An Experimental Evaluation of Prompt Tuning in Code  Intelligence",
    "[98] Fine-Tuning Enhances Existing Mechanisms  A Case Study on Entity  Tracking",
    "[99] Abstract Syntax Tree for Programming Language Understanding and  Representation  How Far Are We",
    "[100] M2TS  Multi-Scale Multi-Modal Approach Based on Transformer for Source  Code Summarization",
    "[101] Advanced Large Language Model (LLM)-Driven Verilog Development   Enhancing Power, Performance, and Area Optimization in Code Synthesis",
    "[102] WizardCoder  Empowering Code Large Language Models with Evol-Instruct",
    "[103] FinGPT-HPC  Efficient Pretraining and Finetuning Large Language Models  for Financial Applications with High-Performance Computing",
    "[104] ETC  Encoding Long and Structured Inputs in Transformers",
    "[105] Attention-Driven Reasoning  Unlocking the Potential of Large Language  Models",
    "[106] TransCoder  Towards Unified Transferable Code Representation Learning  Inspired by Human Skills",
    "[107] Beyond Human Data  Scaling Self-Training for Problem-Solving with  Language Models",
    "[108] Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model  Fine-tuning",
    "[109] Syntax-BERT  Improving Pre-trained Transformers with Syntax Trees",
    "[110] EyeTrans  Merging Human and Machine Attention for Neural Code  Summarization",
    "[111] Noisy Exemplars Make Large Language Models More Robust  A  Domain-Agnostic Behavioral Analysis",
    "[112] Prompt Selection and Augmentation for Few Examples Code Generation in  Large Language Model and its Application in Robotics Control",
    "[113] Unleashing the potential of prompt engineering in Large Language Models   a comprehensive review",
    "[114] Do Prompt-Based Models Really Understand the Meaning of their Prompts",
    "[115] Revisiting Automated Prompting  Are We Actually Doing Better",
    "[116] Revisiting Fine-tuning for Few-shot Learning",
    "[117] Meta-learning for Few-shot Natural Language Processing  A Survey",
    "[118] Learning from Few Examples  A Summary of Approaches to Few-Shot Learning",
    "[119] Few-Shot Learning with a Strong Teacher",
    "[120] True Few-Shot Learning with Prompts -- A Real-World Perspective",
    "[121] Improving and Simplifying Pattern Exploiting Training",
    "[122] Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed  Hinglish  A Feasibility-Driven Transfer Learning Approach with Large Language  Models",
    "[123] Function-constrained Program Synthesis",
    "[124] Zero-Shot Code Representation Learning via Prompt Tuning",
    "[125] xCodeEval  A Large Scale Multilingual Multitask Benchmark for Code  Understanding, Generation, Translation and Retrieval",
    "[126] Grounding Data Science Code Generation with Input-Output Specifications",
    "[127] TabLLM  Few-shot Classification of Tabular Data with Large Language  Models",
    "[128] Pop Quiz! Can a Large Language Model Help With Reverse Engineering",
    "[129] Security for Machine Learning-based Software Systems  a survey of  threats, practices and challenges",
    "[130] Breaking Down the Defenses  A Comparative Survey of Attacks on Large  Language Models",
    "[131] Naturalness of Attention  Revisiting Attention in Code Language Models",
    "[132] Transfer Attacks and Defenses for Large Language Models on Coding Tasks",
    "[133] Exploring Safety Generalization Challenges of Large Language Models via  Code",
    "[134] Survey of Vulnerabilities in Large Language Models Revealed by  Adversarial Attacks",
    "[135] Data",
    "[136] Improving Code Summarization with Block-wise Abstract Syntax Tree  Splitting",
    "[137] Securing Large Language Models  Threats, Vulnerabilities and Responsible  Practices",
    "[138] A Survey on Large Language Model (LLM) Security and Privacy  The Good,  the Bad, and the Ugly",
    "[139] Bird-Eye Transformers for Text Generation Models",
    "[140] Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing  Positional Bias in LLMs",
    "[141] Let's Focus on Neuron  Neuron-Level Supervised Fine-tuning for Large  Language Model",
    "[142] Interactive Coding for Markovian Protocols",
    "[143] Studying the Usage of Text-To-Text Transfer Transformer to Support  Code-Related Tasks",
    "[144] TransformCode  A Contrastive Learning Framework for Code Embedding via  Subtree Transformation",
    "[145] Preprint Déjà Vu  an FAQ",
    "[146] Autoencoders",
    "[147] The Hidden Attention of Mamba Models",
    "[148] GPT Understands, Too",
    "[149] Learn To Pay Attention",
    "[150] LongVQ  Long Sequence Modeling with Vector Quantization on Structured  Memory",
    "[151] A Web of Blocks",
    "[152] A Survey on Visual Mamba",
    "[153] Continued Pretraining for Better Zero- and Few-Shot Promptability",
    "[154] Prompt Engineering Through the Lens of Optimal Control",
    "[155] A Communication Theory Perspective on Prompting Engineering Methods for  Large Language Models",
    "[156] A Systematic Survey of Prompt Engineering in Large Language Models   Techniques and Applications",
    "[157] Code Generation Tools (Almost) for Free  A Study of Few-Shot,  Pre-Trained Language Models on Code",
    "[158] Tuning Language Models as Training Data Generators for  Augmentation-Enhanced Few-Shot Learning",
    "[159] Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task  Generalization",
    "[160] Active PETs  Active Data Annotation Prioritisation for Few-Shot Claim  Verification with Pattern Exploiting Training"
]