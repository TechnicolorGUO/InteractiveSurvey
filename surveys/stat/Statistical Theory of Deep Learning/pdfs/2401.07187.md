# A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models ∗  

Namjoon Suh and Guang Cheng  

Department of Statistics and Data Science, UCLA  

September 17, 2024  

# Abstract  

In this article, we review the literature on statistical theories of neural networks from three perspectives: approximation, training dynamics and generative models. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression (and classification in Appendix B). These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer “how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.” In particular, two well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF) paradigm. Last but not least, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), diffusion models, and in-context learning (ICL) in the Large Language Models (LLMs) from two perpsectives reviewed previously, i.e., approximation and training dynamics.  

# 1 Introduction  

In recent years, the field of deep learning [Goodfellow et al., 2016] has experienced a substantial evolution. Its impact has transcended traditional boundaries, leading to significant advancements in sectors such as healthcare [Esteva et al., 2019], finance [Heaton et al., 2017], autonomous systems [Grigorescu et al., 2020], and natural language processing [Otter et al., 2020]. Neural networks, the mathematical abstractions of our brain, lie at the core of this progression. Nevertheless, amid the ongoing renaissance of artificial intelligence, neural networks have acquired an almost mythical status, spreading the misconception that they are more art than science. It is important to dispel this notion. While the applications of neural networks may evoke awe, they are firmly rooted in mathematical principles. In this context, the importance of deep learning theory becomes evident. Several key points underscore its significance.  

# 1.1 Why theory is important?  

In this subsection, we aim to emphasize the importance of understanding deep learning within mathematical and statistical frameworks. Following are some key points to consider:  

1. Deep learning is a dynamic and rapidly evolving field, producing hundreds of thousands of publications online. Today’s models are characterized by highly intricate network architectures comprising numerous complex sub-components (e.g., Transformer [Vaswani et al., 2017]). Amidst this complexity, it becomes crucial to comprehend the fundamental principles underlying these models. To achieve this understanding, placing these models within a unified mathematical framework is essential. Such a framework serves as a valuable tool for distilling the core concepts from these intricate models, making it possible to extract and comprehend the key principles that drive their functionality.  

2. Applying statistical frameworks to deep learning models allows meaningful comparisons with other statistical methods. For instance, widely used statistical estimators like wavelet or kernel methods can prompt questions about when and why deep neural networks might perform better. This analysis helps us understand when deep learning excels compared to traditional statistical approaches, benefiting both theory and practice.  

3. Hyperparameters, such as learning rate, weight initializations, network architecture choices, activation functions, etc, significantly influence the quality of the estimated model. Understanding the proper ranges for these hyperparameters is essential, not only for theorists but also for practitioners. For instance, in the era of big data where there are millions of samples in one dataset, the theoretical wisdom gives us the depth of the network should scale logarithmically in sample size for the good estimation of compositional functions (e.g., See Schmidt-Hieber [2020]).  

In this review, we provide an overview of papers that delve into these concepts within mathematical settings, offering readers specific insights into the topics discussed above. Here, we try to avoid too many technicalities and make the introductions accessible to as many statisticians as possible. Some overly technical components are deferred to the Appendix.  

# 1.2 Roadmap of the paper  

We classify the existing literature on statistical theories of neural networks into three categories. (Section 2, Section 3, Section 4, respectively.)  

. Approximation theory viewpoint. Recently, there has been a huge collection of works which bridge the approximation theory of neural network models [Yarotsky, 2017, Mhaskar, 1996, Petersen and Voigtlaender, 2018, Schmidt-Hieber, 2020, Montanelli and Du, 2019, Blanchard and Bennouna, 2022, Hornik et al., 1989, Hanin, 2019] with the tools in empirical processes [Van de Geer, 2000] to obtain the fast convergence rates of excess risks both in regression [SchmidtHieber, 2020, Hu et al., 2021] and classification [Hu et al., 2020a, Kim et al., 2021] tasks under non-parametric settings. Approximation theory provides useful perspectives in measuring the fundamental complexities of neural networks for approximating functions in certain classes. Specifically, it enables the explicit constructions of neural networks for the function approximations so that we know how the network width, depth, and number of active parameters should scale in terms of sample size, data dimension, and the function smoothness index to get good statistical convergence rates. For simplicity, we mainly consider the works in which the fully connected neural networks are used as the function estimators. These works include SchmidtHieber [2020], Kim et al. [2021], Shen et al. [2021], Jiao et al. [2021], Lu et al. [2021], Imaizumi and Fukumizu [2019, 2022], Suzuki [2018], Chen et al. [2019], Suzuki and Nitanda [2021], Suh et al. [2022] under various problem settings. Yet, these works assume that the global minimizers of loss functions are obtainable, and are mainly interested in the statistical properties of these minimizers without any optimization concerns. However, this is a strong assumption, given the non-convexity of loss functions arising from the non-linearities of activation functions in the hidden layers.  

2. Training dynamics viewpoint. Understanding the landscape of non-convex loss functions for neural network models and its impact on their generalization capabilities represents a critical next step in the literature. However, the non-trivial non-convexity of this landscape poses significant challenges for the mathematical analysis of many intriguing phenomena observed in neural networks. For example, a seminal empirical finding, Zhang et al. [2021], reveals that neural networks in their experiments trained on a standard image classification training set (CIFAR 10), can fit the (noisy) training data perfectly, but at the same time showing respectable prediction performance. (See Figure 1(c) in Zhang et al. [2021].) This contradicts the classic statistical wisdom of bias-variance tradeoff, which states the overfitted models cannot generalize well. The role of overparameterizations (e.g., Bartlett et al. [2021]) on the non-convex optimization landscape of neural networks has been intensively studied over the past years, and we review the relevant literatures under this context. For instance, Jacot et al. [2018] revealed that the dynamics of highly overparameterized neural networks with large enough width, trained via gradient descent (GD) in $\ell _ { 2 }$ -loss, behave similarly to those of functions in reproducing kernel Hilbert spaces (RKHS), where the kernel is associated with a specific network architecture. Many subsequent works study the training dynamics and the generalization abilities of neural networks in the kernel regime under various settings [Hu et al., 2021, Nitanda and Suzuki, 2020]. However, due to technical constraints (as detailed in Subsection 3.1), networks in the kernel regime fail to explain the essential functionality of neural networks—feature learning [Zhong et al., 2016]. Another important line of work focuses on understanding the learning dynamics of neural networks in the Mean-Field (MF) regime, where feature learning becomes more explainable. Nonetheless, the analysis within the MF regime is challenging to generalize to deep networks and requires infinite widths. Finally, we conclude this section by presenting several approaches that go beyond or unify the two regimes.  

Generative modeling. In this section, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), Diffusion Models, and In-context learning in Large Language Models (LLMs). The works to be introduced are based on the philosophies of two paradigms (approximation & training dynamics). Over the past decade, GANs [Goodfellow et al., 2014] have stood out as a significant unsupervised learning approach, known for their ability to learn the data distributions and efficiently sample the data from it. In this review, we will introduce papers that studied statistical properties of GANs [Arora et al., 2017, Liang, 2021, Chen et al., 2020a, Bai et al., 2018, Zhang et al., 2017, Schreuder et al., 2021]. Recently, another set of generative models, i.e., diffusion model, has shown superior performances in generating impressive qualities of synthetic data in various data modalities including image [Song et al., 2020, Dhariwal and Nichol, 2021], tabular data [Kim et al., 2022, Suh et al., 2023], medical imaging [Mu¨ller-Franzes et al., 2022] etc., beating GAN based models by a large margin. However, given the model’s complex nature and its recent introduction in the community, the theoretical reason why it works so well remains vague. Lastly, we will review an interesting phenomenon commonly observed in Large Language Models referred to as In-context learning (ICL). It refers to the ability of LLMs conditioned on prompt sequence consisting of examples from a task (input-output pairs) along with the new query input, the LLM can generate the corresponding output accurately. Readers can refer to nice survey papers Gui et al. [2021], Yang et al. [2022] on the detailed descriptions of methodologies and the applications of GANs, and diffusion models in various domains. For an overview of ICL, see survey by Dong et al. [2022] which highlights some key findings and advancements in this direction.  

In relation to Subsection 1.1, the advantages of neural networks over classic statistical function estimators are primarily discussed in Sections 2 and 3 under various problem settings. In Section 3, we review the work of Yang and Hu [2020], which suggests appropriate parameter initialization scalings and learning rates for feature learning in large-scale (infinite width) neural networks.  

# 1.3 Existing surveys on deep learning theory  

To our knowledge, there are four existing survey papers [Bartlett et al., 2021, Fan et al., 2021, Belkin, 2021, He and Tao, 2020] on deep learning theory. There is an overlap in certain subjects covered by each of the papers, but their main focuses are different from each other. Bartlett et al. [2021] provided a comprehensive and technical survey on statistical understandings of deep neural networks. In particular, the authors focused on examining the significant influence of overparameterization in neural networks, which plays a key role in enabling gradient-based methods to discover interpolating solutions. Fan et al. [2021] introduced the most commonly employed neural network architectures in practice such as Convolutional Neural Net (CNN), Recurrent Neural Networks (RNN), and training techniques such as batch normalization, and dropout. A brief introduction to the approximation theory of neural networks is provided as well. Similarly as Bartlett et al. [2021], Belkin [2021] reviewed the role of overparameterizations for implicit regularization and benign overfitting observed not only in neural network models but also in classic statistical models such as weighted nearest neighbor predictors. Most notably, they provided intuitions on the roles of the overparameterization of nonconvex loss landscapes of neural networks through the lens of optimization. He and Tao [2020] provides a comprehensive overview of deep learning theory, including the ethics and security problems that arise in data science and their relationships with deep learning theory. We recommend readers review all these papers to gain a comprehensive understanding of this emerging field. Our paper offers a unique and comprehensive survey of the statistical results of neural networks, focusing on approximation theory and training dynamics, while also covering generative models within these two paradigms.  

# 2 Approximation theory-based statistical guarantees  

We outline fully connected networks, which are the main object of interest throughout this review. From a high level, deep neural networks can be viewed as a family of nonlinear statistical models that can encode highly nontrivial representations of data. The specific network architecture $( L , \mathbf { p } )$ consists of a positive integer $L$ , called the number of hidden layers, and a width vector $\mathbf { p } : = \left( \mathbf { p } _ { 0 } , \ldots , \mathbf { p } _ { L + 1 } \right) \in \mathbb { N } ^ { L + 2 }$ , recording the number of nodes from input to output layers in the network. A fully connected neural network, $\tilde { f }$ , is then any function of the form for the input feature $\mathbf { x } \in \mathcal { X }$ :  

$$
\tilde { f } : \mathcal { X } \to \mathbb { R } , \quad \mathbf { x } \to f ( \mathbf { x } ) = W _ { L } \sigma W _ { L - 1 } \sigma W _ { L - 2 } \dots \sigma W _ { 1 } \mathbf { x } ,
$$  

where $\mathbf { W } _ { i } \in \mathbb { R } ^ { p _ { i + 1 } \times p _ { i } }$ is a weight matrix with $\mathbf { p } _ { 0 } = d$ , $\mathbf { p } _ { L + 1 } = 1$ and $\sigma$ is the non-linear activation function. Here, the activation function plays a key-role in neural network allowing the non-linear representations of the given data $\mathbf { x }$ . Popular examples include: ReLU(x)=max $( x , 0 )$ , $\mathrm { S i g m o i d } ( x ) { = } { \frac { 1 } { 1 + e ^ { - x } } }$ We omit the bias terms added on the outputs of pre-activated hidden layers for simplicity. But bias terms are needed for universal approximation if the input data is not appended with a constant entry.  

Under this setting, complexity of the networks is mainly measured through the three metrics: (1) the maximum width, denoted as ${ \bf p } _ { m a x } : = \operatorname* { m a x } _ { i = 0 , \dots , L + 1 } { \bf p } _ { i }$ , (2) the depth, denoted as $L$ , and (3) the number of non-zero parameters, denoted as $\mathcal { N }$ . Letting $\| \mathbf { W } _ { j } \| _ { 0 }$ be the number of non-zero entries of $\mathbf { W } _ { j }$ in the $j ^ { \mathrm { t h } }$ hidden layer, the final form of neural network we consider is given by:  

$$
\mathcal { F } ( L , \mathbf { p } , \mathcal { N } ) : = \bigg \{ \tilde { f } \mathrm { ~ o f ~ t h e ~ f o r m ~ } ( 1 ) : \sum _ { j = 1 } ^ { L } \| \mathbf { W } _ { j } \| _ { 0 } \leq \mathcal { N } \bigg \} .
$$  

In the approximation theoretic literature, the capacity or expressive power of neural network is often characterized by the tuple $( L , \mathbf { p _ { m a x } } , \mathcal { N } )$ . Let $\mathcal { G }$ be a function class where the target function $f _ { \star }$ belongs. The main question frequently asked is: given the fixed approximation error, $\varepsilon _ { \mathrm { A p p r x } }$ , defined as follows:  

$$
\varepsilon _ { \mathrm { A p p r x } } : = \operatorname* { s u p } _ { f _ { \star } \in \mathcal { G } } \operatorname* { i n f } _ { f \in \mathcal { F } ( L , \mathbf { p } , \mathcal { N } ) } \| f - f _ { \star } \| _ { L _ { p } } ,
$$  

how does the network architecture $( L , \mathbf { p _ { m a x } } , \mathcal { N } )$ scale in terms of $\varepsilon _ { \mathrm { A p p r x } }$ ? (henceforth, the subscript Apprx will be omitted in $\varepsilon _ { \mathrm { A p p r x } }$ .) Note the sup is taken over the function class $\mathcal { G }$ and the inf is taken over the neural network class $\mathcal { F }$ . The distance between two functions is measured via $L _ { P }$ -norm.  

# 2.1 Expressive power of fully-connected networks  

In this subsection, we briefly review some important results in the approximation theory of neural networks. For more comprehensive reviews on this topic, readers can refer to DeVore et al. [2021].  

Approximating functions in $\mathcal { G }$ : The specifications of function class $\mathcal { G }$ and $\mathcal { F }$ allow us to derive many interesting insights on the power of neural networks. For instance, the celebrated “universal approximation theorem” states that any continuous functions (i.e., $\mathcal { G } : = \left\{ \begin{array} { r l r l } \end{array} \right.$ {Continuous Functions on $\mathbb { R } ^ { d } \}$ ) can be approximated by a shallow neural network (i.e., one-hidden layer) with sigmoid activation function (i.e., ${ \mathcal { F } } : =$ Shallow Neural Networks ) at an arbitrary accuracy (Cybenko [1989], Hornik et al. [1989, 1990]). However, achieving a good approximation may require an extremely large number of hidden nodes, which significantly increases the capacity of $\mathcal { F }$ . Barron [1993, 1994] developed an approximation theory for function classes $\mathcal { G }$ with limited capacity, measured by the integrability of their Fourier transform. Interestingly, the approximation result is not affected by the dimension of input data $d$ , and this observation matches with the experimental results that deep learning is very  

effective in dealing with high-dimensional data.  

Nonetheless, the capacity of $\mathcal { G }$ in Barron [1994] is rather limited. Another typical route of the analysis is to specifying the smoothness of function classes. Readers can roughly understand that smoothness refers to the highest order of derivatives that the functions can possess. Notably, Yarotsky [2017] demonstrated that deep ReLU networks (1) cannot escape the curse of dimensionality when approximating functions in the unit ball in Sobolev space. They established that the order $\mathcal { N } = \mathcal { O } ( \varepsilon ^ { - \frac { d } { r } } )$ is sharp, with matching lower and upper bounds. Petersen and Voigtlaender [2018] generalized the results to the class of piece-wise smooth functions. Later, Schmidt-Hieber [2020] developed a theory for any network architecture satisfying the set of conditions on $( L , \mathbf { p _ { m a x } } , \mathcal { N } )$ , deep ReLU nets can achieve good approximation rates for functions in H¨older class. (More details on the technical results of Schmidt-Hieber [2020] are deferred in the Appendix A.) This should be contrasted to the Yarotsky [2017] where they proved the “existence” of a network with good approximation. Many researchers have been working on considering either more general (i.e., Besov space) or more specific (i.e., hierarchical compositional function) function classes $\mathcal { G }$ than H¨older class. These considerations have facilitated numerous intriguing comparisons between classic statistical function estimators and deep neural networks in terms of their fundamental limits, specifying the second item in Subsection 1.1.  

The benefits of depth: It has been shown that the expressive power of deep neural networks grows with respect to the number of layers (i.e., $L$ ) by several studies. Delalleau and Bengio [2011] showed there exist families of functions that can be represented much more efficiently with a deep network than with a shallow one (i.e. with substantially fewer hidden units). In the asymptotic limit of depth, Pascanu et al. [2013] showed deep ReLU networks can represent exponentially more piecewise linear functions than their single-layer counterparts can do, given that both networks have the same number of nodes. Later, Montufar et al. [2014] proved that a similar result can be derived with the fixed number of hidden layers. Poole et al. [2016] showed deep neural networks can disentangle highly curved manifolds in an input space into flat manifolds in a hidden space, while shallow networks cannot. Mhaskar et al. [2017] demonstrated that deep ReLU networks can approximate compositional functions with significantly fewer parameters (i.e., $\mathcal { N }$ ) than shallow neural networks need in order to achieve the same level of approximation accuracy.  

Bounded-width: The effects of width on the expressive power of neural networks have recently been studied [Lu et al., 2017, Hanin, 2019, Kidger and Lyons, 2020, Park et al., 2020, Vardi et al., 2022]. Lu et al. [2017] showed that the minimal width for universal approximation (denoted as $w _ { \mathrm { m i n } }$ ) using ReLU networks w.r.t. the $L _ { 1 }$ norm of functions from $\mathbb { R } ^ { d } \to \mathbb { R }$ is $d + 1 \leq w _ { \mathrm { m i n } } \leq d + 4$ . Kidger and Lyons [2020] extended the results to $L _ { p }$ -approximation of functions from $\mathbb { R } ^ { d }  \mathbb { R } ^ { \mathrm { o u t } }$ , and obtained $w _ { \mathrm { m i n } } \leq d + d _ { \mathrm { o u t } } + 1$ . Park et al. [2020] further improved $w _ { \mathrm { m i n } } = \operatorname* { m a x } \{ d + 1 , d _ { \mathrm { o u t } } \}$ . Universal approximations of narrow networks with other activation functions had been studied in Park et al. [2020], Kidger and Lyons [2020], Johnson [2018]. Note that the aforementioned works require the depth of networks to be exponential in input dimension with bounded-width, which are the dual-versions of universal approximation of bounded depth networks from Cybenko [1989], Hornik et al. [1989, 1990]. Interestingly, Vardi et al. [2022] provided an evidence that width of the networks can be less important than depth for the expressive power of neural nets. They showed that the price for making the width small is only a linear increase in the network depth, in sharp contrast to the results mentioned earlier on how making the depth small may require an exponential increase in the network depth.  

# 2.2 Statistical guarantees for regression tasks  

The natural question is: what are the interpretations or consequences of the results in approximation theory for deriving statistical guarantees of neural networks under noisy observations? In this subsection, we focus on reviewing several important results under regression tasks in this regard. As preliminary, we introduce the settings frequently adopted in statistical learning theory.  

Let $\mathcal { X }$ and $\mathcal { V } \subset \mathbb { R }$ be the measurable feature space and output space. We denote $\rho$ as a joint probability measure on the product space $\mathcal { X } \times \mathcal { V }$ , and let $\rho _ { \mathcal { X } }$ be the marginal distribution of the feature space $\mathcal { X }$ . We assume that the noisy data set $\mathcal { D } : = \{ ( \mathbf { x } _ { i } , \mathbf { y } _ { i } ) \} _ { i = 1 } ^ { n }$ are generated from the non-parametric  

regression model:  

$$
\begin{array} { r } { \mathbf { y } _ { i } = f _ { \rho } ( \mathbf { x } _ { i } ) + \varepsilon _ { i } , \quad i = 1 , 2 , \ldots , n , } \end{array}
$$  

where the noise $\varepsilon _ { i }$ is assumed to be centered random variable and $\mathbb { E } ( \varepsilon _ { i } | \mathbf { x } _ { i } ) = 0$ . Our goal is to estimate the regression function $f _ { \rho } ( \mathbf { x } )$ with the given noisy data set $\mathcal { D }$ . Here, it is easy to see regression function $f _ { \rho } : = \mathbb { E } ( \mathbf { y } | \mathbf { x } )$ is a minimizer of the population risk $\mathcal { E } ( f )$ under $\ell _ { 2 }$ -loss defined as:  

$$
\begin{array} { r } { \mathcal { E } ( f ) : = \mathbb { E } _ { ( \mathbf { x } , \mathbf { y } ) \sim \rho } \bigg [ \big ( \mathbf { y } - f ( \mathbf { x } ) \big ) ^ { 2 } \bigg ] . } \end{array}
$$  

However, since the joint distribution $\rho$ is unknown, we cannot find $f _ { \rho }$ directly. Instead, we solve the following empirical risk minimization problem induced from the dataset $\mathcal { D }$ :  

$$
\widehat { f } _ { n } = \operatorname * { a r g m i n } _ { f \in \mathcal { F } ( L , \mathbf { p } , \mathcal { N } ) } \mathcal { E } _ { D } ( f ) : = \operatorname * { a r g m i n } _ { f \in \mathcal { F } ( L , \mathbf { p } , \mathcal { N } ) } \bigg \{ \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \big ( \mathbf { y } _ { i } - f ( \mathbf { x } _ { i } ) \big ) ^ { 2 } \bigg \} .
$$  

Note that the papers introduced in this subsection always assume the empirical risk minimizer ${ \widehat { f } } _ { n }$ is obtainable, ignoring the optimization process. The function estimator $f$ is structurally regulariz db by $\mathcal { N }$ in $( L , \mathbf { p } , \mathcal { N } )$ , which will be specified in sequel.  

Under this setting, the excess risk is an important statistical object measuring the generalizability of the function estimator ${ \widehat { f } } _ { n }$ for unseen data in $\mathcal { X }$ . Mathematically, it can be shown that it is a difference between popul ibon risks of $f _ { \rho }$ and ${ \widehat { f } } _ { n }$ (See Chapter 13 in Wainwright [2019]), which is $\mathbb { E } _ { \mathbf { X } \sim \rho \boldsymbol { x } } \left[ ( \widehat { f } _ { n } ( \mathbf { X } ) - f _ { \rho } ( \mathbf { X } ) ) ^ { 2 } \right]$ . The excess risk ca b e further decomposed as follows (Proposition 4.2 in Suh et al .b[2022]) :  

$$
\mathbb { E } _ { \mathbf { X } \sim \rho _ { \mathcal { X } } } \left[ ( \widehat { f } _ { n } ( \mathbf { X } ) - f _ { \rho } ( \mathbf { X } ) ) ^ { 2 } \right] \leq \frac { \mathrm { C o m p l e x i t y ~ M e a s u r e ~ o f ~ \mathcal { F } _ \rho ~ } } { n } + \mathrm { A p p r o x . ~ E r r o r } ^ { 2 } .
$$  

In the context of excess risk bounds, it is important to note the trade-off between the “Approximation error” and the combinatorial “Complexity measure” of a neural network class $\mathcal { F }$ . Specifically, as the network hypothesis space $\mathcal { F }$ becomes richer, the approximation results improve. However, increasing the hypothesis space $\mathcal { F }$ arbitrarily will eventually lead to an increase in the complexity measure of $\mathcal { F }$ , as described in (6). Researchers (e.g., Bartlett et al. [2019], Schmidt-Hieber [2020]) have examined how various complexity measures, including VC-dimension, pseudo-dimension, and covering number, scale with respect to $( L , \mathbf { p } _ { \operatorname* { m a x } } , \mathcal { N } )$ . Specifically, above papers proved all the three complexity measures increase linearly in $\mathcal { N }$ . For achieving good convergence rates of the excess risks from (6), it is crucial to properly specify the network architecture (i.e., the choices of $( L , \mathbf { p } _ { \mathrm { m a x } } , \mathcal { N } ) )$ that balances the tension between complexity of $\mathcal { F }$ and approximation error in terms of sample size $n$ , data dimension $d$ , and function smoothness $r \geq 0$ .  

Deep sparse ReLU networks v.s. Linear estimators: Among the list of papers to be introduced shortly, the seminal work Schmidt-Hieber [2020] paved the way for providing the statistical guarantees of deep ReLU networks in the sense of (6). In their work, they demonstrated that sparsely connected deep ReLU networks (2) significantly outperform traditional statistical estimators. Specifically, if the unknown regression function $f _ { \rho }$ is a composition of functions that are individually estimable faster than $O \big ( n ^ { - \frac { \cdot 2 r } { 2 r + d } } \big )$ , then a composition-based deep ReLU network is provably more effective than estimators that do n ot utilize compositions, such as wavelet estimators. For further discussions on the paper, interested readers can find the published discussion papers Kutyniok [2020], Ghorbani et al. [2020a], Shamir [2020], Kohler and Langer [2020].  

The sparse network structure manifested in $\mathcal { N }$ in the paper had already been proven to be impressively effective in compressed learning literature [Iandola et al., 2016, Han et al., 2015a,b]. The sparsity of networks can be achieved via pruning technique [Han et al., 2015b]. One of them, Iandola et al. [2016], empirically showed the pruned convolutional neural networks with 50 times fewer parameters achieve the same accuracy level as that of the AlexNet [Krizhevsky et al., 2012] in image classification tasks, and these results pave the way for the employments of neural networks in small devices such as smart  

![](images/58f231a0353e6f76826ce5f9743dcc7b5bd892e770354570a2ea14aada457a1c.jpg)  
$f _ { \rho } \in \mathcal { g }$ : Function Space   
Figure 1: Compared to classical linear estimators (i.e., wavelet, kernel ridge regressors, etc), sparsely connected neural networks are more adaptive in estimating functions $f _ { \rho }$ with special structures. The figure illustrates the different settings of function classes $\mathcal { G }$ where neural networks exhibit superior adaptabilities over the classical estimators.  

phones or smart watches.  

In statistical literature, after the first appearance of Schmidt-Hieber [2020], several works were followed subsequently analyzing sparsely connected networks. Imaizumi and Fukumizu [2019] derived the excess risk convergence rate of sparse ReLU neural networks estimating piece-wise smooth functions, showing that deep learning can outperform the classical linear estimators including kernel ridge regressors, Fourier estimators, splines, and Gaussian processes. (Here, we refer to the estimators as ‘linear’ when they are linearly dependent on the output y.) They pointed out the discrepancy between deep networks and linear estimators appears when the target function is non-smooth. Another work, Suzuki [2018] showed the great adaptiveness of sparse ReLU networks (i.e.,(2)) for the functions in Besov space. The Besov space is a general function space including H¨older space. Specifically, it also allows functions with spatially inhomogeneous smoothness with spikes and jumps. The author mentioned deep networks possess strong adaptiveness in capturing the spatial inhomogeneity of functions, whereas linear estimators are only able to capture the global properties of target functions and cannot capture the variability of local shapes. Later, Hayakawa and Suzuki [2020] proved the linear estimators cannot distinguish the function class and its convex hull. This results in the suboptimality of linear estimators over a simple but non-convex function class, on which sparsely connected deep ReLU nets can attain nearly the minimax-optimal rate. There also have been efforts [Kohler and Langer, 2021, Farrell et al., 2021] to study the statistical guarantees of densely fully-connected networks without sparsity constraints. However, the rates of excess risk they obtained are sub-optimal.  

Avoiding curse of dimensionality: According to the classical result from Donoho and Johnstone [1998], for estimating functions in the H¨older class with smoothness $r \geq 0$ , the un-improvable minimax convergence rate of excess risk is:  

$$
\operatorname* { i n f } _ { \widehat { f } _ { n } } \operatorname* { s u p } _ { f _ { \rho } \in \{ \mathrm { H } \widetilde { \operatorname { o l d e r } } \} } \mathbb { E } _ { \mathbf { X } \sim \rho _ { \mathcal { X } } } \left[ ( \widehat { f } _ { n } ( \mathbf { X } ) - f _ { \rho } ( \mathbf { X } ) ) ^ { 2 } \right] = \mathcal { O } \bigg ( n ^ { - 2 r / 2 r + d } \bigg ) .
$$  

This rate can be problematic when the data dimension is much larger than the smoothness of function space. In this case, the convergence rate in (7) becomes quite slow in $n$ . Nonetheless, high-dimensional data are often observed in real-world applications. For instance, in ImageNet challenge, data are RGB images with a resolution of $2 2 4 \times 2 2 4$ , which means $d = 3 \times 2 2 4 \times 2 2 4$ . Then, the rate in (7) cannot explain the empirical success of deep learning. Motivated by this, many researchers have put in considerable effort to avoid the $d$ -dependence in the denominator of the rate in (7).  

Several routes exist to achieve the goal: One typical approach is to consider the various types of function spaces $\mathcal { G }$ with various smoothness: mixed-Besov space [Suzuki, 2018], Korobov space [Montanelli and Du, 2019]. Another alternative is to impose structural assumption on target function $f _ { \rho }$ .  

Such structures include additive ridge functions [Fang and Cheng, 2022], composite functions with hierarchical structures [Schmidt-Hieber, 2020, Han et al., 2022], generalized single index model [Bauer and Kohler, 2019] or multivariate adaptive regression splines (MARS) [Kohler et al., 2022]. Another line of work focuses on the geometric structure of the feature space $\mathcal { X }$ . These works take the advantage of high-dimensional data having practically low-intrinsic dimensionality [Tenenbaum et al., 2000, Roweis and Saul, 2000]. Under this setting, Nakada and Imaizumi [2020] showed deep neural networks can achieve a fast rate over a broad class of measure on $\mathcal { X }$ , such as data on highly non-smooth fractal sets. This should be contrasted with the fact that linear estimators, which are known to be adaptive to intrinsic dimensions, can achieve fast convergence rates only when the data lie on smooth manifolds. Another work, Chen et al. [2022a], showed the adaptiveness of deep ReLU networks to the data with low-dimensional geometric structures. They were interested in estimating target function $f _ { \rho }$ in H¨older spaces defined over a low-dimensional manifold $\mathcal { M }$ embedded in $\mathbb { R } ^ { d }$ . Figure 1 summarizes the cases on classes $\mathcal { G }$ where neural nets exhibit superior adaptabilities over the classical estimators.  

Recently, Suh et al. [2022] studied deep ReLU networks estimating H¨older functions on a unit sphere and showed that these networks can avoid the curse of dimensionality as function smoothness increases with the data dimension $d$ , i.e., $r = \mathcal { O } ( d )$ . This behavior was not observed in the aforementioned literature, where $\mathcal { X }$ is set as a cube, i.e., $\mathcal { X } : = [ 0 , 1 ] ^ { d }$ . When $\mathcal { X }$ is a cube, several studies [Lu et al., 2021, Jiao et al., 2021, Shen et al., 2021] have attempted to track and reduce the $d$ -dependence in the constant factor hidden in the Big-O notation in (7). Interested readers can find the detailed comparisons of the results from these works in the Appendix C in Suh et al. [2022].  

Remark 1: Note that the aforementioned works are based on the constructions of sparse networks. From a technical perspective, the sparsity assumption is natural in the sense of the equation (6). Nonetheless, as mentioned in Ghorbani et al. [2020a], it is still an open question whether the sparsity (i.e., $\mathcal { N }$ ) is a sufficient complexity measure of $\mathcal { F }$ for generalizability, as densely connected networks are observed more commonly in practice without the regularized penalties. These often lead to overparametrized networks with huge complexity on $\mathcal { F }$ , not guaranteeing good generalizability in the sense of (6). Given this observation, one popular heuristic argument for explaining the good generalizability of dense neural nets is the implicit regularization of gradient-based algorithms; that is, the model complexity is not captured by an explicit penalty but by the dynamics of the algorithms implicitly. For some special cases [Gunasekar et al., 2018, Ji and Telgarsky, 2018], it has been shown that the gradient-based methods provably find the solutions of low-complexity in the huge parameter space. (e.g., low-rank matrix in matrix estimation problem.) A similar phenomenon has been empirically observed in function estimation problems via neural networks [Cao and Gu, 2019, Hu et al., 2020b], sparking further research to be discussed in the next section. For more in-depth discussions on these issues beyond what is covered in this review, readers should consult Section 3 of He and Tao [2020], Neyshabur [2017], and references therein.  

Remark 2: Reviews on approximation-based statistical guarantees of neural networks for classification problems are provided in the Appendix B due to page limit.  

# 3 Training dynamics-based statistical guarantees  

The literature introduced in the Section 2 relied on the assumption that the global minimizer of the empirical risk, ${ \widehat { f } } _ { n }$ in (5), is obtainable. However, due to the non-convex nature of the loss function, neural networ bestimated using commonly employed gradient-based methods lack guarantees of finding ${ \widehat { f } } _ { n }$ , which leads to the following natural question:  

# Does the neural network estimated by gradient-based methods generalize well?  

The papers to be introduced shortly will try to answer to the above question. Due to the complex nature of the problem, most papers to be reviewed have considered the following shallow neural network (i.e., networks with one hidden layer) $f _ { \mathbf { W } } ( \mathbf { x } )$ with $M$ number of hidden neurons:  

$$
f _ { \mathbf { W } } ( \mathbf { x } ) = \frac { \alpha } { M } \sum _ { r = 1 } ^ { M } a _ { r } \sigma ( w _ { r } ^ { \top } \mathbf { x } ) .
$$  

Notation $\mathbf { x } \in \mathcal { X } \subset \mathbb { R } ^ { d }$ is an input vector, $\{ w _ { r } \in \mathbb { R } ^ { d } \} _ { r = 1 } ^ { M }$ are the weights in the first hidden layer, and $\{ a _ { r } \in \mathbb { R } \} _ { r = 1 } ^ { M }$ are the weights in the output layer. Let us denote the pair $\mathbf { W } : = \{ ( a _ { r } , w _ { r } ) \} _ { r = 1 } ^ { M }$ . The network dynamic is scaled with the factor $\textstyle { \frac { \alpha } { M } }$ . If the network width (i.e., $M$ ) is small, the scaling factor has negligible effects on the network dynamics. But for the wide enough networks (i.e., overparametrized setting), the scaling difference yields completely different behaviors in the dynamics. Given the $M$ is large enough, we will focus on two specific regimes:  

1. Neural Tangent Kernel regime [Jacot et al., 2018, Du et al., 2019] with $\alpha = \sqrt { M }$ . (Subsection 3.1)   
2. Mean Field regime [Mei et al., 2018b, 2019] with $\alpha = 1$ . (Subsection 3.2)  

Additionally, we will review several works that try to address the drawbacks of the NTK framework, as well as some that provide unifying perspectives on these two regimes (Subsection 3.3).  

We focus on reviewing the papers that work on $\ell _ { 2 }$ -loss function : $\begin{array} { r } { \mathcal { L } _ { \mathbf { S } } ( \mathbf { W } ) = \frac { 1 } { 2 } \sum _ { i = 1 } ^ { n } \left( y _ { i } - f _ { \mathbf { W } } ( \mathbf { x } _ { i } ) \right) ^ { 2 } } \end{array}$ . Note that in contrast to (5), structural requirements such as $\mathcal { F } ( L , \mathbf { p } , \mathcal { N } )$ are removed. The model parameter pairs $\mathbf { W }$ are updated through the gradient-based methods. Let $\mathbf { W } _ { ( 0 ) }$ be the initialized weight pairs. Then, we have the following gradient descent (GD) update rule with step-size $\eta > 0$ and $k \geq 1$ :  

$$
\mathbf { G D } : \mathbf { W } _ { ( k ) } = \mathbf { W } _ { ( k - 1 ) } - \eta \nabla _ { \mathbf { W } } \mathcal { L } _ { \mathbf { S } } \big ( \mathbf { W } \big ) \mid _ { \mathbf { W } = \mathbf { W } _ { ( k - 1 ) } } .
$$  

Another celebrated gradient-based method is Stochastic Gradient Descent (SGD). The algorithm takes a randomly sampled subset (i.e., $\boldsymbol { B }$ ) of the data $\mathcal { D }$ , computes the gradient with the selected samples, and this significantly reduces the computational burdens in GD. Another frequently adopted algorithm in practice is Noisy Gradient Descent (NGD), which adds the centered Gaussian noise to the gradient of loss function in (9). It is known that adding noises to gradient helps training [Neelakantan et al., 2015] and generalization [Smith et al., 2020] of neural networks.  

# 3.1 Neural Tangent Kernel Perspective  

Over the past few years, Neural Tangent Kernel (NTK) [Arora et al., 2019b, Jacot et al., 2018, Lee et al., 2018, Chizat and Bach, 2018] has been one of the most seminal discoveries in the theory of neural networks. The underpinning idea of the NTK-type theory comes from the observation that in a wide-enough neural net, model parameters updated by gradient descent (GD) stay close to their initializations during the training, so that the dynamics of the networks can be approximated by the first-order Taylor expansion with respect to its parameters at initialization; that is, we denote the output of neural network as $f _ { \mathbf { W } _ { ( k ) } } ( \mathbf { x } ) \in \mathbb { R }$ with input $\mathbf { x } \in \mathcal { X }$ and model parameter $\mathbf { W } _ { ( k ) }$ updated at $k$ -th iteration of gradient descent, then the dynamics of $f _ { \mathbf { W } _ { ( k ) } } ( \mathbf { x } )$ over $k \geq 1$ can be represented as follows:  

$$
f _ { \mathbf { W } _ { ( k ) } } ( \mathbf { x } ) = f _ { \mathbf { W } _ { ( 0 ) } } ( \mathbf { x } ) + \langle \nabla f _ { \mathbf { W } _ { ( 0 ) } } ( \mathbf { x } ) , \mathbf { W } _ { ( \mathbf { k } ) } - \mathbf { W } _ { ( 0 ) } \rangle + o ( \| \mathbf { W } _ { ( \mathbf { k } ) } - \mathbf { W } _ { ( 0 ) } \| _ { \mathrm { F } } ^ { 2 } ) ,
$$  

where $o \big ( \| \mathbf { W } _ { ( \mathbf { k } ) } - \mathbf { W } _ { ( 0 ) } \| _ { \mathrm { F } } ^ { 2 } \big )$ is the small random quantity that tends to $0$ as network width gets close to infinity, measuring the distance between updated model parameter and its initialization in Frobenius norm. Specifically, it can be shown $\begin{array} { r } { \| \mathbf { W } _ { \mathbf { ( k ) } } - \mathbf { W } _ { \mathbf { ( 0 ) } } \| _ { \mathrm { F } } ^ { 2 } \le \mathcal { O } ( \frac { 1 } { \sqrt { M } } ) } \end{array}$ with sufficiently large enough $M$ . (For instance, see Remark 3.1 in Du et al. [2018].) Under this setting, the right-hand side of (10) is linear in the network parameter $\mathbf { W } _ { ( k ) }$ . As a consequence, training on $\ell _ { 2 }$ -loss with gradient descent leads to kernel regression solution with respect to the (random) kernel induced by the feature mapping $\phi ( \mathbf { x } ) : = \nabla _ { \mathbf { W } _ { 0 } } f ( \mathbf { x } )$ for all $\mathbf { x } \in \mathcal { X }$ . The inner-product of two feature mappings evaluated at two data points $\mathbf { x } _ { i } , \mathbf { x } _ { j }$ is denoted as $\mathbf { K } ^ { ( M ) } ( \mathbf { x } _ { i } , \mathbf { x } _ { j } ) : = \langle \phi ( \mathbf { x } _ { i } ) , \phi ( \mathbf { x } _ { j } ) \rangle$ for all $1 \leq i , j \leq n$ .  

Note that the ${ \mathbf { K } } ^ { ( M ) } ( \cdot , \cdot )$ is a random matrix with respect to the initializations $\mathbf { W } _ { ( 0 ) }$ . It is shown that it converges to its deterministic limit (i.e., $M \to \infty$ ) in probability pointwisely [Jacot et al., 2018, Arora et al., 2019b, Lee et al., 2018] and uniformly [Lai et al., 2023] over $\mathcal { X } \times \mathcal { X }$ . The limit matrix is named NTK denoted as $\{ \mathbf { K } ^ { \infty } ( \mathbf { x } _ { i } , \mathbf { x } _ { j } ) \} _ { 1 \leq i , j \leq n } \in \mathbb { R } ^ { n \times n }$ . Hereafter, we write the eigen-decomposition of $\begin{array} { r } { \mathbf { K } ^ { \infty } = \sum _ { j = 1 } ^ { n } \lambda _ { j } \mathbf { v } _ { j } \mathbf { v } _ { j } ^ { \top } } \end{array}$ , where $\lambda _ { 1 } \geq \cdot \cdot \cdot \geq \lambda _ { n } \geq 0$ with corresponding eigenvectors $\mathbf { v } _ { j } \in \mathbb { R } ^ { n }$ .  

Optimization of neural nets in NTK regime. Many papers have come out in the sequel to tackle the optimization properties of neural networks in the NTK regime. Under the above setting, Du et al. [2018] proves the linear convergence of training loss of shallow ReLU networks. Specifically, the authors randomly initialized $a _ { r } \sim \mathrm { U n i f } \{ - 1 , + 1 \}$ and $\mathbf { w } _ { r } \sim \mathcal { N } ( 0 , \mathcal { T } )$ , and train the $\mathbf { w } _ { r }$ via GD with a constant positive step size $\eta = \mathcal { O } ( 1 )$ . Here, the linear convergence rate means that the training loss at $k$ -th gradient descent decays at a geometric rate with respect to the initial training loss, which is explicitly stated in Theorem 4.1. in Du et al. [2018] as:  

$$
\| f _ { \mathbf { W } _ { ( k ) } } ( \mathbf { x } ) - \mathbf { y } \| _ { 2 } ^ { 2 } \leq \bigg ( 1 - \frac { \eta \lambda _ { n } } { 2 } \bigg ) ^ { k } \| f _ { \mathbf { W } _ { ( 0 ) } } ( \mathbf { x } ) - \mathbf { y } \| _ { 2 } ^ { 2 } .
$$  

Their result requires the network width $M$ to be in the order of $\Omega \big ( \frac { n ^ { 6 } } { \lambda _ { n _ { \bullet } } } \big )$ , and the decay rate is dependent on the minimum eigenvalue of NTK, $\lambda _ { n }$ . Here, for the geometric decay rate, $\lambda _ { n }$ needs to be strictly greater than 0 induced from data non-parallel assumption. (i.e., no two inputs are parallel.)  

Afterwards, there have been several attempts to reduce the overparametrization size. One work we are aware of is Song and Yang [2019] where they used matrix Chernoff bound to reduce the width size up to $\begin{array} { r } { M = \Omega \big ( \frac { n ^ { 2 } } { \lambda _ { n } ^ { 4 } } \big ) } \end{array}$ with a slightly stronger assumption than the data non-parallel assumption. Several subsequent works Allen-Zhu et al. [2018], Du et al. [2019], Zou et al. [2018], Wu et al. [2019], Oymak and Soltanolkotabi [2020], Suh et al. [2021] extended the results showing the linear convergence of training loss of deep ReLU networks with $L$ -hidden layers. For a succinct comparison of the overparametrized conditions on $M$ in aforementioned papers, we direct readers to Table 1 in Zou and Gu [2019].  

Spectral bias of shallow ReL $U$ networks. Motivated by the result (11), researchers further studied the spectral bias of deep neural networks, investigating why the neural dynamics learn the lower frequency components of the functions faster than they learn the higher frequency counterparts. The specific results are stated in terms of eigenvalues $\mu _ { 1 } \geq \mu _ { 2 } \geq . . .$ and corresponding orthonormal eigenfunctions $\phi _ { 1 } ( \cdot ) , \phi _ { 2 } ( \cdot ) , . . .$ of integral operator $\mathcal { L } _ { \mathbf { K } ^ { \infty } }$ induced by $\mathbf { K } ^ { \infty }$ :  

$$
\mathcal { L } _ { \mathbf { K } ^ { \infty } } ( f ) ( \mathbf { x } ) : = \int _ { \mathcal { X } } \mathbf { K } ^ { \infty } ( \mathbf { x } , \mathbf { y } ) f ( \mathbf { y } ) \rho ( \mathbf { d } \mathbf { y } ) , \quad \forall f \in \mathcal { L } ^ { 2 } ( \mathcal { X } ) ,
$$  

where $\mathcal { L } ^ { 2 } ( \mathcal { X } )$ is a $L _ { 2 }$ -space on $\mathcal { X }$ . Specifically, Cao et al. [2019], Bietti and Mairal [2019] provided the spectral decay rates of $( \mu _ { k } ) _ { k }$ for shallow ReLU networks when $\mathbf { x }$ is from a unit-sphere equipped with uniform measure as follows: 1  

Proposition 3.1. (Theorem 4.3 in Cao et al. [2019], Proposition 5 in Bietti and Mairal [2019]) For the neural tangent kernel corresponding to a two-layer feed-forward ReLU network, the eigenvalues $( \mu _ { k } ) _ { k }$ satisfy the following:  

$$
\begin{array} { r } { \left\{ \begin{array} { l l } { \mu _ { k } = \Omega ( 1 ) , } & { \mathrm { ~ w h e n ~ } k = 0 , 1 , } \\ { \mu _ { k } = 0 , } & { \mathrm { ~ w h e n ~ } k ( \ge 3 ) \mathrm { ~ i s ~ o d d } , } \\ { \mu _ { k } = \Omega ( \operatorname* { m a x } ( k ^ { - d - 1 } , d ^ { - k - 1 } ) ) , } & { \mathrm { ~ w h e n ~ } k ( \ge 2 ) \mathrm { ~ i s ~ e v e n } . } \end{array} \right. } \end{array}
$$  

The decay rate is exponentially fast in input dimension $d$ for $k \gg d$ . An interesting benefit of having a specific decay rate is that we can measure the size of reproducing kernel Hilbert spaces (RKHS) induced from the kernel $\mathbf { K } ^ { \infty }$ . (We denote this RKHS as $\mathcal { H } ^ { \infty }$ for the later use.) The slower the decay rate is, the larger the RKHS becomes, allowing higher-frequency information of function to be included.  

With the specified eigendecay rates on $\mu _ { k }$ , Theorem 4.2 in Cao et al. [2019] proved the spectral bias of neural network training in the NTK regime. Specifically, as long as the network is wide enough and the sample size is large enough, gradient descent first learns the target function along the eigen directions of NTK with larger eigenvalues, and learns the rest components corresponding to smaller eigenvalues later. Similary, Hu et al. [2019] showed that gradient descent learns the linear component of target functions in the early training stage. But crucially they do not require the network to have a disproportionately large width, and the network is allowed to escape the kernel regime later in training.  

Generalization of neural nets in NTK regime. Here, we review some important works that study the generalizabilities of (8). To the best of our knowledge, Arora et al. [2019a] provided the first step in understanding the role of NTK in the generalizability of neural nets. Specifically, they showed that for $\begin{array} { r } { M = \Omega ( \frac { n ^ { 2 } \log ( n ) } { \lambda _ { n } } ) } \end{array}$ and $\begin{array} { r } { k \geq \tilde { \Omega } \big ( \frac { 1 } { \eta \lambda _ { n } } \big ) } \end{array}$ , the $\ell _ { 2 }$ -population loss of $f _ { \mathbf { W } _ { ( k ) } } ( \mathbf { x } )$ is bounded by:  

$$
\mathbb { E } \Big [ ( f _ { \mathbf { W } _ { ( k ) } } ( \mathbf { x } ) - \mathbf { y } ) ^ { 2 } \Big ] \leq \mathcal { O } \bigg ( \sqrt { \frac { \mathbf { y } ^ { \top } ( \mathbf { K } ^ { \infty } ) ^ { - 1 } \mathbf { y } } { n } } \bigg ) .
$$  

Observe the nominator in the bound can be written as $\begin{array} { r } { \mathbf { y } ^ { \top } ( \mathbf { K } ^ { \infty } ) ^ { - 1 } \mathbf { y } : = \sum _ { i = 1 } ^ { n } \frac { 1 } { \lambda _ { i } } ( \mathbf { v } _ { i } ^ { \top } \mathbf { y } ) ^ { 2 } } \end{array}$ . This implies the projections $\mathbf { v } _ { i } ^ { \top } \mathbf { y }$ that correspond to small eigenvalues $\lambda _ { i }$ should be small for good generalizations on unseen data. This theoretical result is consistent with the empirical finding in Zhang et al. [2017]. Indeed, they performed their own empirical experiments on MNIST and CIFAR-10 datasets, showing the projections $\{ ( \mathbf { v } _ { i } ^ { \mid } \mathbf { y } ) \} _ { i = 1 } ^ { n }$ sharply drops for true labels $\mathbf { y }$ , leading to the fast convergence rate. In contrast, when the projections are close to being uniform for random labels $\mathbf { y }$ , leading to slow convergence. (See Figure 1 in Zhang et al. [2017])  

However, the bound (12) is obtained in the noiseless setting and becomes vacuous under the presence of noise. In this regard, under the noisy setting (4), Nitanda and Suzuki [2020] showed that  

$$
\mathbb { E } \big [ \| f _ { W _ { ( k ) } } ( \mathbf { x } ) - f _ { \rho } ( \mathbf { x } ) \| _ { L _ { 2 } } ^ { 2 } \big ] \leq \mathcal { O } \big ( k ^ { - \frac { 2 r \beta } { 2 r \beta + 1 } } \big ) ,
$$  

where the target function $f _ { \rho }$ belongs to the subset of $\mathcal { H } ^ { \infty }$ , and $f _ { W _ { ( k ) } }$ is a shallow neural network with a smooth activation function that approximates ReLU. Here, the network is estimated via one-pass SGD (take one sample for gradient update and the samples are visited only once during training), minimizing $\ell _ { 2 }$ -regularized expected loss. This setting leads to $k = n$ . The rate (13) is minimax optimal, which is faster than $\begin{array} { r l } { \mathcal { O } ( \frac { 1 } { \sqrt { n } } ) } & { { } } \end{array}$ in Arora et al. [2019a]. It is characterized by two control parameters, $\beta$ and $r$ , where $\beta > 1$ controls the size of $\mathcal { H } ^ { \infty }$ and $r \in [ 1 / 2 , 1 ]$ controls the size of subset of $\mathcal { H } ^ { \infty }$ where $f _ { \rho }$ belongs. The bound (13) has an interesting bias-variance trade-off between these two quantities $\beta$ and $r$ . For large $\beta$ , the whole space $\mathcal { H } ^ { \infty }$ becomes small, and the subspace of $\mathcal { H } ^ { \infty }$ needs to be as large as possible for the faster convergence rate, and vice versa.  

However, as noted by the following work [Hu et al., 2021], the rate in (13) requires the network width $M$ to be exponential in $n$ . The work reduced the size of overparametrization to $\tilde { \Omega } ( n ^ { 6 } )$ when the network parameters are estimated by gradient descent. The paper proved that the overparametrized shallow ReLU networks require $\ell _ { 2 }$ -regularization for GD to achieve minimax convergence rate $\mathcal { O } ( n ^ { - \frac { d } { 2 d - 1 } } )$ . Afterward, Suh et al. [2021] extended the result to deep ReLU networks in the NTK regime, showing that $\ell _ { 2 }$ -regularization is also required for achieving minimax rate for deep networks.  

# 3.2 Mean-Field Perspective  

A “mean-field” viewpoint is another interesting paradigm to help us understand the optimization landscape of neural network models. Recall that neural network dynamics in the Mean-Field (MF) regime corresponds to $\alpha = 1$ in (8).  

The term mean-field comes from an analogy with mean field models in mathematical physics, which analyze the stochastic behavior of many identical particles [Ryzhik, 2023]. Let us denote $\theta _ { r } : = ( a _ { r } , w _ { r } ) \in$ $\mathbb { R } ^ { d + 1 }$ , $\sigma _ { \star } ( \mathbf { x } , \boldsymbol { \theta } _ { r } ) : = a _ { r } \sigma ( w _ { r } ^ { \top } \mathbf { x } )$ in (8). Weight pairs $\{ \theta _ { r } \} _ { r = 1 } ^ { M }$ are considered as a collection of gas particles in $\mathbf { D }$ -dimensional spaces with $\mathbf { D } : = d + 1$ . We consider there are infinitely many gas particles allowing $M  \infty$ which yields the following integral representation of neural dynamics:  

$$
\frac { 1 } { M } \sum _ { r = 1 } ^ { M } \sigma _ { \star } ( \mathbf { x } ; \theta _ { r } ) \xrightarrow { M \to \infty } f ( \mathbf { x } ; \rho ) : = \int \sigma _ { \star } ( \mathbf { x } ; \theta ) \rho ( d \theta ) ,
$$  

where $\theta _ { r } \sim \rho$ for $r = 1 , \ldots , M$ . The integral representation (14) is convenient for the mathematical analysis as it is linear with respect to the measure $\rho$ . For instance, see Bengio et al. [2005].  

Table 1: NTK v.s. Mean-Field regimes   


<html><body><table><tr><td></td><td>NTK Regime</td><td>MF Regime</td></tr><tr><td>Pros</td><td>1.Same scaling as in practice 2.Finite time convergence rate 3.Generalization bounds</td><td>1.Does not require 0(k） to be close to 0(0) 2.Potentially learn a larger class of functions</td></tr><tr><td>Cons</td><td>Require 0(k） to be close to 0(0)</td><td>1. Not the same scaling as in practice 2. No finite-time convergence rate 3.No generalization bounds</td></tr></table></body></html>  

Under this setting, the seminal work [Mei et al., 2018b] studied the evolution of particles $\theta ^ { ( k ) } \in \mathbb { R } ^ { D }$ updated by $k$ -steps of one-pass SGD (take one sample for gradient update and the samples are visited only once during training) under $\ell _ { 2 }$ -loss. Interestingly, they proved the trajectories of empirical distribution of $\theta ^ { ( k ) }$ denoted as $\begin{array} { r } { \widehat { \rho } _ { k } : = \frac { 1 } { M } \sum _ { r = 1 } ^ { M } \delta _ { \theta _ { r } ^ { ( k ) } } } \end{array}$ weakly converges to the deterministic limit $\rho _ { t } \in \mathcal { P } ( \mathbb { R } ^ { \mathcal { D } } )$ as $k  \infty$ and $M \to \infty$ . Tbhe measure $\rho _ { t }$ is the solution of the following nonlinear partial differential equation (PDE):  

$$
\begin{array} { r l r l } & { \partial _ { t } \rho _ { t } = \nabla _ { \theta } \cdot \bigl ( \rho _ { t } \nabla _ { \theta } \Psi ( \theta ; \rho _ { t } ) \bigr ) , } & & { \Psi ( \theta ; \rho _ { t } ) : = \mathcal { V } ( \theta ) + \int \mathcal { U } ( \theta , \bar { \theta } ) \rho _ { t } ( d \bar { \theta } ) , } \\ & { \mathcal { V } ( \theta ) : = - \mathbb { E } \{ \mathbf { y } \sigma _ { \star } ( \mathbf { x } ; \theta ) \} , } & & { \mathcal { U } ( \theta _ { 1 } , \theta _ { 2 } ) : = \mathbb { E } \{ \sigma _ { \star } ( \mathbf { x } ; \theta _ { 1 } ) \sigma _ { \star } ( \mathbf { x } ; \theta _ { 2 } ) \} . } \end{array}
$$  

The above PDE describes the evolution of each particle $\left( \theta _ { r } \right)$ in the force field created by the densities of all the other particles. (We provide more descriptions on the above PDE in the Appendix C.) Denote $\mathcal { R } ( { \boldsymbol \rho } _ { t } ) : = \mathbb { E } [ ( y - f ( \mathbf { x } ; { \boldsymbol \rho } _ { t } ) ) ^ { 2 } ]$ and let $\mathcal { R } _ { M }$ be the empirical version of $\mathcal { R }$ , then under some regularity assumptions on the network, we have:  

$$
\operatorname* { s u p } _ { 0 \leq t \leq T } \left| \mathcal { R } ( \rho _ { t } ) - \mathcal { R } _ { M } ( \widehat { \rho } _ { \lfloor t / 2 \eta _ { l } \rfloor } ) \right| \leq e ^ { C ( T + 1 ) } \cdot \sqrt { \frac { 1 } { M } \vee 2 \eta } \cdot \sqrt { D + \log \frac { M } { 2 \eta } } .
$$  

The condition for the bound to vanish to $0$ is (1) $M \gg D$ , (2) $\begin{array} { r } { \eta \ll \frac { 1 } { D } } \end{array}$ , and (3) PDE converges in $T = \mathcal { O } ( 1 )$ iterations. It is interesting to note that the generic ODE approximation requires the step-size $\eta$ to be less than the order of the total number of parameters in the model (i.e., $\begin{array} { r } { \eta \ll \frac { 1 } { M D } } \end{array}$ ), whereas in this setting the step-size $\begin{array} { r } { \eta \ll \frac { 1 } { D } } \end{array}$ should be enough. Also, recall that the number of sample size $n$ is equivalent to the iteration steps $\begin{array} { r } { k : = \lfloor \frac { T } { \varepsilon } \rfloor } \end{array}$ of one-pass SGD with $T = \mathcal { O } ( 1 )$ . Then, this means $n = \mathcal { O } ( D ) \ll \mathcal { O } ( M D )$ should be enough for a good approximation. Another notable fact is; in contrast to the NTK regime, the evolution of weights $\theta _ { r }$ are non-linear, and in particular, the weights move away from their initialization during training. Indeed under mild assumption, we can show that for small enough step size $\eta$ , $\begin{array} { r } { \operatorname* { l i m } _ { M \to \infty } \| \theta ^ { ( k ) } - \theta ^ { ( 0 ) } \| _ { 2 } ^ { 2 } / M = \Omega ( \eta ^ { 2 } ) } \end{array}$ in the mean-field regime, while $\begin{array} { r } { \operatorname* { s u p } _ { t \geq 0 } \| \theta ^ { ( t ) } - \theta ^ { ( 0 ) } \| _ { 2 } ^ { 2 } / M = { \mathcal { O } } ( n / ( M d ) ) } \end{array}$ in the NTK regime. See Bartlett et al. [2021].  

Despite the nice characterizations of SGD dynamics, still, the bound in (16) has room for the improvement; the number of neurons $M$ is dependent on the ambient data dimension $d$ and the bound is only applicable to the SGD with short convergence iterations $T = \mathcal { O } ( 1 )$ . A follow-up work [Mei et al., 2019] has attempted to tackle these challenges. Particularly, they proved that there exists a constant $K$ that only depends on intrinsic features of the activation and data distribution, such that with high probability, the following holds:  

$$
\operatorname* { s u p } _ { 0 \leq t \leq T } \left| \mathcal { R } ( \rho _ { t } ) - \mathcal { R } _ { M } ( \widehat { \rho } _ { \lfloor t / \varepsilon \rfloor } ) \right| \leq K e ^ { K ( T \eta ) ^ { 3 } } \Bigg \{ \sqrt { \frac { \log ( M ) } { M } } + \sqrt { d + \log ( M ) } \sqrt { \eta } \Bigg \} .
$$  

A remarkable feature of this bound is that as long as $T \eta = \mathcal { O } ( 1 )$ and $K = \mathcal { O } ( 1 )$ , the number of neurons only needs to be chosen $M \ \gg \ 1$ for the mean-field approximation to be accurate. The condition $T \eta = \mathcal { O } ( 1 )$ mitigates the exponential dependence on $T$ and the bound does not need to scale with the ambient dimension $d$ . Later, researchers from the same group generalize the result into multi-layer settings [Nguyen and Pham, 2020].  

# 3.3 Beyond the NTK and Mean-Field regimes  

Despite nice theoretical descriptions on training dynamics of gradient descent in loss functions, Arora et al. [2019b], Lee et al. [2018], Chizat and Bach [2018] empirically found significant performance gaps between NTK and actual training in many downstream tasks. For instance, Arora et al. [2019b] derived the convolution neural network (CNN)-based CNTK, and empirically found the $5 \% \sim 6 \%$ performance gaps between CNN and CNTK-based kernel regressor in image classification tasks. (i.e., CNN performs better.) This indicates the potential benefits of the finite width in neural networks.  

Beyond the NTK regime. These gaps have been theoretically studied in several papers, including Wei et al. [2019], Allen-Zhu and Li [2019], Ghorbani et al. [2020b], Yehudai and Shamir [2019], Chizat and Bach [2018]. They showed that NTK has worse generalization guarantees than finite-width neural networks in some settings. For example, Mei et al. [2018a] demonstrated that training a neural network with one hidden neuron can efficiently learn a single neuron target function with ${ \mathcal { O } } ( d \log d )$ samples, whereas the corresponding RKHS has a test error that is bounded away from zero for any sample size polynomial in $d$ [Yehudai and Shamir, 2019, Ghorbani et al., 2021]. However, kernel methods often perform comparably to neural networks in some image classification tasks [Li et al., 2019, Novak et al., 2018]. Ghorbani et al. [2020b] provided a unified framework under spiked covariate models to explain this divergence, showing that while RKHS and neural networks perform similarly in certain stylized tasks, RKHS performance deteriorates under isotropic covariate distributions (e.g., noisy high-frequency image components), whereas neural networks are less affected by such noise. Another work, Wei et al. [2019] gives an interesting example of where NTK or any kernel methods are statistically limited, whereas regularized neural networks have better sample complexity. They proved that there is a $\Omega ( d )$ sample-complexity gap between the regularized neural net and kernel prediction function for estimating $f _ { \rho } ( { \bf x } ) = { \bf x } _ { 1 } { \bf x } _ { 2 }$ with $\mathbf { x } _ { i } \sim \{ \pm 1 \}$ for $\mathbf { x } \in \mathbb { R } ^ { d }$ .  

The aforementioned works explained the superiority of neural networks over the networks in the NTK regime under some highly stylized settings. There also has been another line of works [Li and Liang, 2018, Allen-Zhu et al., 2019, Bai and Lee, 2019] to explain how the networks estimated through gradient-based methods generalize well but critically do not rely on the linearization of network dynamics. Under the distribution-free setting (i.e., no distributional assumptions on covariates), Allen-Zhu et al. [2019] provided optimization and generalization guarantees for three-layer ReLU networks learning the function classes of three-layer networks with smooth activation functions. They showed that three-layer ReLU networks can learn a larger function class than two-layer ReLU networks do. Unlike NTK techniques, their approach allows non-convex interactions across hidden layers, enabling parameters trained by SGD to move far from their initializations. Motivated from Allen-Zhu et al. [2019], the paper [Bai and Lee, 2019] studied the optimization and generalization of shallow networks with smooth activation function $\sigma ( \cdot )$ via relating the network dynamics $f _ { \mathbf { W } } ( \mathbf { x } )$ in (8) with second-order (or quadratic) approximations. They explicitly showed the sample complexity of the quadratic model is smaller than that of the linear NTK model in learning some stylized target functions by the factor of ${ \mathcal { O } } ( d )$ . Similarly, relying on the tensor decomposition techniques instead of quadratic approximation, Li and Liang [2018] showed the separations of shallow ReLU networks and NTK regressors.  

Unifying views of NTK and MF regimes. There have been several attempts to give a unifying view of the two regimes; NTK and Mean-Field. An attempt we are aware of is Chen et al. [2020b]. The motivation of this paper is summarized in Table 1, along with the pros and cons of NTK and Mean-Field regimes. They showed the two-layer neural networks learned through noisy-gradient descent (NGD) in MF scaling can potentially learn a larger class of functions than networks in NTK scaling can do. One seminal work, Yang and Hu [2020], identifies a set of scales for initialized weights and SGD step sizes where feature learning occurs in deep neural networks in the infinite width limit. They offer a unified framework that encompasses parameterizations in both the NTK and MF regimes. Feature learning is the core property of neural networks: the ability to learn useful features out of raw data [Girshick et al., 2014, Devlin et al., 2018] that adapt to the learning problem. For instance, BERT [Devlin et al., 2018] leveraged this property of neural networks for sentence sentiment analysis. Specifically, in the regime where the network width and data size are comparable, Ba et al. [2022] showed the non-trivial feature learning occurs at the early phase (one gradient step in GD) of shallow neural network training with a large enough step-size. Similarly to Ghorbani et al. [2020b],  

![](images/f507cb44c81f9bb8b6f253344951e10420157d71fa9fae8d4d28b1a5ff616043.jpg)  
Figure 2: Development of the literature (y-axis) on algorithm-based neural network analysis over time (x-axis). We view the ultimate goal (represented as star) of this line of research is to theoretically demystify feature learning of neural nets with deep layers and finite width, closing the gap with the practice. Note that kernel regressor in NTK regime does not exhibit feature learning functionality.  

Ba et al. [2024] examined the advantages of shallow neural networks with finite width over kernel methods under the spiked covariance model. Both studies demonstrated that neural networks and kernel methods benefit from stronger low-dimensional structures (i.e., larger spikes). However, Ba et al. [2024] focused on gradient-based optimization guarantees, while Ghorbani et al. [2020b] provided only approximation-based analysis.  

Interested readers can find more detailed descriptions on the works Wei et al. [2019], Ghorbani et al. [2020b], Bai and Lee [2019], Chen et al. [2020b], Yang and Hu [2020] in the Appendix D and E. In Figure 2, we summarizes our own views on the developments of literatures along this line of research.  

# 4 Statistical Guarantees of Generative Models  

In this section, we sequentially explore the statistical literature on three topics: Generative Adversarial Networks (GANs), diffusion models, and in-context learning in large language models (LLMs).  

# 4.1 Generative Adversarial Networks (GAN)  

Over the past decade, Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] have stood out as a significant unsupervised learning approach, known for their ability to learn the data distribution and efficiently sample the data from it. The main goal of GAN is to learn the target distribution $\mathbf X \sim \nu$ through the adversarial training between a Discriminator and a Generator. Here, the generator takes the input $\mathcal { Z }$ from prior distributions such as Gaussian or Uniform (i.e., $\mathcal { Z } \sim \pi$ ) and the input is push-forwarded by transformation map $g : { \mathcal { Z } }  g ( { \mathcal { Z } } )$ . In this seminal paper, the distribution of random variable $g ( \mathcal { Z } )$ is referred to as an implicit distribution $\mu$ . The primary objective of the generator is to produce synthetic data from $\mu$ that closely resembles samples from the target distribution.  

The adversarial training between the discriminator and generator is enabled through optimizing the following minimax problem w.r.t. functions in the generator class $\mathcal { G }$ and discriminator class $\mathcal { F }$ :  

$$
( g ^ { \star } , f ^ { \star } ) \in \underset { g \in \mathcal { G } } { \arg \operatorname* { m i n } } \underset { f \in \mathcal { F } } { \arg \operatorname* { m a x } } \bigg \{ \mathbb { E } _ { \mathcal { Z } \sim \pi } f ( g ( \mathcal { Z } ) ) - \mathbb { E } _ { X \sim \nu } f ( X ) \bigg \} .
$$  

In practice, the above expectations can be approximated with $m$ training data from $\nu$ and $n$ drawn samples from $\mu$ . The inner maximization problem in (18) is an Integral Probability Metric (IPM, Mu¨ller [1997]), which quantifies the discrepancy between two distributions $\mu$ and $\nu$ w.r.t. a symmetric function class, i.e., $f \in { \mathcal { F } }$ , then $- f \in \mathcal { F }$ ;  

$$
d _ { \mathcal { F } } ( \mu , \nu ) = \operatorname* { s u p } _ { f \in \mathcal { F } } \bigg \{ \mathbb { E } _ { x \sim \mu } f ( x ) - \mathbb { E } _ { y \sim \nu } f ( y ) \bigg \} .
$$  

When $\mathcal { F }$ is taken to be all 1-Lipschitz functions, $d _ { W _ { 1 } } ( \cdot , \cdot )$ is the Wasserstein-1 distance; when $\mathcal { F }$ is the class of all indicator functions, $d _ { \mathrm { T V } } ( \cdot , \cdot )$ is the total variation distance; when $\mathcal { F }$ is taken as a class of neural networks, $d _ { \mathrm { N N } } ( \cdot , \cdot )$ is the “neural net distance” (see Arora et al. [2017]). Under this setting, a generator from $\mathcal { G }$ attempts to minimize the IPM between $\mu$ and $\nu$ .  

Generalization of $G A N s$ . A question that naturally arises is: “What does it mean for GANs to generalize effectively?” The paper Arora et al. [2017] provided a mathematical definition for generalization in GANs in terms of IPM.  

Definition 4.1. Let ${ \widehat { \mu } } _ { n }$ and ${ \hat { \nu } } _ { m }$ be the empirical distributions of $\mu$ and $\nu$ . For some generalization gap $\varepsilon > 0$ , if the followbing ho dbs with high-probability;  

$$
\begin{array} { r } { | d _ { \mathcal { F } } ( \mu , \nu ) - d _ { \mathcal { F } } ( \widehat { \mu } _ { n } , \widehat { \nu } _ { m } ) | \leq \varepsilon , } \end{array}
$$  

with $n$ being polynomially dependent in $\varepsilon$ , the divergence $d _ { \mathcal { F } } ( \cdot , \cdot )$ between distributions generalizes.  

This means that if the absolute discrepancy between population divergence and empirical divergence of $\mu$ and $\nu$ can be arbitrarily controlled with $n$ polynomially generated samples, GAN generalizes well. The same paper proved under this definition, that GAN cannot generalize w.r.t. Wasserstein-1 distance and Jensen-Shannon divergence as $n = \tilde { \mathcal { O } } ( \varepsilon ^ { - \mathrm { p o l y ( d ) } } )$ is required. But it generalizes well w.r.t. neural net distance with $n = \tilde { \mathcal { O } } ( p \log ( L ) \cdot \varepsilon ^ { - 2 } )$ where $p$ is the total number of parameters in discriminator neural network and $L$ is a Lipschitz constant of discriminators w.r.t. parameters.  

Nonetheless, as noted by Chen et al. [2020a], Zhang et al. [2017], Arora et al. [2017], this result has some limitations: (1) the sample complexity is involved with unknown Lipschitz constants $L$ of discriminator. (2) A small neural net distance does not necessarily mean that two distributions are close. (Section 3.4 in Arora et al. [2017]) (3) Sample complexity is not involved with the complexity of generator class $\mathcal { G }$ under the assumption that the generator can approximate well enough the target data distribution. (4) No concrete architecture of discriminator networks is given. Some papers Zhang et al. [2017], Jiang et al. [2018], Bai et al. [2018] attempted to address the first two limitations, and their attempts are nicely summarized in Chen et al. [2020a]. In this review, we introduce works Chen et al. [2020a], Liang [2021] that tackle the issues raised in (3) and (4) concretely through the tools from the approximation theory of neural networks.  

Statistical guarantees of GANs. Note that the functions in discriminator and generator classes can be either classical non-parametric regressors such as random forests, local polynomial, etc., or can be both parametrized by neural networks. Here, we focus on the latter case which is more commonly used in practice. Specifically, let us denote $\mathcal { F } : = \{ f _ { \omega } ( \cdot ) : \mathbb { R } ^ { d }  \mathbb { R } \}$ as the Discriminator class and $\mathcal { G } : = \{ g _ { \boldsymbol { \theta } } ( z ) : \mathbb { R } ^ { m }  \mathbb { R } ^ { d } \}$ (with $m \leq d$ ) as the Generator class with $\omega$ and $\theta$ being denoted as network parameters of respective classes, and we are interested in estimating the parameters by minimizing following optimization problem:  

$$
( \widehat { \theta } _ { m , n } , \widehat { \omega } _ { m , n } ) \in \mathop { \mathrm { a r g } \operatorname* { m i n } } _ { \theta : g _ { \theta } \in \mathcal { G } } \operatorname* { m a x } _ { \omega : f _ { \omega } \in \mathcal { F } } \bigg \{ \widehat { \mathbb { E } } _ { m } f _ { \omega } \big ( g _ { \theta } \big ( \mathcal { Z } \big ) \big ) - \widehat { \mathbb { E } } _ { n } f _ { \omega } \big ( X \big ) \bigg \} ,
$$  

where $\widehat { \mathbb { E } } _ { m } ( \cdot )$ (resp. ${ \widehat { \mathbb { E } } } _ { n } ( \cdot )$ ) denotes the empirical expectation with $m$ generator samples. (resp. $n$ tranin bsamples.)  

Table 2: Summary of depth (i.e., $L$ ) and width (i.e., $\mathbf { p } _ { \mathrm { m a x } }$ ) of generators (i.e., $g _ { \boldsymbol { \theta } }$ ) and discriminators (i.e., $f _ { w }$ ), and the convergences of IPM under three specially-designed scenarios in GAN framework.   


<html><body><table><tr><td colspan="2"></td><td>L</td><td>Pmax</td><td>Conv. Rate</td></tr><tr><td rowspan="2">Scenario 1.</td><td>90</td><td>( 1gn)</td><td>βd 0(d(n))</td><td>(n- 2Fa log²n)</td></tr><tr><td></td><td></td><td>O(qn(a+1)(2α+d) V d)</td><td></td></tr><tr><td rowspan="2">Scenario 2.</td><td>ge f</td><td>(2a 10gn) （ql0gn)</td><td>O(n²vd）</td><td>(n-2a log²n）</td></tr><tr><td>90</td><td>O( logn)</td><td>O(Kdnα)</td><td>O(dn-α)</td></tr><tr><td>Scenario 3.</td><td>f</td><td>O(logn + d)</td><td>O（n）</td><td></td></tr></table></body></html>  

Summary of Liang [2021] Given that the optimal parameters of generator $\widehat { \theta } _ { m , n }$ in (21) can be obtained, Liang [2021] studied how well the implicit distribution estimator $\mu _ { \widehat { \theta } _ { m , n } }$ b(i.e., distribution of random variable $g _ { \widehat { \theta } _ { m , n } } ( Z ) )$ is close to the target distribution $\nu$ in a Total Vabriation distance. Under some regularity asbsumptions on architectures of $g _ { \boldsymbol { \theta } }$ and $f _ { w }$ , Theorem 19 in Liang [2021] proved the existence of $( g _ { \theta } ( z ) , f _ { \omega } )$ pairs satisfying the bound:  

$$
\mathbb { E } d _ { \mathrm { T V } } ^ { 2 } ( \nu , \mu _ { \widehat { \theta } _ { m , n } } ) \leq \sqrt { d ^ { 2 } L \log ( d L ) \bigg ( \frac { \log m } { m } \vee \frac { \log n } { n } \bigg ) } .
$$  

In the rate (22), $L$ and $d$ are the depth and width of the generator networks, respectively. This result allows the very deep network as $L \leq { \sqrt { ( n \land m ) / \log ( n \lor m ) } }$ . It is worth noting that the generator requires the width of the network to be the same as the input dimension $d$ so that the invertibility condition on the generator is satisfied. As for the discriminator, it can be constructed by concatenating two networks that have the same architecture as the one from the generator, and with the additional two layers, i.e., network $f _ { \omega }$ has $L + 2$ layers. The $g _ { \boldsymbol { \theta } }$ and $f _ { \omega }$ used leaky ReLU and dual leaky ReLU as activation functions respectively for their invertibility. However, this invertibility condition is often violated in practical uses of GANs.  

Summary of Chen et al. [2020a] A subsequent work Chen et al. [2020a] provides more flexible network architectures for $g _ { \theta }$ and $f _ { \omega }$ without requiring the invertibility condition on generator and activation function, i.e., the authors consider ReLU activation function. The paper mainly focuses on three interesting scenarios where they impose structural assumptions on target distribution $\nu$ :  

1. The target distribution $\nu$ is assumed to have a $\alpha ( > 0 )$ -H¨older density $p _ { \nu }$ with respect to Lebesgue measure in $\mathbb { R } ^ { d }$ , and the density is lower-bounded away from $0$ on a compact convex subset $\mathcal { X } \subset \mathbb { R } ^ { d }$ .   
2. The target distribution $\nu$ is supported on the $q$ -dimensional (i.e., $q \ll d$ ) linear subspace of the data domain $\boldsymbol { \mathcal { X } } \subset \mathbb { R } ^ { d }$ , where the density function is assumed to be in $\alpha$ -H¨older class.   
3. The target distribution $\nu$ is supported on $\mathcal { X } \subset [ 0 , 1 ] ^ { d }$ being the $q$ -dimensional (i.e., $q \ll d$ ) nonlinear $K$ -mixture components, where each component’s density function is in $\alpha$ -H¨older class.  

In scenario 1, the discriminator class $\mathcal { F }$ is assumed to be $\beta$ -H¨older class for $\beta > 1$ . In scenarios 2 and 3, $\mathcal { F }$ is considered to be a collection of 1-Lipschitz functions. The convergence rate of IPM, the depth $L$ and max-widths $\mathbf { p } _ { \mathrm { m a x } }$ of the generator and discriminator in each scenario are summarized in Table 2. In the first scenario, the convergence rate cannot avoid the exponential dependence on $d$ , aligning with the known minimax lower bound [Tang and Yang, 2022]. The result in the second scenario indicates GANs can avoid the curse of dimensionality being adaptive to the unknown low-dimensional linear structures, achieving faster rates than the parametric rate $n ^ { - \frac { 1 } { 2 } }$ . However, rather than the real-world data being centered in the low-dimensional linear subspace, mixture data are more commonly observed in practice, i.e., MNIST data or images in CIFAR-10. In the third scenario, the rate depends linearly on $d$ and exponentially on $q$ , showing GANs can capture nonlinear data structures. The depth $L$ of networks grows logarithmically with sample size $n$ . In contrast, to Liang [2021], the widths of networks are not the same with the input dimension $d$ .  

# 4.2 Score-based diffusion models  

Score-based diffusion model consists of two processes. The first step, forward process, transforms data into noise. Specifically, the score-based diffusion model Song et al. [2020] uses the following stochastic differential equations (SDEs) [S¨arkk¨a and Solin, 2019] for data perturbation:  

$$
d \mathbf { x } = f ( \mathbf { x } , t ) d t + g ( t ) d W _ { t } ,
$$  

where $f ( \mathbf { x } , t )$ and $g ( t )$ are drift and diffusion coefficient, respectively, and $W _ { t }$ is a standard Wienerprocess (a.k.a. Brownian Motion) indexed by time $t \in [ 0 , T ]$ . Here, the $f$ and $g$ functions are userspecified, and Song et al. [2020] suggests three different types of SDEs, i.e., variance exploding (VE), variance preserving (VP), and sub-variance preserving (sub-VP) SDEs for data perturbation. Allowing diffusion to continue long enough with $T$ being sufficiently large, it can be shown the distribution of $\mathbf { x } _ { t }$ converges to some easy-to-sample distributions $\pi$ such as normal or uniform distributions. Specifically, when $f : = - { \bf x } _ { t }$ and $g : = { \sqrt { 2 } }$ , (23) is known as Ornstein-Uhlenbeck (OU) process, it is proven $p _ { t } : = \mathrm { L a w } ( \mathbf { x } _ { t } )  \pi$ with $\pi$ being normal, exponentially fast in 2-Wasserstein distance. See for instance Bakry et al. [2014]. However, despite this convergence result, it is analytically difficult to know the exact form of $p _ { T }$ , and it is often replaced by the $\pi$ in practice when starting the reverse process.  

The second step, reverse process, is a generative process that reverses the effect of the forward process. This process learns to transform the noise back into the data by reversing SDEs in (23). Through the Fokker-Planck equation of marginal density $p _ { t } ( \mathbf { x } )$ for time $t \in [ t _ { 0 } , T ]$ , the following reverse SDE [Anderson, 1982] can be easily derived:  

$$
d \mathbf { x } = \bigg [ f ( \mathbf { x } , t ) - g ( t ) ^ { 2 } \nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } ) \bigg ] d t + g ( t ) d \bar { W } _ { t } .
$$  

Here, the gradient of $\log p _ { t } ( \mathbf { x } )$ w.r.t to the perturbed data ${ \bf x } ( t )$ is referred to as score function, dt in (24) is an infinitesimal negative time step, and $d \bar { W } _ { t }$ is a Wiener-Process running backward in time, i.e., $t : T  t _ { 0 }$ . In practice, $t _ { 0 }$ is usually chosen to be a small number close to $0$ , but not too close to $0$ to prevent the blow-up of the score function. There are various ways to solve (24); for instance, the discretization scheme such as Euler-Maruyama, or the theory-driven method such as probability-flow. See Song et al. [2020] for more detailed expositions on these methods. The papers introduced in this paper focus on the discretization scheme, and readers can refer Chen et al. [2023b] for the recent theoretical understandings of the probability flow in the diffusion model.  

The score function, $\nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } )$ , is approximated by a time-dependent score-based model $\mathbf { S } _ { \theta } ( \mathbf { x } ( t ) , t )$ which is parametrized by neural networks in practice. The network parameter $\theta$ is estimated by minimizing the following score-matching loss:  

$$
\theta ^ { \star } : = \underset { \theta } { \arg \operatorname* { m i n } } \mathbb { E } _ { t \sim \mathcal { U } [ t _ { 0 } , T ] } \mathbb { E } _ { \mathbf { x } ( \mathbf { t } ) | \mathbf { x } ( 0 ) } \mathbb { E } _ { \mathbf { x } ( 0 ) } \bigg [ \lambda ( t ) ^ { 2 } \left. \mathbf { S } _ { \theta } ( \mathbf { x } ( t ) , t ) - \nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } ( t ) \mid \mathbf { x } ( 0 ) ) \right. _ { 2 } ^ { 2 } \bigg ] ,
$$  

where $\mathcal { U } [ t _ { 0 } , T ]$ is a uniform distribution over $[ t _ { 0 } , T ]$ , and $\lambda ( t ) ( > 0 )$ is a positive weighting function that helps the scales of matching-losses $\| { \bf S } _ { \theta } ( { \bf x } ( t ) , t ) - \nabla _ { \bf x } \log p _ { 0 t } ( { \bf x } ( t ) \mid { \bf x } ( 0 ) ) \| _ { 2 } ^ { 2 }$ to be in the same order across over the time $t \in [ t _ { 0 } , T ]$ . The transition density $p _ { t } ( \mathbf { x } ( t ) \mid \mathbf { x } ( 0 ) )$ is a tractable Gaussian distribution, and ${ \bf x } ( t )$ can be obtained through ancestral sampling [Ho et al., 2020].  

Under this setting, we introduce readers to two lines of attempts aimed at finding answers to the following theoretical questions :  

Q1. Can the diffusion model estimate the target distribution $\nu$ via the learned score function? If so, under which conditions on $\nu$ , do we have the polynomial convergence guarantees in generalization error bound $\varepsilon$ measured in TV or Wasserstein distances?   
Q2. Do neural networks well approximate and learn the score functions? If so, how one should choose network architectures and what is the sample complexity of learning? Furthermore, if the data distribution has a special geometric structure, is the diffusion model adaptive to the structure, just as GAN models do?  

The main statistical object of interest in these two lines of research is the generalization bound measuring the distance between target distribution $\nu$ and estimated distribution $\hat { \mu } _ { \boldsymbol { \theta } }$ from the samples $\mathbf { x } _ { t _ { 0 } }$ by solving the reverse SDE in (24). Here, the score function is substituted byb the estimated timedependent neural network $\mathbf { S } _ { \theta } ( \mathbf { x } ( t ) , t )$ . The first line of work mainly focuses on the sampling perspective of the diffusion model, given that we have good estimates of the score function. The second line of work extends the attention to the score function approximation through neural networks. Furthermore, under some highly stylized settings, they specify the explicit network structures which give good generalization guarantees.  

Attempts to answer $\boldsymbol { Q 1 }$ . Early theoretical efforts to understand the sampling of score-based diffusion model suffered from being either not quantitative [De Bortoli et al., 2021, Liu et al., 2022], or from the curse of dimensionality [De Bortoli, 2022, Block et al., 2020]. Specifically among them, De Bortoli [2022] gave the convergence in 1-Wasserstein distance for distributions with bounded support $\mathcal { M }$ . Their case covers the distributions supported on lower-dimensional manifolds, where guarantees in TV or KL distance are unattainable as there are no guarantees that $\nu$ and ${ \widehat { \mu } } \theta$ have the same support set. For general distributions, their bound on $W _ { 1 } ( \nu , \widehat { \mu } _ { \theta } )$ have the exponentiabl dependence on the diameter of the manifold $\mathcal { M }$ and truncation of reverse prbocess $t _ { 0 }$ as $\mathcal { O } ( \exp ( \mathrm { d i a m } ( \mathcal { M } ) ^ { 2 } / t _ { 0 } ) )$ . For smooth distributions where the Hessian $\nabla ^ { 2 } \log { p _ { t } }$ is available, the bound is further improved with a polynomial dependence on $t _ { 0 }$ with the growth rate of Hessian as $t \to 0$ being on the exponent.  

To the best of our knowledge, Lee et al. [2022] first gives the polynomial guarantees in TV distance under $L ^ { 2 }$ -accurate score for a reasonable family of distributions. However, their result is based on the assumption that the distribution meets certain smoothness criteria and the log-Sobolev inequality, which essentially confines the applicability of their findings to distributions with a single peak. Recently, two works Lee et al. [2023], Chen et al. [2022b] appear online trying to avoid the strong assumptions on the data distributions and to get the polynomial convergence guarantees under general metrics such as TV or Wasserstein. Specifically, Lee et al. [2023] give 2-Wasserstein bounds for any distributions with bounded support. Contrary to De Bortoli [2022], Lee et al. [2022], the results they provide have polynomial complexity guarantees without relying on the functional inequality on distributions such as log-Sobolev inequality. They further give TV bounds with polynomial complexity guarantees under the Hessian availability assumption. Similarly with Lee et al. [2022], under general data distribution assumption, i.e., second moment bound of $\nu$ and $L$ -Lipschtizness of score function, Chen et al. [2022b] give the polynomial TV convergence guarantee, where only $\begin{array} { r } { \tilde { \Theta } \big ( \frac { L ^ { 2 } d } { { \varepsilon } ^ { 2 } } \big ) } \end{array}$ discretization is needed. Here, $\varepsilon$ is a TV generalization error, $d$ being a data dimension.  

Attempts to answer $Q 2$ . Due to its recent theoretical advancements, the list of research attacking the second question is short in the literature. One work [Chen et al., 2023a] proved that the diffusion model is adaptive to estimating the data distribution supported in a lower-dimensional subspace. They design a very specific network architecture for $\mathbf { S } _ { \theta } ( \mathbf { x } ( t ) , t )$ with an encoder-decoder structure and a skip-connection. Under a more general setting, another work [Oko et al., 2023] proves the distribution estimator from the diffusion model can achieve nearly minimax optimal estimation rates. Specifically, they assume the true density is supported on $[ - 1 , 1 ] ^ { d }$ , in the Besov space with a smooth boundary. The Besov space unifies many general function spaces such as H¨older, Sobolev, continuous, or even non-continuous function classes. (Also refer section 2.2.) The result in Oko et al. [2023] is valid for the non-continuous function class, and this should be contrasted with the aforementioned works [Lee et al., 2023, Chen et al., 2022b] where they assume the Lipschitzness of the score function. The exact architecture on the score network is also given in the form of (2).  

# 4.3 In-Context Learning in Large Language Model  

We provide readers with recent theoretical understandings of an interesting phenomenon observed in LLM, i.e., In-Context Learning (ICL). It refers to the ability of LLMs conditioned on prompt sequence consisting of examples from a task (input-output pairs) along with the new query input, the LLM can generate the corresponding output accurately. An instance taken from Garg et al. [2022], these models can produce English translations of French words after being prompted on a few such translations,  

e.g.:  

This capability is quite intriguing as it allows models to adapt to a wide range of downstream tasks on-the-fly without the need to update the model weights after training. Readers can refer to the backbone architecture of LLM (i.e., Transformer) in the seminal paper [Vaswani et al., 2017].  

Toward further understanding ICL, researchers formulated a well-defined problem of learning a function class $\mathcal { F }$ from in-context examples. Formally, let $\mathcal { D } _ { \mathcal { X } }$ be a distribution over inputs and $\mathcal { D } _ { \mathcal { F } }$ be a distribution over functions in $\mathcal { F }$ . A prompt $P$ is a sequence $( x _ { 1 } , f ( x _ { 1 } ) , \ldots , x _ { k } , f ( x _ { k } ) , x _ { \mathrm { q u e r y } } )$ where inputs (i.e., $x _ { i }$ and $x _ { \mathrm { { q u e r y } } }$ ) are drawn i.i.d. from $\mathcal { D } _ { \mathcal { X } }$ and $f$ is drawn from $\mathcal { D } _ { \mathcal { F } }$ . In the above example, it can be understood $\{ ( x _ { 1 } , f ( x _ { 1 } ) , ( x _ { 2 } , f ( x _ { 2 } ) \} : = \{ ( \mathrm { m a i s o n , h o u s e } ) , ( \mathrm { c h a t , c a t } ) \} ,$ $x _ { \mathrm { q u e r y } } = \mathrm { c h i e n }$ , and $f ( x _ { \mathrm { q u e r y } } ) = \mathrm { d o g }$ . Now, we provide the formal definition of in-context learning.  

Definition 4.2 (In-context learning Garg et al. [2022]). Model $M _ { \theta }$ can in-context learn the function class $\mathcal { F }$ up to $\varepsilon$ , with respect to $( \mathcal { D } _ { \mathcal { F } } , \mathcal { D } _ { \mathcal { X } } )$ , if it can predict $f ( x _ { \mathrm { q u e r y } } )$ with an average error  

$$
\begin{array} { r } { \mathbb { E } _ { P \sim ( x _ { 1 } , f ( x _ { 1 } ) , \dots , x _ { k } , f ( x _ { k } ) , x _ { \mathrm { q u e r y } } ) } \big [ \ell ( M _ { \theta } ( P ) , f ( x _ { \mathrm { q u e r y } } ) ) \big ] \le \varepsilon , } \end{array}
$$  

where $\ell ( \cdot , \cdot )$ is some appropriate loss function, such as the squared error.  

The paper Garg et al. [2022] empirically investigated the ICL of Transformer architecture [Vaswani et al., 2017] by training the model $M _ { \theta }$ on random instances from linear functions, two-layer ReLU networks, and decision trees. Here, we omit the definition of Transformer for brevity. Specifically, they showed the predictions of Transformers on the prompt $P$ behave similarly to those of ordinary least-squares (OLS), when the models are trained on the instances from linear function classes $\mathcal { F } ^ { \mathrm { L i n } } : = \{ f \ | \ f ( x ) = w ^ { \top } x , w \in \mathbb { R } ^ { d } \}$ for random weights $w \sim \mathcal { N } ( 0 , I _ { d } )$ . A similar phenomenon was observed for the models trained on sparse linear functions as the predictions behave like those of Lasso estimators.  

These nice observations sparked numerous follow-up theoretical studies of ICL on internal mechanisms [Akyu¨rek et al., 2022, Von Oswald et al., 2023, Dai et al., 2022], expressive power [Akyu¨rek et al., 2022, Giannou et al., 2023] and generalizations Li et al. [2023b]. Among them, papers [Aky¨urek et al., 2022, Von Oswald et al., 2023] investigated the behavior of transformers when trained on random instances from $\mathcal { F } ^ { \mathrm { L i n } }$ , and showed the trained Transformer’s predictions mimic those of the single step of gradient descent. They further provided the construction of Transformers which implements such a single step of gradient descent update. The recent paper Zhang et al. [2023] explicitly proved the model parameters estimated via gradient flow converge to the global minimizer of the non-convex landscape of population risk in (26) for learning $\mathcal { F } ^ { \mathrm { L i n } }$ . Nonetheless, the results in the paper are based on a linear self-attention (LSA) layer without softmax non-linearities and simplified parameterizations. Later, Huang et al. [2023] considered the single head attention with softmax non-linearity and proved the trained model through gradient descent indeed in-context learn ${ \mathcal { F } } ^ { \mathrm { L i n } }$ under highly stylized scenarios (i.e., simplified parameter settings, orthonormal features). Recently, Chen et al. [2024] generalized the setting to the multi-head attention layer with softmax non-linearity for in-context learning multi-task linear regression problems.  

One important work Bai et al. [2023] showed the existence of Transformers that can implement a broad class of standard machine learning algorithms in-context such as least squares, ridge regression, Lasso, and gradient descent for two-layer neural networks. This paper goes beyond and demonstrates a remarkable capability of a single transformer: the ability to dynamically choose different base incontext learning (ICL) algorithms for different ICL instances, all without requiring explicit guidance on the correct algorithm to use in the input sequence. This observation is noteworthy as it mirrors the way statisticians select the learning algorithms for inferences on model parameters.  

# 5 Conclusions & Future Topics  

In this article, we review the papers that studied neural networks mainly from statistical viewpoints. In Section 2, we review statistical literature that primarily relies on approximation-theoretic results of neural networks. This framework allows for interesting comparisons between neural networks and classical linear estimators in various function estimation settings. Specifically, neural networks are highly adaptive to functions with special geometric structures, whereas classical linear estimators are not. See Figure 1. In Section 3, we review papers studying the statistical guarantees of neural networks trained with gradient-based algorithms. The overparameterization of neural networks impacts the landscape of loss function, streamlining the mathematical analysis of training dynamics. We discuss training dynamics in two regimes: Neural Tangent Kernel (NTK) and Mean-Field (MF). Specifically, some works which studied how networks in NTK regime can offer statistical guarantees under noisy observations are introduced. We also introduce attempts to unify and go beyond these regimes, explaining the success of networks with finite widths. In Section 4, we review the statistical guarantees of deep generative models (i.e., GAN and diffusion model) for estimating the target distributions. Neural networks form the fundamental backbone of both frameworks, enabling the adaptive estimation of distributions with specialized structures. Some statistical works on ICL phenomena observed in LLM are also introduced,  

However, aside from these topics, several promising avenues have not been covered in this paper.   
We will briefly them for the references of readers.  

Generative Data Science. In modern machine learning (ML), data is valuable but often scarce and has been referred to as ”the new oil”, a metaphor credited to mathematician Clive Humby. With the rise of deep generative models like GANs, diffusion models, and LLMs, synthetic data is rapidly filling the void of real data and finding extensive utility in various downstream applications. For instance, in the medical field, synthetic data has been utilized to improve patients’ data privacy and the performance of predictive models for disease diagnosis [Chen et al., 2021]. Similarly, structured tabular data is the most common data modality that requires the employment of synthetic data to resolve privacy issues [Li et al., 2023a, Suh et al., 2023] or missing values [Ouyang et al., 2023b]. Furthermore, Synthetic data has been at the core of building reliable AI systems, specifically for the promising issues in fairness [Zeng et al., 2022], and robustness [Ouyang et al., 2023a]. Despite its prevalence in the real world, how to evaluate synthetic data from the dimensions of fidelity, utility and privacy-preserving remains unclear. Specifically, we want to address the following general questions:  

Q1 How well the models trained via synthetic data generalize well to real unseen data? (e.g., Xu et al. [2023a])?   
Q2 How does artificially generated data perform in the various downstream tasks such as classification or regression? (e.g., Li et al. [2023a], Xu et al. [2023b]), and adversarial training (e.g., Xing et al. [2022a,b])?   
Q3 How does the synthetic data generated to satisfy certain privacy constraints (e.g., differential privacy Dwork [2008])?  

Addressing these questions systemmatically requires establishing a new framework of “Generative Data Science”, aiming to elucidates the underlying principles behind generative AI. As evidenced by the above referenced works, this vision is supported by the recent observation that “creating something out of nothing” is possible and beneficial through synthetic data generation.  

Kolmogorov Arnold Network (KAN). As of June 2024, a new type of architecture, KAN [Liu et al., 2024], appears on arXiv and has been receiving enormous attention from the ML community. The model is motivated by the Kolmogorov-Arnold Representation Theorem (KART), stating any continuous and smooth functions on the bounded domain can be represented as compositions and summations of the finite number of univariate functions. Several papers have explored the connections between neural networks and the KART due to their similarities in terms of compositional structure. For instance, see Poggio and Girosi [1989], Girosi and Poggio [1989], Schmidt-Hieber [2021] and references therein. The authors claim that KAN outperforms fully-connected networks in terms of accuracy and interpretability for function approximations on their specially designed tasks. Nonetheless, this model definitely requires further research for better use in the future for both practical and theoretical purposes. From a practical perspective, KAN’s training time is 10 times slower than fully-connected networks, and authors only apply the model to small-size tasks. Interested readers can refer to Section 6 of Liu et al. [2024]. From a theoretical point of view, the authors claim that KAN avoids the curse of dimensionality for function approximation, whereas fully-connected networks cannot. But this argument requires further investigation under more rigorous settings with various types of function spaces $\mathcal { G }$ .  

# DISCLOSURE STATEMENT  

The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review.  

# ACKNOWLEDGMENTS  

We would like to thank an anonymous reviewer and Minshuo Chen for insightful comments on the draft of this review. This survey is partially sponsored by NSF – SCALE MoDL (2134209), NSF CNS (2247795), Office of Naval Research (ONR N00014-22-1-2680) and CISCO Research Grant.  

# Appendix  

We defer some of the technical contents included in the first version of this draft to the Appendix. Following is the list of materials to be covered  

1. We introduce the literature which studied the statistical guarantees of neural networks based on approximation-theory, specifically a result of Schmidt-Hieber [2020] in the Appendix A.   
2. Additional survey on statistical research of approximation-theory based classification tasks is presented in the Appendix B.   
3. The intuition of PDE (Equation. (28)) in the Mean-Field Regime is explained through a rough derivation process in the Appendix C.   
4. More detailed descriptions of the problem settings and results of some works introduced in Subsection 3.3. in the main manuscript are provided in the Appendices D and E.  

# Appendix A Technical Results of Schmidt-Hieber [2020]  

In this section, we would like to provide readers with a little bit of detailed descriptions on the seminal paper Schmidt-Hieber [2020]. The paper assumed that the regression function $f _ { \rho }$ has a special structure; a hierarchical compositional structure:  

$$
f _ { \rho } = g _ { q } \circ g _ { q - 1 } \circ g _ { q - 2 } \circ \cdot \cdot \cdot \circ g _ { 0 } ,
$$  

with $g _ { i } : [ a _ { i } , b _ { i } ] ^ { d _ { i } }  [ a _ { i + 1 } , b _ { i + 1 } ] ^ { d _ { i + 1 } }$ . Denote by $g _ { i } = ( g _ { i j } ) _ { j = 1 , \dots , d _ { i + 1 } } ^ { \mathrm { T } }$ the components of $g _ { i }$ and let $t _ { i }$ be the maximal number of variables on which each of the $g _ { i j }$ depends on. This setting leads to $t _ { i } \leq d _ { i }$ . Each of the $d _ { i + 1 }$ components of $g _ { i }$ belongs to $r _ { i }$ -H¨older class. Finally, the underlying function space to be considered is  

$$
\begin{array} { r l } & { \mathcal { G } ( q , \mathbf { d } , \mathbf { t } , \mathbf { r } , K ) : = \big \lbrace f _ { \rho } = g _ { q } \circ g _ { q - 1 } \circ \cdot \cdot \cdot \circ g _ { 0 } : g _ { i } = ( g _ { i j } ) _ { j } : [ a _ { i } , b _ { i } ] ^ { d _ { i } } \to [ a _ { i + 1 } , b _ { i + 1 } ] ^ { d _ { i + 1 } } , } \\ & { \qquad g _ { i j } \in \mathcal { H } ^ { r _ { i } } ( [ a _ { i } , b _ { i } ] ^ { t _ { i } } , K ) , \quad \forall | a _ { i } | , | b _ { i } | \leq K \big \rbrace , } \end{array}
$$  

where $\mathbf { d } : = ( d _ { 0 } , \ldots , d _ { q + 1 } )$ , $\mathbf { t } : = ( t _ { 0 } , \ldots , t _ { q } )$ , and $\mathbf { r } : = ( r _ { 0 } , \ldots , r _ { q } )$ . By Juditsky et al. [2009], Ray and Schmidt-Hieber [2017], importantly, the induced-smoothness on $f _ { \rho }$ is driven as follows:  

$$
r _ { i } ^ { \star } : = r _ { i } \prod _ { \ell = i + 1 } ^ { q } \big ( r _ { \ell } \wedge 1 \big ) , \quad \forall i \in \{ 0 , \ldots , q \} .
$$  

In Theorem 3 of Schmidt-Hieber [2020], it is proven that the rate $\phi _ { n } : = \operatorname* { m a x } _ { i = 0 , \ldots , q } n ^ { - { \frac { 2 r _ { i } ^ { \star } } { 2 r _ { i } ^ { \star } + t _ { i } } } }$ is the minimax optimal rate over the class $\mathcal { G } ( q , \mathbf { d } , \mathbf { t } , \mathbf { r } , K )$ in the interesting regime $t _ { i } \leq \operatorname* { m i n } \left( d _ { 0 } , \ldots , d _ { i - 1 } \right)$ for all $i$ . This regime avoids the $t _ { i }$ ’s being larger than the input dimension $d _ { 0 }$ . Under this setting on the regression function, the author proves that there exists an empirical risk minimizer ${ \widehat { f } } _ { n } \in { \mathcal { F } } ( L , \mathbf { p } , { \mathcal { N } } )$ with $L \asymp \log n$ , $\mathbf { p } _ { \mathrm { m a x } } \asymp n ^ { C }$ with $C \geq 1$ and $\mathcal { N } \asymp n \phi _ { n } \log n$ such that it achieves th bnearly minimax optimal rate, i.e., $C ^ { \prime } \phi _ { n } L \log ^ { 2 } n$ for the convergence on the excess risk. The constant factor $C ^ { \prime }$ depends on $q , \mathbf { d } , \mathbf { t } , \mathbf { r } , K$ . Importantly, they proved a classic statistical method such as Wavelet cannot achieve the minimax rate, showing the superior adaptiveness of neural networks for estimating $f _ { \rho } \in \mathcal { G } ( q , \mathbf { d } , \mathbf { t } , \mathbf { r } , K )$ .  

Several remarks follow in the sequel. First, the rate $\phi _ { n }$ is dependent on the effective dimension $t _ { i }$ which can be much less than the ambient input dimension $d _ { 0 }$ , and this implies that the neural net can circumvent the curse of dimensionality. Second, the depth $L$ should be chosen to scale with the sample size in a rate ${ \mathcal { O } } ( \log n )$ . Third, the width $\mathbf { p } _ { \mathrm { m a x } }$ can be chosen to be independent of the smoothness indices. Lastly, the result is saying what is important for the statistical performance is not the sample size, but the amount of regularization. This is explicitly reflected through the number of active parameters $\mathcal N \asymp n \phi _ { n } \log n \ll n$ .  

# Appendix B Classification tasks via fully-connected networks  

We consider a binary classification problem via fully-connected networks. Classifiers built with neural networks handle large-scale high-dimensional data, such as facial images from computer vision extremely well, while traditional statistical methods often fail miserably. We will review papers that have attempted to provide theoretical explanations for such empirical successes in high-dimensional classification problems, beyond the existing statistical literature. This is a less studied area than nonparametric regression, although deep learning is often used for classification.  

Binary Classification Task. Let $\{ X _ { i } , Y _ { i } \} _ { i = 1 } ^ { n }$ be i.i.d. random pairs of observations, where $X _ { i } \in \mathbb { R } ^ { d }$ and $Y _ { i } \in \{ 0 , 1 \}$ . Denote $P _ { X }$ the probability distribution of $X _ { i }$ and $\pi = \pi _ { \boldsymbol { X } , \boldsymbol { Y } }$ the joint distribution of $( X , Y )$ . In a classification problem, it is of main interest to estimate the decision rule, where it is determined through a set $G$ . In this paper, we assume $G$ is a Borel subset of $\mathbb { R } ^ { d }$ , where it gives $Y = 1$ if $X \in G$ and $Y = 0$ if $X \not \in G$ . The misclassification risk associated with $G$ is given by:  

$$
R ( G ) : = P ( Y \neq \mathbb { 1 } ( X \in G ) ) = \mathbb { E } [ ( Y - \mathbb { 1 } ( X \in G ) ) ^ { 2 } ] .
$$  

It is widely known that a Bayes classifier minimizes $R ( G )$ denoted as $G _ { \pi } ^ { * } = \{ x : \eta ^ { \star } ( x ) \geq 1 / 2 \}$ , where $\eta ^ { \star } ( x ) = P ( Y = 1 | X = x )$ . However, since we have no information on the joint distribution $\pi$ , it is difficult to directly find the minimizer of $R ( G )$ . There are two ways to circumvent this difficulty. Given the data set $\{ X _ { i } , Y _ { i } \} _ { i = 1 } ^ { n }$ , the first method is to directly estimate the set minimizing the empirical risk;  

$$
R _ { n } ( G ) = { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } { \bigl ( } Y _ { i } - \mathbb { 1 } ( X _ { i } \in G ) { \bigr ) } ^ { 2 } .
$$  

We denote the minimizer of $R _ { n } ( G )$ as ${ \widehat { G } } _ { n }$ . The second method is to estimate the regression function, $\widehat { \eta } ( x )$ , and then plug in the $\widehat { \eta } ( x )$ to th bset $G$ denoted as $\widehat { G } _ { n } : = \{ x \in \mathbb { R } ^ { d } : \widehat { \eta } ( x ) \geq 1 / 2 \}$ . Under these sbettings, we are interested bn how fast $R ( \widehat { G } _ { n } )$ converges bo $R ( G _ { \pi } ^ { * } )$ as $n  \infty$ . Note that these two approaches are different in the sense that thbey impose different assumptions to derive the convergence rate of $R ( { \widehat { G } } _ { n } )$ to $R ( G _ { \pi } ^ { * } )$ .  

Now, wbe state three assumptions commonly imposed on the joint distribution $\pi$ .  

1. Complexity Assumption on the Decision set (CAD) characterizes the smoothness of boundary of Bayes classifier $G _ { \pi } ^ { \star } ( \subset { \mathcal { G } } )$ by assuming the class $\mathcal { G }$ has a suitably bounded $\epsilon$ -entropy.  

Table 3: Comparison table on statistical convergence rates of neural networks in classification tasks.   


<html><body><table><tr><td>Reference</td><td>Function Space</td><td>Loss</td><td>Condition</td><td colspan="2">Rate</td></tr><tr><td rowspan="2">Kim et al. [2021]</td><td rowspan="2">ReLU FNNs</td><td rowspan="2">Hinge</td><td>MA, CAD</td><td>O n</td><td rowspan="2">α(q+1) α(q+2)+(d-1)(q+1)</td></tr><tr><td>MA, CAR</td><td>α(q+1) O n α(q+2)+d</td></tr><tr><td rowspan="3">Feng et al. [2021]</td><td rowspan="6">ReLU CNNs</td><td>Hinge</td><td rowspan="2">CAR</td><td>O</td><td>n2β(d-1)+r(2-7)</td></tr><tr><td>p-norm</td><td>O</td><td colspan="2">n-2(β+1)(d-1)+2pr(2-T)</td></tr><tr><td>2-norm</td><td>MA, CAR</td><td>O n</td><td colspan="2">2rq (2+q)((β+1)(d-1)+2τ)</td></tr><tr><td rowspan="3">Shen et al. [2022]</td><td>Hinge</td><td>MA, CAR</td><td>O n</td><td>d+（+1）</td></tr><tr><td>Logistic</td><td rowspan="2">CAR</td><td>O</td><td>n-2d+4r</td></tr><tr><td>Least Square</td><td>O</td><td>（n-3d+16r</td></tr><tr><td>Zhou and Huo [2023]</td><td>ReLU FNNs</td><td>Hinge</td><td>MA:</td><td>O</td><td>n</td></tr><tr><td>Ko et al. [2023]</td><td>ReLU FNNs</td><td>Logistic</td><td>MA, CAR: BA</td><td>O</td><td>q+1 n 3(q+2)</td></tr><tr><td>Hu et al. [2020a]</td><td>ReLU FNNs</td><td>0-1 Loss</td><td>MA, CAD: Teacher-Student</td><td>O</td><td>n</td></tr></table></body></html>  

2. Complexity Assumption on the Regression function (CAR) refers the regression function $\eta ^ { \star } ( x )$ is smooth enough and belongs to a function class $\Sigma$ having a suitably bounded $\epsilon$ -entropy.  

3. Margin Assumption (MA) describes the behavior of $\eta ^ { \star } ( x )$ near the decision boundary (i.e., $\eta ^ { \star } ( x ) = 1 / 2$ ). Specifically, the assumption parameter $q ( \geq 0 )$ describes how much the regression is bounded away from $1 / 2$ . The larger the $q$ is, the farther $\eta ^ { \star } ( x )$ is bounded away from $1 / 2$ .  

For those who are interested in rigorous treatments of each assumption, we refer readers Mammen and Tsybakov [1999], Tsybakov [2004], Audibert and Tsybakov [2007] and references therein. Note that, in general, there is no connection between assumptions (CAR) and (CAD). Indeed, the fact that $G ^ { * }$ has a smooth boundary does not imply that $\eta ^ { \star } ( x )$ is smooth, and vice versa. However, as noted by Audibert and Tsybakov [2007], the smoothness of $\eta ^ { \star } ( x )$ and (MA) parameter $q$ cannot be simultaneously large; as the smooth $\eta ^ { \star } ( x )$ hits the level $1 / 2$ , it cannot ”take off” from this level too abruptly.  

Results on Neural Networks. The Table 1 summarizes the settings and results of the works that study the convergence rates of excess risks for binary classification problems of neural networks. Hereafter, $\alpha > 0$ indicates the smoothness index of either target function class ( $\Sigma$ ) in CAR or boundary class ( $\mathcal { G }$ ) in CAD, and $q > 0$ denotes the term in the exponent in Margin Assumption (MA).  

The first notable paper Kim et al. [2021] proved that under the smooth boundary (CAD) and smooth regression (CAR) conditions, respectively, there exist neural networks that achieve (nearly) optimal convergence rates of excess risks. The obtained rate under CAR assumption is sub-optimal in a sense Tsybakov [2004] as they prove the minimax optimal rate achievable by neural nets is $\mathcal { O } \left( n ^ { - \alpha ( q + 1 ) / [ \alpha ( q + 2 ) + ( d - 1 ) q ] } \right)$ , and the optimal rate under CAD assumption is achievable. According to their remark, no other estimators achieve fast convergence rates under these scenarios simultaneously.  

Two recent works studied the binary classification by ReLU convolutional neural networks Feng et al. [2021], Shen et al. [2022]. The paper Feng et al. [2021] considered the $p$ -norm loss $\phi ( t ) : = \operatorname* { m a x } \{ 0 , 1 - t \} ^ { p }$ for $p \geq 1$ , and the input data is supported on the sphere $S ^ { d - 1 }$ . In the paper, the approximation error bounds and excess risk bounds are derived under a varying power condition, and the target function class is the Sobolev space $W _ { p } ^ { r } ( S ^ { d - 1 } )$ for $r > 0$ and $p \geq 1$ . Technically, they obtained the approximation error in $L ^ { p }$ norm including the case . Two quantities including $\beta = \operatorname* { m a x } \{ 1 , ( d + 3 + r ) / ( 2 ( d - 1 ) ) \}$ $p = \infty$ and $\tau \in [ 0 , 1 ]$ are involved in the rate. The paper Shen et al. [2022] established the convergence rates of the excess risk for classification with a class of convex loss functions. The target function of interests is also from the Sobolev space $W _ { p } ^ { r } ( [ 0 , 1 ] ^ { d } )$ , and note that the feature domain is $[ 0 , 1 ] ^ { d }$ . Under this setting, the most interesting aspect of this work is that they track the dependencies of the ambient dimension $d$ in the pre-factor hidden in the big- $\boldsymbol { \mathcal { O } }$ notation. They showed the prefactor is polynomially dependent on $d$ .  

The last three works in the above table give the dimension-free rates in the exponent of $n$ under different problem settings. The paper Zhou and Huo [2023] studied the problem of learning binary classifiers through ReLU FNNs. An interesting aspect of this result is that unlike the other results Kim et al. [2021], Feng et al. [2021], Shen et al. [2022], the rate is both dimension and smoothness index free, despite the feature domain being unbounded. The main idea for getting this result is to leverage the fact that Gaussian distributions of features are analytic functions and have a fast decay rate, and this can be captured through ReLU FNNs. Another paper Ko et al. [2023] works on a similar problem but under a different setting. Notably, they are interested in the Barron Approximation (BA) space proposed in Caragea et al. [2023]. Unlike the classical Barron space in Barron [1993], which is essentially a subset of a set of Lipschitz continuous functions, this space includes even discontinuous functions and hence is more general. The CAR assumption is imposed on the functions in the BA class. They proved that the rate $\mathcal { O } ( n ^ { - \frac { q + 1 } { 3 ( q + 2 ) } } )$ is achievable through ReLU FNNs for the estimation, and this rate is indeed minimax optimal. Note that the rate is slower than the parameteric rate $n ^ { - { \frac { 1 } { 2 } } }$ by noting the rate ranges from $n ^ { - \frac { 1 } { 6 } }$ to $n ^ { - \frac { 1 } { 3 } }$ as $q$ varies from $0$ to $\infty$ . This is attributed to the setting that the size of the distribution class is large.  

Lastly, a paper Hu et al. [2020a] studied the convergence rate of excess risk of neural networks under the framework of Mammen and Tsybakov [1999]. Specifically, the authors of the paper consider a Teacher-Student framework. In this setup, one neural network, called student net, is trained on data generated by another neural network, called teacher net. Adopting this framework can facilitate the understanding of how deep neural networks work as it provides an explicit target function with bounded complexity. Furthermore, assuming the target classifier to be a teacher network of an explicit architecture might provide insights into what specific architecture of the student classifier is needed to achieve an optimal excess risk. Under this setting, the rate of convergence is derived as $\tilde { \mathcal { O } } _ { d } ( n ^ { - \frac { 2 } { 3 } } )$ for the excess risk of the empirical $0 - 1$ loss minimizer, given that the student network is deeper and larger than the teacher network. The authors use the notation $\tilde { \mathcal { O } } _ { d }$ to denote the rate is dependent on input dimension $d$ in a logarithmic factor. When data are separable, the rate improves to $\tilde { \mathcal { O } } _ { d } ( n ^ { - 1 } )$ . In contrast, as it has already been shown in Mammen and Tsybakov [1999], under CAD assumption, the optimal rate is $\mathcal { O } \big ( n ^ { - \alpha ( q + 1 ) / [ \alpha ( q + 2 ) + ( d - 1 ) q ] } \big )$ . Clearly, this rate suffers from “Curse of Dimensionality” but interestingly, coincides with the rate $\tilde { \mathcal { O } } _ { d } ( n ^ { - \frac { 2 } { 3 } } )$ when $\alpha = 1$ and $\beta  \infty$ . If we further allow $\alpha  \infty$ (corresponding to separable data), the classical rate above recovers $\mathcal { O } ( n ^ { - 1 } )$ . The paper also got the rate is un-improvable in a minimax sense by showing that the minimax lower bound has the same order as that of the upper bound (i.e., $\tilde { \mathcal { O } } _ { d } ( n ^ { - \frac { 2 } { 3 } } ) .$ ).  

# Appendix C Intuition of PDE Equation (Distributional Dynamics) in Mei et al. [2018b]  

In the Mean-Field setting, recall the trajectories of empirical distribution of $\theta ^ { ( k ) }$ denoted as $\widehat { \rho } _ { k } ^ { ( M ) } : =$ $\begin{array} { r } { \frac { 1 } { M } \sum _ { r = 1 } ^ { M } \delta _ { \theta _ { r } ^ { ( k ) } } } \end{array}$ weakly converges to the deterministic limit $\rho _ { t } \in \mathcal { P } ( \mathbb { R } ^ { \mathcal { D } } )$ as $k  \infty$ and $M \to \infty$ . The measure $\rho _ { t }$ is the solution of the following nonlinear partial differential equation (PDE):  

$$
\begin{array} { r l r } { \partial _ { t } \rho _ { t } = \nabla _ { \theta } \cdot \bigl ( \rho _ { t } \nabla _ { \theta } \Psi ( \theta ; \rho _ { t } ) \bigr ) , } & { } & { \Psi ( \theta ; \rho _ { t } ) : = \mathcal { V } ( \theta ) + \displaystyle \int \mathcal { U } ( \theta , \bar { \theta } ) \rho _ { t } ( d \bar { \theta } ) , } \\ { \mathcal { V } ( \theta ) : = - \mathbb { E } \{ \mathbf { y } \sigma _ { \star } ( \mathbf { x } ; \theta ) \} , } & { } & { \mathcal { U } ( \theta _ { 1 } , \theta _ { 2 } ) : = \mathbb { E } \{ \sigma _ { \star } ( \mathbf { x } ; \theta _ { 1 } ) \sigma _ { \star } ( \mathbf { x } ; \theta _ { 2 } ) \} . } \end{array}
$$  

The PDE in Equation. (28) is often referred as Distributional Dynamics (DD). The intuition of the above PDE can be understood through the derivation process. To aid the understandings of readers, we only present a high-level idea for the derivation without rigor. For each particle $\theta _ { r }$ , we can write  

the $k$ th update of one-pass SGD as follows:  

$$
\theta _ { r } ^ { k + 1 } = \theta _ { r } ^ { k } + 2 \eta _ { k } \nabla _ { \theta _ { r } } \sigma _ { \star } ( \mathbf { x } _ { k } , \theta _ { r } ^ { k } ) \biggl \{ \mathbf { y } _ { k } - \frac { 1 } { M } \sum _ { r = 1 } ^ { M } \sigma _ { \star } ( \mathbf { x } _ { k } ; \theta _ { r } ^ { k } ) \biggr \} .
$$  

By fixing the learning rate $\begin{array} { r } { \eta _ { k } : = \frac { \varepsilon } { 2 } } \end{array}$ with $\varepsilon$ close to $0$ and taking expectation with respect to data $\mathbf { \rho } ( \mathbf { x } , \mathbf { y } )$ on both sides of the equality, we have the ODE desribing the continuum motion of each particle $\theta _ { r }$ :  

$$
\dot { \theta } _ { r } ( t ) = - 2 \nabla _ { \theta _ { \mathbf { r } } } \mathcal { V } ( \theta _ { \mathbf { r } } ) - \frac { 2 } { M } \sum _ { k = 1 } ^ { M } \nabla _ { \theta _ { \mathbf { r } } } \mathcal { U } ( \theta _ { \mathbf { r } } , \theta _ { \mathbf { k } } ) .
$$  

Each particle $\left( \theta _ { r } \right)$ moves to the directions where it minimizes $- \mathcal { V } ( \theta _ { \mathbf { r } } ) : = \mathbb { E } \{ \mathbf { y } \sigma _ { \star } ( \mathbf { x } ; \theta ) \}$ to secure the best fit with the response data y. At the same time, it moves to the directions where it minimizes $\begin{array} { r } { - \frac { 1 } { M } \sum _ { k = 1 } ^ { M } \mathcal { U } ( \theta _ { \mathbf { r } } , \theta _ { \mathbf { k } } ) } \end{array}$ so that the particle $\theta _ { r }$ does not get too close to the other particles $( \theta _ { k } ) _ { k \neq r }$ . Recalling the definition of $\Psi ( \theta ; \rho _ { t } )$ , we rewrite the ODE Equation. (29) in terms of empirical distribution $\widehat { \rho } _ { k } ^ { ( M ) }$ ，  

$$
\begin{array} { r } { \boldsymbol { \dot { \theta } } _ { r } ( t ) : = - \nabla _ { \boldsymbol { \theta } _ { r } } \Psi ( \boldsymbol { \theta } _ { r } ; \boldsymbol { \widehat { \rho } } _ { k } ^ { ( M ) } ) . } \end{array}
$$  

Then, the authors introduced new $M$ i.i.d. trajectories $\left( \boldsymbol { \theta } _ { r } ( t ) \right)$ with $r = 1 , \ldots , M$ with same initializations as for the SGD (i.e., $\theta _ { r } ( 0 ) = \theta _ { r } ( 0 )$ ), where the trajectory $\theta _ { r } ( t )$ is the solution of the ODE $\dot { \bar { \theta } } ( t ) : = - \nabla _ { \bar { \theta } } \Psi ( \bar { \theta } ( t ) ; \rho _ { t } )$ , then proved $\theta _ { r } ( t )$ is actually the solution of the PDE in Equation. (28). Finally, they showed the distance between two trajectories $\theta _ { r } ( t )$ and $\theta _ { r } ( t )$ can be controlled. In other words, the above PDE Equation. (28) describes the evolution of each particle $\left( \theta _ { r } \right)$ in the force field created by the densities of all the other particles.  

# Appendix D Beyond the Kernel Regime  

Despite nice theoretical descriptions on training dynamics of gradient descent in loss functions, Arora et al. [2019b], Lee et al. [2018], Chizat and Bach [1812] empirically found significant performance gaps between NTK and actual training. These gaps have been theoretically studied in Wei et al. [2019], Allen-Zhu and Li [2019], Ghorbani et al. [2020b], Yehudai and Shamir [2019] which established that NTK has provably higher generalization error than training the neural net for specific data distributions and architectures. Here, we introduce two works Ghorbani et al. [2020b], Wei et al. [2019].  

1. Ghorbani et al. [2020b] gives an example of a highly stylized spiked feature model. Consider the case in which $\bf { x } = \bf { U } z _ { 1 } + \bf { U } ^ { \perp } z _ { 2 }$ , where $\mathbf { U } \in \mathbb { R } ^ { d \times d _ { 0 } }$ , $\mathbf { U } ^ { \perp } \in \mathbb { R } ^ { d \times ( d - d _ { 0 } ) }$ , and $[ \mathbf { U } \mid \mathbf { U } ^ { \perp } ] \in \mathbb { R } ^ { d \times d }$ is an orthogonal matrix. Set the signal dimension $d _ { 0 } = \lfloor d ^ { \eta } \rfloor$ with $\eta \in ( 0 , 1 )$ so that $d _ { 0 } \ll d$ . Here, we call $\mathbf { z } _ { 1 }$ a signal feature and $\mathbf { z } _ { 2 }$ a junk feature with the variance $ { \mathbf { C o v } } (  { \mathbf { z } } _ { 1 } ) =  { \mathrm { S N R } } _ { f }  { \mathbb { Z } } _ { d _ { 0 } }$ and $\mathbf { C o v } ( \mathbf { z } _ { 2 } ) = \mathcal { T } _ { d - d _ { 0 } }$ where $\mathrm { S N R } _ { f } = d ^ { \kappa }$ for $0 \leq \kappa < \infty$ . Under this setting, the noisy response has the form $\mathbf { y } = \psi ( \mathbf { U } \mathbf { z } _ { 1 } ) + \varepsilon$ with $\varepsilon$ some random noises. Note that the response only depends on the signal feature. They derived and compared the number of parameters needed for approximating degree $\ell$ -polynomials in $\mathbf { z _ { 1 } }$ in Neural Network (NN), Random Features (RF), and Neural Tangent (NT) models under different levels of $\mathrm { S N R } _ { f }$ :  

<html><body><table><tr><td>F</td><td>NN</td><td>RF</td><td>NT</td><td>inffex |lf*- fl²²</td></tr><tr><td>SNRf = 1</td><td>dme</td><td>dl</td><td>d</td><td>NN > RF = NT</td></tr><tr><td>SNRf >1</td><td>dme</td><td>dme</td><td>dm(l-1)+1</td><td>NN ~ RF > NT</td></tr></table></body></html>  

Here, NN is a collection of shallow network functions $\begin{array} { r } { f _ { N N } ( \mathbf { x } , a , \mathbf { W } ) = \sum _ { r = 1 } ^ { M } a _ { r } \sigma ( \langle w _ { r } , \mathbf { x } \rangle ) } \end{array}$ for $a _ { r } \in \mathbb { R }$ and $w _ { r } \in \mathbb { R } ^ { d }$ , and RF (resp. NT) are a collection of linear functions $f _ { R F } ( \mathbf { x } , a ) \ =$ $\begin{array} { r } { \sum _ { r = 1 } ^ { M } a _ { r } \sigma \big ( \langle w _ { r } ^ { 0 } , \mathbf { x } \rangle \big ) } \end{array}$ for $a _ { r } \in \mathbb { R }$ . (resp. $\begin{array} { r } { f _ { N T } ( \mathbf { x } , \mathbf { W } ) = \sum _ { r = 1 } ^ { M } \langle s _ { r } , \mathbf { x } \rangle \sigma ^ { \prime } ( \langle w _ { r } ^ { ( 0 ) } , \mathbf { x } \rangle ) } \end{array}$ for $s _ { r } \in \mathbb { R } ^ { d }$ .) It is interesting to observe that the approximation power of NN models is independent of $\operatorname { S N R } _ { f }$ , whereas the other two models are affected by it. In their numerical simulation (Figure 2), they also showed larger SNR $f$ induces the larger approximation power of RF, NT as manifested in the above table. This is also consistent with the spectral bias of NT models in that the model learns the low-frequency components of functions faster than the high-frequency counterparts.  

2. Wei et al. [2019] gives an interesting example of where NTK or any kernel methods are statistically limited, whereas regularized neural networks have better sample complexity. Consider the setting where we have feature space $\mathbf { x } \in \mathbb { R } ^ { d }$ with $\mathbf { x } _ { i } \sim \{ \pm 1 \}$ and the function $\mathbf { f } ( \mathbf { x } ) = \mathbf { x } _ { 1 } \mathbf { x } _ { 2 }$ which is only the product of first two arguments of $\mathbf { x }$ . We want to learn the $\mathbf { f } \left( \mathbf { x } \right)$ through functions in NTK-induced RKHS $\begin{array} { r } { \mathbf { \tilde { \mu } } ^ { \mathrm { { z N T K } } } ( a ; \mathbf { x } ) = \sum _ { r = 1 } ^ { M } a _ { r } \mathbf { K } ^ { \infty } ( \mathbf { x } _ { r } , \mathbf { x } ) } \end{array}$ and two-layer neural networks with ReLU activation $\begin{array} { r } { \mathbf { f } ^ { \mathrm { N N } } ( \boldsymbol { \Theta } ; \mathbf { x } ) = \sum _ { r = 1 } ^ { M } a _ { r } \sigma ( w _ { r } ^ { \top } \mathbf { x } ) } \end{array}$ , respectively. The classifier $\mathbf { f } ^ { \mathrm { N T K } } ( a ; \mathbf { x } )$ is attained by minimizing squared los  and f $\mathbf { \partial } ^ { \bullet \mathrm { N N } } ( \Theta ; \mathbf { x } )$ is estimated through $\ell _ { 2 }$ -regularized logistic loss. Under this setting, Theorem 2.1 in Wei et al. [2019] is read as:  

“Functions $\mathbf { f } ^ { \mathrm { N T K } } ( a ; \mathbf { x } )$ require $n = \Omega ( d ^ { 2 } )$ samples to learn the problem with error $\ll \Omega ( 1 )$ . In contrast, regularized NN functions $\mathbf { f } ^ { \mathrm { N N } } ( \Theta ; \mathbf { x } )$ only need $n = \mathcal { O } ( d )$ samples.”  

The result implies that there is a $\Omega ( d )$ sample-complexity gap between the regularized neural net and kernel prediction function. A main intuition behind this gap is that the regularization allows neural networks to adaptively find the model parameters $( a _ { r } , w _ { r } )$ so that the best estimator only chooses 4 neurons in the hidden layer. 2 (Note that the relation between $\ell _ { 2 }$ -regularized two-layer neural network and $\ell _ { 1 }$ -SVM over ReLU features is known in literature Neyshabur et al. [2014].) However, using the NTK, we do a dense combination of existing features as the network output will always involve $\sigma ( w ^ { \top } { \\bf x } )$ where $w$ is a random vector so it includes all the components of $\mathbf { x }$ .  

Beyond the kernel regime. The above two works explained the superiority of neural networks over the networks in the NTK regime under some highly stylized settings. There also have been several theoretical attempts [Bai and Lee, 2019, Allen-Zhu et al., 2019] to explain how the networks estimated through gradient-based methods generalize well to the unseen data but critically do not rely on the linearization of network dynamics. The paper Bai and Lee [2019] studied the optimization and generalization of shallow networks with smooth activation function $\sigma ( \cdot )$ via relating the network dynamics $f _ { \mathbf { W } } ( \mathbf { x } )$ with higher-order approximations. The key idea is to find the subset of $\mathbf { W _ { \lambda } } : =$ $\{ ( a _ { r } , w _ { r } ) \} _ { r = 1 } ^ { M }$ such that the dominating term in Taylor expansion Equation. (10) is not the linear term (i.e., $\left. \nabla f _ { \mathbf { W } _ { 0 } } ( \mathbf { x } ) , \mathbf { W } - \mathbf { W } _ { 0 } \right. )$ but a quadratic term (i.e., $\lVert \mathbf { W } - \mathbf { W } _ { 0 } \rVert _ { \mathrm { F } } ^ { 2 } )$ . The authors find such $\mathbf { W }$ by running SGD on the tailored-regularized loss:  

$$
\begin{array} { r } { \mathbb { E } _ { \pmb { \Sigma } , ( \mathbf { x } , \mathbf { y } ) \sim \mathcal { D } } \bigg [ \ell ( \mathbf { y } , f _ { \mathbf { W } _ { 0 } + \pmb { \Sigma } \mathbf { W } _ { r } } ( \mathbf { x } ) ) \bigg ] + \lambda \| \mathbf { W } \| _ { 2 , 4 } ^ { 8 } . } \end{array}
$$  

The expectation in loss is taken over the diagonal matrix $\pmb { \Sigma }$ with $\Sigma _ { r r } \sim \mathrm { U n i f } \{ - 1 , 1 \}$ over $r \in [ M ]$ and it has the effects of dampening out the linear term. The regularizer $\| \cdot \| _ { 2 , 4 } ^ { 8 }$ controls the distance of moving weights to be $\mathcal { O } ( M ^ { - 1 / 4 } )$ . The authors showed the landscape of optimization problem Equation. (31) has a very nice property that every second-order stationary point of the loss is nearly global optimum, despite the non-convexity of Equation. (31). These points can be easily found by the SGD algorithm. Given that $M$ is sufficiently large enough, i.e., $M \gtrsim n ^ { 4 }$ , they further provided the comparisons of sample complexities between quadratic and linear models under three different scenarios. The ground truth functions estimated in the three scenarios are (1) $f ^ { \star } ( { \bf x } ) = \alpha ( \beta ^ { \top } { \bf x } ) ^ { p }$ for $\alpha \in \mathbb { R }$ , $\beta \in \mathbb { R } ^ { d }$ , and $p = 1$ or even. (2) $f ^ { \star } ( { \bf x } ) = { \bf x } _ { 1 } { \bf x } _ { 2 }$ , and (3) $f ^ { \star } ( { \bf { x } } ) = \langle \Theta ^ { \star } , { \bf { x } } { \bf { x } } ^ { \top } \rangle$ where $\Theta ^ { \star } \in \mathbb { R } ^ { d \times d }$ is a rank- $r$ matrix. We summarize the results in the below table.  

In all three cases, it is shown that the lower bound of sample complexity of the quadratic model is smaller than that of the linear NTK model by the factor of $\tilde { \mathcal { O } } ( d )$ . This can mean that the expressivity of the higher-order approximation of neural dynamics is richer than that of the linear counterpart.  

<html><body><table><tr><td></td><td>X</td><td colspan="2">Quadratic Model</td><td colspan="2">Linear Model</td></tr><tr><td>f*(x)=α(βTx)p</td><td>Sufficiently Isotropic</td><td>n≥ó(</td><td>p2a2112</td><td>n≥</td><td></td></tr><tr><td>f*(x)= X1X2</td><td>{-1,+1}d</td><td>n≥(</td><td></td><td colspan="2">n ≥ Ω(d²)</td></tr><tr><td>f*(x) =(0*,xxT)</td><td>Unif(sd-1(vd)</td><td>n≥o</td><td>2 dr²</td><td colspan="2">n≥o</td></tr></table></body></html>  

# Appendix E Unifying View of NTK and Mean-Field Regime  

There have been attempts to give a unifying view of these two regimes. The first attempt we are aware of is Chen et al. [2020b]. The motivation of this paper is summarized in the above table, along with the pros and cons of NTK and Mean-Field regimes. As mentioned in Table 1, the caveat of MF analysis lies in its difficulties in obtaining the generalization bound, yet the regime allows the parameters to travel far away from their initializations. The key idea of the paper is to bring in the NTK which requires $\lVert { \boldsymbol { \theta } } ^ { ( k ) } - { \boldsymbol { \theta } } ^ { ( 0 ) } \rVert _ { 2 } ^ { 2 }$ to be small. Instead, they worked on the probability measure space ${ \mathcal { P } } ( \mathbb { R } ^ { d } )$ minimizing the following energy functional, where $f ( \mathbf { x } ; \rho ) : = \alpha \int \sigma _ { \star } ( \mathbf { x } ; \theta ) \rho ( d \theta )$ is the integral representation of neural dynamics in Equation. (8):  

$$
\rho ^ { \star } : = \underset { \rho \in \mathcal { P } ( \mathbb { R } ^ { d } ) } { \arg \operatorname* { m i n } } \left\{ \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( \mathbf { y } _ { i } - f ( \mathbf { x } _ { i } ; \rho ) \right) ^ { 2 } + \lambda \mathcal { D } _ { \mathbf { K L } } ( \rho \parallel \rho _ { 0 } ) \right\} ,
$$  

with a hyper-parameter $\lambda > 0$ . Here, $\rho ^ { \star } \in \mathcal { P } ( \mathbb { R } ^ { d } )$ is a global minimizer of Equation. (32). Note that they penalize the KL-divergence between $\rho$ and $\rho _ { 0 }$ instead the distance between $\theta ^ { ( k ) }$ and $\theta ^ { ( 0 ) }$ . The training of the objective functional Equation. (32) is often implemented through the Wasserstein gradient flow Mei et al. [2018b]. Note that the noisy gradient descent (NGD) corresponds to a discretized version of the Wasserstein gradient flow in parameter space, and it has been extensively studied in Mei et al. [2018b, 2019], Chizat and Bach [2018] that the NGD algorithm approximates the joint measure $\rho ^ { \star }$ which is the solution of Equation. (28) with an additional diffusion term.  

Under this setting, Chen et al. [2020b] showed the generalization bound for the binary classification problem with $0 - 1$ loss. (i.e., $\ell ^ { \mathsf { U } - 1 } ( y , y ^ { \prime } ) : = \mathbb { 1 } ( y y ^ { \prime } < 0 )$ .) Given the target function has the form $\mathbf { y } : = f ( \mathbf { x } ; \rho _ { \mathrm { t r u e } } )$ , the generalization bound is given as:  

$$
\begin{array} { r } { \mathbb { E } _ { \mathcal { D } } \big [ \ell ^ { 0 - 1 } \big ( f ( \rho ^ { \star } , \mathbf { x } ) , \mathbf { y } \big ) \big ] \leq \underbrace { \tilde { \mathcal { O } } \bigg ( \sqrt { \frac { \mathcal { D } _ { X ^ { 2 } } \big ( \rho _ { \mathrm { t r u e } } \big | \big | \rho _ { 0 } \big ) } { n } } \bigg ) } _ { \mathrm { N T K } \big ( \alpha = \mathcal { O } ( \sqrt { M } ) \big ) } , \quad \underbrace { \tilde { \mathcal { O } } \bigg ( \sqrt { \frac { \mathcal { D } _ { \mathrm { K L } } \big ( \rho _ { \mathrm { t r u e } } \big | \big | \rho _ { 0 } \big ) } { n } } \bigg ) } _ { \mathrm { M F } \big ( \alpha = \mathcal { O } ( 1 ) \big ) } . } \end{array}
$$  

This implies that when $\alpha$ in Equation. 8 (in main text) is large, i.e., $\alpha = \mathcal { O } ( \sqrt { M } )$ (resp. $\alpha = \mathcal { O } ( 1 )$ ), two-layer neural networks with infinite width trained by noisy gradient descent (NGD) can learn the function class $\mathcal { F } _ { \mathcal { X } ^ { 2 } }$ (resp. the function class $\mathcal { F } _ { \mathbf { K L } }$ ). The two function classes are defined as follows:  

$$
\tau _ { X ^ { 2 } } : = \Bigg \{ \int \sigma _ { \star } ( \mathbf { x } ; \theta ) \rho ( d \theta ) : \mathscr { D } _ { X ^ { 2 } } ( \rho \parallel \rho _ { 0 } ) < \infty \Bigg \} , \quad \mathcal { F } _ { \mathbf { K L } } : = \Bigg \{ \int \sigma _ { \star } ( \mathbf { x } ; \theta ) \rho ( d \theta ) : \mathscr { D } _ { \mathbf { K L } } ( \rho \parallel \rho _ { 0 } ) < \infty \Bigg \} ,
$$  

where $\mathcal { D } _ { X ^ { 2 } } ( \rho \parallel \rho _ { 0 } )$ and ${ \mathcal { D } } _ { \mathbf { K L } } ( \rho \parallel \rho _ { 0 } )$ denote the $\mathcal { X } ^ { 2 }$ and KL-divergences between $\rho$ and $\rho _ { 0 }$ , respectively. As KL-divergence is smaller than $\mathcal { X } ^ { 2 }$ distance, it can be checked ${ \mathcal { F } } _ { { \mathcal { X } } ^ { 2 } } \subsetneq { \mathcal { F } } _ { \mathbf { K L } }$ (a notation $\subsetneq$ denotes a strict subset). This implies that the two-layer neural networks learned through NGD in mean-field scaling can potentially learn a larger class of functions than networks in NTK scaling can learn.  

Another work Yang and Hu [2020] provides a unifying framework encompassing the parametrizations that induce the neural dynamics in NTK and MF regimes. Under the heavy overparameterized setting (i.e., the width of the network is very large), the main differences between these two regimes come from the different scales on initialized weight parameters and learning rates in GD or SGD. In the paper, the proposed abc-parameterization provides a guideline on how to choose the scales on these parameters. Specifically, recall the definition of a fully connected network with $L + 1$ layers in Equation. 1 (in main text). In this context, the width of the network remains consistent at $M$ across all layers. The parametrization is given by a set of numbers $\{ a _ { \ell } , b _ { \ell } \} _ { \ell } \cup \{ c \}$ . The pair $\left( a _ { \ell } , b _ { \ell } \right)$ controls the scale of initialized weight parameters over the layers, and $c$ is the parameter relevant to the learning rate of SGD.  

<html><body><table><tr><td></td><td>Definition</td><td>NTK</td><td>Mean Field (L=1)</td><td>Maximal Update (μP)</td></tr><tr><td>al</td><td>We = M-aewl</td><td>0 l=1 (1/2 l>1</td><td>{o l=1 1 l=2</td><td>-1/2 l=1 0 2≤l≤L</td></tr><tr><td>be</td><td>wβ ~N(0,M-2be)</td><td>0</td><td>0</td><td>1/2 l=L+1 1/2</td></tr><tr><td>cl</td><td>LR=nM-c</td><td>0</td><td>-1</td><td>0</td></tr></table></body></html>  

The parameterization is given as:  

1. Parameterize $\begin{array} { r } { W _ { \ell } = M ^ { - a _ { \ell } } w ^ { \ell } } \end{array}$ where $w ^ { \ell }$ is trained instead of $W _ { \ell }$ for all $\ell \in [ L + 1 ]$ .   
2. Initialize each $w _ { \alpha \beta } ^ { \ell } \sim \mathcal { N } ( 0 , M ^ { - 2 b _ { \ell } } )$ .   
3. SGD learning rate is $\eta M ^ { - c }$ for some width-independent $\eta > 0$ .  

Borrowing the paper’s notation, let us denote $\mathbf { x } _ { t } ^ { \ell } ( \xi )$ as the output of the $\ell$ -th hidden layer of fullyconnected network Equation. 1 (in main text) (after the activation) with the input $\xi \in \mathbb { R } ^ { d }$ for the $t$ -th SGD updates. The paper defines the difference vector $\Delta \mathbf { x } _ { t } ^ { \ell } ( \xi ) : = \mathbf { x } _ { t } ^ { \ell } ( \xi ) - \mathbf { x } _ { 0 } ^ { \ell } ( \xi )$ for all $\ell \in [ L + 1 ]$ with $r$ notation:  

$$
r : = \operatorname * { m i n } ( b _ { L + 1 } , a _ { L + 1 } + c ) + a _ { L + 1 } + c + \operatorname * { m i n } _ { \ell = 1 , \ldots , L } [ 2 a _ { \ell } - 1 ( \ell \neq 1 ) ] .
$$  

Under this setting, they consider the regimes of parameterizations $\{ a _ { \ell } , b _ { \ell } \} _ { \ell } \cup \{ c \}$ where every coordinate of $\mathbf { x } _ { t } ^ { \ell } ( \xi )$ doesn’t blow-up for every $\ell \in [ L + 1 ]$ . Specifically, the regimes for which $r \geq 0$ when $\Delta \mathbf { x } _ { t } ^ { \ell } ( \xi ) : = \Theta ( M ^ { - r } )$ are considered. This is in conjunction with the additional linear constraints on $\{ a _ { \ell } , b _ { \ell } \} _ { \ell } \cup \{ c \}$ where the network outputs at initialization and during training don’t blow-up. (See Theorem 3.2 in Yang and Hu [2020] for the detail.) The high-dimensional polyhedron formed through these constraints is referred to as a stable abc-parameterization region in the sense that training through SGD should be stable avoiding the blow-up of network dynamics. They further showed the stable region is divided into two parts where the dynamics change (non-trivial) and doesn’t change (trivial) in its infinite width limit, i.e., $M  \infty$ during training. The non-trivial stable abc-parameterization is possible on the union of the following two faces of the polyhedron:  

$$
a _ { L + 1 } + b _ { L + 1 } + r = 1 \quad \mathrm { o r } \quad 2 a _ { L + 1 } + c = 1 .
$$  

In this region, the infinite width limit either (1) allows the embedding of the last hidden layer $\mathbf { x } ^ { L } ( \boldsymbol { \xi } )$ to evolve non-trivially with $r = 0$ (MF regime) or (2) is described by kernel gradient descent with $r > 0$ (NTK regime), but not both.  

The corresponding settings of $( a _ { \ell } , b _ { \ell } ) \cup \{ c \}$ for NTK and MF regimes are summarized in the above table. They only focus on the scaling with $M$ and ignore dependence on the intrinsic dimension $d$ . The setting for NTK matches with the Theorem 5 in Du et al. [2019], where the entries of $W _ { \ell }$ is drawn from $\mathcal { N } ( 0 , 1 / M )$ with width-independent $\eta > 0$ . The reason why we are seeing $a _ { \ell } = 0$ for $\ell = 1$ in the NTK regime is to ensure that the output of every hidden layer in the network is of constant order so that parameterization is in a stable regime. For the MF regime, Mei et al. [2018b, 2019] gives us the width-independent parameterizations for weight initializations and learning rate in shallow neural network. Note that the $\textstyle { \frac { 1 } { M } }$ -scale in the output weight layer ( $\ell = 2$ ) is canceled with the $M$ -scale of learning rate in SGD.  

Although not mentioned in the paper, we conjecture the reason for these manipulations is to put the parameterizations in the MF regime to satisfy the constraints Equation. (35). The $\{ a _ { \ell } , b _ { \ell } \} _ { \ell } \cup \{ c \}$ pair for NTK satisfies $\begin{array} { r } { r = { \frac { 1 } { 2 } } } \end{array}$ , meaning the dynamic is described through kernel gradient descent, whereas the pair for MF satisfies $r = 0$ , allowing the feature learning. The authors of this paper showed the feature learning dynamics in the MF regime can be extended to a multi-layer setting, where they call the corresponding parameterizations as Maximal Update parameterization $( \mu P )$ . This parametrization is “Maximal” in the sense that feature learning is enabled throughout the entire layers in the network in the $\infty$ -width limit. As mentioned in the introduction, feature learning is the essence of modern deep learning making techniques such as transfer learning or fine-tuning in the Large Language Model (LLM) possible, and it is remarkable in the sense that the exact neural dynamics are mathematically tractable under this setting.  

# References  

Ekin Aky¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? Advances in Neural Information Processing Systems, 32, 2019.   
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. arXiv e-prints, pages arXiv–1811, 2018.   
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. Advances in Neural Information Processing Systems, 32: 6158–6169, 2019.   
Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313–326, 1982.   
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International conference on machine learning, pages 224–232. PMLR, 2017.   
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322–332. PMLR, 2019a.   
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems, 32:8141–8150, 2019b.   
Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The Annals of statistics, 35(2):608–633, 2007.   
Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. Highdimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems, 35:37932–37946, 2022.   
Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning in the presence of low-dimensional structure: a spiked random matrix perspective. Advances in Neural Information Processing Systems, 36, 2024.   
Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. In International Conference on Learning Representations, 2019.   
Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in gans. arXiv preprint arXiv:1806.10586, 2018.   
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.   
Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov diffusion operators, volume 103. Springer, 2014.   
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930–945, 1993.   
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine learning, 14:115–133, 1994.   
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. The Journal of Machine Learning Research, 20(1):2285–2301, 2019.   
Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. Acta Numerica, 30:87–201, 2021. doi: 10.1017/S0962492921000027.   
Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality in nonparametric regression. 2019.   
Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. Acta Numerica, 30:203–248, 2021.   
Yoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. Advances in neural information processing systems, 18, 2005.   
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In NeurIPS 2019- Thirty-third Conference on Neural Information Processing Systems, volume 32, pages 12873–12884, 2019.   
Moise Blanchard and Mohammed Amine Bennouna. Shallow and deep networks are near-optimal approximators of Korobov functions. In International Conference on Learning Representations, 2022.   
Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising autoencoders and langevin sampling. arXiv preprint arXiv:2002.00107, 2020.   
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. Advances in Neural Information Processing Systems, 32:10836–10846, 2019.   
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.   
Andrei Caragea, Philipp Petersen, and Felix Voigtlaender. Neural network approximation and estimation of classifiers with classification boundary in a barron class. The Annals of Applied Probability, 33(4):3039–3079, 2023.   
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on lowdimensional manifolds using deep ReLU networks: Function approximation and statistical recovery. To appear in Information and Inference: a Journal of the IMA, 2019.   
Minshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Distribution approximation and statistical estimation guarantees of generative adversarial networks. arXiv preprint arXiv:2002.03938, 2020a.   
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on lowdimensional manifolds using deep ReLU networks: Function approximation and statistical recovery. Information and Inference: A Journal of the IMA, 11(4):1203–1253, 2022a.   
Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. arXiv preprint arXiv:2302.07194, 2023a.  

Richard J Chen, Ming Y Lu, Tiffany Y Chen, Drew FK Williamson, and Faisal Mahmood. Synthetic data in machine learning for medicine and healthcare. Nature Biomedical Engineering, 5(6):493–497, 2021.  

Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215, 2022b.   
Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ode is provably fast. arXiv preprint arXiv:2305.11798, 2023b.   
Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024.   
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. A generalized neural tangent kernel analysis for two-layer neural networks. Advances in Neural Information Processing Systems, 33, 2020b.   
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.(2018). arXiv preprint arXiv:1812.07956, 1812.   
L´enaı¨c Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 3040–3050, 2018.   
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314, 1989.   
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn incontext? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022.   
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint arXiv:2208.05314, 2022.   
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr¨odinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695–17709, 2021.   
Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. Advances in neural information processing systems, 24, 2011.   
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   
Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta Numerica, 30:327–444, 2021.   
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780–8794, 2021.   
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.   
David L Donoho and Iain M Johnstone. Minimax estimation via wavelet shrinkage. The Annals of Statistics, 26(3):879–921, 1998.   
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pages 1675–1685. PMLR, 2019.   
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes overparameterized neural networks. In International Conference on Learning Representations, 2018.  

Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and applications of models of computation, pages 1–19. Springer, 2008.  

Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide to deep learning in healthcare. Nature medicine, 25(1):24–29, 2019.   
Jianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning. Statistical science: a review journal of the Institute of Mathematical Statistics, 36(2):264, 2021.   
Zhiying Fang and Guang Cheng. Optimal learning rates of deep convolutional neural networks: Additive ridge functions. arXiv preprint arXiv:2202.12119, 2022.   
Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and inference. Econometrica, 89(1):181–213, 2021.   
Han Feng, Shuo Huang, and Ding-Xuan Zhou. Generalization analysis of CNNs for classification on spheres. IEEE Transactions on Neural Networks and Learning Systems, 2021.   
Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.   
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Discussion of:“nonparametric regression using deep neural networks with relu activation function”. 2020a.   
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020b.   
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. The Annals of Statistics, 49(2):1029–1054, 2021.   
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.   
Federico Girosi and Tomaso Poggio. Representation properties of networks: Kolmogorov’s theorem is irrelevant. Neural Computation, 1(4):465–469, 1989.   
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014.   
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.   
Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning techniques for autonomous driving. Journal of Field Robotics, 37(3):362–386, 2020.   
Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative adversarial networks: Algorithms, theory, and applications. IEEE transactions on knowledge and data engineering, 35(4):3313–3332, 2021.   
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832– 1841. PMLR, 2018.   
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.  

Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015b.  

Zhi Han, Siquan Yu, Shao-Bo Lin, and Ding-Xuan Zhou. Depth selection for deep ReLU nets in feature extraction and generalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.  

Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu activations. Mathematics, 7(10):992, 2019.   
Satoshi Hayakawa and Taiji Suzuki. On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces. Neural Networks, 123:343–361, 2020.   
Fengxiang He and Dacheng Tao. Recent advances in deep learning theory. arXiv preprint arXiv:2012.10931, 2020.   
James B Heaton, Nick G Polson, and Jan Hendrik Witte. Deep learning for finance: deep portfolios. Applied Stochastic Models in Business and Industry, 33(1):3–12, 2017.   
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.   
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.   
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural networks, 3(5):551–560, 1990.   
Tianyang Hu, Zuofeng Shang, and Guang Cheng. Optimal rate of convergence for deep neural network classifiers under the teacher-student setting. arXiv preprint arXiv:2001.06892, 2020a.   
Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng. Regularization matters: A nonparametric perspective on overparametrized neural network. In International Conference on Artificial Intelligence and Statistics, pages 829–837. PMLR, 2021.   
Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on noisily labeled data with generalization guarantee. In International Conference on Learning Representations, 2019.   
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. Advances in Neural Information Processing Systems, 33: 17116–17128, 2020b.   
Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.   
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively. In The 22nd international conference on artificial intelligence and statistics, pages 869–878. PMLR, 2019.   
Masaaki Imaizumi and Kenji Fukumizu. Advantage of deep neural networks for estimating functions with singularity on hypersurfaces. Journal of Machine Learning Research, 23:1–54, 2022.   
Arthur Jacot, Cl´ement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In NeurIPS, 2018.   
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint  

arXiv:1803.07300, 2018.  

Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On computation and generalization of generative adversarial networks under spectrum control. In International Conference on Learning Representations, 2018.   
Yuling Jiao, Guohao Shen, Yuanyuan Lin, and Jian Huang. Deep nonparametric regression on approximately low-dimensional manifolds. arXiv preprint arXiv:2104.06708, 2021.   
Jesse Johnson. Deep, skinny neural networks are not universal approximators. arXiv preprint arXiv:1810.00393, 2018.   
Anatoli B Juditsky, Oleg V Lepski, and Alexandre B Tsybakov. Nonparametric estimation of composite functions. 2009.   
Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In Conference on learning theory, pages 2306–2327. PMLR, 2020.   
Jayoung Kim, Chaejeong Lee, and Noseong Park. Stasy: Score-based tabular data synthesis. In The Eleventh International Conference on Learning Representations, 2022.   
Yongdai Kim, Ilsang Ohn, and Dongha Kim. Fast convergence rates of deep neural networks for classification. Neural Networks, 138:179–197, 2021.   
Hyunouk Ko, Namjoon Suh, and Xiaoming Huo. On excess risk convergence rates of neural network classifiers. Submitted to IEEE Transactions on Information Theory, 2023.   
Michael Kohler and Sophie Langer. Discussion of:“nonparametric regression using deep neural networks with relu activation function”. 2020.   
Michael Kohler and Sophie Langer. On the rate of convergence of fully connected deep neural network regression estimates. The Annals of Statistics, 49(4):2231–2249, 2021.   
Michael Kohler, Adam Krzy˙zak, and Sophie Langer. Estimation of a function of low local dimensionality by deep neural networks. IEEE transactions on information theory, 68(6):4032–4042, 2022.   
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   
Gitta Kutyniok. Discussion of:“nonparametric regression using deep neural networks with relu activation function”. 2020.   
Jianfa Lai, Manyun Xu, Rui Chen, and Qian Lin. Generalization ability of wide neural networks on mathbbR. arXiv preprint arXiv:2302.05933, 2023.   
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870–22882, 2022.   
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946–985. PMLR, 2023.   
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on Learning Representations, 2018.   
Ximing Li, Chendi Wang, and Guang Cheng. Statistical theory of differentially private marginal-based data synthesis algorithms. arXiv preprint arXiv:2301.08844, 2023a.   
Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and implicit model selection in in-context learning. arXiv preprint arXiv:2301.07067, 2023b.   
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In NeurIPS, 2018.   
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.   
Tengyuan Liang. How well generative adversarial networks learn distributions. The Journal of Machine Learning Research, 22(1):10366–10406, 2021.   
Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: Understanding and extending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022.   
Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacˇic´, Thomas Y Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756, 2024.   
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for smooth functions. SIAM Journal on Mathematical Analysis, 53(5):5465–5506, 2021.   
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. Advances in neural information processing systems, 30, 2017.   
Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27(6):1808–1829, 1999.   
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses. The Annals of Statistics, 46(6A):2747–2774, 2018a.   
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018b.   
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388– 2464. PMLR, 2019.   
Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. When and why are deep networks better than shallow ones? In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.   
Hrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural computation, 8(1):164–177, 1996.   
Hadrien Montanelli and Qiang Du. New error bounds for deep ReLU networks using sparse grids. SIAM Journal on Mathematics of Data Science, 1(1):78–92, 2019.   
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. Advances in neural information processing systems, 27, 2014.   
Alfred Mu¨ller. Integral probability metrics and their generating classes of functions. Advances in applied probability, 29(2):429–443, 1997.   
Gustav Mu¨ller-Franzes, Jan Moritz Niehues, Firas Khader, Soroosh Tayebi Arasteh, Christoph Haarburger, Christiane Kuhl, Tianci Wang, Tianyu Han, Sven Nebelung, Jakob Nikolas Kather, et al. Diffusion probabilistic models beat gans on medical images. arXiv preprint arXiv:2212.07501, 2022.   
Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep neural network with intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1–38, 2020.   
Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.   
Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017.   
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.   
Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multilayer neural networks. arXiv preprint arXiv:2001.11443, 2020.   
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime. In International Conference on Learning Representations, 2020.   
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.   
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. arXiv preprint arXiv:2303.01861, 2023.   
Daniel W Otter, Julian R Medina, and Jugal K Kalita. A survey of the usages of deep learning for natural language processing. IEEE transactions on neural networks and learning systems, 32(2): 604–624, 2020.   
Yidong Ouyang, Liyan Xie, and Guang Cheng. Improving adversarial robustness through the contrastive-guided diffusion process. In International Conference on Machine Learning, pages 26699– 26723. PMLR, 2023a.   
Yidong Ouyang, Liyan Xie, Chongxuan Li, and Guang Cheng. Missdiff: Training diffusion models on tabular data with missing values. arXiv preprint arXiv:2307.00467, 2023b.   
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 1(1):84–105, 2020.   
Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approximation. arXiv preprint arXiv:2006.08859, 2020.   
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.   
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using deep ReLU neural networks. Neural Networks, 108:296–330, 2018.   
Tomaso Poggio and Federico Girosi. A theory of networks for appxoimation and learning. 1989.   
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. Advances in neural information processing systems, 29, 2016.   
Kolyan Ray and Johannes Schmidt-Hieber. A regularity class for the roots of nonnegative functions. Annali di Matematica Pura ed Applicata (1923-), 196:2091–2103, 2017.   
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290(5500):2323–2326, 2000.   
Lenya Ryzhik. Lecture notes for math 272, winter 2023. pages 1–183, 2023.   
Simo S¨arkk¨a and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019.   
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. The Annals of Statistics, 48(4):1875–1897, 2020.   
Johannes Schmidt-Hieber. The kolmogorov–arnold representation theorem revisited. Neural networks, 137:119–126, 2021.   
Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak Dalalyan. Statistical guarantees for generative models without domination. In Algorithmic Learning Theory, pages 1051–1071. PMLR, 2021.  

Ohad Shamir. Discussion of:“nonparametric regression using deep neural networks with relu activation function”. 2020.  

Guohao Shen, Yuling Jiao, Yuanyuan Lin, and Jian Huang. Approximation with cnns in sobolev space: with applications to classification. Advances in Neural Information Processing Systems, 35: 2876–2888, 2022.  

Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation characterized by number of neurons. arXiv preprint arXiv:1906.05497, 2021.   
Samuel Smith, Erich Elsen, and Soham De. On the generalization benefit of noise in stochastic gradient descent. In International Conference on Machine Learning, pages 9058–9067. PMLR, 2020.   
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.   
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. arXiv preprint arXiv:1906.03593, 2019.   
Namjoon Suh, Hyunouk Ko, and Xiaoming Huo. A non-parametric regression viewpoint: Generalization of overparametrized deep relu network under noisy observations. In International Conference on Learning Representations, 2021.   
Namjoon Suh, Tian-Yi Zhou, and Xiaoming Huo. Approximation and non-parametric estimation of functions over high-dimensional spheres via deep relu networks. In The Eleventh International Conference on Learning Representations, 2022.   
Namjoon Suh, Xiaofeng Lin, Din-Yin Hsieh, Merhdad Honarkhah, and Guang Cheng. Autodiff: combining auto-encoder and diffusion model for tabular data synthesizing. arXiv preprint arXiv:2310.15479, 2023.   
Taiji Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality. In International Conference on Learning Representations, 2018.   
Taiji Suzuki and Atsushi Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic besov space. Advances in Neural Information Processing Systems, 34: 3609–3621, 2021.   
Rong Tang and Yun Yang. Minimax rate of distribution estimation on unknown submanifold under adversarial losses. arXiv preprint arXiv:2202.09030, 2022.   
Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000.   
Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135–166, 2004.   
Sara A Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.   
Gal Vardi, Gilad Yehudai, and Ohad Shamir. Width is less important than depth in relu neural networks. In Conference on learning theory, pages 1249–1281. PMLR, 2022.   
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   
Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo˜ao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151–35174. PMLR, 2023.  

Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.  

Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets vs their induced kernel. Advances in Neural Information Processing Systems, 32, 2019.   
Xiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods for an over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.   
Yue Xing, Qifan Song, and Guang Cheng. Why do artificially generated data help adversarial robustness. Advances in Neural Information Processing Systems, 35:954–966, 2022a.   
Yue Xing, Qifan Song, and Guang Cheng. Unlabeled data help: Minimax analysis and adversarial robustness. In International Conference on Artificial Intelligence and Statistics, pages 136–168. PMLR, 2022b.   
Shirong Xu, Will Wei Sun, and Guang Cheng. Utility theory of synthetic data generation. arXiv preprint arXiv:2305.10015, 2023a.   
Shirong Xu, Chendi Wang, Will Wei Sun, and Guang Cheng. Binary classification under local label differential privacy using randomized response mechanisms. Transactions on Machine Learning Research, 2023b.   
Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020.   
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796, 2022.   
Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94: 103–114, 2017.   
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. Advances in Neural Information Processing Systems, 32, 2019.   
Xianli Zeng, Edgar Dobriban, and Guang Cheng. Bayes-optimal classifiers under group fairness. arXiv preprint arXiv:2202.09724, 2022.   
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.   
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in gans. arXiv preprint arXiv:1711.02771, 2017.   
Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.   
Guoqiang Zhong, Li-Na Wang, Xiao Ling, and Junyu Dong. An overview on data representation learning: From traditional feature learning to recent deep learning. The Journal of Finance and Data Science, 2(4):265–278, 2016.   
Tian-Yi Zhou and Xiaoming Huo. Classification of data generated by gaussian mixture models using deep relu networks. arXiv preprint arXiv:2308.08030, 2023.   
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. Advances in neural information processing systems, 2019.   
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes overparameterized deep ReLU networks. arxiv e-prints, art. arXiv preprint arXiv:1811.08888, 2018.  