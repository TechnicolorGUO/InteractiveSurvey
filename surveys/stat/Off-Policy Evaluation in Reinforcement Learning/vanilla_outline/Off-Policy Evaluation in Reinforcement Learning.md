# 1 Introduction
Reinforcement learning (RL) has emerged as a powerful framework for solving sequential decision-making problems. A critical aspect of RL is the evaluation of policies, which involves estimating the performance of a policy based on data collected from interactions with an environment. In many practical scenarios, it is either infeasible or undesirable to evaluate a policy directly by interacting with the environment. This leads us to the concept of off-policy evaluation (OPE), which allows us to estimate the value of a target policy using data generated by a different behavior policy.

In this survey, we provide a comprehensive overview of the state-of-the-art methods for off-policy evaluation in reinforcement learning, their applications, and open research questions. The goal is to consolidate existing knowledge and highlight promising directions for future work.

## 1.1 Motivation for Off-Policy Evaluation
Off-policy evaluation is essential in scenarios where direct interaction with the environment is costly, risky, or otherwise impractical. For example, in healthcare decision-making, evaluating a new treatment policy through direct experimentation could expose patients to suboptimal treatments. Similarly, in robotics, repeated physical interactions with the real world can be time-consuming and damaging to hardware. OPE provides a solution by enabling the estimation of policy performance using historical data.

Mathematically, the objective of OPE is to estimate the expected return of a target policy $\pi_e$:
$$
V^{\pi_e} = \mathbb{E}_{\tau \sim \pi_e}[G_t],
$$
where $G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$ represents the discounted return starting at time $t$, and $\tau$ denotes the trajectory under $\pi_e$. However, the data used for estimation comes from a behavior policy $\pi_b$, which may differ significantly from $\pi_e$. Bridging this gap is the central challenge of OPE.

## 1.2 Scope and Objectives
This survey focuses on methods for off-policy evaluation in both discrete and continuous settings. We cover a range of techniques, including direct methods, importance sampling, doubly robust estimators, and fitted Q-evaluation. Additionally, we discuss the strengths and limitations of these approaches, highlighting their applicability in various domains such as healthcare, recommender systems, and robotics.

Our objectives are threefold: (1) to provide a detailed review of existing OPE methods; (2) to analyze their theoretical foundations and empirical performance; and (3) to identify open challenges and potential avenues for future research.

## 1.3 Structure of the Survey
The remainder of this survey is organized as follows:

- **Section 2**: We introduce the fundamentals of reinforcement learning, focusing on Markov decision processes (MDPs), policies, and value functions. We also discuss the importance of policy evaluation and distinguish between on-policy and off-policy evaluation.

- **Section 3**: This section delves into the core methods for off-policy evaluation. We describe direct methods, importance sampling techniques (e.g., ordinary, per-decision, and stationary importance sampling), doubly robust estimators, and fitted Q-evaluation.

- **Section 4**: Here, we explore real-world applications of OPE, including case studies in healthcare, recommender systems, and robotics. We also examine simulation studies conducted in synthetic environments and benchmark comparisons.

- **Section 5**: We critically assess the strengths and weaknesses of current OPE methods, outline open research questions, and discuss ethical considerations related to the deployment of RL systems.

- **Section 6**: Finally, we summarize the key findings of the survey and propose future directions for advancing the field of off-policy evaluation.

# 2 Background

To fully appreciate the intricacies of off-policy evaluation in reinforcement learning (RL), it is essential to establish a foundational understanding of RL concepts and their implications for policy evaluation. This section provides an overview of key principles, starting with the fundamentals of reinforcement learning and progressing to the importance of evaluating policies.

## 2.1 Reinforcement Learning Fundamentals

Reinforcement learning (RL) is a paradigm where agents learn to make decisions by interacting with an environment to maximize cumulative rewards. At its core, RL involves solving **Markov Decision Processes (MDPs)**, which provide a formal framework for modeling sequential decision-making problems.

### 2.1.1 Markov Decision Processes

An MDP is defined as a tuple $(S, A, P, R, \gamma)$, where:
- $S$ is the set of states,
- $A$ is the set of actions,
- $P(s'|s,a)$ is the transition probability function, specifying the probability of transitioning to state $s'$ given action $a$ in state $s$,
- $R(s,a)$ is the expected reward function for taking action $a$ in state $s$, and
- $\gamma \in [0, 1)$ is the discount factor that determines the present value of future rewards.

The goal in an MDP is to find a policy $\pi(a|s)$ that maximizes the expected return, defined as the discounted sum of rewards: 
$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}.
$$

![](placeholder_for_mdp_diagram)

### 2.1.2 Policies and Value Functions

A policy $\pi(a|s)$ defines the agent's behavior at a given time, mapping states to probabilities of selecting each possible action. Two primary value functions are associated with policies:
- The **state-value function** $V^\pi(s)$ represents the expected return starting from state $s$ and following policy $\pi$: 
$$
V^\pi(s) = \mathbb{E}_\pi \left[ G_t | S_t = s \right].
$$
- The **action-value function** $Q^\pi(s,a)$ represents the expected return starting from state $s$, taking action $a$, and thereafter following policy $\pi$: 
$$
Q^\pi(s,a) = \mathbb{E}_\pi \left[ G_t | S_t = s, A_t = a \right].
$$

Optimal policies aim to maximize these value functions, leading to the optimal state-value function $V^*(s)$ and optimal action-value function $Q^*(s,a)$.

## 2.2 Importance of Policy Evaluation

Policy evaluation plays a central role in RL, as it allows agents to assess the quality of their current or candidate policies. This process is critical for both improving existing policies and comparing alternatives.

### 2.2.1 On-Policy vs Off-Policy Evaluation

In RL, policy evaluation can be categorized into two main types:
- **On-policy evaluation**: Estimates the value of the policy currently being followed. For example, Monte Carlo methods or Temporal Difference (TD) learning estimate $V^\pi(s)$ or $Q^\pi(s,a)$ under the same policy used to generate data.
- **Off-policy evaluation**: Estimates the value of a target policy using data collected under a different behavior policy. This is particularly valuable when data collection is expensive or when evaluating multiple policies from a fixed dataset.

Off-policy evaluation enables greater flexibility and efficiency in RL applications, as it decouples the evaluation process from the data-generating policy.

### 2.2.2 Challenges in Off-Policy Evaluation

Despite its advantages, off-policy evaluation faces several challenges:
1. **Data mismatch**: The distribution of states and actions under the behavior policy may differ significantly from those under the target policy, leading to high variance in estimates.
2. **High-dimensional spaces**: In environments with large or continuous state-action spaces, accurately estimating value functions becomes computationally demanding.
3. **Bias-variance tradeoff**: Methods like importance sampling can introduce high variance, while model-based approaches may suffer from bias due to approximation errors.

| Challenge               | Description                                                                                     |
|------------------------|-----------------------------------------------------------------------------------------------|
| Data mismatch          | Differences between behavior and target policies complicate accurate estimation.                 |
| High-dimensional spaces | Large or continuous state-action spaces increase computational complexity.                       |
| Bias-variance tradeoff | Balancing accuracy and stability is crucial for reliable off-policy evaluation.                |

These challenges underscore the need for robust and efficient techniques for off-policy evaluation, which we explore in subsequent sections.

# 3 Methods for Off-Policy Evaluation

Off-policy evaluation (OPE) is a critical component of reinforcement learning, enabling the assessment of a target policy's performance using data collected under a different behavior policy. This section provides an in-depth overview of the primary methods used for OPE, including the direct method, importance sampling, doubly robust estimators, and fitted Q-evaluation.

## 3.1 Direct Method

The direct method estimates the value function of the target policy by explicitly modeling it from the observed data. This approach does not rely on reweighting samples based on the discrepancy between policies but instead focuses on approximating the value function directly.

### 3.1.1 Regression-Based Approaches

Regression-based approaches involve training a model to predict the expected return given a state-action pair. For example, one can use supervised learning techniques to approximate the action-value function $Q^\pi(s, a)$ of the target policy $\pi$. The loss function typically minimizes the mean squared error between the predicted and actual returns:

$$
L(\theta) = \mathbb{E}_{(s, a, r, s') \sim D} \left[ \left(r + \gamma \max_{a'} Q^\pi(s', a'; \theta) - Q^\pi(s, a; \theta)\right)^2 \right],
$$
where $D$ represents the dataset, $\gamma$ is the discount factor, and $\theta$ are the parameters of the model.

This method is computationally efficient and can generalize well when the function approximator is expressive enough. However, its accuracy depends heavily on the quality of the learned model and may suffer from high variance if the dataset is limited or noisy.

### 3.1.2 Model-Based Approaches

Model-based methods estimate the dynamics of the environment, i.e., the transition probabilities $P(s' | s, a)$ and reward function $R(s, a)$, and then use these models to simulate trajectories under the target policy. Once the model is learned, Monte Carlo rollouts can be performed to estimate the value of the target policy.

A key challenge with model-based approaches is the potential mismatch between the learned model and the true environment dynamics, which can lead to biased evaluations. Techniques such as ensemble modeling or uncertainty quantification can mitigate this issue but at the cost of increased computational complexity.

## 3.2 Importance Sampling Methods

Importance sampling (IS) methods address the off-policy problem by reweighting the observed data according to the ratio of the probabilities of actions under the target and behavior policies. These methods are theoretically grounded but often suffer from high variance.

### 3.2.1 Ordinary Importance Sampling

Ordinary importance sampling (OIS) computes the expected return of the target policy by weighting each trajectory's return with the product of importance ratios along the trajectory:

$$
\hat{V}_{\text{OIS}} = \frac{1}{N} \sum_{i=1}^N \rho_i G_i,
$$
where $G_i$ is the return of the $i$-th trajectory, and $\rho_i = \prod_{t=0}^{T-1} \frac{\pi(a_t | s_t)}{\mu(a_t | s_t)}$ is the importance weight, with $\pi$ and $\mu$ denoting the target and behavior policies, respectively.

While OIS is unbiased, its variance grows exponentially with the trajectory length, making it impractical for long-horizon problems.

### 3.2.2 Per-Decision Importance Sampling

Per-decision importance sampling (PDIS) reduces variance by considering the importance weights at each decision point independently. The estimator is defined as:

$$
\hat{V}_{\text{PDIS}} = \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T-1} \rho_{i,t} r_{i,t},
$$
where $\rho_{i,t} = \prod_{k=0}^{t-1} \frac{\pi(a_k | s_k)}{\mu(a_k | s_k)}$.

PDIS mitigates some of the variance issues of OIS but still suffers from high variance in certain scenarios.

### 3.2.3 Stationary Importance Sampling

Stationary importance sampling (SIS) further reduces variance by incorporating the stationary distribution of the target policy into the estimation process. This method assumes that the state distribution induced by the target policy converges to a stationary distribution, allowing for more stable estimates.

$$
\hat{V}_{\text{SIS}} = \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T-1} d_\pi(s_t) \rho_{i,t} r_{i,t},
$$
where $d_\pi(s)$ represents the stationary distribution under $\pi$.

SIS is particularly useful in settings where the target policy induces a significantly different state distribution than the behavior policy.

## 3.3 Doubly Robust Estimators

Doubly robust (DR) estimators combine the advantages of both direct and importance sampling methods, providing unbiased estimates even if one of the components (model or weights) is misspecified.

### 3.3.1 Basic Doubly Robust Estimation

The basic DR estimator combines the direct method's value function approximation with importance sampling corrections:

$$
\hat{V}_{\text{DR}} = \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=0}^{T-1} \rho_{i,t} (r_{i,t} + \gamma Q(s_{i,t+1}, a_{i,t+1}) - Q(s_{i,t}, a_{i,t})) + Q(s_{i,0}, a_{i,0}) \right).
$$

This formulation ensures that the estimator remains unbiased as long as either the value function $Q$ or the importance weights $\rho$ are accurate.

### 3.3.2 Variants and Extensions

Several variants of DR estimators have been proposed to address specific challenges, such as high variance or non-stationary environments. Examples include weighted DR, which assigns higher confidence to trajectories with lower variance, and sequential DR, which accounts for time-dependent biases in the data.

| Variant         | Key Feature                          |
|-----------------|-------------------------------------|
| Weighted DR     | Adjusts weights based on variance   |
| Sequential DR    | Handles time-dependent biases        |

## 3.4 Fitted Q-Evaluation

Fitted Q-evaluation (FQE) extends the idea of regression-based approaches by iteratively refining the estimate of the action-value function $Q^\pi(s, a)$ until convergence. This method leverages the Bellman equation to ensure consistency between the estimated values and the true returns.

### 3.4.1 Overview of FQE

FQE proceeds by iteratively solving the following optimization problem for each iteration $k$:

$$
Q^{(k+1)}(s, a) = \arg\min_Q \mathbb{E}_{(s, a, r, s') \sim D} \left[ \left(r + \gamma \max_{a'} Q^{(k)}(s', a') - Q(s, a)\right)^2 \right].
$$

The process continues until the updates become negligible, yielding an estimate of $Q^\pi(s, a)$.

### 3.4.2 Advantages and Limitations

FQE offers several advantages, including its ability to handle large datasets and its theoretical guarantees of convergence under certain conditions. However, it can be computationally expensive, especially for high-dimensional state-action spaces, and its performance depends on the choice of function approximator and the quality of the dataset.

# 4 Applications of Off-Policy Evaluation

Off-policy evaluation (OPE) has found applications across a wide range of domains, from healthcare to robotics. This section explores both real-world use cases and simulation studies that demonstrate the utility and challenges of OPE techniques.

## 4.1 Real-World Use Cases

The practical applicability of off-policy evaluation is evident in various industries where sequential decision-making under uncertainty plays a critical role. Below, we discuss three prominent examples: healthcare decision making, recommender systems, and robotics control.

### 4.1.1 Healthcare Decision Making

In healthcare, decisions often involve trade-offs between short-term and long-term outcomes, such as choosing treatments for chronic conditions or managing patient care pathways. Reinforcement learning (RL) offers a framework for optimizing these decisions, but deploying RL policies directly in clinical settings can be risky. Off-policy evaluation provides a safer alternative by allowing researchers to assess the performance of new policies using historical data.

For instance, in personalized medicine, OPE methods have been used to evaluate treatment strategies derived from observational datasets. Techniques like importance sampling and doubly robust estimators are particularly valuable here due to their ability to handle confounding variables and high-dimensional state spaces. A notable challenge in this domain is ensuring that the evaluation accounts for the stochastic nature of patient responses, which can be modeled as Markov decision processes (MDPs):
$$
\text{MDP} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle,
$$
where $\mathcal{S}$ represents the state space, $\mathcal{A}$ the action space, $P$ the transition dynamics, $R$ the reward function, and $\gamma$ the discount factor.

![](placeholder_for_healthcare_diagram)

### 4.1.2 Recommender Systems

Recommender systems aim to suggest items (e.g., movies, products) to users based on their preferences. These systems often rely on logged interaction data, which reflects past behaviors rather than optimal recommendations. Evaluating new recommendation policies without affecting user experience is crucial, making OPE an ideal tool.

Methods such as fitted Q-evaluation (FQE) and importance sampling have been successfully applied in this context. For example, FQE approximates the value function of a target policy by iteratively solving Bellman equations:
$$
Q_{k+1}(s, a) = \mathbb{E}_{s', r}[r + \gamma \max_{a'} Q_k(s', a')],
$$
where $Q_k(s, a)$ denotes the estimated value at iteration $k$. Challenges include handling sparse rewards and ensuring accurate representation of user preferences.

| Metric | Description |
|--------|-------------|
| Precision@K | Fraction of recommended items relevant to the user |
| Recall@K | Fraction of relevant items correctly recommended |

### 4.1.3 Robotics Control

Robotics presents another compelling application for OPE, where autonomous agents must learn to interact with complex environments. Training robots through trial-and-error in real-world scenarios is often impractical or unsafe, necessitating offline evaluation of learned policies.

Importance sampling methods, such as per-decision importance sampling, are commonly employed in robotics due to their capacity to correct for discrepancies between behavior and target policies. However, computational efficiency remains a concern when dealing with high-dimensional continuous action spaces. Model-based approaches, which simulate environment dynamics, offer a potential solution but require careful validation to avoid compounding errors.

## 4.2 Simulation Studies

Simulation studies provide controlled environments for testing and comparing OPE algorithms. They enable researchers to systematically analyze performance metrics and identify algorithmic strengths and weaknesses.

### 4.2.1 Synthetic Environments

Synthetic environments allow for precise control over factors such as noise levels, reward structures, and transition probabilities. By designing MDPs with known ground truths, researchers can benchmark the accuracy and variance of different OPE methods. For example, a simple gridworld environment might define states as positions on a grid, actions as movements, and rewards as penalties for suboptimal paths.

$$
\text{Reward Function: } R(s, a) = \begin{cases} 
-1 & \text{if moving into a wall or obstacle}, \\
0 & \text{otherwise}.
\end{cases}
$$

Such environments facilitate detailed comparisons between direct methods, importance sampling, and doubly robust estimators, highlighting trade-offs in bias and variance.

### 4.2.2 Benchmark Comparisons

Benchmark datasets, such as those provided by OpenAI Gym or DeepMind Control Suite, serve as standardized platforms for evaluating OPE algorithms. These benchmarks typically include tasks with varying degrees of complexity, enabling comprehensive assessments of algorithm robustness.

A common metric for comparison is mean squared error (MSE) between estimated and true values:
$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^N (V_{\text{estimated}}(s_i) - V_{\text{true}}(s_i))^2,
$$
where $V_{\text{estimated}}(s_i)$ and $V_{\text{true}}(s_i)$ represent the estimated and true state values, respectively. Results from such studies underscore the importance of selecting appropriate methods based on problem characteristics, such as horizon length and data availability.

# 5 Discussion

In this section, we delve into a critical analysis of the current state of off-policy evaluation (OPE) in reinforcement learning. We discuss the strengths and weaknesses of existing methods, highlight open research questions that warrant further exploration, and address ethical considerations relevant to the application of OPE.

## 5.1 Strengths and Weaknesses of Current Approaches

Off-policy evaluation methods have made significant strides in recent years, offering practitioners tools to estimate the performance of policies without requiring direct interaction with the environment. Among the strengths of these methods are their flexibility and applicability across diverse domains. For instance, importance sampling (IS) methods provide unbiased estimators by reweighting trajectories sampled under a behavior policy to approximate the target policy's value. However, IS methods suffer from high variance, especially when the behavior and target policies differ significantly. This issue is partially mitigated by per-decision importance sampling (PDIS) and stationary importance sampling (SIS), which reduce variance at the cost of introducing bias.

Doubly robust (DR) estimators combine the advantages of model-based predictions and IS methods, achieving lower variance while maintaining unbiasedness under certain conditions. Despite their theoretical appeal, DR estimators rely on accurate models or value function approximations, which can be challenging to achieve in practice. Similarly, fitted Q-evaluation (FQE) offers a computationally efficient approach to estimating policy values but may struggle with function approximation errors in high-dimensional settings.

| Method | Strengths | Weaknesses |
|--------|-----------|------------|
| Importance Sampling | Unbiased estimation | High variance |
| Doubly Robust Estimators | Lower variance, robustness | Requires accurate models |
| Fitted Q-Evaluation | Scalable, data-efficient | Sensitive to approximation errors |

The table above summarizes the trade-offs between different OPE methods, emphasizing the need for careful selection based on the problem context.

## 5.2 Open Research Questions

Despite the progress in OPE, several open research questions remain. One key challenge is addressing the curse of horizon, where the variance of IS-based methods grows exponentially with the length of episodes. Recent work has explored techniques such as trajectory truncation and marginal importance sampling, but more scalable solutions are needed for long-horizon problems.

Another area ripe for exploration is the integration of causal inference principles into OPE. By leveraging structural causal models, researchers could develop methods that better account for unobserved confounders and improve the reliability of policy evaluations. Additionally, there is a growing interest in offline reinforcement learning, where OPE plays a crucial role in assessing the quality of learned policies. Developing OPE methods tailored to offline RL scenarios remains an important direction for future work.

Finally, the development of benchmark datasets and standardized evaluation protocols would facilitate fair comparisons between OPE algorithms. Currently, results are often reported using domain-specific simulations, making it difficult to draw general conclusions about method performance.

## 5.3 Ethical Considerations

As OPE becomes increasingly applied in real-world domains such as healthcare, finance, and autonomous systems, ethical considerations must not be overlooked. A primary concern is the potential for biased estimations due to skewed data distributions or flawed assumptions about the behavior policy. For example, in healthcare decision-making, incorrect OPE estimates could lead to suboptimal treatment recommendations, impacting patient outcomes.

Transparency in the use of OPE methods is also essential. Practitioners should clearly document the assumptions underlying their evaluations and communicate uncertainties associated with estimated policy values. Furthermore, ensuring fairness in policy evaluations requires addressing biases in both the data collection process and the algorithms themselves.

![](placeholder_for_ethics_diagram)

The figure above (to be included) illustrates how ethical concerns intersect with technical aspects of OPE, emphasizing the need for interdisciplinary collaboration to address these challenges.

# 6 Conclusion

In this survey, we have explored the field of off-policy evaluation (OPE) in reinforcement learning (RL), discussing its importance, methods, and applications. This concluding section summarizes the key findings and outlines potential future directions.

## 6.1 Summary of Key Findings

Off-policy evaluation is a critical component of RL that enables the assessment of policies using data generated by a different behavior policy. The survey has highlighted several important insights:

1. **Motivation and Challenges**: OPE addresses the need to evaluate new policies without requiring interaction with the environment, which is particularly valuable in high-stakes domains such as healthcare and robotics. However, challenges like high variance in importance sampling and model bias in direct methods remain significant hurdles.

2. **Methodological Approaches**: Various methods for OPE were discussed, including:
   - **Direct Method**: Estimates the value function directly through regression or modeling. While computationally efficient, it can suffer from bias due to model misspecification.
   - **Importance Sampling Methods**: These techniques reweight logged data based on the ratio of target and behavior policies ($\rho_t = \frac{\pi(a_t|s_t)}{\mu(a_t|s_t)}$). Ordinary Importance Sampling (OIS), Per-Decision Importance Sampling (PDIS), and Stationary Importance Sampling (SIS) each offer trade-offs between variance and bias.
   - **Doubly Robust Estimators**: Combining the strengths of direct methods and importance sampling, these estimators provide reduced variance while maintaining unbiasedness under certain conditions.
   - **Fitted Q-Evaluation (FQE)**: A model-based approach that iteratively refines Q-value estimates, offering a balance between bias and variance.

3. **Applications**: OPE finds practical use in diverse domains, including healthcare decision-making, recommender systems, and robotics control. Simulation studies further validate the effectiveness of these methods in synthetic environments and benchmark comparisons.

4. **Strengths and Weaknesses**: Each method has unique advantages and limitations. For instance, while importance sampling methods are unbiased, they often exhibit high variance, especially in long-horizon problems. Conversely, direct methods and FQE may introduce bias but typically have lower variance.

## 6.2 Future Directions

Despite significant progress, several open research questions and opportunities for advancement remain:

1. **Improving Variance Reduction**: Developing novel techniques to reduce the variance of importance sampling methods, particularly in high-dimensional and long-horizon settings, is an active area of research. Potential solutions include clipping importance weights or leveraging learned proposals.

2. **Integration with Online Learning**: Bridging the gap between off-policy evaluation and online learning could enhance adaptability in dynamic environments. Techniques such as adaptive importance sampling or hybrid methods combining online and offline data could be promising.

3. **Scalability**: As RL applications grow in complexity, scaling OPE methods to handle large datasets and high-dimensional state-action spaces is crucial. Advances in computational efficiency and parallelization will play a pivotal role.

4. **Ethical Considerations**: Ensuring fairness, transparency, and accountability in OPE algorithms is essential, especially in sensitive domains like healthcare and finance. Addressing biases in logged data and ensuring robustness against adversarial manipulations are critical areas for exploration.

5. **Unified Frameworks**: Developing unified frameworks that combine the strengths of multiple OPE methods could lead to more robust and versatile evaluation tools. Such frameworks might leverage ensemble approaches or hierarchical models to adaptively select the most appropriate estimator for a given problem.

In conclusion, off-policy evaluation continues to evolve as a cornerstone of RL research, with vast potential for impacting both theoretical advancements and real-world applications. By addressing current limitations and exploring new avenues, researchers can further enhance the reliability and applicability of OPE methods.

