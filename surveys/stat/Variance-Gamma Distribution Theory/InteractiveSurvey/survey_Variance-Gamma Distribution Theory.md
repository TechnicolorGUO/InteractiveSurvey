# A Survey of Variance-Gamma Distribution Theory

# 1 Abstract


The Variance-Gamma (VG) distribution is a powerful tool in probability and statistics, particularly adept at modeling heavy-tailed and skewed data, such as financial returns. This survey paper provides a comprehensive overview of the VG distribution, focusing on its mathematical derivations, analytical formulations, and practical applications. The paper begins by exploring estimation methods, including the modified method of moments and maximum likelihood estimation, which are crucial for accurately estimating VG parameters. It then delves into the empirical performance of the VG distribution, evaluating its accuracy in parameter estimation, goodness-of-fit, and predictive tasks like option pricing. The survey further examines the analytical properties of the VG distribution, such as the derivation of its probability density function using Mellin transforms and the representation of moments using hypergeometric functions. The paper also discusses the VG distribution's heavy-tailed nature and its applications in financial modeling, earthquake prediction, and non-local equations. The contributions of this survey include a detailed overview of VG theory, a critical evaluation of estimation methods, and a synthesis of the latest research findings. Finally, the survey identifies potential future directions for research, emphasizing the need for further exploration of the VG distribution's applications in emerging areas such as cryptocurrency markets and advanced financial modeling techniques.

# 2 Introduction
The Variance-Gamma (VG) distribution has emerged as a powerful tool in the field of probability and statistics, particularly in modeling phenomena characterized by heavy tails and skewness [1]. Originating from the broader class of infinitely divisible distributions, the VG distribution is defined by four parameters: the shape parameter \( \nu \), the scale parameter \( \alpha \), the skewness parameter \( \beta \), and the location parameter \( \mu \) [2]. This distribution has gained significant attention due to its ability to capture the complex dynamics observed in financial data, such as asset returns and option prices. The VG distribution's flexibility and tractability have made it a preferred choice in various applications, from financial modeling to risk management and beyond.

This survey paper aims to provide a comprehensive overview of the Variance-Gamma (VG) distribution theory, focusing on its mathematical derivations, analytical formulations, and practical applications [2]. The paper begins by exploring the estimation methods for VG distributions, including the modified method of moments estimation (MME) and maximum likelihood estimation (MLE) [3]. These methods are crucial for accurately estimating the parameters of the VG distribution, especially in the presence of heavy-tailed and skewed data. The paper then delves into the empirical performance analysis of the VG distribution, evaluating its accuracy in parameter estimation, goodness-of-fit to empirical data, and performance in predictive tasks such as option pricing [4]. This section highlights the VG distribution's superiority over traditional models in capturing the leptokurtic and skewed nature of financial returns.

The survey further examines the analytical properties of the VG distribution, including the derivation of its probability density function (PDF) using Mellin transforms and the representation of moments using hypergeometric functions [5]. The Mellin transform technique provides a powerful framework for deriving the PDF of the product of independent VG random variables, while hypergeometric functions offer a compact and analytically tractable form for the moments [6]. The paper also discusses the tail behavior and asymptotic analysis of the VG distribution, which are essential for understanding its heavy-tailed nature and its applicability in risk management and option pricing [7].

In addition to its theoretical aspects, the survey paper explores the practical applications of the VG distribution in various domains [4]. In financial modeling, the VG distribution is widely used for option pricing and volatility modeling, where its ability to capture heavy tails and skewness makes it a more realistic alternative to traditional models [8]. The paper also examines the VG distribution's role in earthquake modeling and prediction, where it can better capture the leptokurtic and asymmetric nature of seismic data. Furthermore, the paper discusses the application of the VG distribution in non-local equations and subordination theory, which provide a robust framework for extending classical models to accommodate more complex dynamics.

The contributions of this survey paper are multifaceted. First, it provides a detailed and structured overview of the VG distribution theory, encompassing its mathematical derivations, analytical properties, and practical applications [4]. Second, it offers a critical evaluation of the estimation methods and empirical performance of the VG distribution, highlighting its advantages and limitations. Third, the paper synthesizes the latest research findings and theoretical developments, making it a valuable resource for researchers and practitioners in the fields of probability, statistics, and financial engineering. Finally, the survey identifies potential future directions for research, emphasizing the need for further exploration of the VG distribution's applications in emerging areas such as cryptocurrency markets and advanced financial modeling techniques.

# 3 Mathematical Derivations of VG Distributions

## 3.1 Estimation Methods for VG Distributions

### 3.1.1 Modified Method of Moments Estimation
The method of moments estimation (MME) is a classical approach for parameter estimation in statistical models, including the Variance-Gamma (VG) distribution [3]. In the context of the VG distribution, the MME involves equating the theoretical moments of the distribution to the empirical moments of the data and solving the resulting system of equations for the parameters [3]. For the VG distribution, which is characterized by four parameters \( \nu \), \( \alpha \), \( \beta \), and \( \mu \), the first four moments are typically used to derive the parameter estimates [3]. The first moment (mean) and the second moment (variance) provide straightforward equations, while the third and fourth moments (skewness and kurtosis) are more complex due to the presence of the modified Bessel function of the second kind in the density function.

To enhance the efficiency and accuracy of the MME, modifications have been proposed. One such modification involves estimating the location parameter \( \mu \) using the empirical mean of the data. This simplifies the estimation process by reducing the number of parameters to be estimated simultaneously. Once \( \mu \) is estimated, the remaining parameters \( \nu \), \( \alpha \), and \( \beta \) can be estimated using the first absolute moment \( A = E[9]|] \) and the standard deviation \( \sigma = \sqrt{\text{Var}(X)} \). The ratio \( A / \sigma \) is particularly useful because it depends only on the shape parameter \( \nu \), and not on the scale parameter \( \alpha \) or the location parameter \( \mu \). This property allows for a more robust estimation of \( \nu \).

Simulation studies have shown that this modified MME approach improves the efficiency and accuracy of parameter estimation, especially in small sample sizes. By leveraging the empirical mean and the ratio of the first absolute moment to the standard deviation, the modified MME reduces the complexity of the estimation process and provides more reliable parameter estimates. This method is particularly advantageous in practical applications where the VG distribution is used to model financial returns, which often exhibit heavy tails and skewness [8].

### 3.1.2 Maximum Likelihood Estimation
Maximum Likelihood Estimation (MLE) is a widely used method for estimating the parameters of the Variance-Gamma (VG) distribution, despite the computational challenges posed by the presence of the modified Bessel function of the second kind in the density function. The primary difficulty in applying MLE to the VG distribution lies in the lack of a closed-form expression for the likelihood function and its derivatives, which complicates the optimization process. To circumvent these issues, various strategies have been developed, including the use of numerical methods and approximations.

One common approach is to reduce the number of parameters by imposing constraints or simplifying assumptions, such as setting the location parameter \( \mu \) to zero or assuming symmetry (\( \beta = 0 \)). This simplification can make the likelihood function more tractable and reduce the computational burden. Another strategy is to use approximations of the modified Bessel function, which can be computationally more efficient while maintaining a reasonable level of accuracy [10]. For instance, asymptotic expansions or series approximations of the Bessel function can be employed to facilitate the evaluation of the likelihood function and its derivatives.

Despite these challenges, the Expectation-Maximization (EM) algorithm has emerged as a powerful tool for parameter estimation in the VG distribution. The EM algorithm iteratively refines the parameter estimates by alternating between an expectation step, where the expected value of the complete-data log-likelihood is computed, and a maximization step, where the parameters are updated to maximize this expected value [3]. This iterative process can converge to the maximum likelihood estimates, even in the presence of complex likelihood functions. The EM algorithm's ability to handle missing or latent variables also makes it particularly suitable for the VG distribution, where the latent variables can represent the underlying Gamma processes that define the VG distribution.

### 3.1.3 Empirical Performance Analysis
In the empirical performance analysis of the Variance Gamma (VG) distribution, several key aspects are evaluated to ensure its applicability and robustness in various contexts, particularly in financial modeling [7]. The primary focus is on the accuracy of parameter estimation methods, the goodness-of-fit to empirical data, and the performance in predictive tasks such as option pricing. The VG distribution, characterized by its ability to model heavy tails and skewness, is often compared against traditional models like the lognormal distribution to highlight its superior fit to financial return data [8]. This section delves into the empirical validation of these claims through extensive simulation studies and real-world data applications.

One of the critical components of empirical performance analysis is the evaluation of parameter estimation techniques. Methods such as maximum likelihood estimation (MLE), method of moments (MME), and Bayesian estimation are commonly employed [3]. Each method has its strengths and weaknesses, and their performance is typically assessed through metrics such as bias, mean squared error (MSE), and computational efficiency. For instance, MLE is known for its asymptotic efficiency but can be computationally intensive, especially in high-dimensional settings. In contrast, MME is simpler and faster but may suffer from higher bias and variance. Recent studies have shown that modifications to MME, such as incorporating absolute moments, can significantly improve its performance, particularly in financial applications where the data often exhibit heavy tails and skewness.

Finally, the empirical performance of the VG distribution is also gauged through its application in predictive tasks, such as option pricing and risk management. The VG model's ability to capture the leptokurtic and skewed nature of financial returns often leads to more accurate pricing of options, especially in the presence of market anomalies like volatility smiles and W-shaped implied volatility curves. Empirical studies have consistently shown that the VG model outperforms traditional models in these scenarios, providing a more realistic representation of market dynamics. This section concludes by discussing the implications of these findings for both theoretical and practical applications, emphasizing the importance of the VG distribution in modern financial modeling [11].

## 3.2 Analytical Formulations and Properties

### 3.2.1 Mellin Transforms and PDF Derivations
The Mellin transform technique provides a powerful framework for deriving the probability density function (PDF) of the product of independent random variables, particularly when these variables follow the variance-gamma (VG) distribution [6]. In the context of the VG distribution, the Mellin transform of the PDF can be expressed in terms of the modified Bessel function of the second kind, \( K_m(x) \) [12]. This approach leverages the convolution structure of the Mellin transform, allowing for the derivation of exact formulas for the PDF of the product of independent VG random variables [6]. The Mellin transform of the VG distribution is particularly useful because it simplifies the convolution integral, which is otherwise challenging to solve directly due to the complexity of the modified Bessel function.

For the product \( Z = \prod_{i=1}^N X_i \) of independent VG random variables \( X_i \sim VG(m_i, \alpha_i, \beta_i, 0) \), the Mellin transform of the PDF can be derived and expressed as an infinite series involving the Meijer G-function [7]. This series representation is a generalization of previous results for the product of two independent VG random variables and the symmetric case where \( \beta_1 = \cdots = \beta_N = 0 \). The Meijer G-function, a generalization of the hypergeometric function, is particularly well-suited for representing the PDF in these cases. When the shape parameters \( m_1, \ldots, m_N \) are half-integers, the PDF of \( Z \) can be represented as a finite sum of Meijer G-functions, which simplifies the computation and analysis of the distribution [7].

In the symmetric case where all \( \beta_i = 0 \), the PDF of the product \( Z \) reduces to a single Meijer G-function, providing a more tractable form for practical applications [7]. This simplification is particularly useful in financial modeling, where the VG distribution is often used to model asset returns. The derived formulas for the PDF can be further utilized to obtain closed-form expressions for the cumulative distribution function (CDF) and characteristic function of the product \( Z \). These results not only enhance the theoretical understanding of the VG distribution but also provide practical tools for statistical inference and risk management in financial markets [4].

### 3.2.2 Hypergeometric Functions for Moments
Hypergeometric functions play a crucial role in the derivation of moments for the Variance-Gamma (VG) distribution [5]. Specifically, the moments of a VG random variable \( X \sim \text{VG}(\nu, \alpha, \beta, 0) \) can be expressed in terms of hypergeometric functions, which provide a compact and analytically tractable form [5]. For a positive integer \( r \), the \( r \)-th moment \( m_r(X) \) can be represented using the hypergeometric function \( {}_2F_1 \) [2]. This representation simplifies the computation of moments, especially for higher orders, and facilitates the use of modern computational tools for numerical evaluation.

The derivation of these moments involves expressing the moments as a finite sum involving gamma functions, which is a significant improvement over previous methods that often required complex integrals or series expansions. The hypergeometric function \( {}_2F_1 \) arises naturally in this context due to its ability to encapsulate the intricate relationships between the parameters of the VG distribution and the moments [5]. This function is particularly useful because it can handle a wide range of parameter values, making it a versatile tool for theoretical and applied research in fields such as finance and statistics.

Furthermore, the use of hypergeometric functions in the context of VG moments allows for the development of efficient algorithms for parameter estimation and model fitting. For instance, the method of moments estimation (MME) can be significantly enhanced by leveraging these hypergeometric representations. By solving the system of equations derived from the moments, one can obtain explicit expressions for the parameters in terms of the moments [3]. This approach not only simplifies the estimation process but also improves the accuracy and robustness of the parameter estimates, especially in scenarios where the data exhibit heavy tails and skewness. The hypergeometric function-based formulas thus serve as a bridge between theoretical derivations and practical applications, making the VG distribution more accessible and applicable in various domains.

### 3.2.3 Tail Behavior and Asymptotic Analysis
The tail behavior of the Variance Gamma (VG) distribution is a critical aspect of its utility in financial modeling, particularly due to its ability to capture heavy-tailed phenomena observed in asset returns [7]. For parameters \( m \) and \( n \) within the range \( -1/2 < m, n \leq 0 \), the tails of the distribution become heavier, leading to undefined moments such as the mean, as indicated by Proposition 2.8 and Corollary 2.9 [11]. This heavy-tailed nature is essential for accurately modeling financial data, which often exhibits significant deviations from normality, including leptokurtosis and skewness. The asymptotic approximations for tail probabilities derived in Corollary 2.9 provide valuable insights into the behavior of extreme values, which are crucial for risk management and option pricing.

The asymptotic analysis of the VG distribution extends beyond tail probabilities to encompass the cumulative distribution function (CDF) and the quantile function [4]. For the full range of parameter values \( m, n > -1/2 \), exact formulas for the CDF are derived, expressed as infinite series involving the modified Bessel function of the second kind and the modified Lommel function of the first kind [12]. These formulas are significant because they allow for precise calculations of probabilities and quantiles, which are essential for practical applications such as value-at-risk (VaR) and expected shortfall (ES) computations. The convergence properties of these series are also discussed, ensuring their practical usability in numerical implementations.

Furthermore, the asymptotic analysis of the VG distribution includes the derivation of formulas for absolute moments, which are particularly useful for understanding the distribution's behavior under various transformations and for comparing it with other heavy-tailed distributions [2]. For the symmetric VG distribution (where \( \beta = 0 \)), simpler closed-form expressions for absolute moments are provided, facilitating easier computation and interpretation [2]. These results are not only theoretically important but also have practical implications, as they enable more accurate modeling of financial returns and the development of robust statistical methods for parameter estimation and hypothesis testing. The inclusion of asymptotic approximations for absolute moments further enhances the applicability of the VG distribution in empirical studies, providing a comprehensive toolkit for financial econometricians and risk analysts [5].

## 3.3 Applications and Extensions

### 3.3.1 Financial Modeling and Option Pricing
The Variance Gamma (VG) distribution has emerged as a powerful tool in financial modeling, particularly in the context of option pricing, due to its ability to capture key empirical features of financial asset returns, such as heavy tails and skewness [13]. Unlike the classical lognormal model, which assumes normally distributed returns, the VG distribution allows for more flexible modeling of asset returns by incorporating a time change through a gamma process [8]. This time change introduces stochastic volatility and skewness, making the VG model more consistent with empirical observations. The VG model is defined by five parameters: location (μ), symmetry (δ), volatility (σ), and the gamma parameters of shape (α) and scale (θ) [10]. The characteristic function of the VG distribution has a closed form, which facilitates the computation of option prices and risk measures [10].

In the context of option pricing, the VG model has been extensively applied to price European options, where the absence of an explicit closed-form solution for the probability density function is overcome by using the characteristic function. The VG model's ability to produce a variety of implied volatility smiles and skews makes it particularly suitable for capturing market dynamics. Empirical studies have shown that the VG model outperforms the Black-Scholes model in terms of pricing accuracy, especially for options with extreme strikes and long maturities. The VG model can also accommodate W-shaped implied volatility curves, which are observed in liquid option markets and are indicative of divergent investor expectations [14]. This feature is crucial for accurately pricing options in volatile market conditions.

Moreover, the VG model's flexibility extends beyond vanilla options to more complex derivatives, such as barrier options and American options. The model's ability to handle jumps and stochastic volatility provides a more realistic framework for pricing these instruments. Recent advancements in computational methods, such as the fast Fourier transform (FFT) and the fractional FFT, have made the VG model computationally feasible for practical applications [10]. These methods enable efficient computation of option prices and Greeks, making the VG model a viable alternative to traditional models in both academic research and industry practice [13]. The VG model's robustness and flexibility make it a valuable tool for financial engineers and risk managers, particularly in the context of derivative pricing and portfolio management.

### 3.3.2 Earthquake Modeling and Prediction
Earthquake modeling and prediction represent a critical area within geophysical sciences, aiming to understand the mechanisms underlying seismic activities and forecast potential earthquakes to mitigate risks. The Variance-Gamma (VG) distribution, characterized by its ability to model heavy-tailed and skewed data, has emerged as a promising tool in this domain. Unlike traditional models that often assume normal distributions, the VG distribution can better capture the leptokurtic and asymmetric nature of seismic data, making it particularly suitable for modeling the magnitude and frequency of earthquakes. This section reviews the application of the VG distribution in earthquake modeling, focusing on its theoretical foundations and practical implications.

The VG distribution is defined by four parameters: a shape parameter, a scale parameter, a skewness parameter, and a location parameter [10]. These parameters allow the VG distribution to flexibly model various aspects of seismic data, including the clustering of events and the presence of extreme values. Recent studies have applied the VG distribution to model the temporal and spatial patterns of earthquakes, demonstrating its effectiveness in capturing the complex dynamics of seismic activities. For instance, the VG model has been used to analyze the inter-event times and magnitudes of earthquakes, providing insights into the underlying physical processes that govern seismic occurrences.

Moreover, the integration of the VG distribution with advanced statistical techniques, such as the Expectation-Maximization (EM) algorithm, has facilitated the estimation of model parameters from empirical data. This approach not only enhances the accuracy of earthquake predictions but also provides a robust framework for incorporating geological factors into the modeling process [15]. By considering the effects of geological variables, such as fault structures and tectonic stresses, the VG model can offer more nuanced and reliable forecasts of seismic activities. Future research in this area may explore the extension of the VG model to incorporate multi-dimensional data and develop real-time prediction systems that can inform disaster management and public safety strategies.

### 3.3.3 Non-Local Equations and Subordination Theory
Non-Local Equations and Subordination Theory provide a robust framework for extending classical models in finance and other fields to accommodate more complex dynamics. The Variance Gamma (VG) model, a prominent example of a subordinated process, is derived by substituting the physical time in a Classical Lognormal Model (CLM) with an independent Gamma process, known as the subordinator [10]. This transformation introduces additional parameters, such as the shape (α) and scale (θ) of the Gamma process, which allow for greater flexibility in modeling asset returns. Unlike the CLM, the VG model does not have an explicit closed-form probability density function (PDF), but its characteristic function is known and can be used for parameter estimation and option pricing [16].

The subordination technique is particularly useful in capturing the heavy-tailed and skewed nature of financial returns, which are often observed in empirical data [17]. The VG model's ability to model these features is due to its infinite divisibility property, which ensures that the process can be decomposed into smaller, independent increments. This property is crucial for the derivation of non-local equations, which describe the evolution of the process in terms of its characteristic function rather than its PDF. These equations are essential for understanding the behavior of the VG process, especially in the presence of jumps and other non-Gaussian phenomena.

In the context of non-local equations, the VG process can be described using a combination of space and time operators, which differ from the classical differential operators used in the CLM. Recent developments in this area have introduced new equations that involve time-operators, providing a more comprehensive framework for analyzing the VG process. These equations are particularly useful for studying the behavior of the VG process at different time scales and for deriving analytical solutions for option pricing and other financial applications. The subordination theory, therefore, not only enriches the mathematical structure of the model but also enhances its practical applicability in financial modeling and risk management.

# 4 Theoretical Analysis of Subordinators and Lévy Processes

## 4.1 Subordinator and Lévy Process Properties

### 4.1.1 Generalized Fractional Negative Binomial Processes
In the realm of stochastic processes, the Generalized Fractional Negative Binomial Process (GFNBP) represents a significant advancement by extending the classical negative binomial process (NB) and the fractional Poisson process (FPP) [18]. The GFNBP is constructed by time-changing the FPP with an independent Mittag-Leffler (ML) Lévy subordinator, which introduces a layer of complexity and flexibility not present in its predecessors. This construction allows the GFNBP to exhibit overdispersion and long-range dependence (LRD), making it particularly suitable for modeling phenomena where events are not only rare but also exhibit clustering and memory effects.

The GFNBP can be mathematically described as a process that combines the counting nature of the NB process with the time-fractional dynamics of the FPP. Specifically, the GFNBP is defined as \( Q_\beta(t, \lambda) \), where \( Q_\beta \) is the fractional NB process obtained by subordinating the Poisson process with a gamma process [18]. The introduction of the ML subordinator further enriches the process by allowing for a more nuanced control over the temporal dynamics. The resulting process is not infinitely divisible, which is a departure from many classical stochastic processes and adds to its utility in modeling real-world phenomena that do not conform to the assumptions of infinite divisibility.

The properties of the GFNBP, such as its probability density function (pdf) and Lévy density, have been studied to understand its behavior under various conditions. The GFNBP's overdispersion and LRD properties make it a valuable tool in fields such as finance, where it can be used to model the occurrence of rare but significant events, and in statistical physics, where it can capture the complex dynamics of systems with long-range interactions. The GFNBP's flexibility in modeling both the frequency and the timing of events makes it a promising candidate for a wide range of applications, from risk assessment in insurance to the analysis of biological systems.

### 4.1.2 Tempered Stable and α-Stable Subordinators
Tempered stable and α-stable subordinators are fundamental in the study of stochastic processes, particularly in the context of subordination and time-changed processes [19]. An α-stable subordinator is a non-decreasing Lévy process with a characteristic exponent given by \( \psi(u) = u^\alpha \) for \( 0 < \alpha < 1 \). These subordinators exhibit heavy-tailed behavior, making them suitable for modeling phenomena with extreme events. The tempered stable subordinator, on the other hand, is a generalization that introduces an exponential tempering to the α-stable subordinator, leading to a characteristic exponent of the form \( \psi(u) = (u + \lambda)^\alpha - \lambda^\alpha \), where \( \lambda > 0 \) is the tempering parameter. This tempering results in a lighter tail, which is crucial for applications where the heavy-tailed nature of α-stable subordinators is too extreme.

The properties of tempered stable subordinators, such as their probability density function (pdf) and Lévy density, have been extensively studied [19]. The pdf of a tempered stable subordinator can be expressed in terms of the Mittag-Leffler function, which captures the transition from heavy-tailed to light-tailed behavior as the tempering parameter increases. The Lévy density of a tempered stable subordinator is given by \( \nu(dx) = \frac{\alpha \lambda^\alpha}{\Gamma(1 - \alpha)} x^{-1 - \alpha} e^{-\lambda x} \, dx \), highlighting the exponential decay that characterizes the tempering effect. This Lévy density is crucial for understanding the small and large jumps of the subordinator, which in turn influence the behavior of the subordinated process [20].

In the context of subordination, both α-stable and tempered stable subordinators are used to time-change other stochastic processes, such as Brownian motion or Poisson processes [19]. For instance, the subordination of Brownian motion by an α-stable subordinator results in a process with a characteristic exponent \( \psi(u) = u^\alpha \), which is a fractional Brownian motion. Similarly, subordinating Brownian motion by a tempered stable subordinator leads to a process with a characteristic exponent that reflects the tempered heavy-tailed behavior [21]. These subordinated processes have found applications in various fields, including finance, where they are used to model asset prices and other financial variables that exhibit both heavy-tailed and tempered behaviors.

### 4.1.3 Dickman Subordinator and Renewal Theorems
The Dickman subordinator plays a pivotal role in the study of renewal processes and aging phenomena, particularly in the context of Lévy processes. This subordinator is characterized by its unique Lévy measure, which leads to a specific form of the Laplace exponent. The Dickman subordinator is often associated with the Dickman function, a special function that arises in the analysis of smooth numbers in number theory and has found applications in various fields, including probability theory and stochastic processes. The Dickman function, denoted by \(\rho(x)\), is defined as the solution to a delay differential equation and is intimately connected to the asymptotic distribution of the largest prime factor of integers.

In the context of renewal theorems, the Dickman subordinator provides a framework for understanding the long-term behavior of renewal processes with heavy-tailed inter-arrival times. The renewal function, which describes the expected number of renewals by a given time, exhibits a non-trivial asymptotic behavior when the inter-arrival distribution is regularly varying with a specific index. The Dickman subordinator allows for a precise characterization of this behavior, particularly in the case where the inter-arrival times follow a Pareto distribution. The renewal theorem for the Dickman subordinator states that the expected number of renewals grows logarithmically with time, reflecting the slow convergence to equilibrium typical of heavy-tailed processes.

Furthermore, the Dickman subordinator has been used to study the asymptotic properties of various stochastic processes, including the behavior of Lévy trees and the exploration process of these trees [22]. The exploration process, which can be seen as a continuous-time Markov chain, is closely related to the height process of a Lévy tree [22]. The Dickman subordinator helps in understanding the distribution of the height of the tree and the time it takes to explore the entire tree. This connection is particularly useful in the analysis of branching processes and the study of random trees, where the Dickman subordinator provides a powerful tool for deriving precise asymptotic results [22].

## 4.2 Scaling Limits and Diffusion Models

### 4.2.1 Scaling Limits of Random Walks
The scaling limits of random walks play a pivotal role in the development of stochastic processes, particularly in the context of Lévy processes and Brownian motion [23]. A simple random walk, characterized by discrete, equally spaced time points and random steps, converges to Brownian motion under appropriate scaling [23]. This convergence, first described by Einstein in 1905, provides a foundational link between microscopic particle movements and macroscopic diffusion phenomena. The central limit theorem ensures that the sum of independent, identically distributed (i.i.d.) random variables, when properly normalized, converges to a normal distribution, which is the basis for Brownian motion. However, the scope of scaling limits extends beyond this classical setting to encompass more complex processes.

Lévy processes, which are infinitely divisible with independent stationary increments, represent a broader class of scaling limits for random walks [10]. Unlike Brownian motion, Lévy processes can accommodate jumps of arbitrary size, making them suitable for modeling phenomena with heavy-tailed distributions. When the steps of a random walk follow a power-law distribution, the resulting Lévy process can exhibit super-diffusive behavior, where particles spread faster than in normal diffusion [23]. This is particularly relevant in physical systems where long-range interactions or anomalous transport mechanisms are present. The scaling limit of such random walks is governed by a space-fractional diffusion equation, reflecting the non-local nature of the underlying process.

Continuous-time random walks (CTRWs) further generalize the concept of scaling limits by allowing both the step sizes and waiting times between steps to be random. This framework can model both super-diffusive and sub-diffusive behaviors, depending on the distributions of step sizes and waiting times. When the waiting times follow a power-law distribution, the resulting process is non-Markovian and is described by a time-fractional diffusion equation. CTRWs have found extensive applications in fields such as finance, statistical physics, and biology, where they capture the memory effects and long-range correlations often observed in empirical data. The flexibility of CTRWs in modeling a wide range of diffusion processes underscores the importance of understanding their scaling limits in the context of stochastic modeling.

### 4.2.2 Fractional Diffusion Equations
Fractional diffusion equations (FDEs) extend the classical diffusion equation by incorporating fractional derivatives in time or space, or both, to model anomalous diffusion processes. These equations are particularly useful in describing systems where the mean squared displacement of particles scales non-linearly with time, a phenomenon observed in various physical, biological, and financial systems. The fractional derivative in time, often represented by the Caputo or Riemann-Liouville derivative, accounts for memory effects, while the fractional derivative in space captures long-range interactions or jumps. The general form of a time-fractional diffusion equation is given by \( \partial^\alpha_t u(x,t) = K \nabla^\beta u(x,t) \), where \( \alpha \) (0 < \( \alpha \) < 1) and \( \beta \) (1 < \( \beta \) < 2) are the orders of the fractional derivatives in time and space, respectively, and \( K \) is the diffusion coefficient. This equation can be derived from a continuous-time random walk (CTRW) model with power-law waiting times and jump lengths, leading to a non-Markovian process [23].

The solutions to fractional diffusion equations exhibit rich behavior, including heavy-tailed distributions and non-Gaussian scaling, which are not captured by classical diffusion models. The fundamental solution to the time-fractional diffusion equation is often expressed in terms of the Mittag-Leffler function, which generalizes the exponential function and captures the memory effects inherent in the system. For space-fractional diffusion equations, the fundamental solution is typically described by a stable distribution, reflecting the long-range interactions [23]. These solutions have been extensively studied in the context of anomalous transport in porous media, turbulent flows, and financial markets, where traditional diffusion models fail to accurately describe the observed phenomena.

Recent advances in the theory of fractional diffusion equations have focused on the development of numerical methods and analytical techniques to solve these equations in more complex geometries and under various boundary conditions. The use of fractional calculus in modeling has also led to the introduction of tempered fractional derivatives, which incorporate exponential tempering to account for finite-size effects and truncation in physical systems. These tempered models have found applications in modeling processes with both long-range and short-range interactions, such as in the study of tracer particles in heterogeneous media and in the analysis of financial time series. The versatility of fractional diffusion equations in capturing a wide range of physical and biological phenomena continues to drive research in this area, with ongoing efforts to develop more robust and efficient methods for their solution and application.

### 4.2.3 Subdiffusive Processes and Fokker-Planck Equations
Subdiffusive processes, characterized by a slower-than-linear growth of the mean squared displacement over time, are often modeled using fractional Fokker-Planck equations (FFPEs). These equations generalize the classical Fokker-Planck equation by incorporating fractional derivatives in time, which capture the memory effects inherent in subdiffusive dynamics. The FFPEs are derived from the continuous time random walk (CTRW) framework, where the waiting times between particle jumps follow a heavy-tailed distribution, leading to anomalous diffusion. The fractional derivative in the FFPEs typically takes the form of a Caputo or Riemann-Liouville derivative, reflecting the non-Markovian nature of the process.

The mathematical structure of the FFPEs allows for a detailed analysis of the statistical properties of subdiffusive processes. For instance, the solution to the FFPE provides the probability density function (PDF) of the particle's position, which exhibits a stretched exponential decay in the long-time limit, contrasting with the Gaussian decay observed in normal diffusion. This PDF can be used to derive the mean squared displacement and other moments, offering insights into the transport properties of the system. Additionally, the FFPEs can incorporate spatial heterogeneity and external forces, making them applicable to a wide range of physical and biological systems, such as the movement of molecules in crowded cellular environments or the spread of contaminants in porous media.

Furthermore, the FFPEs can be extended to include more complex scenarios, such as the presence of multiple subdiffusive regimes or the interaction with other stochastic processes. For example, the combination of subdiffusion with reaction kinetics leads to fractional reaction-diffusion equations, which are crucial for understanding chemical reactions in subdiffusive media. The study of these equations not only enhances our theoretical understanding of subdiffusive processes but also provides a powerful tool for modeling and predicting the behavior of systems where traditional diffusion models fail to capture the observed dynamics.

## 4.3 Probabilistic and Analytical Techniques

### 4.3.1 Integro-Differential Operators and Generators
Integro-differential operators play a crucial role in the study of Feller processes, particularly in characterizing the infinitesimal generators of these processes [24]. The generator \( \mathcal{A} \) of a Feller process \( (X_t)_{t \geq 0} \) is an integro-differential operator that acts on a suitable domain \( \mathcal{D}(\mathcal{A}) \) within the space of continuous functions vanishing at infinity, \( C_0(\mathbb{R}^d) \) [24]. For a jump-Feller process, the generator can be expressed as:
\[
\mathcal{A}f(x) = b(x) \cdot \nabla f(x) + \int_{\mathbb{R}^d \setminus \{0\}} \left( f(x+y) - f(x) - y \cdot \nabla f(x) \mathbf{1}_{|y| < 1} \right) \nu(x, dy),
\]
where \( b(x) \) is the drift coefficient, and \( \nu(x, dy) \) is the Lévy measure that describes the intensity and distribution of jumps.

The integro-differential structure of \( \mathcal{A} \) reflects the combined effects of diffusion and jumps in the dynamics of \( X_t \). The drift term \( b(x) \cdot \nabla f(x) \) captures the local behavior of the process, while the integral term accounts for the non-local jumps. This decomposition allows for a detailed analysis of the process's behavior, especially in the context of small time asymptotics and long-term stability. The Lévy measure \( \nu(x, dy) \) is crucial in determining the nature of the jumps, including their frequency and magnitude, and thus influences the overall dynamics of the process.

Furthermore, the probabilistic symbol \( p(x, \xi) \) of the process, defined as the Fourier multiplier of the generator \( \mathcal{A} \), provides a powerful tool for analyzing the spectral properties of the process. The symbol \( p(x, \xi) \) encodes the infinitesimal characteristics of the process and can be used to derive various properties, such as the transition probabilities and the potential theory associated with the process. By studying the symbol, one can gain insights into the regularity of the process, the existence of moments, and the behavior of the process near boundaries or singular points. This approach has been instrumental in extending the classical results of Liggett, Szekli, and Rischendorf to a broader class of Feller processes, thereby enriching the theoretical framework for the analysis of stochastic processes with jumps [24].

### 4.3.2 Extreme First Hitting Times
Extreme first hitting times (FHTs) are a critical aspect of stochastic processes, particularly in scenarios involving multiple searchers or particles. These times represent the earliest instance when any of the searchers reaches a target, which is crucial in understanding the dynamics of systems where the first arrival triggers an event [25]. In the context of Brownian motion and Lévy flights, the study of extreme FHTs has revealed significant differences in the behavior of these processes [20]. For instance, while Brownian motions tend to follow the shortest path to the target, Lévy flights exhibit a more complex behavior, characterized by long jumps that can significantly reduce the search time [20].

The mathematical formulation of extreme FHTs involves the consideration of a collection of independent and identically distributed (i.i.d.) first passage times, \(\{\tau_1, \tau_2, \ldots, \tau_N\}\), where \(N\) is the number of searchers. The extreme FHT, denoted as \(\tau_{\text{min}} = \min\{\tau_1, \tau_2, \ldots, \tau_N\}\), captures the first instance when any of the searchers hits the target [20]. The distribution of \(\tau_{\text{min}}\) is of particular interest, as it provides insights into the efficiency of the search process. Recent studies have shown that the distribution of \(\tau_{\text{min}}\) can be highly skewed, with a significant probability of very short times, especially when the number of searchers is large.

Understanding the asymptotic behavior of extreme FHTs is essential for applications in various fields, including biology, physics, and finance. For example, in gene regulation, the time it takes for a transcription factor to find a specific gene location can be modeled using extreme FHTs, where the fastest few factors out of a large population determine the regulatory response. Similarly, in financial markets, the time it takes for a stock price to reach a certain threshold can be modeled using extreme FHTs, providing insights into market dynamics and risk assessment. The study of extreme FHTs thus bridges theoretical developments in stochastic processes with practical applications, highlighting the importance of these times in understanding complex systems.

### 4.3.3 Potential Density and Lévy Density Analysis
In the analysis of subordinators, the potential density and Lévy density play crucial roles in understanding the long-term behavior and the jump structure of the process, respectively. The potential density \( u(x) \) of a subordinator provides insight into the expected occupation time of the process in a given interval, reflecting the process's tendency to linger in certain regions. For a subordinator \( Y = \{Y_t\}_{t \geq 0} \), the potential density \( u(x) \) is defined as the density of the potential measure \( U(dx) = \mathbb{E}\left[26] \). This measure encapsulates the average amount of time the subordinator spends in the infinitesimal interval \( [27] \).

The Lévy density \( \nu(dx) \) of a subordinator, on the other hand, characterizes the intensity of jumps of the process [24]. Specifically, \( \nu(dx) \) describes the expected number of jumps of size \( dx \) per unit time. For a subordinator, the Lévy measure \( \nu \) is supported on \( (0, \infty) \) and must satisfy the integrability condition \( \int_0^\infty (1 \wedge x) \nu(dx) < \infty \). This condition ensures that the subordinator does not have too many small jumps that would make the process explode in finite time. The relationship between the potential density and the Lévy density is intricate, as the potential density can often be expressed in terms of the Lévy measure through the Laplace exponent of the subordinator [21].

In the context of specific subordinators such as the tempered stable subordinator and the inverse Gaussian subordinator, the asymptotic forms of the potential density and Lévy density provide valuable insights into the scaling properties and the tail behavior of these processes [21]. For instance, the potential density of a tempered stable subordinator typically exhibits polynomial decay, reflecting the heavy-tailed nature of the process. Similarly, the Lévy density of the inverse Gaussian subordinator shows exponential decay, indicating a lighter tail compared to the tempered stable subordinator [21]. These asymptotic behaviors are crucial for understanding the long-term dynamics and the statistical properties of the processes, particularly in applications involving anomalous diffusion and financial modeling.

# 5 Financial Applications of Lévy Models

## 5.1 Option Pricing and Volatility Modeling

### 5.1.1 Stochastic Correlation and Random Jumps
Stochastic correlation and random jumps are pivotal components in modeling financial markets, particularly in capturing the dynamics of asset returns and their volatilities. Traditional models often assume constant correlation and continuous price paths, which fail to account for the empirical evidence of sudden market shifts and heavy-tailed distributions. To address these limitations, advanced models incorporate stochastic correlation, allowing the correlation between assets to vary over time, and introduce random jumps to reflect the occurrence of rare but significant market events. These enhancements are crucial for accurately pricing derivatives and managing risk in a volatile market environment.

The integration of stochastic correlation and random jumps into asset pricing models typically involves the use of Lévy processes, which extend Brownian motion by including jumps. A notable example is the Ornstein-Uhlenbeck (OU) process driven by a Lévy process, where the background driving Lévy process (BDLP) can be a compound Poisson process with a variance gamma distribution. This setup allows for the modeling of both mean-reverting behavior and heavy-tailed distributions, providing a more realistic representation of market dynamics. The OU process with a BDLP captures the persistence in volatility and the clustering of extreme events, which are key characteristics of financial time series.

To further enhance the model's flexibility and accuracy, the BDLP can be subordinated by an inverse Gaussian process, leading to a time-changed OU process. This subordination technique introduces an additional layer of stochasticity, effectively modeling the irregular timing of market events and the varying intensity of jumps. The resulting model can better capture the complex interplay between stochastic correlation, volatility, and jump occurrences. For practical applications, such as option pricing, the model parameters can be estimated using historical data, and the option prices can be computed through numerical methods like Monte Carlo simulations or polynomial expansions. These advanced models offer a robust framework for understanding and predicting market behavior, especially during periods of high uncertainty and extreme market movements.

### 5.1.2 Bitcoin Volatility Index and Subordinated Lévy Processes
The construction of a Bitcoin Volatility Index (BVIX) using subordinated Lévy processes represents a significant advancement in modeling the unique characteristics of cryptocurrency markets [17]. Unlike traditional financial assets, cryptocurrencies exhibit extreme volatility, heavy-tailed distributions, and significant skewness, necessitating more sophisticated models. A double subordinated Normal Inverse Gaussian (NIG) Lévy process is employed to capture these features effectively [2]. This process involves a primary NIG Lévy process, which is further subordinated by another stochastic process, typically a subordinator, to introduce additional variability and flexibility in the model.

The double subordination framework allows for the modeling of both the magnitude and timing of price jumps, which are crucial for accurately capturing the dynamics of bitcoin prices. By incorporating a subordinator, the model can account for the clustering of volatility and the irregular spacing of jumps, which are common in high-frequency financial data. This approach leads to a more realistic representation of the underlying price process, particularly in periods of market stress or rapid price movements. The resulting model is not only capable of explaining the heavy-tailed and skewed nature of bitcoin returns but also provides a robust foundation for the computation of implied volatilities and the pricing of derivatives.

To apply this model to the construction of the BVIX, the equivalent martingale measure (EMM) is derived under the assumption of no-arbitrage. This EMM is then used to price options on bitcoin, from which the implied volatilities are extracted using the Cboe variance swap fair-value formula adapted for the double subordinated NIG process. The resulting BVIX offers a more accurate reflection of market expectations regarding future volatility, thereby providing valuable insights for risk management and investment strategies in the volatile cryptocurrency market. The enhanced accuracy of the BVIX, compared to simpler models like the Student’s t, underscores the importance of incorporating advanced stochastic processes in the analysis of complex financial instruments.

### 5.1.3 Mixed Levy Subordinated Price Processes
Mixed Lévy subordinated price processes represent a sophisticated class of models designed to capture the intricate dynamics of financial markets, particularly their heavy-tailed and skewed distributions. These processes extend traditional Lévy processes by incorporating a time-change mechanism through subordinators, which are non-decreasing Lévy processes [21]. The primary motivation behind this extension is to better model the empirical characteristics of asset returns, such as volatility clustering and leptokurtosis, which are often inadequately represented by standard Brownian motion or even pure-jump Lévy processes. By subordinating a base Lévy process with another Lévy process, the mixed Lévy subordinated model introduces additional flexibility in the temporal structure, allowing for a more nuanced representation of market volatility and jump behavior.

In the context of financial modeling, the Normal Inverse Gaussian (NIG) Lévy process is frequently employed as the base process due to its ability to capture both skewness and heavy tails effectively [28]. When this NIG process is further subordinated by an Inverse Gaussian subordinator, the resulting model can account for the complex temporal dependencies observed in financial data. This double subordination framework not only enhances the model's capacity to describe the empirical distribution of asset returns but also facilitates the construction of arbitrage-free option pricing models. The subordinated NIG process, therefore, serves as a powerful tool for pricing derivatives, particularly in markets characterized by significant volatility and frequent extreme events. The enhanced accuracy in capturing these features is crucial for risk management and portfolio optimization, as it allows for more reliable estimates of potential losses and more effective hedging strategies.

Moreover, the mixed Lévy subordinated price processes provide a robust foundation for developing advanced financial metrics, such as the revised VIX, which measures market volatility. By incorporating the double subordinated NIG Lévy process, the revised VIX can more accurately reflect the true state of market uncertainty, thereby offering a more reliable indicator for investors and policymakers. The ability to model the heavy-tailed and skewed nature of financial returns through these processes also aids in the development of more sophisticated risk-reward measures, which are essential for reducing maximum drawdowns and managing financial downturns. Thus, the mixed Lévy subordinated price processes not only enhance the theoretical understanding of financial markets but also have practical implications for financial decision-making and market regulation.

## 5.2 Financial Derivatives and Risk Management

### 5.2.1 Temperature Derivatives and Time-Changed Lévy Models
Temperature derivatives, a class of financial instruments designed to hedge against temperature-related risks, have gained prominence in the energy and agricultural sectors. The pricing and risk management of these derivatives are inherently complex due to the non-Gaussian and often heavy-tailed nature of temperature data. Time-changed Lévy models, particularly those involving double subordinated Normal Inverse Gaussian (NIG) Lévy processes, offer a robust framework for addressing these challenges [2]. These models extend the classical Lévy process by incorporating a subordinator, which introduces a stochastic time change, thereby capturing the irregular and clustered nature of temperature fluctuations.

In the context of temperature derivatives, the double subordinated NIG Lévy process is particularly advantageous. The NIG process, known for its ability to model skewness and heavy tails, is further enhanced by the subordination mechanism, which allows for the modeling of long-range dependence and volatility clustering. This is crucial for accurately capturing the statistical properties of temperature data, which often exhibit significant deviations from normality. By using a double subordination, the model can account for both the intrinsic variability of temperature and the external factors that influence its dynamics, such as seasonal patterns and climate change. The resulting model provides a more realistic representation of temperature processes, which is essential for the accurate pricing of temperature derivatives.

The application of time-changed Lévy models to temperature derivatives also involves the estimation of model parameters, which is a critical step in ensuring the model's practical utility. Methods such as maximum likelihood estimation (MLE) and Bayesian inference are commonly employed to calibrate the model parameters to historical temperature data. Once calibrated, the model can be used to compute the implied volatility of temperature, which is a key input in the pricing of temperature derivatives. The ability of the double subordinated NIG Lévy process to capture the heavy-tailed and skewed nature of temperature data, combined with its flexibility in modeling stochastic time changes, makes it a powerful tool for managing temperature-related financial risks.

### 5.2.2 VIX and NIG Lévy Processes
The VIX, often referred to as the "fear index," is a key measure of market expectations of near-term volatility conveyed by S&P 500 index option prices. Traditional models, such as the Black-Scholes framework, often fail to capture the observed skew and fat-tailed distributions of option prices, leading to inaccuracies in volatility estimation. To address these limitations, the Normal Inverse Gaussian (NIG) Lévy process has been proposed as a more robust alternative [2]. The NIG Lévy process is characterized by its ability to model heavy-tailed and skewed distributions, which are common in financial time series [2]. This process is particularly useful in constructing a revised VIX that better reflects the true market conditions.

The double subordinated NIG Lévy process further enhances the modeling capabilities by incorporating an additional layer of stochastic time change. This approach allows for a more nuanced representation of the market dynamics, where the time evolution of the process is driven by a subordinator, typically a gamma or inverse Gaussian process. The double subordination introduces a second subordinator, which can capture the varying degrees of market activity and volatility clustering. This results in a more flexible and realistic model for S&P 500 option prices, leading to improved in-sample implied volatility estimates. The arbitrage-free nature of the model ensures that it can be used for both pricing and hedging purposes, providing a comprehensive tool for risk management.

In practice, the parameters of the double subordinated NIG Lévy process are estimated using advanced statistical techniques, such as maximum likelihood estimation or Bayesian inference. These methods allow for the calibration of the model to historical data, ensuring that the implied volatilities derived from the model are consistent with market observations. The resulting revised VIX not only captures the skew and fat-tailed properties of the S&P 500 option prices but also provides a more accurate measure of market volatility, which is crucial for investors and risk managers [28]. The enhanced accuracy of the VIX can help in making more informed decisions, particularly during periods of market stress, where traditional models may fall short.

### 5.2.3 Cross-Spectrum Method for Weak Signal Detection
The cross-spectrum method is a powerful technique for detecting weak signals in noisy environments, particularly when the signal-to-noise ratio is low [29]. This method leverages the simultaneous measurement of the signal using multiple independent instruments, thereby enhancing the detection sensitivity and reliability. Each instrument captures a version of the signal corrupted by its unique noise characteristics. By computing the cross-spectrum, which is the Fourier transform of the cross-correlation function between the signals from different instruments, the method effectively isolates the common signal component from the uncorrelated noise. This approach is particularly advantageous in scenarios where the signal of interest is buried in non-Gaussian noise, as it can capture both linear and nonlinear relationships between the instruments' outputs.

In practice, the cross-spectrum method involves repeating the experiment multiple times to gather a sufficient number of observations [29]. The cross-spectrum is estimated by averaging the cross-spectral densities across the repetitions, which helps to reduce the impact of random noise and improve the accuracy of the signal estimation [30]. The estimation of the measurement uncertainty is typically performed using Type A evaluation, as defined by the International Vocabulary of Metrology (VIM). This involves statistical analysis of the repeated measurements to quantify the variability and provide a robust estimate of the signal strength. The method's effectiveness is further enhanced by the use of advanced signal processing techniques, such as maximum likelihood estimation, which can refine the parameter estimates and enhance the detection of weak signals.

The cross-spectrum method has found applications in various fields, including astrophysics, finance, and engineering, where the detection of weak signals is crucial. In astrophysics, for instance, it is used to detect faint signals from distant celestial objects, while in finance, it can help in identifying subtle market trends or anomalies in high-frequency trading data. The method's ability to handle non-normal distributions and heavy-tailed phenomena makes it particularly suitable for analyzing complex systems where traditional methods based on second moments may fall short. By providing a more comprehensive and accurate representation of the signal, the cross-spectrum method contributes significantly to the advancement of signal processing and data analysis techniques.

## 5.3 Empirical Studies and Calibration

### 5.3.1 Calibration of Multivariate Lévy-Driven Ornstein-Uhlenbeck Processes
The calibration of multivariate Lévy-driven Ornstein-Uhlenbeck processes (LDOUPs) is a critical step in accurately modeling the dynamics of financial assets, particularly in contexts where heavy-tailed distributions and stochastic volatility are prevalent. These processes extend the classical Ornstein-Uhlenbeck (OU) framework by incorporating Lévy processes as the background driving noise, thereby capturing more complex market behaviors such as jumps and heavy tails. The calibration process involves estimating the parameters of the Lévy process and the OU process, including the mean reversion rate, the long-term mean, and the volatility parameters. This is typically achieved through maximum likelihood estimation (MLE) or methods based on empirical characteristic functions, which are particularly useful for handling the non-Gaussian nature of the driving Lévy process.

In the context of multivariate LDOUPs, the calibration process becomes more intricate due to the interdependencies between multiple assets. The use of multivariate Lévy processes allows for the modeling of joint jumps and correlated stochastic volatility, which are essential for capturing the comovements observed in financial markets. Techniques such as the empirical characteristic function (ECF) method have been adapted to the multivariate setting to estimate the parameters of the Lévy process and the OU dynamics simultaneously. The ECF method leverages the Fourier transform of the observed data to construct a system of equations that can be solved to obtain the parameter estimates. This approach is particularly advantageous when dealing with high-dimensional data, as it can handle the complexity and computational demands of multivariate models [31].

Moreover, the calibration of multivariate LDOUPs often involves the use of Bayesian inference, which provides a flexible framework for incorporating prior information and handling model uncertainty. Bayesian methods allow for the estimation of posterior distributions of the parameters, providing a more comprehensive understanding of the parameter uncertainty. This is particularly useful in financial applications where model robustness and reliability are crucial. The combination of advanced statistical techniques and computational methods, such as Markov Chain Monte Carlo (MCMC) algorithms, enables the effective calibration of multivariate LDOUPs, thereby enhancing the accuracy and reliability of financial models used for risk management, asset pricing, and portfolio optimization.

### 5.3.2 Simulations and Maximum Likelihood Estimation
In the context of estimating parameters for complex financial models, such as the double subordinated NIG Lévy process, simulations and maximum likelihood estimation (MLE) play a pivotal role. The simulation methods are essential for generating synthetic data that closely mimic real-world financial time series, thereby allowing for the testing and validation of the models. Specifically, the simulation of the observations \(X(0), X(t_1), \ldots, X(t_m)\) is achieved through the Fourier inversion of the characteristic function of the process [32]. This method leverages the known closed-form characteristic function of the innovation term \(Z^*(\Delta)\), which is a mixture of variance gamma distributions, to produce exact simulations of the process [32]. The accuracy of these simulations is crucial for the subsequent estimation procedures.

The maximum likelihood estimation (MLE) technique is then applied to estimate the parameters of the model using the simulated data. MLE is particularly advantageous in this context because it provides consistent and asymptotically efficient estimates, even when the underlying process is non-Gaussian and exhibits heavy-tailed behavior. The likelihood function for the process is derived from the characteristic function of \(Z^*(\Delta)\), which is known in closed form. This allows for the direct computation of the likelihood, facilitating the optimization process. The MLE approach is further enhanced by the use of numerical optimization algorithms, which can handle the potentially high-dimensional parameter space of the model.

To ensure robustness, the MLE procedure is often complemented by a thorough analysis of the simulation results. This includes assessing the convergence of the optimization algorithm, evaluating the goodness-of-fit of the estimated model, and conducting sensitivity analyses to understand the impact of different parameter settings. The combination of simulation and MLE not only provides a powerful framework for parameter estimation but also enables a deeper understanding of the underlying stochastic processes governing financial returns. This approach is particularly useful in the context of option pricing and volatility modeling, where accurate parameter estimates are critical for risk management and investment strategies.

### 5.3.3 Empirical Validation and Performance Comparison
In the empirical validation and performance comparison section, we focus on the robustness and accuracy of the proposed models, particularly the WVAG-OU and OU-WVAG processes, in capturing the heavy-tailed and skewed nature of financial asset returns. Through extensive simulations, we demonstrate that the parameters of both processes can be accurately estimated, although the sequential method used for the OU-WVAG process, due to its computational complexity, yields slightly less precise results compared to the WVAG-OU process [32]. The simulation study confirms the effectiveness of the maximum likelihood estimation (MLE) approach, leveraging the AR(1) structure and the properties of the innovation term \( Z^*(\Delta) \), which is crucial for the accurate estimation of the model parameters.

To further validate the models, we compare their performance against historical volatility measures and other established models in the literature. The intrinsic time approach, which accounts for the heavy tails and skewness in asset returns, is shown to outperform traditional historical volatility in terms of capturing the current risk of speculative assets, especially in the presence of volatility clustering. This is particularly evident in the context of cryptocurrency markets, where historical volatility often fails to adequately reflect the dynamic risk profile. The empirical results highlight the importance of incorporating heavy-tailed and skewed distributions in financial modeling to better understand and manage market risks.

Finally, we numerically implement the calibration and simulation methods for both the WVAG-OU and OU-WVAG processes, demonstrating their practical applicability [32]. The results of these implementations are compared with those obtained from Monte Carlo simulations and polynomial approximations, such as Taylor and cubic spline polynomials [33]. The numerical experiments show that the proposed models provide a more accurate and efficient estimation of asset prices, particularly in scenarios with significant market volatility. These findings underscore the value of the proposed models in both theoretical and applied finance, offering a more nuanced understanding of financial market dynamics.

# 6 Future Directions


The Variance-Gamma (VG) distribution, while a powerful tool in modeling heavy-tailed and skewed data, still faces several limitations and gaps that need to be addressed. One of the primary limitations is the computational complexity associated with parameter estimation, particularly when using maximum likelihood estimation (MLE) due to the presence of the modified Bessel function in the density function. This complexity can be a barrier to practical implementation, especially in high-dimensional settings or with large datasets. Additionally, the VG distribution's flexibility, while advantageous, can also lead to overfitting if not properly constrained. Another limitation is the difficulty in extending the VG distribution to multivariate settings, where dependencies between variables need to be accurately captured. Current methods for estimating multivariate VG distributions often rely on simplifying assumptions that may not hold in real-world applications.

To address these limitations, future research could focus on developing more efficient and robust parameter estimation methods for the VG distribution. One promising direction is the exploration of hybrid methods that combine the strengths of different estimation techniques, such as the modified method of moments (MME) and MLE. For instance, initial parameter estimates could be obtained using MME, and then refined using MLE. This approach could reduce computational burden while maintaining estimation accuracy. Another direction is the development of approximate methods, such as variational inference or Bayesian methods, which can provide a balance between computational efficiency and model accuracy. These methods could be particularly useful in high-dimensional settings or with large datasets.

Furthermore, there is a need to extend the VG distribution to more complex and realistic multivariate settings. This could involve the development of copula-based models that allow for flexible dependence structures between variables while maintaining the heavy-tailed and skewed properties of the VG distribution. Additionally, the integration of machine learning techniques, such as neural networks, could help in capturing complex dependencies and nonlinear relationships in multivariate data. These models could be particularly useful in financial applications, where the relationships between assets and risk factors are often intricate and nonlinear.

The potential impact of these proposed future research directions is significant. More efficient and robust parameter estimation methods could make the VG distribution more accessible and practical for a wide range of applications, from financial modeling to risk management and beyond. By reducing computational complexity, these methods could enable real-time analysis and decision-making, which is crucial in fast-paced environments such as financial markets. Extending the VG distribution to multivariate settings with flexible dependence structures could lead to more accurate and reliable models, improving the understanding and management of complex systems. This, in turn, could enhance the ability to predict and mitigate risks, leading to better-informed decisions and more effective strategies. Ultimately, these advancements could contribute to the development of more sophisticated and robust models, enhancing the field of probability and statistics and its applications in various domains.

# 7 Conclusion



The survey paper provides a comprehensive overview of the Variance-Gamma (VG) distribution, covering its mathematical derivations, analytical formulations, and practical applications. Key findings include the detailed exploration of estimation methods such as the modified method of moments estimation (MME) and maximum likelihood estimation (MLE), which are crucial for accurately estimating the parameters of the VG distribution. The empirical performance analysis highlights the VG distribution's superior fit to financial data, particularly in capturing heavy tails and skewness, and its effectiveness in predictive tasks like option pricing. The paper also delves into the analytical properties of the VG distribution, including the derivation of its probability density function using Mellin transforms and the representation of moments using hypergeometric functions. These properties are essential for understanding the distribution's heavy-tailed nature and its applicability in risk management and financial modeling. The survey further examines the VG distribution's practical applications in financial modeling, earthquake prediction, and non-local equations, demonstrating its versatility and robustness across various domains.

The significance of this survey lies in its detailed and structured overview of the VG distribution theory, making it a valuable resource for researchers and practitioners in probability, statistics, and financial engineering. The paper's critical evaluation of estimation methods and empirical performance provides a clear understanding of the VG distribution's advantages and limitations, enhancing its practical utility. By synthesizing the latest research findings and theoretical developments, the survey offers a comprehensive reference for those seeking to apply the VG distribution in their work. Moreover, the identification of potential future research directions emphasizes the need for further exploration of the VG distribution's applications in emerging areas such as cryptocurrency markets and advanced financial modeling techniques.

In conclusion, this survey paper serves as a foundational resource for anyone interested in the Variance-Gamma distribution. It not only consolidates existing knowledge but also highlights the distribution's potential for addressing complex real-world problems. We encourage researchers and practitioners to build upon the insights provided in this survey, further advancing the theoretical and practical applications of the VG distribution. Future work could focus on developing more efficient computational methods for parameter estimation, extending the VG distribution to more complex models, and exploring its applications in new and emerging fields. The continued development and refinement of the VG distribution will undoubtedly contribute to more accurate and robust models in various scientific and financial disciplines.

# References
[1] On the Pricing of Currency Options under Variance Gamma Process  
[2] Absolute moments of the variance-gamma distribution  
[3] Modified Method of Moments for Generalized Laplace Distribution  
[4] The Variance-Gamma Distribution  A Review  
[5] On the moments of the variance-gamma distribution  
[6] The distribution of the product of independent variance-gamma random  variables  
[7] The variance-gamma product distribution  
[8] Wasserstein and Kolmogorov error bounds for variance-gamma approximation  via Stein's method I  
[9] Unknown PDF  
[10] Fitting Infinitely divisible distribution  Case of Gamma-Variance Model  
[11] The variance-gamma ratio distribution  
[12] On the cumulative distribution function of the variance-gamma  distribution  
[13] Pricing European Options under Stochastic Volatility Models  Case of  five-Parameter Variance-Gamma  
[14] W-shaped implied volatility curves in a variance-gamma mixture model  
[15] Quantifying effect of geological factor on distribution of earthquake  occurrences by inhomogeneous  
[16] Variance-Gamma (VG) model  Fractional Fourier Transform (FRFT)  
[17] Bitcoin Volatility and Intrinsic Time Using Double Subordinated Levy  Processes  
[18] Generalized Fractional Negative Binomial Process  
[19] Mixtures of Tempered Stable Subordinators  
[20] Extreme statistics of superdiffusive Levy flights and every other Levy  subordinate Brownian motion  
[21] Potential Theory of Normal Tempered Stable Process  
[22] The structure of the local time of Markov processes indexed by Levy  trees  
[23] Scaling Limit of Dependent Random Walks  
[24] On the association and other forms of positive dependence for Feller  processes  
[25] Extreme statistics of anomalous subdiffusion following a fractional  Fokker-Planck equation  Subdiff  
[26] Unknown PDF  
[27] Unknown PDF  
[28] Beyond the Traditional VIX  A Novel Approach to Identifying Uncertainty  Shocks in Financial Markets  
[29] Cross-Spectrum Measurement Statistics  
[30] The Statistics of the Cross-Spectrum and the Spectrum Average   Generalization to Multiple Instrumen  
[31] Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers  
[32] Calibration for multivariate Lévy-driven Ornstein-Uhlenbeck processes  with applications to weak sub  
[33] Valuing Exchange Options Under an Ornstein-Uhlenbeck Covariance Model  