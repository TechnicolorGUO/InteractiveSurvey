# A Review of Barren Plateaus in Variational Quantum Computing  

Martín Larocca, $^ { 1 , 2 }$ Supanut Thanasilp, ${ } ^ { 3 , 4 }$ Samson Wang,5 Kunal Sharma, $^ 6$ Jacob Biamonte, $^ { 7 , 8 }$ Patrick J. Coles, $^ { 9 }$ Lukasz Cincio, $^ { 1 , 1 0 }$ Jarrod R. McClean, $^ { 1 1 }$ Zoë Holmes,3 and M. Cerezo $^ { 1 2 }$ , 10  

$^ 1$ Theoretical Division, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA $^ 2$ Center for Non-Linear Studies, Los Alamos National Laboratory, 87545 NM, USA   
$^ 3$ Institute of Physics, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne CH-1015, Switzerland $^ 4$ Chula Intelligent and Complex Systems, Department of Physics, Faculty of Science, Chulalongkorn University, Bangkok 10330, Thailand $^ { 5 }$ Institute for Quantum Information and Matter, Caltech, Pasadena, CA 91125, USA $^ 6$ IBM Quantum, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA $^ 7$ Quantum Artificial Intelligence Laboratory, NASA Ames Research Center, Moffett Federal Airfield, Mountain View, California 94035, USA   
Simons Institute for the Theory of Computing, University of California, Berkeley, California 94720-2190, USA $^ { 9 }$ Normal Computing Corporation, New York, New York, USA $^ { 1 0 }$ Quantum Science Center, Oak Ridge, TN 37931, USA $^ { 1 1 }$ Google Quantum AI, Venice, CA 90291, USA $^ { 1 2 }$ Information Sciences, Los Alamos National Laboratory, 87545 NM, USA  

Variational quantum computing offers a flexible computational paradigm with applications in diverse areas. However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm—choices of ansatz, initial state, observable, loss function and hardware noise—can lead to BPs when ill-suited. Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory. This article provides a comprehensive review of the current understanding of the BP phenomenon.  

# I. INTRODUCTION  

Variational quantum computing has gathered momentum in recent years as a promising approach for quantum computers [1–7]. At their core, most variational quantum algorithms follow the same principle: Convert a problem into an optimization task, and use a classical device to train a parametrized quantum learning model. The popularity of this computational paradigm can be attributed in part to its flexibility, opening up applications in diverse areas in basic science and machine learning, but also because of the hope that it is more robust to the constraints of near-term quantum hardware.  

In contrast to conventional quantum algorithms, which generally come with provable guarantees, variational quantum algorithms share much of the heuristic nature of classical machine learning. However, as opposed to classical machine learning, we do not yet have the hardware required to test these algorithms at scale. Moreover, it quickly becomes clear to any practitioner that out-ofthe-box variational schemes can be hard to train as the number of qubits becomes larger and larger. This has prompted efforts to analyze the scalability of these algorithms and to determine trainability barriers that could obstruct their practical use in realistic problem sizes.  

Among the sources for potential untrainability in variational quantum algorithms, this review focuses on the Barren Plateau (BP) phenomenon [8, 9]. On a BP, the loss gradients, or more generally loss differences [10], vanish exponentially with the size of the system. Combining this issue with the fact that information can only be extracted from a quantum computer through a finite set of measurements, the presence of a BP landscape implies that for the overwhelming majority of parameter choices an exponentially large number of measurement shots are needed to identify the loss minimizing direction.  

By now, researchers have uncovered a number of causes of BPs, as well as several methods to mitigate or avoid them. The quest to understand this phenomenon has highlighted that BPs are a symptom of a deeper issue with variational quantum computing. Namely, that the exponentially large Hilbert space dimension—the very same feature which was hoped to provide variational quantum computing with computational capabilities beyond those achievable by classical learning models—can lead to issues if not handled properly. That is, in essence, BPs ultimately arise from a curse of dimensionality [11].  

In this review we survey the substantial body of literature on BPs that has emerged in the last five years. Our main goal is to provide guidelines for the numerous do’s and don’t’s that our community has identified. However, we also aim at showing that the quest to uncover the mysteries of BPs is a path filled with insights into the very nature of the information processing capabilities of quantum computers, the resources that make something quantum “quantum”, the connection between absence of BPs and classical simulability, and the potential paths for alternative variational paradigms.  

![](images/de400e4b1db63adbfc2bbbfbbdeb5ea9bb4c4209d56c256d03ef08ec1bb67f3b.jpg)  
FIG. 1. Variational quantum computing. An $n$ -qubit quantum computer is initialized to a state $\rho$ that is evolved through a PQC $\boldsymbol { { \mathit { u } } _ { \theta } }$ . At the output of the circuit we perform a finite set of measurements which are used to estimate a loss function $\ell _ { \pmb { \theta } } ( \rho , O )$ as in Eq. (3). A classical computer takes as input the finite sample estimate of $\ell _ { \pmb { \theta } } ( \rho , O )$ (or its partial derivatives) and attempts to find new sets of parameters which train the PQC and minimize the loss. As such, at the core of variational quantum computing lies the manipulation and comparison—via polynomially many samples—of the exponentially large operators $\rho ( \pmb \theta ) = \mathcal { U } _ { \pmb \theta } ( \rho )$ and $O$ .  

We start our review in Section II with a brief review of variational quantum computing. In Section III we recall definitions for different type of BPs, with their sources being identified in Sections IV. We then survey architectures (Section V) and methods that can (Section VI) and cannot (Section VII) mitigate BPs, or even attempt to prevent BPs altogether. Section VIII presents examples of the impact that the BP study has had beyond the field of variational quantum computing, and Section $1 \lambda$ discusses other issues beyond BPs such as local minima and the intriguing connection between absence of BPs and classical simulability. In Section X we link the BP phenomenon with vanishing gradients in classical machine learning. We finish in Section XI with some closing remarks and outlook into the future. We note that we have added all throughout our reviews boxes with include important details and more technical discussions.  

# II. VARIATIONAL QUANTUM COMPUTING  

# A. Framework  

The key idea behind variational quantum computing is to combine the power of both quantum and classical computers. In this hybrid computational paradigm, one uses the exponentially large Hilbert space of the quantum  

# Box 1: Quantum and classical data for QML  

By quantum data, we mean that $\rho$ is obtained from some quantum mechanical process of interest within a quantum device. By classical data $\mathbf { \boldsymbol { x } } \in \mathcal { X }$ we mean data from some domain $\mathcal { X }$ that can be efficiently stored in a classical computer (e.g., images, financial information, events from a particle accelerator), and that we want to process in a quantum computer. Such classical data needs to be encoded in a quantum state through an embedding map $\mathcal { E } : \mathcal { X }  B ( \mathcal { H } _ { 0 } )$ . We also note that the classical data $_ { x }$ can also be used to construct the PQC in Eq. (1) through data re-uploading methods [12].  

device to process information and estimate some quantity of interest, and then feeds those outputs to a classical computer that analyzes them and (potentially) determines the next experiment on the quantum computer. Many such hybrid algorithms employ an optimization, or a learning-based, approach whereby the quantum experiments are characterized by a sequence of parameters that a classical optimizer attempts to train in order to solve a given task. Given that variational computing reduces the burden on quantum hardware, they are naturally well suited for near-term devices. However, their usage is by no means confined to the near-term and it is hoped that variational schemes will further flourish in the fault-tolerant era when noise is no longer an issue.  

It has become standard to divide variational quantum computing schemes into two main categories: Variational Quantum Algorithms (VQAs) [1–3] and Quantum Machine Learning (QML) models [4–6]. VQAs can be considered as problem-driven, since the algorithms tackle a specific task such as preparing the ground state of some Hamiltonian. QML models on the other hand are datadriven. QML has been envisioned for virtually all learning tasks such as supervised and unsupervised learning, generative modeling, etc [6]. Throughout this work, we will study in parallel VQAs and QML models as they mostly share a common structure.  

# B. Components of variational quantum computing: data, ansatz, measurements, and classical information processing  

As shown in Fig. 1, in variational quantum computing one first initializes the quantum system to some $n$ -qubit initial state $\rho$ , belonging to $\boldsymbol { B } ( \mathcal { H } _ { 0 } )$ , the set of bounded linear operators on an $2 ^ { n }$ -dimensional complex Hilbert space $\mathcal { H } _ { 0 }$ . In VQAs, $\rho$ is usually some easy-to-prepare fiducial state, while in QML $\rho$ is taken from a set (or distribution) of training states that encode either quantum or classical data (see Box 1).  

Next, one sends $\rho$ through a Parametrized Quantum  

Inductive biases, broadly speaking, capture the assumptions and choices we make when constructing the quantum algorithm. In the pipeline of variational quantum computing these come in many forms. For instance, when working with VQAs, while there is a significant amount of freedom when choosing the input state, the measurement operator is usually more restricted. This is due to the fact that in most VQAs the problem information is encoded in the measurement operator (e.g., in the variational quantum eigensolver [13] we measure the expectation value of the Hermitian operator whose ground state we want to prepare). In QML, the input states tend to be fixed, with the encoding scheme (for classical data), the measurement operator and choice of loss function being generally more arbitrary, and thus leading to extra design freedom. Similarly, while Eq. (1) shows a general PQC, different instantiations of these computational models are distinguished form each other by the inductive biases that go into $\mathcal { U } _ { \pmb { \theta } _ { l } }$ , i.e., the choices in their specific form. This includes properties such as the number and types of gates, their placement, and how the parameters are initialized.  

Circuit (PQC), also known as a quantum neural network in the context of QML. A PQC can be expressed as a sequence of $L$ parametrized unitaries  

$$
U ( \pmb \theta ) = \prod _ { i = 1 } ^ { L } U _ { i } ( \pmb \theta _ { i } ) ,
$$  

where $\pmb \theta = ( \pmb \theta _ { 1 } , \dots , \pmb \theta _ { L } )$ is a set of trainable parameters. In the presence of hardware noise, or if the parameterized quantum circuit includes adding or discarding qubits, the PQC can be more generally expressed as a sequence of $L$ parametrized channels  

where each $\mathcal { U } _ { \pmb { \theta } _ { l } } : B ( \mathcal { H } _ { l - 1 } )  B ( \mathcal { H } _ { l } )$ is a completely positive and trace-preserving map between bounded operators. Finally, as indicated in Fig. 1, at the output of the PQC we perform a (finite) number of measurements to estimate a quantity of interest such as the expectation value of some set of observables or the probability of a given outcome.  

$$
\mathcal { U } _ { \pmb \theta } = \mathcal { U } _ { \pmb \theta _ { L } } \circ \cdot \cdot \cdot \circ \mathcal { U } _ { \pmb \theta _ { 1 } } ,
$$  

In the simplest case, one has a single input state and is interested in estimating the expectation value of a single observable $O$ , which leads to the loss function  

$$
\ell _ { \pmb { \theta } } ( \rho , O ) = \mathrm { T r } [ \rho ( \pmb { \theta } ) O ] ,
$$  

with $\rho ( \pmb \theta ) = \mathcal { U } _ { \pmb \theta } ( \rho )$ . While more general loss functions will be discussed below (some built by computing several quantities such as $\ell _ { \pmb { \theta } } ( \rho , O ) .$ ), we will mostly restrict  

# Box 3: Effects of finite sampling  

In practice, we can only estimate the expectation value of the loss function $\ell _ { \pmb { \theta } } ( \rho , O ) = \mathbb { E } _ { \pmb { \rho } ( \pmb { \theta } ) } [ O ]$ up to some finite precision determined by the number of measurement shots. To differentiate, $\ell _ { \pmb { \theta } } ( \rho , O )$ from its $N$ -shot approximation, we use $\bar { \ell } _ { \pmb { \theta } } ( \rho , O )$ to denote the latter. Assuming that the experiment repetition used to obtain $\bar { \ell } _ { \pmb { \theta } } ( \rho , O )$ are independent, we recover the sample variance (denoted as $\mathrm { V a r } _ { s }$ ) of the estimate  

$$
\begin{array} { r l } & { \mathrm { V a r } _ { s } [ \overline { { \ell } } _ { \pmb { \theta } } ( \rho , O ) ] = \frac { \mathrm { V a r } _ { \rho ( \pmb { \theta } ) } [ O ] } { N } } \\ & { \qquad = \frac { \mathbb { E } _ { \rho ( \pmb { \theta } ) } [ O ^ { 2 } ] - \mathbb { E } _ { \rho ( \pmb { \theta } ) } [ O ] ^ { 2 } } { N } . } \end{array}
$$  

Equation (5) quantifies the uncertainty to which $\bar { \ell } _ { \pmb { \theta } } ( \rho , O )$ is computed. Clearly, there are two contributions to this uncertainty. First, the one in the nominator arises from the fact that $\rho ( \pmb \theta )$ is generally not an eigenstate of $O$ , and hence different measurements can yield different outcomes. Second, the factor $1 / N$ indicates that we are estimating the expectation value with a finite number of samples. Thus, in regions of a landscape with exponentially small gradients, exponentially many shots are typically required to determine a loss minimizing direction.  

our attention to those of the form in Eq. (3), as the lessons learnt therein can often be extrapolated to other, more general, scenarios. In either case, one usually uses a quantum device to estimate the loss, and leverages the power of classical optimizers to solve the optimization task  

$$
\arg \operatorname* { m i n } _ { \pmb { \theta } } \ell _ { \pmb { \theta } } ( \rho , O ) .
$$  

Despite the (relative) simplicity of the variational quantum computing framework, solving Eq. (4) and training the model’s parameters can be a hard task. For instance, the optimizer might get stuck in a sub-optimal local minima [14–19], there might not exist a solution in the parameter hyperspace [20, 21], or the landscape might exhibit BPs, the focus of this review. Importantly, and as we will see below, the hardness of training the loss will strongly depend on the inductive biases of the quantum algorithm (see Box 2).  

# III. TYPES OF BARREN PLATEAUS AND LOSS CONCENTRATION  

We say that a variational quantum computing scheme exhibits a BP if its loss function, or its gradients, concentrate exponentially about their mean in the number of qubits $n$ . Intuitively, this means that the optimization landscape is mostly flat and featureless and that slightly changing the model’s parameters $\pmb \theta$ results in only an exponentially small change in $\ell _ { \pmb { \theta } } ( \rho , O )$ or $\partial _ { \mu } \ell _ { \pmb { \theta } } ( \rho , O ) = \partial \ell _ { \pmb { \theta } } ( \rho , O ) / \partial \theta _ { \mu }$ (where ${ \theta } _ { \mu } \in \theta$ ). One can readily see that this will cause serious issues when minimizing the loss function, as optimizers find descent directions by comparing the loss at different points (gradientfree) or by using derivative information (e.g., gradientbased). In particular, since we can only estimate expectation values from a quantum computer with a finite number of measurements $N$ , these come with a statistical uncertainty that scales inversely proportional to $\sqrt { N }$ (see Box 3). Thus, if we want to resolve exponentially small changes and optimize the loss, we need to spend an exponential amount of shots, rendering the algorithm inefficient and non-scalable. Indeed, if a smaller number of shots are used, the optimizer will follow changes produced by the uninformative statistical fluctuations, meaning that the parameter updates lead to a useless random walk in the loss landscape.  

# A. Probabilistic concentration and narrow gorges  

The most common type of loss function concentration encountered in the literature is known as probabilistic concentration. By defining $\mathbb { E } _ { \theta }$ as the expectation value over the parameter landscape, we say a loss exhibits a probabilistic BP if  

$$
\operatorname { V a r } _ { \pmb { \theta } } [ \ell _ { \pmb { \theta } } ( \rho , O ) ] \ \mathrm { o r } \ \mathrm { V a r } _ { \pmb { \theta } } [ \partial _ { \mu } \ell _ { \pmb { \theta } } ( \rho , O ) ] \in \mathcal { O } \left( \frac { 1 } { b ^ { n } } \right) ,
$$  

for some $b > 1$ , and for some (but not necessarily all) $\mu$ . This implies through Chebyshev’s inequality that for any $\delta > 0$  

$$
\operatorname* { P r } ( | \ell _ { \pmb { \theta } } ( \rho , O ) - \mathbb { E } _ { \pmb { \theta } } [ \ell _ { \pmb { \theta } } ( \rho , O ) ] | \geqslant \delta ) \in O \left( \frac { 1 } { b ^ { n } } \right) ,
$$  

or $\begin{array} { r } { \operatorname* { P r } ( | \partial _ { \mu } \ell _ { \pmb { \theta } } ( \rho , O ) - \mathbb { E } _ { \pmb { \theta } } [ \partial _ { \mu } \ell _ { \pmb { \theta } } ( \rho , O ) ] | \geqslant \delta ) \in \mathcal { O } \left( \frac { 1 } { b ^ { n } } \right) } \end{array}$ . These equations indicate that the probability that the loss, or its partial derivative, deviate by more than $\delta$ (for some $\delta \in \Omega ( 1 / \operatorname { p o l y } ( n ) ) )$ from its mean is exponentially small.  

Here we find it important to remark that while the definition of a BP in terms of the loss is unambiguous, it is less so when stated in terms of partial derivatives. As such, our statement above that a BP arises when some (but not all) partial derivatives must concentrate highlights that concentration of the loss is a stronger means of diagnosing a BP. In fact, it is entirely possible that a loss contains some partial derivatives that are concentrated while others are not. Hence, care must be taken when establishing the absence of a BP by studying $\operatorname { V a r } _ { \pmb { \theta } } [ \ell _ { \pmb { \theta } } ( \rho , O ) ]$ as it is entirely possible that a nonconcentrated loss arises only from a few well-behaved set of parameters (and thus still has a BP). However, when every partial derivative is concentrated one can prove that loss concentration and partial derivative concentration are equivalent [10, 22]. Similarly, one can show that if first-order derivatives concentrate, then so do higherorder ones [23].  

Interestingly, while a probabilistic BP implies a mostly featureless, flat landscape, there can still be regions with large gradients around some minima (sometimes called fertile valleys [24]). The caveat is that these regions have to be small and the minima therein are always exponentially narrow (meaning that their relative volume in parameter space is always exponentially small [10]). These properties led researchers to name these minima as narrow gorges. Here, it is worth mentioning that while narrow gorges bring to mind an image of a very steep well, the slope of the walls of a narrow gorge cannot be too large as it bounded by the loss’s Lipschitz constant which in most cases is $\mathcal { O } ( \mathrm { p o l y } ( n ) )$ [10].  

# B. Deterministic concentration  

While probabilistic BPs lead to mostly flat and featureless landscapes, they still admit the presence of wellbehaved regions such as narrow gorges. In contrast, there exists a second type of concentration, known as deterministic BPs, where all of the landscape is truly flat [25]. A deterministic BP arises when $\forall \pmb { \theta }$  

$$
| \ell _ { \pmb \theta } ( \rho , O ) - \mathbb { E } _ { \pmb \theta } [ \ell _ { \pmb \theta } ( \rho , O ) ] | \in { \mathcal O } \left( \frac { 1 } { b ^ { n } } \right) ,
$$  

for some $b > 1$ . Clearly, Eq. (8) implies that for all parameter values $\begin{array} { r } { | \partial _ { \mu } \ell _ { \pmb { \theta } } ( \rho , O ) - \mathbb { E } _ { \pmb { \theta } } [ \partial _ { \mu } \ell _ { \pmb { \theta } } ( \rho , O ) ] | \ \in \ \mathcal { O } \left( \frac { 1 } { b ^ { n } } \right) } \end{array}$ . From here it is obvious that a deterministically concentrated loss cannot admit any type of extrema that is well separated from the loss’s mean. In fact, Eq. (8) precludes any type of non-exponentially suppressed features in the landscape.  

# C. Methods for analyzing variance  

At the simplest level, one can heuristically determine if the loss or its gradients are concentrating by evaluating them at randomly sampled parameters and computing how the statistical variance scales with the number of qubits [8, 18, 26]. Similarly, one can also compute other quantities which have been proposed as surrogates and thus are able to study if a BP occurs. These include the landscape’s information content [27], the local state’s entanglement [28], or the landscape’s Fourier coefficients [29–31]. While such approaches can provide a rough estimate of the scaling, care must be taken as they suffer from statistical errors (as we measure the loss a finite number of times), and are generally unable to distinguish between the two types of BPs.  

A preferred, but more theoretically demanding, method for detecting BPs is by analytically studying the scaling of the variance. Here, one needs to make  

# Box 4: From quantum circuits to Lie algebras and groups  

The Lie subalgebras ${ \mathfrak { g } } \subseteq { \mathfrak { s u } } ( { \mathcal { H } } )$ play a fundamental role in BP analysis. Recalling that $\mathfrak { s u } ( \mathcal { H } )$ is the subspace of traceless anti-Hermitian linear operators on $\mathcal { H }$ , any subspace ${ \mathfrak { g } } \subseteq { \mathfrak { s u } } ( { \mathcal { H } } )$ that is closed under commutation constitutes a subalgebra. Given a unitary noiseless PQC as in Eq. (1) where $U ( { \pmb \theta } _ { l } ) = e ^ { - i \theta _ { l } H _ { l } }$ , its dynamical Lie algebra (DLA) [18, 69–71] is defined as  

$$
\mathfrak { g } = \langle \{ i H _ { l } \} _ { l } \rangle _ { \mathrm { L i e } } \subseteq \mathfrak { s u } ( \mathcal { H } ) ,
$$  

where $\langle S \rangle _ { \mathrm { L i e } }$ , for any subset $S \subseteq { \mathfrak { s u } } ( { \mathcal { H } } )$ , denotes its Lie closure, the real vector space spanned by every nested commutator between the elements of $S$ . The DLA is a fundamental object in the study of PQCs because its associated Lie group $G = e ^ { \mathfrak { g } }$ contains all unitaries that can ever be achieved by a choice of parameters $\pmb \theta$ or number of layers $L$ . One can explicitly verify this by using the Baker–Campbell–Hausdorff formula to re-express the composition of layers as the exponential of a linear combination of the nested commutators of the generators, which by definition belongs in $\mathfrak { g }$ .  

assumptions regarding the PQC to make the problem more tractable. For instance, if $\mathcal { U } _ { \theta }$ is a deep unitary circuit whose distribution of unitaries forms at least a 2-design [32] over a Lie group (see Boxes 4-6), one can use representation theory to exactly evaluate the variances [8, 18, 33–43] via Weingarten calculus [44, 45]. Instead, for shallow circuits that do not form designs, one can instead assume that the local gates are sampled from a local group, thus mapping the variance evaluation to a Markov chain-like process which can be analyzed analytically [46–56], numerically via Monte Carlo [57] and tensor networks [58, 59], or studied via XZ-calculus [60]. One can also take an iterative approach to explicitly integrate over the random parameters within a given angle range [24, 54, 61, 62] or exploit tools from Floquet theory [63, 64]. Then, if $\mathcal { U } _ { \theta }$ is a noisy, one can study the variance by making assumptions regarding to the type of noise acting throughout the PQC [25, 65–68].  

# IV. ORIGINS OF BPS  

While theoretical and numerical calculations can be used to determine the flatness of the landscape, these only identify a symptom of the variational quantum computing scheme. In this section we discuss recent findings which argue that BPs appear as a form of curse of dimensionality, as well as showcase how this phenomenon reveals itself under different scenarios. By understanding the underlying connection between all of them, we  

# Box 5: Averages & Haar-measure & Weingarten calculus  

Evaluating $\mathbb { E } _ { \pmb { \theta } } [ \mathrm { T r } [ U ( \pmb { \theta } ) \rho U ^ { \dagger } ( \pmb { \theta } ) O ] ^ { t } ]$ can become tractable for unitary PQCs under certain assumptions. First, note that every set of parameters $\pmb \theta$ , determines a unitary through Eq. (1), meaning that we can opt to evaluate the average over the induced distribution of unitaries $\boldsymbol { S }$ , and an associated distribution $d U$ over $\boldsymbol { S }$ . That is, we can evaluate $\mathbb { E } _ { \pmb { \theta } } [ \mathrm { T r } [ U ( \pmb { \theta } ) \rho U ^ { \dagger } ( \pmb { \theta } ) O ] ^ { t } ]$ as  

$$
\int _ { S } d U [ \mathrm { T r } [ U \rho U ^ { \dagger } O ] ^ { t } ] = \mathrm { T r } [ \tau _ { S } ^ { ( t ) } ( \rho ^ { \otimes t } ) O ^ { \otimes t } ] ,
$$  

where $\begin{array} { r } { \tau _ { S } ^ { ( t ) } ( \cdot ) = \int _ { S } d U U ^ { \otimes t } ( \cdot ) ( U ^ { * } ) ^ { \otimes t } } \end{array}$ is the $t$ -th fold twirl over $\boldsymbol { S }$ . (In the vectorized picture [44], we can also write Eq. (10) as $\langle \langle \rho ^ { \otimes t } | \widehat { \tau } _ { S } ^ { ( t ) } | O ^ { \otimes t } \rangle \rangle$ with τ (t) = dU U ⊗t ⊗ (U ∗)⊗t known bas the t-th mobment operator.) If $\boldsymbol { S }$ forms a group $G$ , and if $d U$ is the uniform measure over this group, we say that the PQC is Haar random over $G$ , in which tcharseou τ t(hoer $\hat { \tau } _ { S } ^ { ( t ) }$ i)ngcarnt bn ceavlcaluluats.ed aSnpaelcyiftiicallly, given a basis $\{ B _ { \sigma } \} _ { \sigma }$ for the $t$ -th fold commutant of $G$ , $\mathrm { c o m m } ^ { ( t ) } ( G ) = \{ A \in \mathcal { B } ( \mathcal { H } ^ { \otimes t } ) | [ A , U ^ { \otimes t } ] =$ $0 \forall U \in G \}$ , we have  

$$
\tau _ { S } ^ { ( t ) } ( \cdot ) = \sum _ { \sigma \pi } W _ { \pi \sigma } ^ { - 1 } \mathrm { T r } [ B _ { \sigma } ^ { \dagger } ( \cdot ) ] B _ { \pi } ,
$$  

where $W$ is the basis’ Gram matrix. For a pedagogical introduction to Weingarten calculus with a focus in quantum information we point the reader to [44].  

will see that one should be able to predict if a model will exhibit an exponentially concentrated loss.  

# A. A curse of dimensionality  

Recently it has been argued that the very same feature that makes variational quantum computing appealing— the exponentially large dimension of the Hilbert space— is at the very core of the BP phenomenon [11, $3 7 -$ 39, 72, 73]. To see that this is the case, let us begin by assuming that the PQC is a unitary as in Eq. (1). Then, a loss of the form in Eq. (3) can be expressed as $\ell _ { \pmb { \theta } } ( \rho , O ) = \langle \rho ( \pmb { \theta } ) , O \rangle = \langle \rho , O ( \pmb { \theta } ) \rangle$ , where we introduced $O ( \pmb \theta ) = U ( \pmb \theta ) ^ { \dag } O U ( \pmb \theta )$ and with $\langle A , B \rangle = \operatorname { T r } [ A ^ { \dagger } B ]$ . That is, the loss is the Hilbert-Schmidt inner product between two vectors in the exponentially large space $\boldsymbol { B } ( \mathcal { H } _ { 0 } )$ . This simple realization shows that optimizing the loss can be understood as variationally trying to anti-align two exponentially large vectors via their inner product. This is already troublesome as—under quite general assumptions such as random initialization—the inner product between two exponentially large parametrized objects will be (on average) exponentially small and concentrated.  

Now, of course, this does not mean that comparing any pair of exponentially large objects will always lead to exponentially small signals. Indeed, conventional quantum algorithms [74] manipulate vectors in the exponentially large Hilbert space by carefully choreographing the patterns of constructive and destructive interference of the probability amplitudes [75]. Indeed such algorithms have even been adapted to establish advantage proofs for quantum machine learning [76, 77]. Yet, there are not many such algorithms in the variational setting as their construction requires carefully and purposely designed circuits with appropriate inductive biases for the task they are trying to solve. How well variational quantum algorithms can be coerced into finding and exploiting such structure is very much an open question.  

In what follows, we will take a closer look at $\langle \rho ( \pmb { \theta } ) , O \rangle$ and illustrate how its different components can lead to a curse of dimensionality-induced exponential concentration due to the choice of circuits, initial states, measurement operators, and also due to the effects of hardware noise.  

# Box 6: Expressive power of the PQC  

# B. Circuit expressiveness  

The expressive power of a unitary PQC quantifies the breath of unitaries that the circuit is likely to produce in an unbiased manner. For instance, one can quantify the potential, or ultimate expressiveness of the PQC by its dynamical Lie algebra $\mathfrak { g }$ defined in Box 4—since any unitary $U ( \pmb \theta )$ produced by the PQC belongs in the dynamical Lie group $G \ = \ e ^ { \mathfrak { g } }$ . Here, the larger the dimension of the algebra, the more expressive the circuit is, with controllable circuits being defined as those for which ${ \mathfrak { g } } = { \mathfrak { s u } } ( 2 ^ { n } )$ . Of course, shallow circuits will not be able to cover the whole dynamical Lie group $G$ , as they can only produce unitaries in some subset $S \subseteq G$ . To deal with this situation, we can instead consider the current expressiveness which is quantified by the difference between τ (t) and $\tau _ { G } ^ { ( t ) }$ . When those operators match, we sSay that the PQC forms a $t$ -design [32]. Importantly, if $| \tau _ { S } ^ { ( 2 ) } - \tau _ { G } ^ { ( 2 ) } | \leqslant \varepsilon$ (in operator norm) then [37]  

$$
\begin{array} { r } { | \mathrm { V a r } _ { \pmb { \theta } } [ \ell _ { \pmb { \theta } } ( \rho , O ) ] - \mathrm { V a r } _ { G } [ \ell _ { \pmb { \theta } } ( \rho , O ) ] | \in \mathcal { O } ( \varepsilon \| O \| _ { 1 } ) , } \end{array}
$$  

showing that we can quantify how much the variance of a shallow circuit deviates from that of a Haar random PQC over $G$ via $\varepsilon$ .  

Perhaps the main culprit responsible for the curse of dimensionality is the PQC as it has been shown that its expressive power (see Box 6) is directly linked to BPs. Roughly speaking, the larger the space the PQC explores in an unbiased manner, the more it is prone—when randomly initialized—to lead to input states that have exponentially small inner products with the measurement operator. At a first glance, this result seems counterintuitive as we would ideally like to choose a PQC that is as expressive as possible to try to guarantee that a solution for Eq. (4) exists within the parameter landscape. However, the fact that PQCs with unbiased highly expressive features lead to BPs should be understood as a form of No-Free-Lunch theorem: The more uniformly expressive the model (i.e., the less inductive biases), the harder to train. Said otherwise, there is no one $P Q C$ to rule them all.  

The previous connection between expressiveness and BPs has been mathematically formalized for unitary [8, 18, 37–39, 46, 78, 79] and noisy [80] circuits, as one can show that the more expressive a circuit is, the more the loss can concentrate. A similar statement can be made in terms of the PQC’s capacity [81, 82], a measure of how many functions it can represent, as the larger the model’s capacity is, the more its loss can concentrate [83].  

In particular, let us review the recent approach in Refs. [37–39] for obtaining an exact analytical form for the loss function variance of deep unitary PQCs. By deep we refer to the regime where the number of layers is large enough to guarantee that the distribution of unitaries corresponding to random parameter choices forms a 2- design [32]. That is, that it approximately matches the Haar distribution on $e ^ { { \mathfrak { g } } }$ (where $\mathfrak { g }$ is the circuit’s dynamical Lie algebra (DLA), defined in Box 4) up to second moments with small additive approximation error. Assume for simplicity that either $\rho$ or $O$ belong to a groupmodule $\mathcal { M } \subseteq B ( \mathcal { H } )$ that is non-trivial under the adjoint action of the circuit. (We recall that a group-module is a vector space that holds a representation for the action of a group. In the case of $\boldsymbol { B }$ , $\mathcal { M }$ is a subspace that is closed under the adjoint action of $e ^ { { \mathfrak { g } } }$ , i.e., if $A \in { \mathcal { M } }$ , then $U A U ^ { \dagger }$ also belongs to $\mathcal { M }$ for any $U \in e ^ { { \mathfrak { g } } }$ [84]). One finds  

$$
\operatorname { V a r } _ { \pmb { \theta } } [ \ell _ { \pmb { \theta } } ( \rho , O ) ] = \frac { P _ { \mathcal { M } } ( \rho ) P _ { \mathcal { M } } ( O ) } { \mathrm { d i m } ( \mathcal { M } ) } .
$$  

To finish, let us note that in order to obtain Eq. (12) we assumed that the circuit is deep enough so that it forms a 2-design over $e ^ { { \mathfrak { g } } }$ . The number of layers $L$ for which this assumption is satisfied can be quantified via the results in [37, 38]. Interestingly, one can still bound how much the variance will deviate from that in Eq. (12) when the circuit is not a 2-design (see for instance Box 6 and the results in [37, 38, 78, 79]). Moreover, for shallow circuits where Eq. (12) does not hold, the computation of the loss function variance can become more intricate but a similar idea holds: If the circuit’s adjoint action can move the measurement operator (or the initial state) in an exponentially large subspace of operator space, then a BP can arise.  

# C. Input states and measurements  

While highly expressive PQC bear significant fault in leading to BPs, there exist cases where circuits with small expressiveness can still exhibit BPs depending on who the initial state and measurement operator are. For instance, let us consider Eq. (12) again, and assume that the ensuing DLA is not exponentially large, i.e., that we have $\dim ( { \mathfrak { g } } ) \in { \mathcal { O } } ( { \mathrm { p o l y } } ( n ) )$ . This case will arise when $\mathcal { U } _ { \theta }$ has strong inductive biases such as encoding some symmetries (see below for specific examples). Now, despite using a circuit with a non-exponentially large underlying algebra, the loss can still be exponentially concentrated. For instance, if we pick $O$ belonging to an exponentially large module (i.e., small algebras can admit exponentially large modules), then one will find exponential concentration from the denominator in Eq. (12). However, if $O$ belongs to a module whose dimension grows only polynomially with $n$ , such as $i O \in { \mathfrak { g } }$ , we will have that the variance’ denominator $\dim ( { \mathcal { M } } ) = \dim ( { \mathfrak { g } } )$ only vanishes polynomially with $n$ , meaning that the circuit expressiveness cannot be responsible for a BP. In fact, in this case the loss is obtained by comparing objects that live in a polynomially large subspace, thus potentially avoiding the curse of dimensionality. However, a deterministic BP is still possible if the norm of the projection of $\rho$ into the module is exponentially small, i.e., if $\mathcal { P } _ { \mathcal { M } } ( \rho ) \in \mathcal { O } ( 1 / 2 ^ { n } )$ . In Box 7 we present an example of an inexpressive circuits for which a BP can arise either because $O$ belongs to an exponentially large $\mathcal { M }$ , or because $O$ is taken from a polynomial module but the state is ill-aligned.  

We refer the reader to Refs. [37, 39] for a conceptual discussion on the interplay between PQC, measurement and initial state. Therein, it was argued that the module purities ${ \mathcal { P } } _ { { \mathcal { M } } } ( { \boldsymbol { \rho } } )$ can be considered as generalized forms of entanglement [85, 86], thus providing operational meaning to those quantities. Moreover, Refs. [46, 47, 87–94] and [28, 34, 35, 95] studied problems where the measurement operator or the state, respectively, can determine the presence of a BP. Then, Ref. [96] showed that midcircuit measurements can change the amount of entanglement in the state and thus determine whether a BP will arise. For the case of variational quantum computing with classical data, it has been shown that the encoding scheme can lead to initial states that are ill-aligned with the PQC and measurement operator, further highlighting the importance of choosing a well-design data embedding scheme [40, 81, 97–101].  

# D. Noise  

While in the previous sections we have considered unitary circuits, it has been shown that the presence of hard  

# Box 7: Single-qubit rotation PQCs can have BPs  

Consider a unitary circuit composed of general single qubit gates as $\begin{array} { r } { U ( \pmb { \theta } ) = \prod _ { j = 1 } ^ { n } U _ { j } ( \pmb { \theta } _ { j } ) } \end{array}$ , where $U _ { j } ( \pmb \theta _ { j } )$ acts only on the $j$ -th qubit. Clearly, this circuit is fairly inexpressive as it doesn’t even generate entanglement. Moreover, its DLA is simply given by ${ \mathfrak { g } } = \oplus _ { j = 1 } ^ { n } { \mathfrak { s u } } ( 2 )$ , and thus of dimension $\dim ( { \mathfrak { g } } ) = 3 n \ \in \ { \mathcal { O } } ( { \mathrm { p o l y } } ( n ) )$ . Yet, we can see that if $O = Z ^ { \otimes n }$ , then the operator belongs to an exponentially large module composed of all $3 ^ { n }$ Pauli operators acting non trivially on all the qubits. Assuming that each $U _ { j } ( \pmb \theta _ { j } )$ forms a 2- design over $\mathbb { S } \mathbb { U } ( 2 )$ [37], a direct calculation reveals that for all $\rho$ , $\operatorname { V a r } _ { \pmb { \theta } } [ \ell _ { \pmb { \theta } } ( \rho , O ) ] \ \in \ \mathcal { O } ( 1 / 2 ^ { n } )$ . However, if $O = Z _ { 1 }$ , then the measurement belongs to a module composed of the 3 Pauli operators acting on the first qubit. Now, Eq. (12) leads to $\mathrm { V a r } _ { \theta } [ \ell _ { \theta } ( \rho , O ) ] = 2 ( \mathrm { T r } [ \rho _ { 1 } ^ { 2 } ] - 1 / 2 ) / 3$ , with $\rho _ { 1 }$ the reduced state of $\rho$ on the first qubit. The choice of measurement operator can lead to a BP. When $O = Z ^ { \otimes n }$ the loss is obtained by comparing operators that can move in exponentially large modules, while for $O = Z _ { 1 }$ , the circuit can only move in the subspace of operators acting only on the first qubit. However, while the PQC and measurements are not responsible for BPs, the initial state can lead to an exponentially concentrated loss. In particular, if the state is too entangled and $( \mathrm { T r } [ \rho _ { 1 } ^ { 2 } ] - 1 / 2 ) \in \mathcal { O } ( 1 / 2 ^ { n } )$ (which occurs if $\rho$ satisfies a volume law of entanglement [102]), then a deterministic BP occurs.  

ware noise acting throughout the PQC can significantly affect the loss function optimization landscape, even being able to induce BPs [25, 65–68, 103]. When modeling noise it is standard to assume that the channels $\mathcal { U } _ { \pmb { \theta } _ { l } }$ are composed of a unitary parametrized gates followed by some noise channel $\mathcal { N }$ as $\mathcal { U } _ { \pmb { \theta } _ { l } } = \mathcal { N } ( U _ { l } ( \pmb { \theta } _ { l } ) \rho U _ { l } ^ { \dagger } ( \pmb { \theta } _ { l } ) )$ . In this picture, it has been shown that any noisy PQC whose noise channels have the maximally mixed state as its fixed point (i.e., such that $\mathcal { N } \circ \cdot \cdot \cdot \circ \mathcal { N } ( \rho ) \sim \mathbb { 1 } / 2 ^ { n } )$ will induce a deterministic BP in the large number of layers regime (see Box 8 for an example with global depolarizing noise) [25, 103]. Importantly, we note that recent results have shown that non-unital noise can alleviate the BP issue, indicating that the interplay between noise and loss function concentration is still an open research question [65, 80, 104, 105].  

While not obvious a priori, one can see that noiseinduced BPs still count as a form of curse of dimensionality [80] by recalling that noisy processes can always be understood as an entangling operation with some environment of $n _ { E }$ qubits. Thus, if we purify [74, 106] the noisy state $\rho _ { \pmb { \theta } } = \mathrm { T r } _ { E } [ V _ { \pmb { \theta } } ( | 0 \rangle \langle 0 | ^ { \otimes n _ { E } } \otimes \rho ) V _ { \pmb { \theta } } ^ { \dagger } ]$ we see that the loss becomes $\ell _ { \pmb { \theta } } ( \rho , O ) = \langle V _ { \pmb { \theta } } ( | 0 \rangle \langle 0 | ^ { \otimes n _ { E } } \otimes \rho ) V _ { \pmb { \theta } } ^ { \dagger } , \mathbb { 1 } _ { E } \otimes O \rangle$ , with  

Let us consider a noisy PQC which we model as unitary gates interleaved with global depolarizing noise channels $\mathcal { N } ( \rho ) = ( 1 - p ) \rho + p \mathbb { 1 } / 2 ^ { n }$ [74, 106]. That is, we will write $\begin{array} { r c l } { { \mathcal { U } _ { \pmb { \theta } _ { l } } ( \rho ) } } & { { = } } & { { ( 1 \mathrm { ~ - ~ } } } \end{array}$ $p ) U ( \pmb { \theta } _ { l } ) \rho U ^ { \dagger } ( \pmb { \theta } _ { l } ) + p \mathbb { 1 } / 2 ^ { n }$ , which indicates that with probability $( 1 - p )$ the unitary gate acts, whereas with probability $p$ the state is fully depolarized into the maximally mixed state. One can readily verify that the loss becomes $\ell _ { \pmb { \theta } } ( \rho , O ) =$ $( 1 - p ) ^ { L } \mathrm { T r } [ U ( \pmb { \theta } ) \rho U ^ { \dag } ( \pmb { \theta } ) O ] + ( 1 - ( 1 - p ) ^ { L } ) \mathrm { T r } [ O ] / 2 ^ { n }$ , implying a deterministic concentration (towards $\operatorname { T r } [ O ] / 2 ^ { n } )$ as the circuit’s depth $L$ increases or as the depolarizing probability becomes one. Note that this noise-induced BP will appear irrespective of whether the noiseless loss function is exponentially concentrated or not. That is, even if the noise-free loss landscape does not have BP i.e., $\begin{array} { r } { \mathrm { ~  ~ \psi ~ } \sqrt { \mathrm { a r } _ { \pmb { \theta } } \left[ \mathrm { T r } [ U ( \pmb { \theta } ) \rho U ^ { \dag } ( \pmb { \theta } ) O ] \right] } ~ \in ~ \Omega ( 1 / \operatorname { p o l y } ( n ) ) } \end{array}$ , the presence of global depolarizing noise can lead to a BP if the circuit depth $L$ is sufficiently large.  

$\Im _ { E }$ the identity over the environment’s Hilbert space. Thus, noise leads to an extended PQC $V _ { \pmb { \theta } }$ which can act in a much larger space, and thus substantially aggravates the curse of dimensionality. This intuition is supported from the fact that noise can be shown to increase a PQC’s channel expressiveness [80], as well as its capacity [37, 66, 107], thus further indicating that at the end of the day, noise-induced BPs constitutes a form of generalized expressiveness-induced BP.  

# V. BARREN-PLATEAU-PRONE ARCHITECTURES  

In the previous section we have provided some intuition regarding the origins of BPs. In this section we review some BP-prone architectures. We note that in most cases it has been shown that deep versions of these architectures can lead to exponentially concentrated losses, as their DLAs are exponentially large. However, this does not mean that they cannot be useful in their shallow forms, or when leveraging smart initialization strategies.  

problem one is trying to solve, but because they are easy to implement on the device (e.g., one only picks entangling gates that follow the device connectivity to avoid compiling overhead [87, 109, 110]). Under quite general conditions, hardware efficient ansatzes will be universal [18], meaning that their DLA is ${ \mathfrak { g } } = { \mathfrak { s u } } ( 2 ^ { n } )$ . As such, deep versions of these circuits are as expressive as possible, and therefore will exhibit BPs irrespective of the initial state or measurement operator [8, 33, 46, 111– 114]. Here it is important to note that several standard architectures exist in the literature, such as entangling gates acting on a brick-like fashion on alternating pairs of qubits or on a ladder. While these can indeed exhibit different performances in their shallower forms, they will ultimately lead to a BP if the number of layers is too large.  

# B. Some problem-inspired ansatzes  

Just like hardware efficient is used for a wide-range of problem-agnostic circuits, problem-inspired ansatzes is a blanket term used for models which embed inductive biases about the problem at hand into the circuit architecture itself. In most cases, problem-inspired ansatzes change the circuit expressiveness by using particular sets of gates, entangling gate topologies, or by correlating the parameters to respect some symmetry of the task. Here, one can generally expect that $\mathfrak { g }$ will be smaller than $\mathfrak { s u } ( 2 ^ { n } )$ [37]. While we will present below circuits which can in fact sufficiently reduce the circuit expressive power to potentially avoid BPs, we here discuss probleminspired architectures that can still have exponentially concentrated loss functions despite their inductive biases.  

Hamiltonian Variational Ansatz (HVA). HVAs [18, 63, 115, 116] were originally introduced within the context of the variational quantum eigensolver [13]. Here, the goal is to find the ground state of a Hamiltonian $H$ which we assume can be expressed as a sum of non commuting terms $\begin{array} { r } { H = \sum _ { s } H _ { s } } \end{array}$ (i.e., $[ H _ { s } , H _ { s ^ { \prime } } ] ~ \neq ~ 0$ if $s \neq s ^ { \prime }$ ). The HVA circui iPs expressed as $\begin{array} { r } { U ( \pmb { \theta } ) = \prod _ { l } \prod _ { s } e ^ { - i \theta _ { l s } H _ { s } } } \end{array}$ , from where we can clearly see that the ensuing DLA will strongly depend on what the Hamiltonian of interest $H$ is. As such, the presence or absence of BPs will be extremely problem-dependent and a case-by-case analysis will be needed. For instance, recent results have shown that the vast majority of one-dimensional translational invariant Hamiltonians $H$ will lead to exponentially large algebras and thus to BPs [70].  

# A. Hardware efficient ansatz and deep unstructured circuits  

Hardware efficient ansatz [108] is a generic term employed for unstructured PQCs composed of single-qubit rotation gates interleaved with (fixed or parametrized) entangling gates. The term hardware-efficient spans from the fact that the gates of the circuit are chosen not because they encode some strong inductive bias from the  

Quantum Alternating Operator Ansatz (QAOA). The QAOA [117, 118] is widely used to variationally attempt to solve combinatorial optimization problems. Here, one wishes to find the ground state of a Hamiltonian $H _ { P }$ that is diagonal in the computational basis and which encodes in its ground state the solution to some combinatorial problem of interest. The QAOA is constructed as $\begin{array} { r } { U ( \pmb { \theta } ) = \prod _ { l } e ^ { - i \theta _ { l M } H _ { M } } e ^ { - i \theta _ { l P } H _ { P } } } \end{array}$ , where $H _ { M }$ is the so-called mixer Hamiltonian [119–121] usually taken to be $\begin{array} { r } { H = \sum _ { j = 1 } ^ { n } X _ { j } } \end{array}$ . The presence of BPs in QAOA for maximum-cut tasks has been heuristically analyzed in [18, 122], where it has been shown that the loss will exhibit a BP for the vast majority of graphs if the circuit is deep enough. Moreover, this exponential concentration phenomenon has been also studied in QAOA variants such as the multi-angle QAOA where every gate in the circuit is assigned its own parameter [123]. Here, one can show that for all graphs with non-trivial cut value the circuit’s DLA dimension will be exponentially large implying that the loss will be concentrated [124]. Notably, the onset of BPs can even occur at depth-one for certain types of problems [124].  

Unitary Coupled Singles and Doubles (UCCSD) ansatz. The UCCSD is a problem inspired PQC used to find the ground state of a fermionic molecular Hamiltonian via the variational quantum eigensolver [13, 125, 126]. Here, it has been shown that standard versions of UCC composed of controlled single excitation gates are particle-conserving universal unitaries [55]. Indeed, this implies that deep UCCSD ansatzes will exhibit a BP if the state of interest lies within a subspace with a number of excitations that scales with the problem size [127].  

# VI. STRATEGIES TO AVOID OR MITIGATE BPS  

Ever since the discovery of BPs, there have been attempts to mitigate their detrimental effects. A common feature shared between the tools discussed here is that they break the assumptions which lead to the curse of dimensionality. First, we will review techniques which somehow transform the loss into the inner product between two objects that live in polynomially, rather than in exponentially, large spaces [11] (see Box 9). Second, we will review methods that mitigate BPs even when operating in exponentially large spaces. Here, we particularly refer the reader to the alternative initialization section, as this appear to be one of the most promising ways to jump start the training in a region of large gradients, rather than in a random place of the optimization landscape.  

# A. Shallow circuits  

Perhaps one of the simplest ways to avoid BPs is to employ shallow PQCs. Here, even if the loss is concentrated for sufficiently deep versions of the circuit (either because it is too expressive or due to the effects of noise), if the number of gates is sufficiently small one can expect that that the effect of BPs is reduced for certain measurements. Examples of such cases are logarithmicdepth hardware-efficient ansatzes [46, 56, 102] with local measurements, or quantum convolutional neural networks [48, 60] which are shallow by design. Intuitively, we can understand that these circuits avoid BPs—even  

# Box 9: Avoiding BPs by working in small spaces  

Recently it has been noted that several strategies to avoid BPs do so by effectively encoding the relevant dynamics in the loss function to a polynomially-large subspace of the operator space, thus avoiding the curse of dimensionality that causes BPs. As discussed in the main text, we can think about the adjoint action of a unitary PQC over the initial state $U ( \pmb \theta ) \rho U ^ { \dagger } ( \pmb \theta )$ , or the measurement operator $U ^ { \dagger } ( { \pmb \theta } ) O U ( { \pmb \theta } )$ as only being able to exactly, or mostly, explore a polynomially large subspace $B _ { \lambda }$ of $\boldsymbol { B } ( \mathcal { H } _ { 0 } )$ . That is $\dim ( B _ { \Lambda } ) \in { \mathcal { O } } ( { \mathrm { p o l y } } ( n ) )$ Denoting $A ^ { \lambda }$ as the projection of an operator onto $B _ { \lambda }$ , the loss function becomes $\ell _ { \pmb \theta } ( \rho , O ) = \langle \rho _ { \pmb \theta } ^ { \lambda } , O ^ { \lambda } \rangle = \langle \rho ^ { \lambda } , O _ { \pmb \theta } ^ { \lambda } \rangle$ . Here, it is extremely important to note that $B _ { \lambda }$ is not necessarily a module of the Lie group associated with the quantum circuit. As such, the loss function variance will not always be given by Eq. (12). Still, since we can only move in a small subspace, we can expect that when one varies the parameters the variance can decay polynomially as $\Omega ( 1 / \dim ( B _ { \lambda } ) )$ . Of course, the input state and the measurement operators still need to be well aligned with $B _ { \lambda }$ , as if either $\rho$ or $O$ have exponentially small component in the subspace, the loss will be deterministically concentrated.  

if randomly initialized—by constraining the space that the measurement operator can explore. These insights have been used in the literature to show that when using the variational quantum eigensolver, along with a shallow circuit, it is better to employ problem-encoding schemes that lead to more local measurement operators [47, 128].  

It is important to note that while shallow circuits can provably have non-exponentially vanishing gradients, this is only true for certain measurements (for instance, a logarithmic-depth hardware efficient ansatz can still exhibit a BP for a non-local measurement [46]. The question still remains as to whether a solution exists within the set of parameters. By using few gates, it is entirely possible that there is no optimal set of parameters [83]. This phenomenon is known as a reachability deficit [21]. In general, there is no simple way of knowing whether the model will suffer from reachability deficits and a case-bycase analysis is necessary.  

# B. Small dynamical Lie algebras  

While in the previous section we presented probleminspired ansatzes which are still too expressive to avoid exponential concentration, there do exist cases of PQCs whose DLA’s dimension only grows polynomially with the number of qubits. In these cases, and critically depending on the choice of initial state and measurement operator, the circuit’s adjoint action can lead to small modules and thus avoid expressiveness induced BPs according to Eq. (12) even in its deeper forms.  

One can attempt to reduce the circuit’s expressiveness by using circuits acting on smaller number of qubits [128– 130], or encoding sufficient inductive biases about the problem one is trying to solve. In this context, it has been recently pointed out that one particularly important source of inductive biases is the set of symmetries that a problem satisfies. That is, if one knows that the task’s solution must respect some symmetry, then one should construct a variational models that only explore the symmetry-respecting solution space [6, 45, 131– 136]. For instance, it has been shown that permutationequivariant unitary PQCs (circuits that remain invariant under qubit permutations) have a DLA whose dimension scales as $\mathcal { O } ( n ^ { 3 } )$ [40, 137]. Similarly, $U ( 1 )$ -equivariant circuits (i.e., circuits that preserve the Hamming weight of an initial state) have been shown to not exhibit BPs when they act on initial states whose Hamming weight does not scale with $n$ [18, 138, 139]. To finish, we note that [140] studied a PQCs where all the gates commute, meaning that the size of the algebra is exactly equal to the number of distinct gate generators.  

Another archetypal example of a PQC with small DLA is that of parametrized matchgate circuits (also known as Ising-model HVAs [139, 141–146]), where one can find that ${ \mathfrak { g } } \simeq { \mathfrak { s o } } ( 2 n )$ and therefore $\dim ( { \mathfrak { g } } ) \in { \mathcal { O } } ( n ^ { 2 } )$ [18, 39, 70, 147] (see Box 10). These circuits can be shown to have a reduced expressive power as they can only implement free-fermionic evolutions. Along a similar line, it has been shown that linear optics continuous variable PQCs acting on coherent light in $n$ modes have an associated algebra ${ \mathfrak { g } } \simeq { \mathfrak { o } } ( 2 n )$ [148, 149] thus also potentially leading to BP-free loss functions. Interestingly, these models have also been explored in classical machine learning, where they are known as efficient unitary neural networks [150, 151], and where it was already noted that they do not exhibit vanishing gradients at large depths.  

While the previous examples showcase the fact that one can indeed find circuits with polynomial-sized DLAs, and thus to their action being reducible to some polynomial-sized space, it is worth noting that such cases are quite rare. In fact, as previously mentioned most circuits will exhibit exponentially large algebras [18, 70, 124], and the quest for other PQC architectures with small algebras is an open area of research.  

# Box 10: Its all relative (to the modules)  

Until very recently, the BP study was performed on a case-by-case basis, analyzing one architecture at a time. While such fragmented patchwork analysis was pivotal to the recent development of a unified theory of BPs [37], it lead to several misconceptions in the literature where lessons learnt in one architecture are taken to be generically true and extrapolated to other PQCs. These include claim such as “global” measurements such as $O _ { G } = X _ { 1 } Z _ { 2 } \cdot \cdot \cdot X _ { n - 1 } Z _ { n }$ (i.e., when $O$ act nontrivially on all the qubits) lead to BPs, while local measurements such as ${ \cal O } _ { L } \ : = \ : X _ { n / 2 }$ (i.e., we measure just one qubit) prevent them. While these claims are indeed true for certain architectures such as logarithmic depth hardware efficient ansatzes [35, 46, 102] (or to the single-qubit rotation ansatz of Box 7), they are certainly not generically true. For instance, a parametrized matchgate circuit can avoid BPs with $O _ { G }$ , but will lead to exponentially concentrate loss functions if we pick $O _ { L }$ [39]. To understand why those two cases behave so differently, we need to (again) study the module, or subspace, that the adjoint action of the PQC leads to when action over $O$ (see also Box 9). For the single qubit rotation ansatz, $O _ { G }$ belongs to an exponentially large module, while $O _ { L }$ to a polynomially small one. This statement is fully reversed in the matchgate case as $O _ { G }$ is in a small module, but $O _ { L }$ in an exponentially large one.  

able to employ highly-expressive gate-sets by only exploring specific regions of the ansatz architecture hyperspace. Several machine learning-aided strategies have been employed within this context such as evolutionary algorithms, short-depth compilation algorithms, automachine learning, among others [152–164]. In particular, we highlight that one of the most-promising and well-studied variable ansatz strategy is the ADAPTVQE [154, 155, 165, 166]. Here, one constructs a circuit for quantum chemistry applications with gates based on the UCCSD ansatz, and it has been shown that such strategy can lead to hardware-friendly circuits with few entangling gates which can be constrained to only explore regions of large gradients and potentially reach good solutions.  

# C. Variable structure ansatzes  

Variable structured PQCs attempt to mitigate expressiveness and noise induced BPs by leveraging classical machine learning protocol to iteratively grow the quantum circuit by placing (and removing) gates that empirically lower the loss function and keep large gradients during training. As such, these approaches are  

# D. Alternative initialization strategies  

Perhaps the greatest limitation of studying probabilistic BPs by analyzing the scaling of $\operatorname { V a r } _ { \pmb { \theta } } [ \ell _ { \pmb { \theta } } ( \rho , O ) ]$ or $\mathrm { V a r } _ { \pmb { \theta } } [ \partial _ { \mu } \ell _ { \pmb { \theta } } ( \rho , O ) ]$ is that these quantities can only tell us how concentrated the loss is in average across the whole landscape. Such information is of course important as it allows us to understand what to expect if we randomly initialize the model’s parameters. However, it is well known that randomly initializing the parameters is never a good idea, and that the initialization method can be critical in determining the model’s ultimate performance.  

Motivated by this observation, several strategies to initialize a variational quantum computing scheme have been proposed. These include restricted small angle initializations (where one still randomly initializes the parameters but in a more structured way, such as in a special region) [61, 62, 64, 167–175], pre-training via classical [154, 176–178] or quantum [179, 180] methods, parameter transfer [181–189] and iterative learning strategies [24]. For the latter case, we note that analogous sample complexity lower bounds have been derived for alternative iterative variational approaches via the McLachlan principle [190, 191].  

Here we highlight that some of these initialization strategies have shown heuristic success and appear to be one of the most promising avenues to mitigate BPs. In fact, classical machine learning models can exhibit vanishing gradients issues just like their quantum counterpart (see below for more on this), and smartinitializations have been one of the main tools used therein. Hence, we expect that warm-start will play a fundamental role in the future of variational quantum computing. Still, care must be taken, as several limitations to these strategies have been reported [192, 193]. These include concerns such as the fact that one can initialize in a region which might have large gradients, but is not well connected to a global minima, or that might be classically simulable [11]. Indeed several workarounds have been proposed to prevent a smartly initialized circuit to run into problematic regions of the optimization landscape (such as monitoring the reduced state’s entanglement [28], or using state-of-the-art optimizers [19, 194–196]).  

# VII. STRATEGIES THAT CANNOT AVOID BPS  

While many promising strategies have been devised to prevent or alleviate BPs, some intuitive approaches can fail due as they are unable to change the underlying reasons why a BP appears. Here we review two such strategies.  

Changing the optimization method. It is common to find claims that it is possible to navigate a BP landscape (when randomly initialized) by changing the optimization method. For instance, one can assume that since the gradients are suppressed, one could find a loss minimizing direction by either using higher-order gradient information, gradient-free methods, quantum naturalgradients [197, 198] or other quantum fisher informationbased schemes. However, it has been shown that such approaches can still require an exponential number of shots to obtain a loss minimizing direction [23, 97, 199].  

Error mitigation and noise-induced BPs. Given the critical limitations that noise imposes on variational quantum algorithms, it is interesting to determine whether error mitigation techniques [200–202] are capable of sufficiently denoising the loss function to find a loss minimizing direction. Unfortunately, it has been shown that in the worse case an exponential number of resources need to be expended to mitigate the effect of noise-induced BPs [103, 203–205] even when the circuit is of sub-logarithmic depth for a class of worst-case circuits [206].  

# VIII. EXPONENTIAL CONCENTRATIONELSEWHERE  

The study of BPs has thus been informed by, and in turn impacted, work in other areas of quantum computing and machine learning. Here we briefly review some of these results.  

Quantum generative models. In quantum generative modeling one aims to optimize a quantum state such that its probability distribution (resulting from some set of measurements) matches a target distribution [54, 207– 216]. Quantum generative models are intrinsically implicit models as they only provide access to samples (and not the distributions themselves as do explicit models). Correspondingly, one can draw a distinction between explicit losses, which are formulated explicitly in terms of the model and target probabilities, and implicit losses, which compare samples from the model and the training distribution [212]. The tension between using an explicit loss, such as the Kullback-Leibler divergence, with an implicit model, was shown in [90, 212] for quantum circuit Born machines to lead to a BP. Conversely, implicit losses, such as the maximum mean discrepancy [210, 212] and some losses used for quantum generative adversarial networks [54, 90], can be viewed as the expectation value of an observable and so can be analyzed with the tools discussed previously.  

BPs have also been studied in quantum Boltzmann machines [215], where one uses trains a parametrized thermal quantum state model by minimizing a log-likelihood loss. In this case, the landscape is convex and so it is possible to derive convergence guarantees with only polynomial sample complexity [216]. Inherently quantum strategies for generative modelling also pose an interesting avenue for finding quantum generative models that avoid BPs [212, 217].  

Kernel-based quantum algorithms. Exponential concentration also arises in kernel-based schemes [98, 218– 225], where a quantum computer is used as an enhanced feature-space from which inner products or similarity measures between the data states can be estimated ( $a$ la kernel trick) [226–228]. Unlike PQCs, one strength of kernel methods is the optimal model is guaranteed to be found after training due to the convex training landscape. However, with exponentially concentrated kernels, this leads to a trivial model where the predictions on unseen inputs are independent of the training data inputs [219].  

Quantum optimal control. While exponentially concentrated optimization landscapes can arise in quantum optimal control schemes [229], their presence had not been truly studied until the BP literature started blossoming. The intrinsic connection between these two areas of study was first reported in [230] and later in [231]. Then, in [18] it was noted that tools from quantum optimal control, such as the study of the PQC’s DLA, can be used to diagnose BPs. Nowadays, the cross-fertilzation goes the other way and tools developed in variational quantum computing have been used in optimal control schemes [232, 233].  

Trainability of tensor networks. Several works have studied BPs in tensor networks. For instance, it has been shown that tensor network based machine learning models exhibit BPs for global but not local losses [50], and isometric tensor network states can avoid BPs [52]. Similarly, BPs have been studied in quantum circuits inspired by matrix product states, tree tensor networks, and the multiscale entanglement renormalization ansatz [51, 234].  

Learnability. Insights from BPs have allowed researchers to present no-go theorems on the learnability of typical scrambling unitaries (i.e., typical random unitaries) [53, 235]. There is an interesting commonality between these results and statistical query learning [236] no-go theorems for learning typical random unitaries [237–239]. These no-go theorems can be viewed more exotically as having implications for the learnability of black hole dynamics [235, 240]. Here it is worth noting that while BPs are fundamentally an architecture dependent phenomenon (but can be applied to a wide range of problems), tools from statistical query learning can provide architecture-independent bounds for a given learning problem [239].  

# IX. BEYOND BARREN PLATEAUS IN VARIATIONAL QUANTUM COMPUTING  

The study of BPs is clearly not the end of the story if one wants to guarantee that a variational quantum computing model can achieve a quantum advantage. Here we briefly review some of the other questions that need to be addressed.  

Local minima & solution quality. It has been shown that quantum landscapes can be plagued with exponentially many sub-optimal local minima, potentially making their optimization extremely hard [14, 15, 17, 19, 241– 243]. Critically, the local minima issue appears for both shallow circuits where no BP arises but where no good solutions exist, but also in deep circuits where a solution might exist but reaching it is still computationally difficult. Indeed, we know that getting rid of the local minima by overparametrizing the PQC requires exponentially deep circuits [83, 193, 229, 244]. This realization actually leads to a very powerful insight: If we have a randomly initialized circuit with $\mathcal { O } ( \mathrm { p o l y } ( n ) )$ parameters which has a BP because the DLA is exponentially large (and thus the PQC is not overparametrized [83]), even if we allow for infinite measurements and get rid of the sampling noise, we will still not be able to train since the optimizer is exponentially likely to get stuck in a local minima [83, 193, 229, 244]. Addressing the problem of local minima will be fundamental to guarantee that a model will be useful (this will be particularly important when using alternative-initialization techniques).  

Classical simulability of BP-free models. As we have discussed above, many existing techniques that avoid the BP curse of dimensionality do so by encoding the relevant dynamics in some subspace that grows polynomially in the number of qubits $n$ . Since an exponentially large space is no longer being used, one should be able to simulate the information processing ability of the PQC, and thus classically estimate the loss function [11, 245] (note that for non-classically simulable initial states or measurements the quantum computer may be needed to gather some data in an initial data collection phase). Not having to implement a PQC on the quantum device could lead to algorithms that are better suited for near-term quantum computers as the data acquisition phase could be less noisy than fully implementing the parametrized quantum circuit [11, 246, 247]. Elsewhere, recent results have showcased that classical algorithms, when equipped with measurements from a quantum computer, can be significantly more powerful than their fully classical counterparts [218, 248–251].  

# X. LINK WITH VANISHING GRADIENT PROBLEM  

One of the first questions one raises when studying BP phenomena is the relationship to vanishing gradient problems in classical deep learning. Identifying a precise connection and closest analogy between these vanishing gradients has been challenging, in part because there are many different types of vanishing gradients classically. Here we review some of the potential similarities and differences that may lead to a multitude of perspectives.  

Differences between the two phenomena. The most well-known variety of classical vanishing gradients is likely the vanishing or exploding norm with depth, caused by growing or shrinking weights due to the activation functions [252]. This phenomenon is already conceptually different than that of BPs in variational quantum computing as (non-noise-induced) BPs arise as the number of qubits increases, not as the depth increases. In fact, for fixed number of qubits there comes a point where adding more layers does not worsen the concentration phenomenon [37, 38].  

Classical solutions to vanishing gradients. Many solutions to the vanishing of classical gradients with depth have been developed, including batch normalization [253], layer-wise normalization [254] and the use of  

Re-LU activation functions [255], although it is worth mentioning that these techniques might still fail in some situations [256, 257]. As these solutions target the problem of shrinking weights at large depths due to the nonlinearity of the activation function, these solutions seem not be directly applicable to the quantum case.  

Some deep classical neural networks still suffer problems even with the solutions listed above. In particular, formally infinite depth networks, such as recurrent neural networks used in language or speech processing, can suffer related problems [256]. Some solutions to this problem are skip-connections, residual connections, and general LSTM models [257]. This strategies could be emulated using simultaneous preparations of quantum states, but direct copying is challenging due to the no-cloning theorem.  

Finally, restricted angle range initialization strategies have been shown to lead to larger initialization gradients and can be implemented in both classical and quantum contexts [61–64, 175]. However, given that quantum landscapes have been shown to have numerous local minima in under-parameterized regimes [16] this strategy might be seen as less promising in quantum contexts.  

The role of exponentially large spaces. A common refrain when discussing concentration of measure is that the dimension of quantum spaces can be so much larger than classical spaces. However, there are subtleties here, as classical models can also explore exponentially large spaces (e.g., one can classically optimize a $2 ^ { n }$ - dimensional probability distribution on $n$ bits via probabilistic parametrized bit-flips). Thus, it seems that it is not the size of the space explored by quantum models per se but rather how the space is used. Indeed, it is the hope of quantum computing that we may find novel, less computationally intense, paths through similar spaces of problems. Hence, it appears that a key step forward in resolving the problem of BPs is not just a powerful ansatz, but a parametrization that has a natural bias towards problems of interest.  

The cost of precision. In the classical case, the value of the loss and gradient can often be obtained to precision $\epsilon$ with resources that scale as ${ \mathcal { O } } ( \log ( 1 / \epsilon ) )$ , so that even if a gradient is rapidly vanishing, we may compensate with additional precision. In contrast, for most quantum models based on expected values, we are relegated to stochastic sampling at a cost of $1 / \epsilon ^ { 2 }$ or $1 / \epsilon$ with clever use of amplitude amplification. This means that any problems of decaying gradients are often exponentially magnified in the quantum case. However, there are classical models in use that make use of stochastic sampling to determine the value of a loss and gradient [258, 259], so quantum models are not entirely alone in this difficulty. An interesting strategy to sidestep this problem in quantum models could be to take inspiration from the classical side and move away from expected values to most-likely readout values or another deterministic output that can be reliably determined to high precision without sampling.  

# XI. IMPLICATIONS AND OUTLOOK  

The concerted effort by the community has led to significant advancements in the understanding of the BP phenomenon. The primary causes of the phenomenon, which in essence all stem from a curse of dimensionality, have been pinned down. Similarly, progress has been made in identifying architectures that circumvent BPs. However, the ultimate impact of the BP phenomenon on the scalability of variational quantum models is still to be determined.  

The questions that remain largely boil down to the fact that the BP phenomenon is an average case notion. If a landscape has a BP then the probability that any randomly chosen parameter setting has non-negligible loss gradients is exponentially small. Given that quantum loss functions are precision limited, i.e., they are estimated from a finite number of measurement shots, the probability of obtaining an informative signal at any randomly chosen parameter setting is exponentially small. Nonetheless, in exponentially narrow regions, a BP landscape can exhibit substantial gradients. The question is whether these regions are easy to find, and even if we find them whether they are useful and can allow for meaningful training. Addressing this will require understanding the complex interplay between the BPs, local minima, expressivity limitations and, most importantly, classical simulability.  

At this point, we are thus faced with the question of what next? Classical machine learning has been driven by heuristics and is substantially more successful than what would be expected from theoretical analysis. However, we do not yet have the quantum hardware needed to perform large-scale heuristic implementations of variational quantum models. Indeed, the original motivation for studying BPs was to understand limitations that could prevent scaling variational models to non-classically simulable regimes. In this regard, the field of BP has successfully carved out settings where we know a quantum advantage will be unlikely. Still, much work remains to be done. We envision that the BP study will continue to serve as a guide to determine if, and where, a practical variational quantum advantage exists, and to hint towards the development of new variational methods that go beyond the standard procedure of Fig. 1.  

# ACKNOWLEDGEMENTS  

We thank Maria Kieferova, Pablo Bermejo, and Tom O’Leary for their feedback on our manuscript. M.L. was supported by the Center for Nonlinear Studies at Los Alamos National Laboratory (LANL). M.L. acknowledges support by the Laboratory Directed Research and Development (LDRD) program of LANL under project number 20230049DR. S.T. and Z.H. acknowledge support from the Sandoz Family Foundation-Monique de Meuron program for Academic Promotion. This research was partly supported (L.C.) by the Quantum Science Center, a National Quantum Science Initiative of the Department of Energy, managed by Oak Ridge National Laboratory. L.C. was also supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, under Computational Partnerships program. M.C. acknowledges support by LANL ASC Beyond Moore’s Law project and by LDRD program of LANL under project number 20230527ECR.  

[1] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii, J. R. McClean, K. Mitarai, X. Yuan, L. Cincio, and P. J. Coles, Variational quantum algorithms, Nature Reviews Physics 3, 625–644 (2021).   
[2] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea, A. Anand, M. Degroote, H. Heimonen, J. S. Kottmann, T. Menke, et al., Noisy intermediatescale quantum algorithms, Reviews of Modern Physics 94, 015004 (2022).   
[3] S. Endo, Z. Cai, S. C. Benjamin, and X. Yuan, Hybrid quantum-classical algorithms and quantum error mitigation, Journal of the Physical Society of Japan 90, 032001 (2021).   
[4] M. Schuld, I. Sinayskiy, and F. Petruccione, An introduction to quantum machine learning, Contemporary Physics 56, 172 (2015).   
[5] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd, Quantum machine learning, Nature 549, 195 (2017).   
[6] M. Cerezo, G. Verdon, H.-Y. Huang, L. Cincio, and P. J. Coles, Challenges and opportunities in quantum machine learning, Nature Computational Science 10.1038/s43588-022-00311-3 (2022).   
[7] A. Di Meglio, K. Jansen, I. Tavernelli, C. Alexandrou, S. Arunachalam, C. W. Bauer, K. Borras, S. Carrazza, A. Crippa, V. Croft, et al., Quantum computing for high-energy physics: state of the art and challenges. summary of the qc4hep working group, arXiv preprint arXiv:2307.03236 (2023).   
[8] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven, Barren plateaus in quantum neural network training landscapes, Nature Communications 9, 1 (2018).   
[9] H. Qi, L. Wang, H. Zhu, A. Gani, and C. Gong, The barren plateaus of quantum neural networks: review, taxonomy and trends, Quantum Information Processing 22, 435 (2023).   
[10] A. Arrasmith, Z. Holmes, M. Cerezo, and P. J. Coles, Equivalence of quantum barren plateaus to cost concentration and narrow gorges, Quantum Science and Technology 7, 045015 (2022).   
[11] M. Cerezo, M. Larocca, D. García-Martín, N. L. Diaz, P. Braccia, E. Fontana, M. S. Rudolph, P. Bermejo, A. Ijaz, S. Thanasilp, et al., Does provable absence of barren plateaus imply classical simulability? or, why we need to rethink variational quantum computing, arXiv preprint arXiv:2312.09121 (2023).   
[12] A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, and J. I. Latorre, Data re-uploading for a universal quantum classifier, Quantum 4, 226 (2020)   
[13] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.- Q. Zhou, P. J. Love, A. Aspuru-Guzik, and J. L. O’brien, A variational eigenvalue solver on a photonic quantum processor, Nature Communications 5, 1 (2014).   
[14] L. Bittel and M. Kliesch, Training variational quantum algorithms is NP-hard, Phys. Rev. Lett. 127, 120502 (2021).   
[15] E. Fontana, M. Cerezo, A. Arrasmith, I. Rungger, and P. J. Coles, Non-trivial symmetries in quantum landscapes and their resilience to quantum noise, Quantum 6, 804 (2022).   
[16] E. R. Anschuetz and B. T. Kiani, Beyond barren plateaus: Quantum variational algorithms are swamped with traps, Nature Communications 13, 7760 (2022).   
[17] E. R. Anschuetz, Critical points in quantum generative models, International Conference on Learning Representations (2022).   
[18] M. Larocca, P. Czarnik, K. Sharma, G. Muraleedharan, P. J. Coles, and M. Cerezo, Diagnosing Barren Plateaus with Tools from Quantum Optimal Control, Quantum 6, 824 (2022).   
[19] P. Bermejo, B. Aizpurua, and R. Orús, Improving gradient methods via coordinate transformations: Applications to quantum machine learning, Physical Review Research 6, 023069 (2024).   
[20] S. Sim, P. D. Johnson, and A. Aspuru-Guzik, Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms, Advanced Quantum Technologies 2, 1900070 (2019).   
[21] V. Akshay, H. Philathong, M. E. Morales, and J. D. Biamonte, Reachability deficits in quantum approximate optimization, Physical Review Letters 124, 090504 (2020).   
[22] Q. Miao and T. Barthel, Equivalence of cost concentration and gradient vanishing for quantum circuits: an elementary proof in the riemannian formulation, arXiv preprint arXiv:2402.07883 (2024).   
[23] M. Cerezo and P. J. Coles, Higher order derivatives of quantum neural networks with barren plateaus, Quantum Science and Technology 6, 035006 (2021).   
[24] R. Puig i Valls, M. Drudis, S. Thanasilp, and Z. Holmes, Variational quantum simulation: a case study for understanding warm starts, arXiv preprint arXiv:2404.10044 https://doi.org/10.48550/arXiv.2404.10044 (2024).   
[25] S. Wang, E. Fontana, M. Cerezo, K. Sharma, A. Sone, L. Cincio, and P. J. Coles, Noise-induced barren plateaus in variational quantum algorithms, Nature Communications 12, 1 (2021).   
[26] J. L. Cybulski and T. Nguyen, Impact of barren plateaus countermeasures on the quantum neural network capacity to learn, Quantum Information Processing 22, 442 (2023).   
[27] A. Pérez-Salinas, H. Wang, and X. Bonet-Monroig, Analyzing variational quantum landscapes with information content, npj Quantum Information 10, 27 (2024).   
[28] S. H. Sack, R. A. Medina, A. A. Michailidis, R. Kueng, and M. Serbyn, Avoiding barren plateaus using classical shadows, PRX Quantum 3, 020365 (2022).   
[29] S. Okumura and M. Ohzeki, Fourier coefficient of parameterized quantum circuits and barren plateau problem, arXiv preprint arXiv:2309.06740 (2023).   
[30] N. A. Nemkov, E. O. Kiktenko, and A. K. Fedorov, Fourier expansion in variational quantum algorithms, Phys. Rev. A 108, 032406 (2023).   
[31] M. Stęchły, L. Gao, B. Yogendran, E. Fontana, and M. Rudolph, Connecting the hamiltonian structure to the qaoa energy and fourier landscape structure, arXiv preprint arXiv:2305.13594 (2023).   
[32] C. Dankert, R. Cleve, J. Emerson, and E. Livine, Exact and approximate unitary 2-designs and their application to fidelity estimation, Physical Review A 80, 012304 (2009).   
[33] K. Sharma, M. Cerezo, L. Cincio, and P. J. Coles, Trainability of dissipative perceptron-based quantum neural networks, Physical Review Letters 128, 180505 (2022).   
[34] T. L. Patti, K. Najafi, X. Gao, and S. F. Yelin, Entanglement devised barren plateau mitigation, Physical Review Research 3, 033090 (2021).   
[35] C. O. Marrero, M. Kieferová, and N. Wiebe, Entanglement-induced barren plateaus, PRX Quantum 2, 040316 (2021).   
[36] Z. Holmes, A. Arrasmith, B. Yan, P. J. Coles, A. Albrecht, and A. T. Sornborger, Barren plateaus preclude learning scramblers, Physical Review Letters 126, 190501 (2021).   
[37] M. Ragone, B. N. Bakalov, F. Sauvage, A. F. Kemper, C. O. Marrero, M. Larocca, and M. Cerezo, A unified theory of barren plateaus for deep parametrized quantum circuits, arXiv preprint arXiv:2309.09342 (2023).   
[38] E. Fontana, D. Herman, S. Chakrabarti, N. Kumar, R. Yalovetzky, J. Heredge, S. Hari Sureshbabu, and M. Pistoia, The adjoint is all you need: Characterizing barren plateaus in quantum ansätze, arXiv preprint arXiv:2309.07902 (2023).   
[39] N. L. Diaz, D. García-Martín, S. Kazi, M. Larocca, and M. Cerezo, Showcasing a barren plateau theory beyond the dynamical lie algebra, arXiv preprint arXiv:2310.11505 (2023).   
[40] L. Schatzki, M. Larocca, Q. T. Nguyen, F. Sauvage, and M. Cerezo, Theoretical guarantees for permutationequivariant quantum neural networks, npj Quantum Information 10, 10.1038/s41534-024-00804-1 (2024).   
[41] J. Liu, Z. Lin, and L. Jiang, Laziness, barren plateau, and noise in machine learning, Machine Learning: Science and Technology 5, 015058 (2024).   
[42] M. T. West, J. Heredge, M. Sevior, and M. Usman, Provably trainable rotationally equivariant quantum machine learning, arXiv preprint arXiv:2311.05873 (2023).   
[43] D. García-Martín, M. Larocca, and M. Cerezo, Deep quantum neural networks form gaussian processes, arXiv preprint arXiv:2305.09957 (2023).   
[44] A. A. Mele, Introduction to haar measure tools in quantum information: A beginner’s tutorial, arXiv preprint arXiv:2307.08956 (2023).   
[45] M. Ragone, Q. T. Nguyen, L. Schatzki, P. Braccia, M. Larocca, F. Sauvage, P. J. Coles, and M. Cerezo, Representation theory for geometric quantum machine learning, arXiv preprint arXiv:2210.07980 (2022).   
[46] M. Cerezo, A. Sone, T. Volkoff, L. Cincio, and P. J. Coles, Cost function dependent barren plateaus in shallow parametrized quantum circuits, Nature Communications 12, 1 (2021).   
[47] A. Uvarov and J. D. Biamonte, On barren plateaus and cost function locality in variational quantum algorithms, Journal of Physics A: Mathematical and Theoretical 54, 245301 (2021).   
[48] A. Pesah, M. Cerezo, S. Wang, T. Volkoff, A. T. Sornborger, and P. J. Coles, Absence of barren plateaus in quantum convolutional neural networks, Physical Review X 11, 041011 (2021).   
[49] V. Heyraud, Z. Li, K. Donatella, A. L. Boité, and C. Ciuti, Efficient estimation of trainability for variational quantum circuits, PRX Quantum 4, 040335 (2023).   
[50] Z. Liu, L.-W. Yu, L.-M. Duan, and D.-L. Deng, The presence and absence of barren plateaus in tensornetwork based machine learning, Physical Review Letters 129, 270501 (2022).   
[51] T. Barthel and Q. Miao, Absence of barren plateaus and scaling of gradients in the energy optimization of isometric tensor network states, arXiv preprint arXiv:2304.00161 (2023).   
[52] Q. Miao and T. Barthel, Isometric tensor network optimization for extensive hamiltonians is free of barren plateaus, arXiv preprint arXiv:2304.14320 (2023).   
[53] R. J. Garcia, C. Zhao, K. Bu, and A. Jaffe, Barren plateaus from learning scramblers with local cost functions, Journal of High Energy Physics 2023, 1 (2023).   
[54] A. Letcher, S. Woerner, and C. Zoufal, Tight and efficient gradient bounds for parameterized quantum circuits, arXiv preprint arXiv:2309.12681 (2023).   
[55] J. M. Arrazola, O. Di Matteo, N. Quesada, S. Jahangiri, A. Delgado, and N. Killoran, Universal quantum circuits for quantum chemistry, Quantum 6, 742 (2022).   
[56] H.-K. Zhang, S. Liu, and S.-X. Zhang, Absence of barren plateaus in finite local-depth circuits with long-range entanglement, Physical Review Letters 132, 150603 (2024).   
[57] J. Napp, Quantifying the barren plateau phenomenon for a model of unstructured variational ansätze, arXiv preprint arXiv:2203.06174 (2022).   
[58] P. Braccia, P. Bermejo, L. Cincio, and M. Cerezo, Computing exact moments of local random quantum circuits via tensor networks, arXiv preprint arXiv:2403.01706 (2024).   
[59] H.-Y. Hu, A. Gu, S. Majumder, H. Ren, Y. Zhang, D. S. Wang, Y.-Z. You, Z. Minev, S. F. Yelin, and A. Seif, Demonstration of robust and efficient quantum property learning with shallow shadows, arXiv preprint arXiv:2402.17911 (2024).   
[60] C. Zhao and X.-S. Gao, Analyzing the barren plateau phenomenon in training quantum neural networks with the ZX-calculus, Quantum 5, 466 (2021).   
[61] K. Zhang, L. Liu, M.-H. Hsieh, and D. Tao, Escaping from the barren plateau via gaussian initializations in deep variational quantum circuits, in Advances in Neural Information Processing Systems (2022).   
[62] Y. Wang, B. Qi, C. Ferrie, and D. Dong, Trainability enhancement of parameterized quantum circuits via reduced-domain parameter initialization, arXiv preprint arXiv:2302.06858 (2023).   
[63] C.-Y. Park and N. Killoran, Hamiltonian variational ansatz without barren plateaus, Quantum 8, 1239 (2024).   
[64] C.-Y. Park, M. Kang, and J. Huh, Hardware-efficient ansatz without barren plateaus in any depth, arXiv preprint arXiv:2403.04844 (2024).   
[65] A. Sannia, F. Tacchino, I. Tavernelli, G. L. Giorgi, and R. Zambrini, Engineered dissipation to mitigate barren plateaus, arXiv preprint arXiv:2310.15037 (2023).   
[66] J. Liu, F. Wilde, A. A. Mele, L. Jiang, and J. Eisert, Stochastic noise can be helpful for variational quantum algorithms, arXiv preprint arXiv:2210.06723 (2022).   
[67] M. Schumann, F. K. Wilhelm, and A. Ciani, Emergence of noise-induced barren plateaus in arbitrary layered noise models, arXiv preprint arXiv:2310.08405 (2023).   
[68] P. Singkanipa and D. A. Lidar, Beyond unital noise in variational quantum algorithms: noise-induced barren plateaus and fixed points, arXiv preprint arXiv:2402.08721 (2024).   
[69] R. Zeier and T. Schulte-Herbrüggen, Symmetry principles in quantum systems theory, Journal of mathematical physics 52, 113510 (2011).   
[70] R. Wiersema, E. Kökcü, A. F. Kemper, and B. N. Bakalov, Classification of dynamical lie algebras for translation-invariant 2-local spin systems in one dimension, arXiv preprint arXiv:2309.05690 (2023).   
[71] D. Wierichs, R. D. East, M. Larocca, M. Cerezo, and N. Killoran, Symmetric derivatives of parametrized quantum circuits, arXiv preprint arXiv:2312.06752 (2023).   
[72] M. J. Bremner, C. Mora, and A. Winter, Are random pure states useful for quantum computation?, Physical review letters 102, 190502 (2009).   
[73] D. Gross, S. T. Flammia, and J. Eisert, Most quantum states are too entangled to be useful as computational resources, Physical review letters 102, 190501 (2009).   
[74] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information (Cambridge University Press, Cambridge, 2000).   
[75] J. Horgan, Scott aaronson answers every ridiculously big question i throw at him, Scientific American. https://blogs. scientificamerican. com/crosscheck/scott-aaronson-answers-every-ridiculously-bigquestion-i-throw-at-him 21 (2016).   
[76] Y. Liu, S. Arunachalam, and K. Temme, A rigorous and robust quantum speed-up in supervised machine learning, Nature Physics , 1 (2021).   
[77] J. Jäger and R. V. Krems, Universal expressiveness of variational quantum classifiers and quantum kernels for support vector machines, Nature Communications 14, 576 (2023).   
[78] Z. Holmes, K. Sharma, M. Cerezo, and P. J. Coles, Connecting ansatz expressibility to gradient magnitudes and barren plateaus, PRX Quantum 3, 010313 (2022).   
[79] L. Friedrich and J. Maziero, Quantum neural network cost function concentration dependency on the parametrization expressivity, Scientific Reports 13, 9978 (2023).   
[80] M. Duschenes, D. García-Martín, M. Larocca, Z. Holmes, and M. Cerezo, Channel expressivity measures and their operational meaning, Manuscript in preparation.   
[81] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner, The power of quantum neural networks, Nature Computational Science 1, 403 (2021).   
[82] T. Haug, K. Bharti, and M. S. Kim, Capacity and Quantum Geometry of Parametrized Quantum Circuits, PRX Quantum 2, 040309 (2021).   
[83] M. Larocca, N. Ju, D. García-Martín, P. J. Coles, and M. Cerezo, Theory of overparametrization in quantum neural networks, Nature Computational Science 3, 542 (2023).   
[84] Technically speaking, group modules are irreducible representations of the dynamical Lie group over the vector space $\boldsymbol { B } ( \mathcal { H } _ { 0 } )$ .   
[85] R. Somma, G. Ortiz, H. Barnum, E. Knill, and L. Viola, Nature and measure of entanglement in quantum phase transitions, Physical Review A 70, 042311 (2004).   
[86] R. D. Somma, Quantum computation, complexity, and many-body physics, arXiv preprint quant-ph/0512209 (2005).   
[87] S. Khatri, R. LaRose, A. Poremba, L. Cincio, A. T. Sornborger, and P. J. Coles, Quantum-assisted quantum compiling, Quantum 3, 140 (2019).   
[88] A. Uvarov, J. D. Biamonte, and D. Yudin, Variational quantum eigensolver for frustrated quantum systems, Physical Review B 102, 075104 (2020).   
[89] M. Kashif and S. Al-Kuwari, The impact of cost function globality and locality in hybrid quantum neural networks on nisq devices, Machine Learning: Science and Technology 4, 015004 (2023).   
[90] C. Leadbeater, L. Sharrock, B. Coyle, and M. Benedetti, F-divergences and cost function locality in generative modelling with quantum circuits, Entropy 23, 1281 (2021).   
[91] M. Cerezo, K. Sharma, A. Arrasmith, and P. J. Coles, Variational quantum state eigensolver, npj Quantum Information 8, 1 (2022).   
[92] B. T. Kiani, G. De Palma, M. Marvian, Z.-W. Liu, and S. Lloyd, Learning quantum data with the quantum earth mover’s distance, Quantum Science and Technology 7, 045002 (2022).   
[93] L. Zambrano, A. D. Muñoz-Moller, M. Muñoz, L. Pereira, and A. Delgado, Avoiding barren plateaus in the variational determination of geometric entanglement, Quantum Science and Technology 9, 025016 (2024).   
[94] O. Ogunkoya, K. Morris, and D. M. Kürkçüoglu, Investigating parameter trainability in the snapdisplacement protocol of a qudit system, arXiv preprint arXiv:2309.14942 (2023).   
[95] B. Zhang and Q. Zhuang, Energy-dependent barren plateau in bosonic variational quantum circuits, arXiv preprint arXiv:2305.01799 (2023).   
[96] R. Wiersema, C. Zhou, J. F. Carrasquilla, and Y. B. Kim, Measurement-induced entanglement phase transitions in variational quantum circuits, SciPost Physics 14, 147 (2023).   
[97] S. Thanasilp, S. Wang, N. A. Nghiem, P. Coles, and M. Cerezo, Subtleties in the trainability of quantum machine learning models, Quantum Machine Intelligence 5, 21 (2023).   
[98] R. Shaydulin and S. M. Wild, Importance of kernel bandwidth in quantum machine learning, Physical Review A 106, 042407 (2022).   
[99] M. Kashif and S. Al-Kuwari, The unified effect of data encoding, ansatz expressibility and entanglement on the trainability of hqnns, International Journal of Parallel, Emergent and Distributed Systems 38, 362 (2023).   
[100] S. Das, S. Martina, and F. Caruso, The role of data embedding in equivariant quantum convolutional neural networks, arXiv preprint arXiv:2312.13250 (2023).   
[101] H. Mhiri, L. Monbroussou, M. Herrero-Gonzalez, S. Thabet, E. Kashefi, and J. Landman, Constrained and vanishing expressivity of quantum fourier models, arXiv preprint arXiv:2403.09417 (2024).   
[102] L. Leone, S. F. Oliviero, L. Cincio, and M. Cerezo, On the practical usefulness of the hardware efficient ansatz, arXiv preprint arXiv:2211.01477 (2022).   
[103] D. Stilck França and R. Garcia-Patron, Limitations of optimization algorithms on noisy quantum devices, Nature Physics 17, 1221 (2021).   
[104] A. A. Mele, A. Angrisani, S. Ghosh, S. Khatri, J. Eisert, D. S. França, and Y. Quek, Noise-induced shallow circuits and absence of barren plateaus, arXiv preprint arXiv:2403.13927 (2024).   
[105] B. Fefferman, S. Ghosh, M. Gullans, K. Kuroiwa, and K. Sharma, Effect of non-unital noise on random circuit sampling, arXiv preprint arXiv:2306.16659 (2023).   
[106] M. M. Wilde, Quantum information theory (Cambridge University Press, 2013).   
[107] D. García-Martín, M. Larocca, and M. Cerezo, Effects of noise on the overparametrization of quantum neural networks, Phys. Rev. Res. 6, 013295 (2024).   
[108] A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink, J. M. Chow, and J. M. Gambetta, Hardwareefficient variational quantum eigensolver for small molecules and quantum magnets, Nature 549, 242 (2017).   
[109] Z. He, L. Li, S. Zheng, Y. Li, and H. Situ, Variational quantum compiling with double q-learning, New Journal of Physics 23, 033002 (2021).   
[110] L. Moro, M. G. A. Paris, M. Restelli, and E. Prati, Quantum compiling by deep reinforcement learning, Communications Physics 4, 178 (2021).   
[111] K. Beer, D. Bondarenko, T. Farrelly, T. J. Osborne, R. Salzmann, D. Scheiermann, and R. Wolf, Training deep quantum neural networks, Nature Communications 11, 808 (2020).   
[112] G. Buonaiuto, F. Gargiulo, G. De Pietro, M. Esposito, and M. Pota, The effects of quantum hardware properties on the performances of variational quantum learning algorithms, Quantum Machine Intelligence 6, 9 (2024).   
[113] X. Liu, G. Liu, H.-K. Zhang, J. Huang, and X. Wang, Mitigating barren plateaus of variational quantum eigensolvers, IEEE Transactions on Quantum Engineering , 1 (2024).   
[114] H.-K. Zhang, C. Zhu, G. Liu, and X. Wang, Exponential hardness of optimization from the locality in quantum neural networks, in Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38 (2024) pp. 16741– 16749.   
[115] D. Wecker, M. B. Hastings, and M. Troyer, Progress towards practical quantum variational algorithms, Physical Review A 92, 042303 (2015).   
[116] R. Wiersema, C. Zhou, Y. de Sereville, J. F. Carrasquilla, Y. B. Kim, and H. Yuen, Exploring entanglement and optimization within the Hamiltonian variational ansatz, PRX Quantum 1, 020319 (2020).   
[117] E. Farhi, J. Goldstone, and S. Gutmann, A quantum approximate optimization algorithm, arXiv preprint arXiv:1411.4028 (2014).   
[118] S. Hadfield, Z. Wang, B. O’Gorman, E. G. Rieffel, D. Venturelli, and R. Biswas, From the quantum apnating operator ansatz, Algorithms 12, 34 (2019).   
[119] A. Bärtschi and S. Eidenbenz, Grover mixers for qaoa: Shifting complexity from mixer design to state preparation, in 2020 IEEE International Conference on Quantum Computing and Engineering (QCE) (IEEE, 2020) pp. 72–82.   
[120] F. G. Fuchs, K. O. Lye, H. Møll Nilsen, A. J. Stasik, and G. Sartor, Constraint preserving mixers for the quantum approximate optimization algorithm, Algorithms 15, 202 (2022).   
[121] R. Tate, J. Moondra, B. Gard, G. Mohler, and S. Gupta, Warm-started qaoa with custom mixers provably converges and computationally beats goemans-williamson’s max-cut at low circuit depths, Quantum 7, 1121 (2023).   
[122] B. Zhang, A. Sone, and Q. Zhuang, Quantum computational phase transition in combinatorial problems, npj Quantum Information 8, 1 (2022).   
[123] R. Herrman, P. C. Lotshaw, J. Ostrowski, T. S. Humble, and G. Siopsis, Multi-angle quantum approximate optimization algorithm, Scientific Reports 12, 1 (2022).   
[124] S. Kazi, M. Larocca, M. Farinati, P. Coles, R. Zeier, and M. Cerezo, The landscape of qaoa maxcut lie algebras, In preparation.   
[125] A. G. Taube and R. J. Bartlett, New perspectives on unitary coupled-cluster theory, International journal of quantum chemistry 106, 3393 (2006).   
[126] J. Lee, W. J. Huggins, M. Head-Gordon, and K. B. Whaley, Generalized unitary coupled cluster wave functions for quantum computation, Journal of chemical theory and computation 15, 311 (2018).   
[127] R. Mao, G. Tian, and X. Sun, Barren plateaus of alternated disentangled ucc ansatzs (2023).   
[128] S. Cichy, P. K. Faehrmann, S. Khatri, and J. Eisert, Non-recursive perturbative gadgets without subspace restrictions and applications to variational quantum algorithms, arXiv preprint arXiv:2210.03099 (2022).   
[129] S. C. Marshall, C. Gyurik, and V. Dunjko, High dimensional quantum machine learning with small quantum computers, Quantum 7, 1078 (2023).   
[130] K. Zhang, M.-H. Hsieh, L. Liu, and D. Tao, Toward trainability of quantum neural networks, arXiv preprint arXiv:2011.06258 (2020).   
[131] M. Larocca, F. Sauvage, F. M. Sbahi, G. Verdon, P. J. Coles, and M. Cerezo, Group-invariant quantum machine learning, PRX Quantum 3, 030341 (2022).   
[132] Q. T. Nguyen, L. Schatzki, P. Braccia, M. Ragone, M. Larocca, F. Sauvage, P. J. Coles, and M. Cerezo, A theory for equivariant quantum neural networks, arXiv preprint arXiv:2210.08566 (2022).   
[133] J. J. Meyer, M. Mularski, E. Gil-Fuster, A. A. Mele, F. Arzani, A. Wilms, and J. Eisert, Exploiting symmetry in variational quantum machine learning, PRX Quantum 4, 010328 (2023).   
[134] A. Skolik, M. Cattelan, S. Yarkoni, T. Bäck, and V. Dunjko, Equivariant quantum circuits for learning on weighted graphs, npj Quantum Information 9, 47 (2023).   
[135] T. Volkoff and P. J. Coles, Large gradients via correlation in random parameterized quantum circuits, Quantum Science and Technology 6, 025008 (2021).   
[136] F. Sauvage, M. Larocca, P. J. Coles, and M. Cerezo, Building spatial symmetries into parameterized quanL Technology 9, 015029 (2024).   
[137] S. Kazi, M. Larocca, and M. Cerezo, On the universality of $s _ { n }$ -equivariant $k$ -body gates, arXiv preprint arXiv:2303.00728 (2023).   
[138] L. Monbroussou, J. Landman, A. B. Grilo, R. Kukla, and E. Kashefi, Trainability and expressivity of hamming-weight preserving quantum circuits for machine learning, arXiv preprint arXiv:2309.15547 (2023).   
[139] S. Raj, I. Kerenidis, A. Shekhar, B. Wood, J. Dee, S. Chakrabarti, R. Chen, D. Herman, S. Hu, P. Minssen, et al., Quantum deep hedging, Quantum 7, 1191 (2023).   
[140] J. Lee, A. B. Magann, H. A. Rabitz, and C. Arenz, Progress toward favorable landscapes in quantum combinatorial optimization, Physical Review A 104, 032401 (2021).   
[141] R. Jozsa and A. Miyake, Matchgates and classical simulation of quantum circuits, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 464, 3089 (2008).   
[142] K. Wan, W. J. Huggins, J. Lee, and R. Babbush, Matchgate shadows for fermionic quantum simulation, Communications in Mathematical Physics , 1 (2023).   
[143] F. De Melo, P. Ćwikliński, and B. M. Terhal, The power of noisy fermionic quantum computation, New Journal of Physics 15, 013015 (2013).   
[144] M. Oszmaniec, N. Dangniam, M. E. Morales, and Z. Zimborás, Fermion sampling: a robust quantum computational advantage scheme using fermionic linear optics and magic input states, PRX Quantum 3, 020328 (2022).   
[145] G. Matos, C. N. Self, Z. Papić, K. Meichanetzidis, and H. Dreyer, Characterization of variational quantum algorithms using free fermions, Quantum 7, 966 (2023).   
[146] N. L. Diaz, P. Braccia, M. Larocca, J. M. Matera, R. Rossignoli, and M. Cerezo, Parallel-in-time quantum simulation via page and wootters quantum time, arXiv preprint arXiv:2308.12944 (2023).   
[147] E. Kökcü, T. Steckmann, Y. Wang, J. Freericks, E. F. Dumitrescu, and A. F. Kemper, Fixed depth hamiltonian simulation via cartan decomposition, Physical Review Letters 129, 070501 (2022).   
[148] T. J. Volkoff, Efficient trainability of linear optical modules in quantum optical neural networks, Journal of Russian Laser Research 42, 250 (2021).   
[149] T. Volkoff, Z. Holmes, and A. Sornborger, Universal compiling and (no-)free-lunch theorems for continuousvariable quantum learning, PRX Quantum 2, 040327 (2021).   
[150] M. Arjovsky, A. Shah, and Y. Bengio, Unitary evolution recurrent neural networks, in International conference on machine learning (PMLR, 2016) pp. 1120–1128.   
[151] L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun, M. Tegmark, and M. Soljačić, Tunable efficient unitary neural networks (eunn) and their application to rnns, in International Conference on Machine Learning (PMLR, 2017) pp. 1733–1741.   
[152] M. Bilkis, M. Cerezo, G. Verdon, P. J. Coles, and L. Cincio, A semi-agnostic ansatz with variable structure for variational quantum algorithms, Quantum Machine Intelligence 5, 43 (2023).   
[153] Y. Du, T. Huang, S. You, M.-H. Hsieh, and D. Tao, Quantum circuit architecture search: error mitigation and trainability enhancement for variational quantum solvers, npj Quantum Information 8, 62 (2022).   
[154] H. R. Grimsley, S. E. Economou, E. Barnes, and N. J. Mayhall, An adaptive variational algorithm for exact molecular simulations on a quantum computer, Nature Communications 10, 1 (2019).   
[155] H. L. Tang, V. Shkolnikov, G. S. Barron, H. R. Grimsley, N. J. Mayhall, E. Barnes, and S. E. Economou, qubit-adapt-vqe: An adaptive algorithm for constructing hardware-efficient ansätze on a quantum processor, PRX Quantum 2, 020310 (2021).   
[156] S. Sim, J. Romero, J. F. Gonthier, and A. A. Kunitsa, Adaptive pruning-based optimization of parameterized quantum circuits, Quantum Science and Technology 6, 025019 (2021).   
[157] Z.-J. Zhang, T. H. Kyaw, J. Kottmann, M. Degroote, and A. Aspuru-Guzik, Mutual information-assisted adaptive variational quantum eigensolver, Quantum Science and Technology 10.1088/2058-9565/abdca4 (2021).   
[158] N. V. Tkachenko, J. Sud, Y. Zhang, S. Tretiak, P. M. Anisimov, A. T. Arrasmith, P. J. Coles, L. Cincio, and P. A. Dub, Correlation-informed permutation of qubits for reducing ansatz depth in vqe, PRX Quantum 2, 020337 (2021).   
[159] D. Claudino, J. Wright, A. J. McCaskey, and T. S. Humble, Benchmarking adaptive variational quantum eigensolvers, Frontiers in Chemistry 8, 1152 (2020).   
[160] A. G. Rattew, S. Hu, M. Pistoia, R. Chen, and S. Wood, A domain-agnostic, noise-resistant, hardware-efficient evolutionary variational quantum eigensolver, arXiv preprint arXiv:1910.09694 (2019).   
[161] D. Chivilikhin, A. Samarin, V. Ulyantsev, I. Iorsh, A. Oganov, and O. Kyriienko, Mog-vqe: Multiobjective genetic variational quantum eigensolver, arXiv preprint arXiv:2007.04424 (2020).   
[162] L. Cincio, Y. Subaşı, A. T. Sornborger, and P. J. Coles, Learning the quantum algorithm for state overlap, New Journal of Physics 20, 113022 (2018).   
[163] S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and H. Yao, Differentiable quantum architecture search, Quantum Science and Technology 7, 045023 (2022).   
[164] K. Wada, R. Raymond, Y. Sato, and H. C. Watanabe, Sequential optimal selection of a single-qubit gate and its relation to barren plateau in parameterized quantum circuits, arXiv preprint arXiv:2209.08535 (2022).   
[165] H. R. Grimsley, N. J. Mayhall, G. S. Barron, E. Barnes, and S. E. Economou, Adaptive, problem-tailored variational quantum eigensolver mitigates rough parameter landscapes and barren plateaus, npj Quantum Information 9, 19 (2023).   
[166] P. G. Anastasiou, Y. Chen, N. J. Mayhall, E. Barnes, and S. E. Economou, Tetris-adapt-vqe: An adaptive algorithm that yields shallower, denser circuit ansätze, Physical Review Research 6, 013254 (2024).   
[167] E. Grant, L. Wossnig, M. Ostaszewski, and M. Benedetti, An initialization strategy for addressing barren plateaus in parametrized quantum circuits, Quantum 3, 214 (2019).   
[168] N. Jain, B. Coyle, E. Kashefi, and N. Kumar, Graph neural network initialisation of quantum approximate optimisation, Quantum 6, 861 (2022).   
[169] A. Skolik, J. R. McClean, M. Mohseni, P. van der Smagt, and M. Leib, Layerwise learning for quantum neural networks, Quantum Machine Intelligence 3, 1 (2021)   
[170] A. Kulshrestha and I. Safro, Beinit: Avoiding barren plateaus in variational quantum algorithms, in 2022 IEEE International Conference on Quantum Computing and Engineering (QCE) (IEEE, 2022) pp. 197–203.   
[171] A. Rad, A. Seif, and N. M. Linke, Surviving the barren plateau in variational quantum circuits with bayesian learning initialization, arXiv preprint arXiv:2203.02464 (2022)   
[172] N. Astrakhantsev, G. Mazzola, I. Tavernelli, and G. Carleo, Phenomenological theory of variational quantum ground-state preparation, Physical Review Research 5, 033225 (2023).   
[173] T. Haug and M. Kim, Optimal training of variational quantum algorithms without barren plateaus, arXiv preprint arXiv:2104.14543 (2021).   
[174] M. Kashif, M. Rashid, S. Al-Kuwari, and M. Shafique, Alleviating barren plateaus in parameterized quantum machine learning circuits: Investigating advanced parameter initialization strategies, arXiv preprint arXiv:2311.13218 (2023).   
[175] X. Shi and Y. Shang, Avoiding barren plateaus via gaussian mixture model, arXiv preprint arXiv:2402.13501 (2024).   
[176] L. Friedrich and J. Maziero, Avoiding barren plateaus with classical deep neural networks, Physical Review A 106, 042433 (2022).   
[177] M. S. Rudolph, J. Miller, D. Motlagh, J. Chen, A. Acharya, and A. Perdomo-Ortiz, Synergistic pretraining of parametrized quantum circuits via tensor networks, Nature Communications 14, 8367 (2023).   
[178] G. Marin-Sanchez, J. Gonzalez-Conde, and M. Sanz, Quantum algorithms for approximate function loading, Physical Review Research 5, 033114 (2023).   
[179] A. Cervera-Lierta, J. S. Kottmann, and A. AspuruGuzik, The meta-variational quantum eigensolver (meta-vqe): Learning energy profiles of parameterized Hamiltonians for quantum simulation, PRX Quantum 2, 020329 (2021).   
[180] M. L. Goh, M. Larocca, L. Cincio, M. Cerezo, and F. Sauvage, Lie-algebraic classical simulations for variational quantum computing, arXiv preprint arXiv:2308.01432 (2023).   
[181] F. G. Brandao, M. Broughton, E. Farhi, S. Gutmann, and H. Neven, For fixed control parameters the quantum approximate optimization algorithm’s objective function value concentrates for typical instances, arXiv preprint arXiv:1812.04170 (2018).   
[182] L. Zhou, S.-T. Wang, S. Choi, H. Pichler, and M. D. Lukin, Quantum approximate optimization algorithm: Performance, mechanism, and implementation on nearterm devices, Physical Review X 10, 021067 (2020).   
[183] J. Wurtz and D. Lykov, Fixed-angle conjectures for the quantum approximate optimization algorithm on regular maxcut graphs, Physical Review A 104, 052419 (2021).   
[184] S. Boulebnane and A. Montanaro, Predicting parameters for the quantum approximate optimization algorithm for max-cut from the infinite-size limit, arXiv preprint arXiv:2110.10685 (2021).   
[185] A. Galda, X. Liu, D. Lykov, Y. Alexeev, and I. Safro, Transferability of optimal qaoa parameters between random graphs, in 2021 IEEE International Conference on Quantum Computing and Engineering (QCE) (IEEE, 2021) pp. 171–180.   
[186] E. Farhi, J. Goldstone, S. Gutmann, and L. Zhou, The quantum approximate optimization algorithm and the sherrington-kirkpatrick model at infinite size, Quantum 6, 759 (2022).   
[187] R. Shaydulin, P. C. Lotshaw, J. Larson, J. Ostrowski, and T. S. Humble, Parameter transfer for quantum approximate optimization of weighted maxcut, ACM Transactions on Quantum Computing 4, 1 (2023).   
[188] A. A. Mele, G. B. Mbeng, G. E. Santoro, M. Collura, and P. Torta, Avoiding barren plateaus via transferability of smooth solutions in a hamiltonian variational ansatz, Physical Review A 106, L060401 (2022).   
[189] H.-Y. Liu, T.-P. Sun, Y.-C. Wu, Y.-J. Han, and G.-P. Guo, Mitigating barren plateaus with transferlearning-inspired parameter initializations, New Journal of Physics 25, 013039 (2023).   
[190] Z.-J. Zhang, J. Sun, X. Yuan, and M.-H. Yung, Lowdepth hamiltonian simulation by an adaptive product formula, Phys. Rev. Lett. 130, 040601 (2023).   
[191] J. Gacon, J. Nys, R. Rossi, S. Woerner, and G. Carleo, Variational quantum time evolution without the quantum geometric tensor, Phys. Rev. Res. 6, 013143 (2024).   
[192] E. Campos, D. Rabinovich, V. Akshay, and J. Biamonte, Training saturation in layerwise quantum approximate optimization, Physical Review A 104, L030401 (2021).   
[193] E. Campos, A. Nasrallah, and J. Biamonte, Abrupt transitions in variational quantum circuit training, Physical Review A 103, 032607 (2021).   
[194] D. Fitzek, R. S. Jonsson, W. Dobrautz, and C. Schäfer, Optimizing variational quantum algorithms with qbang: Efficiently interweaving metric and momentum to navigate flat energy landscapes, Quantum 8, 1313 (2024).   
[195] J. Nádori, G. Morse, Z. Majnay-Takács, Z. Zimborás, and P. Rakyta, The promising path of evolutionary optimization to avoid barren plateaus, arXiv preprint arXiv:2402.05227 (2024).   
[196] G. Acampora, A. Chiatto, and A. Vitiello, A comparison of evolutionary algorithms for training variational quantum classifiers, in 2023 IEEE Congress on Evolutionary Computation (CEC) (IEEE, 2023) pp. 1–8.   
[197] J. Stokes, J. Izaac, N. Killoran, and G. Carleo, Quantum natural gradient, Quantum 4, 269 (2020).   
[198] B. Koczor and S. C. Benjamin, Quantum natural gradient generalized to noisy and nonunitary circuits, Physical Review A 106, 062416 (2022).   
[199] A. Arrasmith, M. Cerezo, P. Czarnik, L. Cincio, and P. J. Coles, Effect of barren plateaus on gradient-free optimization, Quantum 5, 558 (2021).   
[200] K. Temme, S. Bravyi, and J. M. Gambetta, Error mitigation for short-depth quantum circuits, Physical review letters 119, 180509 (2017).   
[201] Y. Li and S. C. Benjamin, Efficient variational quantum simulator incorporating active error minimization, Phys. Rev. X 7, 021050 (2017).   
[202] Z. Cai, R. Babbush, S. C. Benjamin, S. Endo, W. J. Huggins, Y. Li, J. R. McClean, and T. E. O’Brien, Quantum error mitigation, Reviews of Modern Physics 95, 045005 (2023).   
[203] K. Wang, Y.-A. Chen, and X. Wang, Mitigating quantum errors via truncated neumann series, Science China Information Sciences 66, 180508 (2023).   
[204] R. Takagi, S. Endo, S. Minagawa, and M. Gu, Fundamental limits of quantum error mitigation, npj Quan114 (2022).   
[205] R. Takagi, H. Tajima, and M. Gu, Universal sampling lower bounds for quantum error mitigation, Physical Review Letters 131, 210602 (2023).   
[206] Y. Quek, D. S. França, S. Khatri, J. J. Meyer, and J. Eisert, Exponentially tighter bounds on limitations of quantum error mitigation, arXiv preprint arXiv:2210.11505 (2022).   
[207] A. Perdomo-Ortiz, M. Benedetti, J. Realpe-Gómez, and R. Biswas, Opportunities and challenges for quantumassisted machine learning in near-term quantum computers, Quantum Science and Technology 3, 030502 (2018).   
[208] X. Gao, E. R. Anschuetz, S.-T. Wang, J. I. Cirac, and M. D. Lukin, Enhancing generative models via quantum correlations, Phys. Rev. X 12, 021037 (2022).   
[209] M. Benedetti, D. Garcia-Pintos, O. Perdomo, V. Leyton-Ortega, Y. Nam, and A. Perdomo-Ortiz, A generative modeling approach for benchmarking and training shallow quantum circuits, npj Quantum Information 5, 45 (2019).   
[210] J.-G. Liu and L. Wang, Differentiable learning of quantum circuit born machines, Phys. Rev. A 98, 062324 (2018).   
[211] B. Coyle, D. Mills, V. Danos, and E. Kashefi, The born supremacy: quantum advantage and training of an ising born machine, npj Quantum Information 6, 60 (2020).   
[212] M. S. Rudolph, S. Lerch, S. Thanasilp, O. Kiss, S. Vallecorsa, M. Grossi, and Z. Holmes, Trainability barriers and opportunities in quantum generative modeling, arXiv preprint arXiv:2305.02881 (2023).   
[213] C. Zoufal, Generative quantum machine learning, arXiv preprint arXiv:2111.12738 (2021).   
[214] S. Lloyd and C. Weedbrook, Quantum generative adversarial learning, Physical Review Letters 121, 040502 (2018).   
[215] M. H. Amin, E. Andriyash, J. Rolfe, B. Kulchytskyy, and R. Melko, Quantum boltzmann machine, Phys. Rev. X 8, 021050 (2018).   
[216] L. Coopmans and M. Benedetti, On the sample complexity of quantum boltzmann machine learning, arXiv preprint arXiv:2306.14969 (2023).   
[217] M. Kieferova, O. M. Carlos, and N. Wiebe, Quantum generative training using rényi divergences, arXiv preprint arXiv:2106.09567 (2021).   
[218] H.-Y. Huang, M. Broughton, M. Mohseni, R. Babbush, S. Boixo, H. Neven, and J. R. McClean, Power of data in quantum machine learning, Nature Communications 12, 1 (2021).   
[219] S. Thanasilp, S. Wang, M. Cerezo, and Z. Holmes, Exponential concentration in quantum kernel methods, arXiv preprint arXiv:2208.11060 (2022).   
[220] J. Kübler, S. Buchholz, and B. Schölkopf, The inductive bias of quantum kernels, Advances in Neural Information Processing Systems 34, 12661 (2021).   
[221] A. Canatar, E. Peters, C. Pehlevan, S. M. Wild, and R. Shaydulin, Bandwidth enables generalization in quantum kernel models, arXiv preprint arXiv:2206.06686 (2022).   
[222] Y. Suzuki and M. Li, Effect of alternating layered ansatzes on trainability of projected quantum kernel, arXiv preprint arXiv:2310.00361 (2023).   
[223] Y. Suzuki, H. Kawaguchi, and N. Yamamoto, Quantum fisher kernel for mitigating the vanishing similarity is0210.1 (2022)   
[224] W. Xiong, G. Facelli, M. Sahebi, O. Agnel, T. Chotibut, S. Thanasilp, and Z. Holmes, On fundamental aspects of quantum extreme learning machines, arXiv preprint arXiv:2312.15124 (2023).   
[225] L.-W. Yu, W. Li, Q. Ye, Z. Lu, Z. Han, and D.-L. Deng, Expressibility-induced concentration of quantum neural tangent kernels, arXiv preprint arXiv:2311.04965 (2023).   
[226] M. Schuld and F. Petruccione, Supervised learning with quantum computers, Vol. 17 (Springer, 2018).   
[227] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta, Supervised learning with quantum-enhanced feature spaces, Nature 567, 209 (2019).   
[228] M. Schuld, Supervised quantum machine learning models are kernel methods, arXiv preprint arXiv:2101.11020 (2021).   
[229] B. T. Kiani, S. Lloyd, and R. Maity, Learning unitaries by gradient descent, arXiv preprint arXiv:2001.11897 (2020).   
[230] A. B. Magann, C. Arenz, M. D. Grace, T.-S. Ho, R. L. Kosut, J. R. McClean, H. A. Rabitz, and M. Sarovar, From pulses to circuits and back again: A quantum optimal control perspective on variational quantum algorithms, PRX Quantum 2, 010101 (2021).   
[231] X. Ge, R.-B. Wu, and H. Rabitz, The optimization landscape of hybrid quantum– classical algorithms: From quantum control to NISQ applications, Annual Reviews in Control https://doi.org/10.1016/j.arcontrol.2022.06.001 (2022).   
[232] L. Broers and L. Mathey, Mitigated barren plateaus in the time-nonlocal optimization of analog quantumalgorithm protocols, Physical Review Research 6, 013076 (2024).   
[233] H.-X. Tao, J. Hu, and R.-B. Wu, Unleashing the expressive power of pulse-based quantum neural networks, arXiv preprint arXiv:2402.02880 (2024).   
[234] E. C. Martín, K. Plekhanov, and M. Lubasch, Barren plateaus in quantum tensor network optimization, Quantum 7, 974 (2023).   
[235] Z. Holmes, A. Arrasmith, B. Yan, P. J. Coles, A. Albrecht, and A. T. Sornborger, Barren plateaus preclude learning scramblers, Physical Review Letters 126, 190501 (2021).   
[236] V. Feldman, Statistical query learning, in Encyclopedia of Algorithms, edited by M.-Y. Kao (Springer New York, New York, NY, 2016) pp. 2090–2095.   
[237] A. Angrisani, Learning unitaries with quantum statistical queries, arXiv preprint arXiv:2310.02254 (2023).   
[238] C. Wadhwa and M. Doosti, Learning quantum processes with quantum statistical queries, arXiv preprint arXiv:2310.02075 (2023).   
[239] A. Nietner, Unifying (quantum) statistical and parametrized (quantum) algorithms, arXiv preprint arXiv:2310.17716 (2023).   
[240] L. Yang and N. Engelhardt, The complexity of learning (pseudo) random dynamics of black holes and other chaotic systems, arXiv preprint arXiv:2302.11013 (2023).   
[241] E. R. Anschuetz and B. T. Kiani, Quantum variational algorithms are swamped with traps, Nature Communications 13, 7760 (2022).   
[242] X. You and X. Wu, Exponentially many local minima in quantum neural networks, in International Confere on Machine Learning (PMLR, 2021) pp. 12144–12155.   
[243] J. Rajakumar, J. Golden, A. Bärtschi, and S. Eidenbenz, Trainability barriers in low-depth qaoa landscapes, arXiv preprint arXiv:2402.10188 (2024).   
[244] A. Tikku and I. H. Kim, Circuit depth versus energy in topologically ordered systems, arXiv preprint arXiv:2210.06796 (2022).   
[245] A. Basheer, Y. Feng, C. Ferrie, and S. Li, Alternating layered variational quantum circuits can be classically optimized efficiently using classical shadows, arXiv preprint arXiv:2208.11623 (2022).   
[246] K. Bharti and T. Haug, Iterative quantum-assisted eigensolver, Physical Review A 104, L050401 (2021).   
[247] K. Bharti and T. Haug, Quantum-assisted simulator, Physical Review A 104, 042418 (2021).   
[248] R. M. Parrish, E. G. Hohenstein, P. L. McMahon, and T. J. Martínez, Quantum computation of electronic transitions using a variational quantum eigensolver, Physical review letters 122, 230401 (2019).   
[249] S. Jerbi, C. Gyurik, S. C. Marshall, R. Molteni, and V. Dunjko, Shadows of quantum machine learning, arXiv preprint arXiv:2306.00061 (2023).   
[250] C. Gyurik, R. Molteni, and V. Dunjko, Limitations of measure-first protocols in quantum machine learning, arXiv preprint arXiv:2311.12618 (2023).   
[251] A. Elben, S. T. Flammia, H.-Y. Huang, R. Kueng, J. Preskill, B. Vermersch, and P. Zoller, The randomized measurement toolbox, Nature Review Physics 10.1038/s42254-022-00535-2 (2022).   
[252] X. Glorot and Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in Proceedings of the thirteenth international conference on artificial intelligence and statistics (JMLR Workshop and Conference Proceedings, 2010) pp. 249–256.   
[253] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in International conference on machine learning (pmlr, 2015) pp. 448–456.   
[254] J. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization, arXiv preprint arXiv:1607.06450 (2016).   
[255] V. Nair and G. E. Hinton, Rectified linear units improve restricted boltzmann machines, in Proceedings of the 27th international conference on machine learning (ICML-10) (2010) pp. 807–814.   
[256] S. Hochreiter, The vanishing gradient problem during learning recurrent neural nets and problem solutions, International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 6, 107 (1998).   
[257] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural computation 9, 1735 (1997).   
[258] L. Baird and A. Moore, Gradient descent for general reinforcement learning, Advances in neural information processing systems 11 (1998).   
[259] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, Evolution strategies as a scalable alternative to reinforcement learning, arXiv preprint arXiv:1703.03864 (2017).  