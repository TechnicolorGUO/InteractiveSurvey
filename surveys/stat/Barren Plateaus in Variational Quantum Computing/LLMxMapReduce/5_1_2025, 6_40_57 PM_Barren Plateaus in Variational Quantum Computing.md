# 5/1/2025, 6:40:57 PM_Barren Plateaus in Variational Quantum Computing  

0. Barren Plateaus in Variational Quantum Computing  

# 1. Introduction  

Variational Quantum Computing (VQC), encompassing Variational Quantum Algorithms (VQAs), has emerged as a leading approach for leveraging the capabilities of Noisy Intermediate-Scale Quantum (NISQ) devices [2,7,12]. This hybrid quantumclassical paradigm utilizes a quantum processor to execute a parameterized quantum circuit and evaluate a cost function, while a classical optimizer iteratively adjusts the circuit parameters to minimize this cost [4,15]. This structure is particularly well-suited for the limitations of current quantum hardware, which faces constraints on qubit count, circuit depth, and resilience to noise [2,9,12]. VQC is being actively explored for a diverse range of applications, including quantum simulation, optimization, quantum machine learning, and quantum chemistry, holding significant promise for demonstrating quantum advantage in the near term [2,4,21,29].  

Despite the potential of VQC, a significant impediment to its trainability and scalability is the phenomenon known as barren plateaus [4,5,19]. Barren plateaus manifest as a severe flattening of the cost function landscape, where the gradients of the cost function with respect to the circuit parameters vanish exponentially with increasing system size or circuit depth [5,13]. This makes parameter optimization exceedingly difficult, hindering the ability of classical algorithms to find the optimal circuit configurations [5]. The barren plateau problem poses a fundamental challenge to the practical implementation and scalability of VQC algorithms, analogous in impact to vanishing or exploding gradients in classical deep learning [4]. Addressing this issue is therefore crucial for realizing the full potential of VQC in the NISQ era and beyond [4,5].  

This survey provides a comprehensive overview of the barren plateau problem in variational quantum computing. We delve into the theoretical underpinnings of this phenomenon, explore various proposed mechanisms driving its appearance, and review strategies aimed at its mitigation. The survey concludes by discussing the implications of barren plateaus for the future development and application of VQC.​  

# 2. Background on Variational Quantum Computing  

Variational Quantum Algorithms (VQAs) represent a prominent class of algorithms for Near-Intermediate Scale Quantum (NISQ) computers, leveraging a hybrid quantum-classical computational model [2,3,4,7,9].  

This architecture involves an iterative feedback loop where specific computational tasks are executed on a quantum processor, and the results are processed by a classical computer to guide the next quantum execution [2,4,7,9,15,17,21].  

At the core of a VQA lies a parameterized quantum circuit (PQC), often referred to as the ansatz \(U(\boldsymbol{\theta})\), where \(\boldsymbol{\theta}\) is a vector of trainable parameters [2,3,4,9,15,18,21,29]. The function of the ansatz is to prepare a parameterized quantum state or unitary \(U(\boldsymbol{\theta})\) [2,15], effectively encoding the problem or preparing a trial solution state within the quantum processor [29]. The structure of this circuit, consisting of parameterized and non-parameterized quantum gates, defines a hypothesis space \(H\) of possible output states or unitaries [3,18].  

To quantify the quality of the state or unitary produced by the ansatz for a given set of parameters \(\boldsymbol{\theta}\), a cost function \(C(\boldsymbol{\theta})\) is employed [2,5,7,8,9,15,24]. Analogous to loss functions in classical machine learning, the cost function maps the quantum circuit’s output to a real number, with the objective of the VQA being to find the parameter set \(\boldsymbol{\theta}^\*\) that minimizes \(C(\boldsymbol{\theta})\) [2,8,9,20,24]. The cost function is typically defined based on the expectation value of a Hermitian observable or a set of observables measured on the output state of the quantum circuit [9,17,18,19,21]. For instance, in the Variational Quantum Eigensolver (VQE), the cost function is the expectation value of the problem Hamiltonian $\backslash ( \mathsf { H } \backslash )$ :​  

where \(\mid \psi(\boldsymbol{\theta}) \rangle $\mathbf { \Sigma } = \mathbf { \Sigma }$ U(\boldsymbol{\theta}) \mid \Phi \rangle\) is the state prepared by the ansatz from an initial state \(\mid \Phi \rangle\), and $\backslash ( \mathsf { H } \backslash )$ can be decomposed into a sum of Pauli strings [17,21]. Evaluating this expectation value requires preparing the state on the quantum computer and performing measurements [9].  

The classical component of the VQA framework is a classical optimizer. This optimizer receives the cost function value (and potentially its gradient) from the quantum computer and uses this information to update the parameters \ (\boldsymbol{\theta}\) for the next iteration, aiming to find the minimum of the cost landscape [2,9,15,17,21]. Standard classical optimization tools, including gradient-based methods like Gradient Descent or Adam, and gradient-free methods like SPSA or Powell, are employed for this task [2,15]. The efficiency of this classical optimization step is crucial for the overall performance of the VQA [2].  

VQAs constitute a versatile framework applicable to a wide range of problems [2,7]. Specific instantiations of VQAs are tailored to different problem domains. For example, the Variational Quantum Eigensolver (VQE) is designed primarily for finding the ground state energy of molecular Hamiltonians or other quantum systems by minimizing the energy expectation value [2,7,17,19,21]. The Quantum Approximate Optimization Algorithm (QAOA) is another notable VQA tailored for solving combinatorial optimization problems, inspired by adiabatic quantum computing and utilizing a specific layered ansatz structure [2,7,20]. Other VQAs include those for quantum machine learning (QML) and quantum simulation [3,7,17,19]. This broad applicability underscores VQAs as a promising pathway towards achieving quantum advantage on near-term hardware [3].​  

# 2.1 The Ansatz (Parameterized Quantum Circuit)  

The parameterized quantum circuit (PQC), often referred to as the ansatz, represents the core trainable component of Variational Quantum Computing (VQC) algorithms. Its primary function is to generate parameterized quantum states or unitaries  

$U ( \pmb \theta )$ [2,15]. The specific form and structure of the ansatz determine the parameter landscape and significantly impact the trainability of the VQC algorithm, playing a crucial role in the emergence of barren plateaus [4,9]. The problem-dependent nature of VQC often allows for tailoring the ansatz structure to leverage specific problem information, although universal, problem-independent ansatz architectures also exist [2].  

Ansatz architectures can be broadly categorized based on their structure and origin [29]. A common approach is the layered structure, where the unitary is composed of a sequence of $p$ layers,   
\​   
with each layer $\mathbf { \mathcal { L } } _ { i }$ containing parameterized gates specified by $\pmb { \alpha } _ { i }$ [15]. These layers often consist of parameterized singlequbit rotations and entangling two-qubit gates [15,17]. A specific type of layered ansatz designed for permutation symmetry takes the form​   
\​   
where​   
\​   
and $H _ { l }$ are Hermitian generators [18]. The parameters $\pmb \theta$ (or $\pmb { \alpha }$ ) are optimized during the VQC training process to minimize a cost function [2].  

<html><body><table><tr><td>Ansatz Type</td><td>Description</td><td>Structure Examples</td><td>Barren Plateau Relevance</td></tr><tr><td>Problem-Inspired</td><td>Leverages domain knowledge for structure.</td><td>UCC, QAOA, Custom (Growth/Shrinkage)</td><td>Can potentially alleviate BPs by restricting search space.</td></tr><tr><td>Hardware-Efficient</td><td>Designed for specific hardware, minimal gates/depth, native gate sets.</td><td>Layers of parameterized single-qubit and</td><td>BP susceptibility depends on connectivity and depth.</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>fixed two-qubit gates.</td><td></td></tr><tr><td>Random Circuits</td><td>Often used for initialization or exploration.</td><td>Alternating parameterized single-qubit and fixed entangling layers.</td><td>Highly susceptible to BPs, especially when deep.</td></tr><tr><td>Tensor Network- based</td><td>Leverages TN structures to constrain entanglement/para meter space.</td><td>MPS, Tree Tensor Networks.</td><td>Low-entanglement TNs (like MPS) can avoid BPs.</td></tr></table></body></html>  

Several types of ansatz architectures are employed in VQC:  

• Problem-Inspired Ansätze: These leverage domain-specific knowledge to structure the quantum circuit. The Unitary Coupled Cluster (UCC) ansatz, particularly the UCCSD truncation including single and double excitations, is a prime example used in quantum chemistry. It is defined by a unitary operator​   
\​   
where​   
\​   
with $\hat { \tau }$ operators constructed from Fermionic creation and annihilation operators on occupied $( i , j )$ and unoccupied $( a , b$   
) orbitals [21]. The Quantum Approximate Optimization Algorithm (QAOA) ansatz is another problem-specific structure for   
combinatorial optimization, built using alternating layers of problem Hamiltonian $H _ { C }$ and mixer Hamiltonian $\scriptstyle { H _ { B } }$ ​   
evolution, resulting in the state​   
\​  

[20]. Custom-designed ansätze can also be constructed through automated procedures, such as a two-phase growth and shrinkage approach that iteratively adds and removes entangling blocks based on their contribution to fidelity [17].  

• Hardware-Efficient Ansätze: These are designed to minimize the number of gates and depth while utilizing the native gate set of specific quantum hardware. They typically consist of layers of parameterized single-qubit rotations followed by fixed, native two-qubit entangling gates, implemented via control pulses like microwave pulses in superconducting qubits [3,6].​  

• Random Circuits: Frequently used, especially as initial configurations, to explore the state space [1,4]. A common structure involves alternating parameterized single-qubit gates (e.g., random Pauli rotations) and fixed entangling layers (e.g., a ladder of controlled-Z gates) [4]. Instantaneous Quantum Polynomial-time (IQP) circuits, a specific type of random-like ansatz, are noted for their classical simulation hardness even at constant depth [1].​  

• Tensor Network-based Ansätze: These architectures leverage tensor network structures like Matrix Product States (MPS) or tree tensor networks to constrain the entanglement and parameter space [3].  

The design of an ansatz involves critical trade-offs between complexity, expressibility, entangling capability, and their influence on trainability and barren plateaus [3]. Complexity is generally related to the number of parameters and circuit depth. Expressivity—the ability of the ansatz to generate a diverse set of quantum states or implement complex unitaries— tends to increase with complexity and entangling capability [3,17]. Entanglement, often generated by two-qubit gates, is essential for exploring the vast Hilbert space [6,17].​  

However, high expressivity and deep circuits, especially those with all-to-all or random entanglement patterns, are strongly linked to the barren plateau phenomenon [1,4]. In such ansätze, gradients of the cost function with respect to parameters tend to vanish exponentially with the number of qubits, making training practically impossible [4]. Hardware-efficient ansätze, while potentially mitigating some issues through limited depth and structure, can still face barren plateaus depending on their specific connectivity and depth. Problem-specific ansätze, like UCCSD, can restrict the searchable state space based on physical or chemical principles, potentially alleviating barren plateaus by reducing the effective Hilbert  

space dimension and providing a more structured parameter landscape, although this comes at the cost of universality [2,21]. Techniques like layering with parameter re-utilization from previous stages have been explored to improve trainability and avoid local minima [8,15]. Ultimately, selecting an appropriate ansatz requires balancing the need for sufficient expressivity to solve the problem with the constraints imposed by hardware limitations and the severe challenge posed by barren plateaus, a balance that theories like the Quantum Neural Tangent Kernel (QNTK) aim to help understand [9].​  

# 2.2 Cost Function  

In Variational Quantum Algorithms (VQAs), the definition and evaluation of a cost function is paramount, serving as the objective function minimized by a classical optimizer to train the quantum circuit parameters [2,24]. Analogous to loss functions in classical machine learning, the cost function  

maps the trainable parameters  

θ  

of the parameterized quantum circuit  

U(θ)  

to a real number, where the optimization seeks the minimum value, ideally the global minimum  

$\theta =$ operatorname{argmin}_{θ} C(θ)  

[2,9,20,24].  

The cost function is typically defined based on the expectation value of a Hermitian observable or a set of observables measured on the output state of the quantum circuit [9,17,18,19,21]. For instance, in the Variational Quantum Eigensolver (VQE), the cost function is the expectation value of the problem Hamiltonian  

H,​ which can be decomposed into a weighted sum of Pauli strings:  

$$
{ \mathsf { C } } ( \theta ) = \bigtriangledown \boldsymbol { \Psi } ( \theta ) \mid \mathsf { H } \mid \boldsymbol { \Psi } ( \theta ) \bigtriangledown , \quad \mathsf { w h e r e } \quad \mathsf { H } = \boldsymbol { \Sigma } \bigtriangledown \mathsf { c } \bigtriangledown \mathsf { P } \bigotimes \bigtriangledown
$$  

[17,21]. More generally, the cost function can be expressed as a sum involving input states {ρₖ} and observables {Oₖ}:  

$$
\mathsf { C } ( \boldsymbol { \Theta } ) = \boldsymbol { \Sigma } \boxed { \mathsf { X } } \mathsf { f } \boxed { \bigstar } \mathsf { T r } ( \cdot )
$$  

[2]. Evaluation of this function on a quantum computer involves preparing the state  

$$
{ \mathsf { U } } ( \Theta ) \mathsf { \rho } { \big \boxtimes } { \mathsf { U } } + ( \Theta )
$$  

and performing measurements to estimate the expectation values of the observables $\boldsymbol { \mathrm { 0 } } \boldsymbol { \boxtimes }$ [9]. Desirable properties for a cost function include faithfulness (the minimum corresponds to the solution), efficient estimability on quantum hardware, operational meaning (quantifying solution quality), and trainability (avoiding vanishing gradients) [2,8,15].  

The specific definition or encoding strategy of the cost function significantly impacts the optimization landscape and the susceptibility to barren plateaus [3,5].  

<html><body><table><tr><td>Feature</td><td>Global Cost Function</td><td>Local Cost Function</td><td>Barren Plateau Susceptibility</td></tr><tr><td>Definition</td><td>Involves measurements/prop erties of the entire state.</td><td>Depends on measurements on a constant number of qubits.</td><td>Higher for Global, Lower for Local (especially with shallow circuits).</td></tr><tr><td>Observable Type</td><td>Non-local observables.</td><td>Local observables.</td><td>Global cost functions strongly linked to BPs.</td></tr><tr><td>Gradient Scaling</td><td></td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td>Examples</td><td>exponentially with N (qubits).</td><td>polynomially or</td><td>exponentially for</td></tr><tr><td></td><td>energy expectation</td><td>values of local Pauli</td><td>(Global) vs. C2</td></tr><tr><td></td><td>systems due to</td><td>larger systems.</td><td>Global cost functions exacerbate</td></tr></table></body></html>  

A key distinction lies between global and local cost functions. Global cost functions, which involve measurements of nonlocal observables or properties of the entire quantum state, have been strongly linked to the phenomenon of barren plateaus—where the variance of the gradient exponentially decreases with the number of qubits [5,19]. For an n-qubit system, the variance of the gradient of a global cost function​  

$$
{ \mathsf { C } } ( \boldsymbol { \Theta } ) = { \mathsf { T r } } ( \cdot )
$$  

is known to scale as  

$$
\bigstar \bigstar ( \mathsf { e } ^ { \wedge } ( - \mathsf { p } \mathsf { n } ) )
$$  

for some positive constant p, particularly when the circuit is sufficiently deep or expresses sufficient entanglement [19].  

In contrast, local cost functions, which depend only on measurements performed on a constant number of qubits or involve local observables, tend to exhibit flatter landscapes locally but can potentially mitigate barren plateaus, resulting in gradients whose variance does not vanish exponentially with system size—provided the cost function itself is sufficiently local [5].​  

The Variational Quantum State Diagonalization (VQSD) algorithm provides a clear example illustrating the impact of cost function locality [8,15]. The goal is to find a unitary  

U(α)​⃗  

that diagonalizes an initial state ρ, quantified by measuring how far  

$$
\bigstar \bigstar \bigstar \bigstar | \bigstar \bigstar | | \bigstar \bigstar | | \bigstar | | \bigstar | | \bigstar | | \bigstar
$$  

is from being diagonal [15]. Two cost functions, C₁ and $C _ { 2 } ,$ are proposed [8,15]. C₁ is defined using a global dephasing channel $\boxtimes$ , which removes all off-diagonal elements in the standard basis:  

$$
\mathsf { C } \bigotimes ( \mathsf { U } \bigotimes ( \mathsf { a } \bigotimes \mathsf { X } ) = \mathsf { T r } ( \mathsf { \rho } ^ { 2 } ) - \mathsf { T r } ( \bigotimes ( \bigotimes \mathsf { \rho } ) ^ { 2 } )
$$  

[15]. An alternative perspective defines  

$$
\mathsf C _ { 1 } = \| \boldsymbol Z ( \bigtriangledown \mathsf { \boldsymbol \rho } ) - \mathsf \boldsymbol \rho \| ^ { 2 } ,
$$  

representing the squared difference after global dephasing [8]. $C _ { 2 }$ , however, uses a sum of local dephasing channels ℨⱼ applied to individual qubits j:​  

$$
\mathsf { C } \bigotimes ( \mathsf { U } | \bigotimes ( \mathsf { a } | \bigotimes ) ) = \mathsf { T r } ( \mathsf { \rho } ^ { 2 } ) - ( \mathsf { 1 } / \mathsf { n } ) \ \Sigma \bigotimes \bigotimes \bigotimes \mathsf { T r } ( \bigotimes \bigotimes ( \bigotimes ) ^ { 2 } )
$$  

[15]. The overall cost is a weighted average given by either  

$$
\mathsf { C } = \mathsf { q } \mathsf { C } _ { 1 } + ( 1 - \mathsf { q } ) \mathsf { C } _ { 2 } \quad \mathsf { o r } \quad \mathsf { C } = \mathsf { C } _ { 1 } + \mathsf { q } \mathsf { C } _ { 2 }
$$  

[8,15]. The analysis reveals that while C₁ accurately measures the global diagonality, its gradient can vanish exponentially with increasing n, making training difficult [8]. In contrast, $C _ { 2 }$ , leveraging local information, provides a more favorable gradient scaling—particularly for larger systems where the parameter q might be reduced to give more weight to $C _ { 2 }$ [8]. This demonstrates how the choice between a globally defined measure (C₁) and a locally averaged measure $( C _ { 2 } )$ directly influences the trainability and the likelihood of encountering barren plateaus. Designing cost functions with local properties is thus a crucial strategy for mitigating barren plateaus in VQAs.​  

# 2.3 Classical Optimizer  

Classical optimizers play a crucial role in Variational Quantum Algorithms (VQAs), operating within an iterative feedback loop where a quantum computer evaluates the cost or loss function for a given set of parameters, and a classical computer updates these parameters to find the minimum [9,15]. This process aims to find the optimal parameters $\pmb { \alpha } _ { \mathrm { o p t } }$ ​ that minimize the cost function $C ( U _ { p } ( { \pmb \alpha } ) )$ associated with a parameterized quantum circuit $U _ { p } ( \pmb { \alpha } )$ [15]. The efficiency and reliability of this optimization method are paramount to the success of any VQA [2].​  

<html><body><table><tr><td>Optimizer Category</td><td>Examples</td><td>Relies on Gradient?</td><td>Barren Plateau Challenge</td><td>Notes</td></tr><tr><td></td><td>Descent, Adam, BFGS, L-BFGS, Quantum Natural Gradient.</td><td></td><td>hindered by exponentially vanishing gradients; requires high precision/meas</td><td>when gradients are large and well-defined.</td></tr><tr><td></td><td>Bayesian Optimization (BO), BOIS.</td><td></td><td>Less directly affected by vanishing gradients,but may still struggle with flat/noisy landscapes; often requires more function</td><td>Can bypass gradient estimation issues, but potentially slower convergence in non-flat regions.</td></tr></table></body></html>  

Classical optimization methods employed in VQAs can be broadly categorized into gradient-based and gradient-free approaches [2,15]. Gradient-based methods, such as standard Gradient Descent, iteratively adjust parameters in the direction indicated by the negative gradient $\nabla L ( \theta _ { t } )$ of the loss function $L ( \theta _ { t } )$ , following an update rule typically expressed as​  

where $\eta$ is the learning rate [2,3]. The gradient can often be computed using techniques like the parameter shift rule [3]. Specific implementations include Adam, which adapts the step size for potentially more efficient convergence, and BFGS or L-BFGS, commonly used for tasks like tuning QAOA parameters or exploring energy landscapes in quantum chemistry calculations, the latter often requiring explicit gradient evaluation [2,20,21]. Advanced gradient-based methods like the Quantum Natural Gradient aim to accelerate convergence by incorporating a metric tensor that reflects the sensitivity of the quantum state to parameter variations [2]. Gradient-based methods are generally efficient when the gradient is well-defined and sufficiently large, offering a clear path towards optimization objectives [8].​  

However, gradient-based methods face significant challenges in VQAs due to the phenomenon of barren plateaus [1,2,7,17]. Barren plateaus are characterized by cost function landscapes where the magnitude of the gradient decreases exponentially with the number of qubits, causing it to vanish across vast regions of the parameter space [1]. This effectively creates a flat “terrain” where gradient descent provides little to no directional information, making convergence extremely slow or impossible. Furthermore, the inherent noise in Near-Intermediate Scale Quantum (NISQ) devices can corrupt the estimation of gradients, further hindering the performance of gradient-based optimizers in these challenging, potentially flat, and noisy landscapes [2,7,17]. Classical gradient descent methods may also struggle with the high-dimensional data involved in quantum problems compared to some quantum-enhanced approaches [24].  

Gradient-free methods offer an alternative by not relying on the explicit calculation of the gradient, potentially circumventing the issue of vanishing gradients in flat regions, although they may still struggle in practice [2,7,17]. Examples include SPSA, which approximates the gradient using finite differences calculated along randomly chosen directions, and the Powell algorithm, which performs optimization without gradient information [2,15]. Other gradient-free strategies might involve matching function dependencies by optimizing subsets of parameters iteratively [2]. Advanced methods like Bayesian Optimization (BO) use a surrogate model of the global cost function to guide the search, potentially making them more robust to noise and capable of exploring complex landscapes [17]. Variants like BOIS employ parallel Bayesian optimizers to enhance efficiency [17]. Global optimization strategies, such as Basin-hopping (which uses a local minimizer like L-BFGS within a Metropolis criterion), can also be employed to explore energy landscapes and potentially avoid getting trapped in local minima or flat regions, though they are often computationally intensive [21]. While potentially less susceptible to the vanishing gradient problem in principle, gradient-free methods can be slower to converge in regions with clear gradients and may still face difficulties navigating extremely flat or noisy landscapes, highlighting that the choice of optimizer is crucial for achieving convergence in minimizing the empirical loss function [18]. Comparisons, such as that between VGON and stochastic gradient descent, suggest that alternative optimization paradigms can offer faster convergence in certain contexts [10]. The suitability of different classical optimization algorithms for noisy and potentially flat landscapes remains a critical area of investigation for the advancement of VQAs [2,7,17].​  

# 3. Understanding Barren Plateaus: Theoretical Foundations  

The phenomenon of barren plateaus (BPs) represents a significant challenge in the training of variational quantum algorithms (VQAs). Formally, barren plateaus are characterized as regions within the parameter space of a variational quantum circuit where the gradients of the cost function with respect to the variational parameters vanish or become exponentially small as the size of the quantum system, typically measured by the number of qubits $N$ , increases [4,5,6,8,19]. This leads to an effectively flat optimization landscape, hindering the ability of gradient-based optimization algorithms to find a clear path towards the optimal parameters [1,5,18].  

A key mathematical signature of barren plateaus is the scaling of the variance of the cost function gradients [4,19,27].  

For many standard VQA ansätze and cost functions, particularly with random parameter initialization, theoretical analysis reveals that the variance of the gradient of the cost function with respect to a parameter $\theta _ { i }$ decays exponentially with the number of qubits $N$ [1,4,13,27]. This exponential scaling implies that the probability of observing a non-negligible gradient, i.e., a gradient larger than some small constant $\epsilon > 0$ , decreases exponentially with system size [1,13]. Mathematically, this can be expressed as:​  

$$
P \left( \left| \frac { \partial C } { \partial \theta _ { i } } \right| > \epsilon \right) \leq e ^ { - \alpha N }
$$  

where $C$ is the cost function and $\alpha$ is a positive constant [13]. This exponential decrease in the likelihood of finding steep slopes renders parameter updates during optimization ineffective for large systems [13]. The absence of barren plateaus, conversely, is characterized by a gradient variance that does not vanish exponentially with problem size, ensuring sufficient signal for training [18].​  

The mathematical origins of barren plateaus are deeply rooted in the concentration of measure phenomenon, which is prevalent in high-dimensional vector spaces, including the Hilbert space and the parameter space of large quantum circuits [6,8]. In high dimensions, the volume of the space is concentrated near the mean, meaning most randomly sampled points are close to the average. Consequently, smooth functions defined on these spaces tend to take values close to their mean across the vast majority of the domain, resulting in small gradients [6,8]. From the perspective of the Quantum Neural Tangent Kernel (QNTK), this is reflected in the mean kernel value being inversely proportional to the square of the Hilbert space dimension, which grows exponentially with $N$ , contributing to vanishing gradients [9]. Concentration inequalities serve as crucial theoretical tools for bounding the gradient variance and quantifying the degree of concentration [16].  

Theoretical arguments link specific circuit properties to the onset of barren plateaus [4,13,19,27]. Deep quantum circuits, particularly those with random parameter initialization, tend to approximate random unitary operations and can form approximate 2-designs over the space of unitaries [4,13]. This property directly contributes to the concentration of the cost function expectation around its mean over the parameter landscape, leading to exponentially vanishing gradients [4,6,13]. The expressibility of the quantum circuit ansatz is another critical factor; while high expressibility allows access to a wider range of states, excessive expressibility can lead to overly complex and flat landscapes prone to BPs [16,19,27]. Furthermore, the entanglement generated by the circuit plays a significant role. Highly entangled states contribute to the concentration of measure and the formation of BPs [16,19,26,27]. Conversely, architectures that restrict entanglement, such as certain tensor networks like Matrix Product States, or limit expressibility through symmetry, like $S _ { n }$ -equivariant QNNs, can avoid barren plateaus by preventing the exponential vanishing of gradients [18,26]. The choice of cost function also interacts with these circuit properties; global cost functions are more susceptible to BPs than local ones [5,16,19]. Noise is also recognized as a factor that can induce barren plateaus, particularly in deep circuits [19].  

# 3.1 Definition and Mathematical Characterization  

Barren plateaus (BPs) are characterized as regions in the parameter space of variational quantum algorithms where the cost function gradients vanish, rendering the optimization landscape flat and impeding effective training of the quantum circuit [5,6,8]. A key mathematical indicator of the barren plateau phenomenon is the scaling of the expectation value and, particularly, the variance of these gradients [4,19,27]. For a variational quantum circuit with randomly initialized parameters, a necessary condition for trainability is that the probability of finding a non-negligible cost function derivative is not vanishingly small; this probability is bounded by the variance of the derivative [19].  

Theoretical studies demonstrate that for many common variational ansatz types and cost functions, the variance of the gradient typically scales exponentially to zero with increasing system size, specifically the number of qubits, denoted by $N$ [1,4,13,27]. This exponential decay implies that gradients become vanishingly small in high-dimensional Hilbert spaces, a characteristic reflected in the exponential decrease in the probability of observing a non-zero gradient beyond a certain precision [1,13]. Mathematically, this scaling is often expressed in the form:  

$$
P \left( \left| \frac { \partial C } { \partial \theta _ { i } } \right| > \epsilon \right) \leq e ^ { - \alpha N }
$$  

where $C$ is the cost function, $\theta _ { i }$ is a variational parameter, $\epsilon > 0$ is a fixed precision, $N$ is the number of qubits, and $\alpha$ is a positive constant [13]. This exponential decrease in the probability indicates that for large system sizes, the landscape becomes overwhelmingly flat, making parameter updates negligible and optimization ineffective [13]. This phenomenon has been observed, for instance, in the sample variance of energy gradients for local Pauli terms [4].  

The prevalence of barren plateaus is closely linked to the concentration of measure phenomenon in high-dimensional vector spaces. In such spaces, the vast majority of volume is concentrated in a narrow shell, meaning most randomly sampled points are close to the average. Consequently, smooth functions on these spaces tend to take values close to their mean, leading to small gradients [6,8]. The analysis of gradient scaling, including the mean and variance, is a standard approach to identify the presence or absence of barren plateaus, for example, in tensor network-based machine learning models [26].​  

Furthermore, insights from Quantum Neural Tangent Kernel (QNTK) theory support this understanding, revealing that while the mean kernel can scale with the number of parameters, its magnitude is inversely proportional to the square of the Hilbert space dimension, which grows exponentially with the number of qubits. This exponential dependence on system size directly contributes to excessively small gradients and the barren plateau effect [9].  

Conversely, the absenceof barren plateaus is mathematically characterized by demonstrating that the variance of partial derivatives does not vanish exponentially with problem size, ensuring that gradients remain sufficiently large for successful training [18]. For instance, Theorem 1 in [18] provides a specific formula for the variance of partial derivatives in permutation-equivariant QNNs that confirms non-exponential vanishing.  

Theoretical tools such as concentration inequalities play a crucial role in quantifying the bounds on gradient variance and other properties related to the concentration phenomenon [16]. These inequalities can provide analytical bounds related to sources contributing to concentration, including the expressivity of data embedding, global measurements, entanglement, and noise [16]. By providing bounds on the variance, concentration inequalities help rigorously establish the conditions under which gradients vanish exponentially, characterizing the barren plateau problem.  

# 3.2 Causes and Physical Origins  

Barren plateaus in variational quantum computing stem from several interconnected physical and theoretical factors, fundamentally related to the high dimensionality of Hilbert space and the properties of quantum circuits and cost functions used in Variational Quantum Algorithms (VQAs).​  

A primary identified cause is the use of random or highly expressive quantum circuits as ansätze, particularly with random parameter initialization in deep circuits [1,13]. These circuits, especially as depth increases, tend to approximate random unitary operations and can form approximate 2-designs over the space of unitaries [4,13]. This property leads to the phenomenon of concentration of measure, where the expectation value of the cost function becomes highly concentrated around its mean over the parameter landscape [6,13]. Consequently, the gradients of the cost function with respect to the circuit parameters vanish exponentially with the number of qubits, creating barren plateaus where optimization becomes effectively impossible [4,6,13]. This challenge is exacerbated by the inherent high dimensionality of the Hilbert space, which makes finding regions with non-vanishing gradients exceedingly difficult [4,6,13]. From the perspective of the Quantum Neural Tangent Kernel (QNTK), barren plateaus can arise when the mean kernel value $\bar { K }$ is small, which theoretical analysis suggests can occur when it is inversely proportional to the square of the Hilbert space dimension $N$ [9].​  

The entanglement properties of the quantum state generated by the ansatz also play a significant role in the formation of barren plateaus. High entanglement, particularly in tensor-network-based models, leads to a complex and rugged optimization landscape, resulting in exponentially vanishing gradients and thus barren plateaus [26,27]. Conversely, tensor networks with limited entanglement, such as Matrix Product States (MPS), can avoid this issue [26]. Entanglement in encoded quantum states, especially entanglement with a local observable, is also identified as a mechanism contributing to the exponential concentration of states, which underlies barren plateaus [16].  

The choice of cost function significantly impacts the likelihood of encountering barren plateaus. Global cost functions, which typically involve measurements over all qubits or a large subsystem, are particularly prone to exacerbating the problem [5,16]. When using global cost functions, barren plateaus can arise when the number of qubits or gates exceeds a certain threshold [5]. For instance, for a specific cost function $C _ { 1 }$ ​ , gradients may change drastically near local minima but vanish far from them, illustrating the plateau phenomenon [8]. Furthermore, for the fidelity kernel, which relies on global measurements, exponential concentration can occur even if the expressivity of the embedding and the entanglement of data states are low [16].  

Noise is another critical factor contributing to barren plateau formation in NISQ devices. Generic noise can induce barren plateaus [19]. System noise leads to the decay of VQA expressivity with increasing circuit depth, which can contribute to the optimization difficulties [3,16]. Theoretical analysis suggests that for the global case with alternating layered ansätze of arbitrary depth, a barren plateau is inevitable in the presence of generic noise [19].​  

Finally, the expressibility of the quantum circuit ansatz is a key factor. While high expressibility might seem desirable, excessive expressibility can lead to barren plateaus by creating overly complex landscapes [16,27]. Conversely, restricting the expressibility of the model can help avoid barren plateaus. For example, incorporating symmetry in quantum neural networks, such as using $S _ { n }$ ​ -equivariant QNNs, restricts expressibility and prevents the optimization landscape from becoming excessively complex and flat, thereby ensuring that gradients do not vanish exponentially and maintaining trainability [18].​  

# 3.3 Types of Barren Plateaus  

Barren plateaus (BPs) in Variational Quantum Computing (VQC) represent a significant challenge, manifesting in diverse forms with distinct origins, scopes, and implications for trainability.  

<html><body><table><tr><td>Type /Classification</td><td>Scope</td><td>Primary Origins</td><td>Characteristics / Impact</td></tr><tr><td>Global BP</td><td>Affects the entire parameterspace.</td><td>Deep random circuits, approximate 2- designs.</td><td>Exponentially vanishing gradients across the whole landscape.</td></tr><tr><td>Local BP</td><td>Affects specific regions of parameter space.</td><td>(Less emphasis in text, but implied by landscape features).</td><td>Might allow escape or finding local minima.</td></tr><tr><td>Expressibility- Induced</td><td>Can be Global or Local depending on circuit structure.</td><td>Highly expressive circuits.</td><td>Creates complex, random-like, flat landscapes.</td></tr></table></body></html>  

<html><body><table><tr><td>Entanglement- Induced</td><td>Can be Global or Local depending on entanglement spread.</td><td>Circuits generating high entanglement (vs.low- entanglement structures like MPS).</td><td>Leads to concentration of measure; strong link to global BPs.</td></tr><tr><td>Noise-lnduced</td><td>Can be Global or Local depending on noise type/level.</td><td>Generic noise, particularly in deep circuits.</td><td>Exacerbates training difficulties, can induce BPs.</td></tr></table></body></html>  

These can be broadly categorized by their scope—global or local—and their underlying causes, such as expressibility, entanglement, noise, or the specific problem instance [27]. Understanding these different types is crucial for developing effective mitigation strategies.  

One primary distinction lies in the scope of the plateau: global versus local. Global barren plateaus are characterized by exponentially vanishing gradients across the entire parameter space of the VQC circuit [13,18]. This widespread flatness makes gradient-based optimization practically impossible, as the signal-to-noise ratio for parameter updates becomes prohibitively low. A common origin for global BPs is the structure of deep random quantum circuits, which can cause the cost function landscape to become increasingly flat as the number of qubits grows [13]. In contrast, local barren plateaus might only affect specific regions of the parameter space, potentially allowing an optimizer to find a tractable training path or a local minimum. This difference in scope has a direct implication for the choice of cost function; barren plateaus are particularly prevalent when using global cost functions, whereas cost functions defined locally can sometimes be minimized effectively even in systems with a larger number of qubits [5,26].  

Beyond scope, the origin of BPs provides another critical classification [27]. Expressibility-induced barren plateaus, for example, can arise from the properties of the variational circuit itself. Highly expressive circuits, while theoretically capable of approximating complex functions, may generate landscapes that are effectively random and featureless, leading to vanishing gradients. This type of BP has been discussed in the context of tensor network machine learning, where the architecture's ability to generate entanglement and correlations plays a role [27].  

Entanglement also serves as a significant origin for BPs. Circuits capable of generating high levels of entanglement across many qubits are more prone to barren plateaus compared to those that maintain low entanglement [18,26]. The mechanism often involves the concentration of measure phenomenon; high entanglement quickly spreads information throughout the quantum state, causing observables (like cost functions) to concentrate around their mean, leading to small gradients. This is highlighted by contrasting the behavior of Matrix Product State (MPS)-based tensor networks, which are restricted to low entanglement and tend to avoid BPs, with more general tensor networks or random circuits that generate high entanglement [18,26].​  

Examples illustrating these types include the observation of global BPs in deep random quantum circuits [13]. Conversely, architectural choices can mitigate global BPs; for instance, Sₙ-equivariant Quantum Neural Networks are shown to avoid exponential vanishing of gradients across the entire parameter space by leveraging symmetry, thus not exhibiting a global BP [18]. The susceptibility difference between low-entanglement (MPS) and high-entanglement tensor networks serves as a clear example of entanglement-induced BPs [18,26]. Furthermore, the success in training with local cost functions in scenarios where global cost functions fail demonstrates the distinction between scope-based BP types and suggests a dependency on the problem structure [5,26]. While noise and specific problem instances are also cited as potential origins [27], the provided literature primarily details BPs arising from circuit properties (randomness, expressibility, entanglement) and their interaction with cost function choice.​  

# 3.4 Impact on VQC Trainability and Optimization  

Barren plateaus (BPs) have been identified as a major bottleneck significantly impeding the trainability and optimization of Variational Quantum Algorithms (VQAs) [2,13,26]. The defining characteristic of BPs is the exponential vanishing of the partial derivatives of the loss function with the system size [2], resulting in exponentially small gradients [9]. This phenomenon makes parameter updates during the optimization process largely ineffective [13], as the gradient essentially disappears into a flat landscape [1]. Consequently, parameter optimization is severely hindered [1,4,6,9,26,27], leading to extremely slow convergence or even complete failure to converge to optimal parameters [4,8,26]. In essence, the presence of a BP can prevent the algorithm from effectively navigating the optimization landscape to find target states, such as the minimum energy state in optimization problems [5,8].  

Accurately estimating these exponentially small gradients poses a further challenge. In the presence of finite sampling noise, resolving such tiny gradients requires an exponential level of precision [2]. This translates directly into a requirement for an exponentially large number of measurements or shots to obtain reliable gradient estimates [13,16]. If the number of measurements is limited to a practical, polynomial scale, the resulting gradient estimates become highly unreliable. In some cases, estimating kernel values with polynomial measurements can lead to the trained model becoming independent of the input data, rendering it useless [16].​  

The combined effect of exponentially vanishing gradients and the requirement for exponentially large measurement resources severely limits the scalability of VQC algorithms. As the number of qubits increases, the optimization process becomes exponentially harder [13], making it impractical to train VQAs for larger system sizes [9,13,19]. This fundamental challenge in training and scaling due to barren plateaus directly undermines the potential for VQC algorithms to achieve quantum advantage or quantum speedup over classical methods [2,5,7].​  

Factors contributing to trainability issues in the presence of BPs include the expressivity of the quantum circuit, where excessive expressivity can be detrimental to parameter optimization and lead to larger generalization errors [3,27]. The complexity of the optimization landscape itself, potentially featuring numerous saddle points [8] or challenging configurations of local minima and high energy transition states [21], further complicates the task of finding optimal parameters. Challenges also arise from the choice of initialization, such as using random circuits for multi-qubit systems [4]. The observation that mitigating BPs through techniques like VGON [10], careful cost function design [8], or the use of specific architectures like $S _ { n }$ -equivariant Quantum Neural Networks [18] leads to improved trainability, polynomial resource requirements, faster convergence, and better generalization [18] further substantiates the significant negative impact of barren plateaus on VQC trainability and optimization.  

# 4. Mitigation Strategies for Barren Plateaus  

Barren plateaus, characterized by the exponential vanishing of gradients with increasing system size or circuit depth, pose a fundamental challenge to the trainability of variational quantum algorithms (VQAs) [2,13]. Addressing this challenge is crucial for realizing the potential of near-term quantum computers. This section provides a systematic review and classification of the diverse strategies developed to combat barren plateaus.  

![](images/0b290a074c938e73f0675118a222c9d247d53bb0408ee5716cc4785794c25e4a.jpg)  

These strategies can be broadly categorized based on which component of the VQA framework they target: the design of the variational circuit (ansatz), the selection of initial parameter values, the definition of the cost function, or the optimization algorithm employed. Additionally, approaches leveraging non-unitary dynamics offer an alternative paradigm.  

Mitigation strategies involving Ansatz Engineering focus on tailoring the structure of the quantum circuit itself [2,13]. This includes designing shallow or structured circuits, limiting entanglement, or incorporating problem symmetries to constrain the parameter space and improve gradient scaling [1,8,18,24,26]. A critical aspect here is balancing the expressibility of the ansatz—its capacity to represent complex quantum states or functions—with its trainability, as highly expressive circuits are often more susceptible to barren plateaus [16].  

Parameter Initialization Techniques aim to select initial parameter values that place the optimization process in a region of the cost landscape with sufficiently large gradients [1,2,4]. Strategies include informed initialization through layer-wise training or pre-training, and data-driven approaches that leverage information from similar problems or initial explorations of the landscape [8,14,15,17,20].  

Cost Function Engineering involves designing the objective function to mitigate the vanishing gradient problem. A key distinction is made between global cost functions, which are generally prone to BPs, and local cost functions, which have been shown to exhibit more favorable scaling [2,5]. Other approaches include constructing problem-specific or combined cost functions and using measurement operators that respect problem symmetries [8,15,16,18,19].​  

Optimization Algorithm Modifications explore alternative or enhanced search strategies to navigate the cost landscape [17,24]. This includes modifying gradient-based methods to improve efficiency or turning to gradient-free optimization techniques like Bayesian optimization or kernel methods, which are not directly reliant on gradient magnitudes [1,17,24].  

Finally, Leveraging Non-Unitarity and Dissipation explores incorporating open-system dynamics and engineered dissipation into the variational framework. This approach posits that controlled non-unitary evolution can reshape the optimization landscape and facilitate trainability [19].  

Each category encompasses a variety of specific techniques, each with its own theoretical basis, practical feasibility, and associated trade-offs in terms of computational overhead, resource requirements, and performance gains. The subsequent subsections will delve into these strategies in detail, comparing and contrasting different approaches within this established framework.​  

# 4.1 Ansatz Engineering  

<html><body><table><tr><td>Strategy</td><td>Description</td><td>Mechanism /How it Helps BP</td><td>Notes /Trade-offs</td></tr><tr><td>Limit Depth / Entanglement</td><td>Design shallow circuits or those with low entanglement capacity.</td><td>Reduces concentration of measure;avoids approx.2-design behavior.</td><td>May limit expressivity; trade- off with problem complexity.</td></tr><tr><td>Structured Circuits</td><td>Use specific architectures like quantum convolutional networks or tensor networks.</td><td>Imposes structure on parameter space; avoids randomness/excessi ve entanglement.</td><td>Balance structure with needed expressivity.</td></tr><tr><td>Exploit Problem Symmetries</td><td>Design ansätze that respect problem symmetries (e.g., S区-equivariant).</td><td>Constrains search space to trainable subspaces; prevents exponential vanishing of gradients.</td><td>Requires problem- specific analysis; not universally applicable.</td></tr><tr><td>Problem-Tailored / Adaptive</td><td>Iteratively build or adapt circuit structure based on problem/performanc e.</td><td>Balances expressivity and trainability; manages complexity.</td><td>Can be computationally expensive; design complexity.</td></tr></table></body></html>  

Architectural choices in variational quantum algorithms profoundly influence the susceptibility to barren plateaus (BPs) [2,13]. Engineering the ansatz to incorporate structure and constraints is recognized as a crucial strategy for mitigating BPs [13].​  

One prominent approach is to limit the depth or entanglement capacity of the quantum circuit. Shallow or short-depth circuits can potentially improve gradient scaling and are suggested as a mitigation strategy [8,24]. Similarly, ansätze that inherently limit entanglement generation, such as those based on tensor network architectures like Matrix Product States (MPS), have been shown to effectively prevent the emergence of BPs while retaining sufficient model expressivity for certain problems [1,26]. This contrasts with highly expressive and entangling architectures, which are often more prone to BPs [16]. Structured circuits, including quantum convolutional networks, are also proposed to mitigate BPs by imposing a specific architecture [1].​  

Furthermore, designing ansätze that exploit the inherent structure or symmetries of the problem can lead to more favorable optimization landscapes [16,18]. By using structured ansätze that limit the explored parameter space, the optimizer can  

potentially avoid vast regions susceptible to flat gradients [2]. A key example is the development of $S _ { n }$ ​ -equivariant ansätze for problems with permutation symmetry. Ensuring the quantum circuit respects this symmetry significantly reduces susceptibility to BPs compared to generic, symmetry-agnostic circuits [18]. Guidelines for constructing such unitary $S _ { n }$ ​ - equivariant QNNs exist, focusing on the appropriate choice of equivariant generators [18]. Beyond explicit symmetry, classically constructed, problem-tailored ansätze balance expressivity with hardware constraints and optimization considerations [17]. These can be built iteratively, for instance, through growth and shrinkage phases to adapt the circuit structure to the specific problem instance while managing redundancy [17]. Problem-inspired embeddings are also favored over problem-agnostic ones, which tend to be highly expressive and entangling, thus contributing to better trainability [16]. Investigations into simplifying complex ansätze, such as truncating Unitary Coupled Cluster Singles and Doubles (UCCSD) wavefunctions, also aim to manage the ansatz complexity and potential optimization challenges [21].​  

The design of ansätze involves a critical trade-off between expressibility and trainability. Highly expressive ansätze, capable of representing complex quantum states, often suffer from poor trainability due to the prevalence of BPs. Conversely, overly constrained or shallow ansätze, while trainable, may lack the expressivity needed to reach the optimal solution. Effective ansatz engineering seeks to find a balance, providing sufficient expressivity for the problem at hand while ensuring a landscape conducive to optimization. Techniques like using low-entanglement structures or incorporating problem symmetries aim to strike this balance, achieving adequate expressivity within a trainable subspace [1,16,17,26].  

# 4.2 Parameter Initialization Techniques  

<html><body><table><tr><td>Strategy</td><td>Description</td><td>Mechanism /How it Helps BP</td><td>Application Context / Notes</td></tr><tr><td>Informed Initialization</td><td>Use results from simpler problems or circuit structures.</td><td>Starts optimization in a potentially less flat region.</td><td>Layer-wise training (use params from p layers for p+1).</td></tr><tr><td>Initialization</td><td>Leverage classical data analysis (e.g., clustering) to suggest parameters.</td><td>Uses patterns in data/instances to find good starting points.</td><td>Used in QAOA parameter setting (clustering angle values,instance features).</td></tr><tr><td>Initialization for Meta-Optimizers</td><td>Select initial points for surrogate models in global optimization.</td><td>Crucial for effectiveness of methods like Bayesian Optimization.</td><td>Space-filling designs (e.g., Latin hypercube sampling) for BO initial points.</td></tr></table></body></html>  

Parameter initialization is recognized as a critical strategy for mitigating or avoiding barren plateaus (BPs) in variational quantum algorithms [2]. Selecting appropriate initial parameters can significantly influence the initial gradient magnitudes and the subsequent optimization trajectory, potentially guiding the search away from flat regions of the cost landscape [2]. Various structured initialization strategies have been proposed to address the BP problem [1,4].​  

One notable approach is informed initialization through layer-wise training. This technique involves using the optimized parameters learned for a variational circuit with $p$ layers as the starting point for the optimization of a circuit with $p + 1$ layers [15]. This layer-wise optimization strategy is seen as a form of informed initialization [15] and is suggested to help mitigate issues like local minima [8]. By building upon successful optimization from shallower circuits, this method aims to position the initial state of the deeper circuit's optimization in a more favorable region of the parameter space.  

Beyond sequential layer optimization, other structured approaches leverage external data or learned patterns. For instance, in the context of Quantum Approximate Optimization Algorithm (QAOA), unsupervised learning techniques, such as Kmeans clustering, can be used to identify promising parameter values based on prior observations [20]. By clustering angle values, instance features, and graph embeddings, this method recommends initial angle settings for new problem instances, effectively using a data-driven approach to initialize parameters rather than relying on random sampling or heuristic rules [20]. This can be seen as a way to start the optimization closer to potentially good solutions identified in similar problem instances.  

In Bayesian Optimization (BO) applied to VQA, the process begins with an initial pool of data points, which are cost function evaluations at selected parameter values used to initialize the surrogate model [17]. These initial points are often selected using space-filling designs like Latin hypercube sampling [17]. While this initial step might not directly target avoiding BPs in the VQC cost landscape itself, the choice and distribution of these initial points are crucial for the effectiveness of the BO process and represent a form of initializing the meta-optimization strategy. The sharing of these initial points across different optimizers can further enhance efficiency [17].​  

The underlying rationale across these methods is to escape the exponential concentration of measure that leads to vanishing gradients in randomly initialized, deep quantum circuits. By providing structured, non-random initial parameters, these techniques aim to ensure that the initial state or the region explored initially possesses sufficiently large gradients to facilitate optimization. The unique properties of quantum geometry are also suggested as a potential source of inspiration for designing more effective initialization strategies for quantum neural networks [6].  

It is worth noting that the importance of specific parameter initialization techniques can depend on the circuit architecture itself. For instance, certain circuit designs, such as $S _ { n }$ ​ -equivariant Quantum Neural Networks, are inherently constructed to avoid BPs, thereby reducing the reliance on specific initialization schemes to escape flat regions of the landscape [18]. In such cases, the architectural design provides a more fundamental solution to the BP problem than initialization alone [18].  

In summary, parameter initialization offers a promising avenue for mitigating barren plateaus by influencing the starting point of the optimization trajectory. Techniques range from informed sequential optimization like layer-wise training [15] and data-driven approaches for specific algorithms like QAOA [20] to the initialization procedures within meta-optimizers like BO [17]. These structured approaches aim to provide larger initial gradients or position the search in more favorable regions compared to uniform random initialization, although the effectiveness can also be influenced by the inherent properties of the quantum circuit architecture [18].  

# 4.3 Cost Function Engineering  

<html><body><table><tr><td>Strategy</td><td>Description</td><td>Mechanism/How it Helps BP</td><td>Examples/Notes</td></tr><tr><td>Use Local Cost Functions</td><td>Define cost based on measurements of local observables (constant number of qubits).</td><td>Limits spread of correlations; gradients scale polynomially (or better).</td><td>Sum of local expectation values; C2 in VQSD example.</td></tr><tr><td>Combined Cost Functions</td><td>Use a weighted sum of different cost functions (e.g., global and local).</td><td>Leverages the favorable gradient scaling of local components for large N.</td><td>C=q C,+ (1-q) Cin VQSD example (adjust q with N).</td></tr><tr><td>Symmetric Measurements</td><td>Use measurement operators that respect problem symmetries.</td><td>Leads to better- behaved cost functions that don't contribute to BPs.</td><td>S区-equivariant measurement operators.</td></tr></table></body></html>  

The definition of the cost function significantly influences the structure of the variational landscape and, consequently, the susceptibility to barren plateaus (BPs) [2]. A primary distinction exists between global and local cost functions. Global loss functions, typically involving measurements or comparisons across the entire system or in exponentially large Hilbert spaces, are prone to exhibiting BPs as system size increases [2]. In contrast, local cost functions—which depend only on measurements of a limited number of qubits or focus on single-qubit properties—have been shown to mitigate the BP phenomenon [2]. Using local cost functions can limit the spread of correlations across the quantum circuit, potentially reducing the severity of barren plateaus [8]. Specifically, local cost functions can exhibit gradients that vanish polynomially in the number of qubits $( n )$ , provided the circuit depth is at most logarithmic in $n$ [2]. This allows for successful minimization in systems up to 100 qubits, whereas global cost functions typically face BPs on systems larger than 20 qubits [5]. For shallow circuits, local cost functions can even be immune to barren plateaus [19].​  

Beyond the global–local distinction, engineered cost functions can be designed to address specific challenges. In the context of Variational Quantum State Diagonalization (VQSD), a combined cost function $C$ has been proposed, defined as a weighted average of two functions, $C _ { 1 }$ ​ and $C _ { 2 }$ ​ :​  

$$
C ( U _ { p } ( \alpha ) ) = q C _ { 1 } ( U _ { p } ( \alpha ) ) + ( 1 - q ) C _ { 2 } ( U _ { p } ( \alpha ) )
$$  

or equivalently,  

$$
C = C _ { 1 } + q C _ { 2 }
$$  

[8,15]. Here, $q$ is a tunable parameter. For smaller system sizes $( n )$ , setting $q \approx 1$ is feasible because the landscape for $C _ { 1 }$ is less flat. For larger $\scriptstyle n$ , a smaller $q$ is preferred as $C _ { 2 }$ ​ is designed to provide the necessary gradient for training [15]. This combined approach aims to mitigate the vanishing gradient problem [8].  

Another strategy involves using measurement operators that respect problem symmetries, such as $S _ { n }$ ​ -equivariant operators. Such symmetric measurements can lead to better-behaved cost functions that do not contribute to barren plateaus [18]. Similarly, constructing projected quantum kernels by measuring local subsystems has been shown to maintain good generalization performance in scenarios where global fidelity kernels fail [16]. The provided digests do not detail techniques specifically aimed at reducing the variance of gradient estimators.​  

# 4.4 Optimization Algorithm Modifications  

<html><body><table><tr><td>Strategy</td><td>Description</td><td>Mechanism/How it Helps BP</td><td>Examples/Notes</td></tr><tr><td>Improved Gradient- Based Methods</td><td>Modify gradient descent or estimation to be more efficient or robust.</td><td>Makes gradient estimation more feasible or less resource intensive.</td><td>LCU-based gradient descent (reduces quantum resource needs).</td></tr><tr><td>Gradient-Free Optimization</td><td>Methods that do not rely on explicit gradient computation.</td><td>Bypasses the issue of vanishing gradients in flat regions.</td><td>Bayesian Optimization (BO), SPSA, Powell, BOIS.</td></tr><tr><td>Alternative Optimization Paradigms</td><td>Fundamentally different search or training frameworks.</td><td>Offers new ways to navigate complex or flat landscapes.</td><td>Variational Generative Optimization Networks (VGON), Kernel Methods (avoiding gradients).</td></tr></table></body></html>  

Addressing the challenge of barren plateaus necessitates exploring modifications to existing optimization algorithms or employing alternative optimization strategies. Standard gradient descent methods are particularly susceptible to barren plateaus, where gradients become exponentially small, hindering progress toward optimal solutions.  

One approach involves modifying gradient-based algorithms. For instance, an improved version of the quantum gradient descent algorithm has been proposed [24]. This modification leverages the LCU (Linear Combination of Unitaries) method, aiming to enhance efficiency by reducing the number of required quantum state copies and the overall circuit depth [24]. While specific adaptive learning rate schedules or the incorporation of momentum—techniques commonly used in classical optimization to navigate flat regions—are not detailed in the provided digests, modifications focused on making the gradient estimation itself more feasible or less resource intensive in the quantum context represent a relevant direction.​  

Alternatively, researchers explore gradient‐free optimization methods that bypass the explicit computation of gradients, thus potentially mitigating the direct impact of vanishing gradients in barren plateaus. Bayesian optimization (BO) is one such family of methods. A parallel optimization scheme, Bayesian Optimization with Information Sharing (BOIS), has been introduced for VQA problems parameterized by physical parameters [17]. BOIS utilizes an array of Bayesian optimizers that share quantum measurement results to optimize a collection of related problems in parallel [17]. This approach employs a lower-confidence bound (LCB) acquisition function defined as  

$$
a _ { \mathrm { L C B } } ( \theta ) = \mu ( \theta ) - \kappa \sigma ( \theta )
$$  

where $\mu$ and $\sigma ^ { 2 }$ are the mean and variance of a Gaussian process surrogate model [17]. The hyperparameter $\kappa$ is dynamically adjusted, decreasing linearly over iterations from an initial value $\kappa _ { 0 }$ ​ to zero, given by  

$$
\kappa _ { t } = \kappa _ { 0 } \frac { N - t } { N }
$$  

at iteration $t$ out of $N$ total iterations [17]. This strategy balances exploration (higher $\kappa$ ) and exploitation (lower $\kappa$ ) without relying on gradient information.  

Other gradient‐free or alternative strategies have also been suggested. It is proposed that avoiding gradients entirely and utilizing kernel methods could serve as an alternative optimization approach [1]. Furthermore, Variational Quantum Optimization based on Generative Networks (VGON) has been presented as an alternative to traditional methods like stochastic gradient descent, demonstrating faster convergence in some cases [10].  

Comparing gradient‐based and gradient‐free methods in the context of small or noisy gradients—characteristic of barren plateaus—highlights their respective trade‐offs. Gradient‐based methods, even with modifications like the LCU method, are fundamentally dependent on the magnitude and quality of gradient information. When gradients are small and noisy, their progress is severely hampered. Gradient‐free methods, on the other hand, are not directly affected by the gradient magnitude. However, they often require a larger number of function evaluations, which can be computationally expensive on quantum hardware. Their performance and resource requirements vary significantly depending on the specific algorithm. For instance, Bayesian optimization, while avoiding gradients, builds and queries a surrogate model, and parallel approaches like BOIS aim to improve efficiency across multiple related problems [17]. Kernel methods and generative network-based optimization offer different paradigms for searching the parameter space [1,10]. It is also important to note that while optimizing the algorithm is one approach, addressing the barren plateau problem can also involve designing inherently trainable circuit architectures, which may reduce the need for specialized optimization techniques altogether [18].​  

# 4.5 Leveraging Non-Unitarity and Dissipation  

While variational quantum algorithms are typically framed within a unitary evolution paradigm, exploring non-unitary dynamics and engineered dissipation presents a potential avenue for influencing and possibly mitigating barren plateaus. This approach postulates that controlled deviations from unitarity can reshape the optimization landscape in a manner conducive to gradient-based learning [19].  

One method for incorporating non-unitary dynamics involves designing a non-unitary ansatz operating on the quantum state. This can be conceptualized as a sequence of unitary operations parameterized by $\theta$ and non-unitary superoperators parameterized by σ [19]. Specifically, a non-unitary ansatz acting on a state $\rho$ can take the form  

$$
\Phi ( \sigma , \theta ) \rho = \bigtriangledown ( \sigma ) \cup ( \theta ) \rho \cup + ( \theta ) ,
$$  

where ${ \mathsf { U } } ( \theta )$ is a parametric unitary quantum circuit and $\bigstar ( \sigma )$ is a non-unitary superoperator [19]. Under a Markovian assumption, the non-unitary component $\bigstar ( \sigma )$ can be generated by a parametric Liouvillian $\bigstar ( \sigma )$ , such that  

$$
\begin{array} { r } { \bigstar ( \sigma ) = \mathbf { e } ^ { \displaystyle \wedge } ( \bigstar ( \sigma ) \Delta \mathrm { t } ) } \end{array}
$$  

for an interaction time $\Delta { \sf t }$ [19]. This Liouvillian dictates the dynamics according to the Gorini–Kossakowski–Sudarshan– Lindblad (GKLS) Master Equation:  

$$
\begin{array} { r } { \dot { \rho } = \mathcal { L } \rho \equiv - i + \displaystyle \sum _ { i } \gamma _ { i } \left( L _ { i } \rho L _ { i } ^ { \dagger } - \frac { 1 } { 2 } \{ L _ { i } ^ { \dagger } L _ { i } , \rho \} \right) } \end{array}
$$  

Here, $\boxtimes$ is the Hamiltonian, $\gamma \bigtriangledown$ are damping rates, and $\mathsf { L } \boxtimes$ are jump operators [19]. To ensure tractability and potentially beneficial properties for optimization, specific conditions can be imposed on the Liouvillian $\bigstar ( \sigma )$ . These conditions include expressing $\boxtimes$ as a sum of Q superoperators $\bigstar _ { - } \mathsf { q } ( \sigma _ { - } \mathsf { q } )$ , each acting nontrivially on at most K qubits, and requiring that all generators $\mathtt { \boxtimes \_ q }$ commute with each other [19]. Furthermore, each generator $\mathtt { \boxtimes \_ q }$ must possess exactly one stationary state ρₛₛ,q, and crucially, all generators $\mathtt { \boxtimes \_ q }$ should converge to their respective stationary states at the same rate [19].  

Engineering dissipation with such controlled properties represents a deliberate attempt to steer the state evolution and the corresponding cost function landscape away from problematic barren plateau regions.  

Beyond deliberately engineered dissipation, the broader context of noise in quantum systems also intersects with the barren plateau problem. Noise is frequently identified as a source of barren plateaus, particularly due to its effect on the expectation value gradient scaling with the number of qubits. However, the relationship between noise and optimization landscapes is complex and can be counterintuitive. It has been noted that noise might, in certain scenarios, prove helpful [9]. Specifically, noise has been suggested as a mechanism that could aid in escaping local optima or saddle points within the optimization landscape [9]. This presents a duality: while uncontrolled noise often exacerbates the barren plateau problem by washing out gradients, certain types or levels of noise might offer a limited, perhaps unintentional, benefit by perturbing the system away from suboptimal stationary points. Research into non-unitary dynamics and dissipation aims to move beyond this uncontrolled noise, seeking to harness dissipation in a structured manner to deterministically improve the optimization process.  

# 5. Barren Plateaus in Specific Contexts and Applications  

Barren plateaus represent a significant challenge hindering the practical application and scalability of Variational Quantum Computing (VQC) across diverse domains.  

This section examines the prevalence and impact of barren plateaus within key VQC application areas, including Quantum Machine Learning (QML), Quantum Chemistry, and Quantum Optimization. For each domain, the analysis focuses on how the specific problem structure, typical variational ansätze, and cost or objective functions influence the manifestation and severity of the barren plateau problem, drawing insights from relevant theoretical analyses and case studies [4,13,20,21].  

In Quantum Machine Learning (QML), particularly within Quantum Neural Networks (QNNs) and quantum kernel methods, barren plateaus manifest as exponentially vanishing gradients during training, impeding the ability to learn complex patterns and limiting the potential for quantum advantage [4,13,16,26,30]. The architectural design of QNNs profoundly influences the gradient landscape, with random or highly entangled circuits being susceptible, while low-entanglement or symmetry-informed structures like $S _ { n }$ ​ -equivariant networks may offer mitigation [6,18,26,30]. Similarly, in quantum kernel methods, barren plateaus are linked to the exponential concentration of kernel values, a phenomenon influenced by the expressivity and design of the data encoding or quantum feature map [3,16,18].​  

For Quantum Chemistry problems tackled by the Variational Quantum Eigensolver (VQE), barren plateaus contribute to the difficulty of optimizing parameterized circuits to find molecular ground state energies [1,10,11,17]. The complexity of the energy landscape in VQE is shaped by the chosen ansatz, such as Unitary Coupled-Cluster Singles and Doubles (UCCSD), and the inherent complexity of the quantum system being simulated, affecting the distribution of local minima and the severity of plateaus [3,21]. Case studies on molecules like LiH and $N _ { 2 }$ ​ demonstrate how ansatz construction impacts the landscape structure, highlighting the challenge of navigating these complex landscapes [21].  

In Quantum Optimization, algorithms like the Quantum Approximate Optimization Algorithm (QAOA) also face barren plateaus, where gradients of the objective function vanish exponentially with increasing system size, complicating parameter training [20]. The problem structure of the optimization task dictates the specific landscape that must be navigated. To address the challenge of gradient-based training in QAOA under these conditions, alternative parametersetting strategies, such as leveraging unsupervised learning techniques like clustering based on problem instances or variational graph autoencoder outputs, are being explored to find suitable parameters without relying on gradient computations [20]. This approach has shown promise in achieving competitive performance with reduced resource requirements for problems like MaxCut [20].  

Across these applications, the underlying issue of barren plateaus poses a fundamental barrier to efficient VQC training on near-term quantum hardware. While the specific mechanisms and contributing factors vary, ranging from circuit depth and entanglement to data encoding strategies and problem complexity, the result is a flattened optimization landscape that impedes parameter convergence. Consequently, significant research efforts are directed towards understanding these domain-specific manifestations and developing tailored mitigation strategies to enable the successful deployment of VQCs.  

# 5.1 Quantum Machine Learning (QML)  

Quantum Machine Learning (QML), particularly within the paradigm of Variational Quantum Algorithms (VQAs) applied to quantum neural networks (QNNs) and quantum kernel methods, represents a significant area of research aimed at  

leveraging quantum computation for machine learning tasks [9,26,30]. A principal impediment to the successful training and deployment of these models is the phenomenon of barren plateaus (BPs), characterized by the exponential vanishing of gradients with increasing system size or circuit depth [4,13]. This section analyzes the specific impact of barren plateaus on QML applications, discussing how this phenomenon limits trainability, affects the capacity to learn complex patterns, and poses challenges for achieving a demonstrable quantum advantage.  

In the context of QNNs, barren plateaus arise during the optimization process, where gradients of the loss function with respect to the variational parameters become exponentially small, making parameter updates ineffective [4,13]. This issue is particularly pronounced in randomly initialized or deeply entangled circuits [2]. The architectural design of QNNs is critically linked to the presence or absence of barren plateaus [6,30]. For instance, while random circuits tend to exhibit BPs, specific architectures, such as those with low entanglement or those incorporating geometric priors like $S _ { n }$ ​ -equivariance, have been theoretically demonstrated to potentially mitigate or avoid these training impediments [18,26]. The trainability and expressivity, often quantified via measures like the covering number, are influenced by both the chosen ansatz and the presence of noise [3]. Practical implementations on near-term hardware highlight the ongoing efforts to design architectures that are simultaneously implementable and trainable [6,30].​  

For quantum kernel methods, a related challenge manifests as exponential concentration in the values of the quantum kernel, leading to a similar flattening of the landscape and hindering the ability to distinguish between different data points [16]. This concentration is often tied to the expressivity of the quantum feature map used to encode classical data into quantum states [16]. The design of these quantum feature maps, which transform classical data points $x _ { i }$ ​ into quantum states $\rho _ { i n } ^ { i }$ , is paramount [3]. An ineffective or overly expressive encoding can contribute to the exponential concentration and subsequent barren plateaus, limiting the model's capacity to learn relevant patterns [16,18]. Effective encoding must represent information appropriately within trainable subspaces [18].  

Collectively, barren plateaus significantly constrain the trainability of QML models, impacting both QNNs and quantum kernel methods by rendering the optimization landscape flat. This limitation directly impedes the ability of these models to learn complex patterns from data and presents a substantial obstacle to demonstrating a practical quantum advantage over classical algorithms [26,30]. Research efforts are focused on understanding the underlying causes related to circuit architecture, entanglement, noise, and data encoding [2,3], as well as exploring mitigation strategies, including but not limited to, carefully designed architectures and potentially leveraging techniques such as knowledge transferability [14,18]. The analogy between QNN barren plateaus and kernel concentration highlights shared fundamental challenges across different QML paradigms [16]. Concepts like Quantum Neural Tangent Kernel (QNTK) theory are being explored to better understand these challenges [9,27].​  

# 5.1.1 Quantum Neural Networks (QNNs)  

Quantum Neural Networks (QNNs) are conceptualized as parameterized quantum circuits employed as machine learning models [1]. A significant challenge in the training of QNNs is the phenomenon of barren plateaus (BPs) [4,13]. These plateaus manifest as exponentially vanishing gradients in the parameter space, rendering optimization effectively impossible [4,13]. The occurrence and impact of barren plateaus in QNNs have been a primary focus of research in this area [4,13].​  

The architectural design of a QNN profoundly influences its gradient landscape and, consequently, its trainability [6,30]. Studies indicate that the use of random circuits in QNNs can lead to barren plateaus [13]. Conversely, careful architectural choices can potentially mitigate or avoid these issues. For instance, employing low-entanglement QNN architectures may offer a pathway to bypass barren plateaus and enhance the trainability of these models [26]. Furthermore, specifically designed architectures, such as those that are equivariant under the action of the symmetric group $S _ { n }$ ​ , have been theoretically shown to not suffer from barren plateaus and exhibit desirable properties like generalization from limited data [18].​  

Different structural ansätze for QNNs also exhibit varying characteristics that relate to their trainability and expressivity. For example, analyses of the covering number for QNNs employing hardware-efficient ansätze, tensor-network-based ansätze with matrix product state structure, and tensor-network-based ansätze with tree structure provide insights into their capabilities [3]. In ideal scenarios, the covering number for a QNN with hardware-efficient ansätze is upper bounded by $e ^ { O ( r ) }$ , where $r$ is related to the circuit depth or number of parameters [3]. However, the presence of system noise, such as depolarization noise, can alter these bounds, with the covering number potentially upper bounded by $e ^ { O ( r - m ) }$ [3]. These technical details regarding covering numbers offer a quantitative perspective on how architecture and noise jointly impact the theoretical expressiveness, which in turn affects the potential for successful training.  

Specific implementations of QNNs highlight the practical implications of architectural design. A novel deep quantum neural network model, demonstrated experimentally on a superconducting system, utilizes a layered qubit structure where parameterized quantum circuits function as inter-layer perceptrons [30]. A backpropagation-like algorithm was developed for training this model, achieving high fidelity in learning quantum channels [30]. Another example includes QNN models designed specifically for near-term quantum processors, showcasing potential for tasks like image classification through supervised learning [6]. These practical examples underscore the ongoing efforts to develop QNN architectures that are both implementable on current hardware and trainable.  

The challenge posed by barren plateaus in QNNs is also analogous to issues encountered in related areas, such as the exponential concentration observed in quantum kernel methods [16]. Concepts like the Quantum Neural Tangent Kernel (QNTK) theory are explored in the context of QNNs, further linking their trainability challenges to those in quantum kernelbased learning [9,27]. This analogy suggests that fundamental properties leading to barren plateaus may also underlie difficulties in other quantum machine learning paradigms [16]. Thus, the architectural design of QNNs is not merely about constructing circuits but critically about sculpting the loss landscape to enable efficient training and avoid the debilitating effects of barren plateaus [6,30].​  

# 5.1.2 Data Encoding and Feature Maps  

A fundamental step in applying quantum machine learning (QML) algorithms to classical data involves encoding the classical input into a quantum state. This process typically maps the classical data point $x _ { i }$ to an input quantum state $\rho _ { i n } ^ { i }$ [3]. This encoding defines a quantum feature map, transforming the classical data into a high-dimensional quantum Hilbert space where computation occurs. The design of this feature map is crucial and can significantly impact the performance and trainability of QML models, including the susceptibility to barren plateaus.  

Research indicates that the characteristics of the data encoding—or the quantum feature map—directly influence the likelihood of encountering barren plateaus. Specifically, in the context of quantum kernel methods, the expressivity of the data embedding has been identified as a source of exponential concentration in quantum kernels [16]. Exponential concentration implies that the kernel values for different data points become very similar, leading to vanishing gradients and thus barren plateaus during training or optimization processes. This highlights a trade-off where highly expressive feature maps, while potentially capable of capturing complex relationships, may suffer from trainability issues due to this concentration phenomenon.​  

Furthermore, the specific choice of encoding is critical for the effective utilization of quantum models. It has been noted that the data encoding must be designed to represent the relevant information required for tasks like classification within invariant subspaces [18]. The ability of the quantum neural network (QNN) to effectively process information within these subspaces depends heavily on how the data is encoded [18]. An encoding that fails to capture or appropriately structure the relevant features may lead to difficulties in learning, potentially manifesting as trainability issues akin to barren plateaus, as the model struggles to extract meaningful signals from the encoded state. Different approaches to encoding exist, such as using pre-computed features or employing learned embeddings from techniques like Variational Graph Auto-Encoders, as explored in the context of parameter setting for QAOA [20]. Each encoding strategy can define a distinct feature space structure, which could have varying implications for the landscape of the cost function and susceptibility to barren plateaus. Therefore, the design and selection of the data encoding method represent a critical factor influencing the trainability and success of variational QML algorithms.​  

# 5.2 Quantum Chemistry (VQE)  

The Variational Quantum Eigensolver (VQE) stands as a primary algorithm for tackling problems in quantum chemistry and condensed matter physics on near-term quantum computers, particularly for tasks such as determining molecular ground state energies [11,17,33]. Applied to electronic structure Hamiltonians [11], including specific molecules like $H _ { 2 }$ ​ , LiH, and $H _ { 3 }$ ​ [17], VQE leverages parameterized quantum circuits (ansätze) and classical optimization to find approximate solutions. However, optimizing the parameters of these circuits presents a significant challenge on Noisy Intermediate-Scale Quantum (NISQ) devices [1], largely due to issues with the structure of the objective function's landscape, most notably the presence of barren plateaus [10].​  

Barren plateaus, characterized by exponentially vanishing gradients across the parameter space, severely hinder the convergence of VQE algorithms by making parameter updates ineffective. The susceptibility of a VQE implementation to barren plateaus and the complexity of its energy landscape are critically influenced by both the choice of ansatz and the complexity of the underlying quantum system being studied.​  

Specifically focusing on the Unitary Coupled-Cluster Singles and Doubles (UCCSD) ansatz, a common choice in quantum chemistry due to its connection to established classical methods, research has investigated its energy landscapes for benchmark systems like lithium hydride (LiH) and the nitrogen molecule $( N _ { 2 } )$ [21]. These studies reveal that the structure of the energy landscape, including the presence and distribution of local minima and transition states, changes significantly depending on how the UCCSD ansatz is constructed, specifically the number and order of operators included [21]. This variability in the landscape directly impacts the optimizer's ability to find the global minimum and can contribute to convergence failures or trapping in suboptimal local minima.​  

The expressivity of the chosen ansatz also plays a role in VQE performance [3]. The expressivity of ansätze, including UCCSD, has been quantified, and this property is linked to the capacity of the ansatz to represent the target quantum state space [3]. While high expressivity is generally desirable to reach accurate solutions, it can also contribute to landscape ruggedness and potential barren plateaus. For the UCCSD ansatz, a corollary regarding its covering number provides insight into the volume of the state space it can explore, implicitly affecting the complexity of the optimization landscape over which VQE operates [3].​  

Furthermore, the complexity of the molecular system or condensed matter model itself dictates the dimensionality and structure of the Hilbert space and the corresponding VQE optimization problem. Studies on molecules ranging from simple $H _ { 2 }$ ​ and $H _ { 3 }$ ​ to LiH and $N _ { 2 }$ ​ highlight the increasing complexity of the electronic structure problem [17,21], which in turn affects the energy landscape's features, including its susceptibility to barren plateaus [21]. More complex systems typically lead to higher-dimensional parameter spaces and more intricate landscapes.  

Approaches to mitigate these issues include developing improved optimization strategies or modifying the ansatz structure or problem formulation. For example, methods like Variational Generative Optimization Networks (VGON) have been explored to address barren plateaus in variational quantum circuits applied to quantum problems such as finding the ground state of a 1D quantum spin model [10]. Similarly, approximation methods like contextual subspace VQE partition the problem to potentially simplify the quantum optimization task for electronic structure Hamiltonians [11].  

In summary, the performance and convergence of VQE for quantum chemistry and condensed matter physics are deeply intertwined with the nature of the energy landscape. This landscape is shaped by the chosen ansatz (such as UCCSD) and the complexity of the quantum system, influencing the prevalence of local minima and the severity of barren plateaus. Understanding and mitigating these landscape issues are crucial for the successful application of VQE on near-term quantum hardware.  

# 5.3 Quantum Optimization (QAOA)  

Variational Quantum Algorithms (VQAs), including the Quantum Approximate Optimization Algorithm (QAOA), are promising candidates for tackling complex optimization problems on near-term quantum hardware. However, their effectiveness is significantly challenged by the phenomenon of barren plateaus, characterized by the exponential vanishing of gradients in the parameter landscape as the number of qubits increases. This issue severely impedes the trainability of the QAOA parameters, making it difficult to effectively navigate the optimization landscape to find approximate solutions [20].  

To circumvent the difficulties associated with gradient‐based training in the presence of barren plateaus, alternative strategies are being explored for efficiently setting QAOA parameters [20]. One such approach leverages unsupervised learning techniques to determine suitable parameter values without relying on gradient computations. This can mitigate the impact of vanishing gradients and improve trainability [20]. Methods involve using clustering algorithms, which can utilize various features for grouping—such as the angle values of parameters themselves, features derived directly from the structure of the problem instance, or outputs from a variational graph autoencoder [20]. By identifying patterns or similarities within problem instances or parameter spaces, these techniques can guide parameter initialization or narrow down the search space, thereby reducing the necessity for extensive and potentially gradient‐challenged optimization loops.​  

These unsupervised parameter‐setting strategies offer tangible benefits for the practical application of QAOA on current quantum devices. They have been shown to reduce the number of required calls to quantum circuits, a crucial factor given the limitations of near-term hardware regarding noise and coherence times [20]. Their utility extends to frameworks like Recursive-QAOA (RQAOA) [20]. For instance, applying clustering-based unsupervised learning to the MaxCut problem on Erdős-Rényi graphs within RQAOA achieved competitive approximation ratios (a median of 0.94 over 200 graphs up to depth 3) while requiring only a limited number of QAOA circuit calls per iteration [20]. This highlights unsupervised learning, particularly clustering, as a promising direction for efficiently determining QAOA parameters and achieving good performance with reduced quantum resources, offering a potential pathway to navigate the challenges posed by barren plateaus in quantum optimization.​  

# 5.3.1 Unsupervised Learning for Parameter Setting in QAOA  

Gradient-based optimization methods, commonly employed in Variational Quantum Algorithms (VQAs) like QAOA, can face challenges such as the barren plateau phenomenon, where gradients vanish exponentially with the number of qubits. An alternative strategy to mitigate this issue and efficiently find effective QAOA parameters is the application of unsupervised learning techniques [20] that aim to determine suitable parameters without relying on gradient computations and, in doing so, potentially alleviate problems associated with vanishing gradients.  

One specific implementation of this strategy involves utilizing clustering techniques to guide the parameter search [20]. Different features can be used as input for the clustering algorithm. These include clustering the angle values directly, using features derived from the problem instance itself, or employing outputs obtained from a variational graph autoencoder [20]. By grouping similar instances or parameter spaces, these methods can suggest parameter initializations or narrow down the search space, reducing the need for extensive gradient-based optimization loops.​  

These unsupervised parameter-finding strategies have been shown to reduce the number of required calls to quantum circuits—a crucial consideration for near-term quantum devices given noise and limited coherence times [20]. Their utility has also been demonstrated within frameworks such as Recursive-QAOA (RQAOA) [20]. For instance, when applied to the MaxCut problem on Erdős-Rényi graphs, this approach achieved a median approximation ratio of 0.94 over a set of 200 graphs using RQAOA up to depth 3, while critically limiting the number of QAOA circuit calls per iteration to just 3 [20]. This result indicates that unsupervised learning—particularly clustering—offers a promising avenue for efficiently setting QAOA parameters and achieving good performance with reduced quantum resource requirements, thereby providing a potential pathway around the difficulties posed by barren plateaus.  

# 6. Experimental Observations and Benchmarking  

Empirical investigation into Variational Quantum Computing (VQC) on noisy intermediate-scale quantum (NISQ) hardware is crucial for understanding their practical feasibility and limitations, particularly concerning phenomena like barren plateaus. Experimental implementations have been conducted across various hardware platforms, including superconducting processors [30], nuclear magnetic resonance systems [24], and trapped ions [33], among others. These experiments provide valuable insights into VQC behavior under realistic conditions.  

Characterizing the loss landscape and performing accurate gradient estimation on NISQ devices presents significant challenges. Hardware constraints such as limited qubit counts, shallow circuit depths, and the presence of noise (including decoherence, gate infidelity, and measurement error) fundamentally impact the ability to precisely probe the landscape and measure gradients [15,28]. For instance, experiments on a 1-qubit system on the 8Q-Agave quantum computer observed cost landscapes with non-zero minima, attributed to a combination of these noise sources [8,15]. The convergence behavior of the cost function on quantum hardware has been observed to differ from that in ideal or noisy simulations, highlighting the effect of hardware-specific characteristics and tuning [8,15]. While directly demonstrating barren plateaus via the exponential decay of gradients with qubit number on large NISQ devices remains challenging due to these limitations, studies reporting sample variance of gradients showing exponential decay with increasing qubit count underscore the relevance of this theoretical problem in practice [4]. Furthermore, phenomena like “lazy training,” observed in experiments on IBM quantum computers, provide empirical context for how parameters evolve (or fail to evolve) during training on real hardware, potentially related to landscape flatness [9]. Experimental efforts aim to isolate the effects of barren plateaus from those induced purely by hardware noise, necessitating the development of sophisticated characterization techniques.​  

Benchmarking different strategies aimed at mitigating barren plateaus and generally improving VQC performance is a vital area of experimental research on quantum hardware. These studies assess the practical effectiveness and feasibility of  

proposed methods under realistic noise conditions. Benchmarking efforts compare techniques ranging from improved optimization algorithms to novel parameter setting strategies and information-sharing protocols. For example, an improved quantum gradient descent algorithm using a Linear Combination of Unitaries (LCU) approach was benchmarked on a 4- qubit system, demonstrating reduced gate operations and circuit depth as key performance indicators [24]. Similarly, the Variational Generation Optimization Network (VGON) has been benchmarked against stochastic gradient descent, showing superior performance in convergence speed and solution accuracy on quantum tasks [10]. Different strategies for parameter setting, such as using instance encodings in unsupervised learning for QAOA, have been benchmarked, demonstrating resource savings in terms of circuit calls while maintaining performance [20]. Furthermore, benchmarking of informationsharing strategies within a Bayesian Optimization framework on IBMQ devices illustrates the trade-offs between communication overhead and optimization efficiency [17]. Empirical validation through hardware experiments confirms that successful strategies yield tangible improvements in metrics like convergence speed, accuracy, or resource efficiency, often involving careful consideration of practical costs introduced by added complexity [17,20]. While some empirical validations focus on demonstrating the absence of barren plateaus in specific ansatz designs through numerical simulations, the experimental benchmarking on hardware provides crucial data on performance under realistic noise [18].  

These experimental observations and benchmarking results are indispensable for guiding the development of practically viable VQC algorithms for NISQ-era devices and beyond.  

# 6.1 Characterizing Barren Plateaus on NISQ Hardware  

Experimental characterization of barren plateaus on Noisy Intermediate-Scale Quantum (NISQ) hardware presents unique challenges due to inherent constraints and noise. These limitations affect the ability to perform accurate measurements and execute long circuit depths, both of which are relevant when investigating phenomena like barren plateaus. Practical implementations on NISQ devices aim to provide empirical insights into algorithm behavior under realistic conditions. For instance, an experimental implementation utilizing a 4-qubit system has been conducted to explore the performance of an algorithm on NISQ hardware [24]. While such experiments offer valuable data points on hardware performance, the presence of noise complicates the characterization of barren plateaus, particularly in tasks involving gradient estimation. Hardware noise can introduce errors that might mask or exacerbate the vanishing gradients associated with barren plateaus. Consequently, future experimental efforts aimed at specifically characterizing barren plateaus on NISQ devices will likely need to develop and employ sophisticated techniques to isolate the effects of barren plateaus from those induced purely by hardware noise.​  

# 6.2 Benchmarking Mitigation Strategies  

<html><body><table><tr><td>Mitigation Strategy / Method Improved</td><td>Application / Task</td><td>Benchmarked Against</td><td>Key Benchmarked</td><td>Outcome Highlights</td></tr><tr><td>Quantum Gradient</td><td>Performance on NISQ</td><td>Previous methods</td><td>Gate operations, circuit depth.</td><td>Reduced resource</td></tr><tr><td>Optimization</td><td>(e.g.,1D spin model ground</td><td>Gradient Descent (SGD)</td><td>speed, solution accuracy. compared to</td><td>and accuracy</td></tr><tr><td>Parameter Setting</td><td></td><td></td><td>calls, approximation</td><td>achieving</td></tr><tr><td>Optimization</td><td>devices</td><td>BO baselines</td><td>efficiency,</td><td>between</td></tr></table></body></html>  

<html><body><table><tr><td>Information</td><td></td><td></td><td>communication overhead.</td><td>sharing and</td></tr><tr><td>Sharing (BOIS)</td><td></td><td></td><td></td><td>performance gains.</td></tr></table></body></html>  

Evaluating the practical effectiveness of strategies aimed at mitigating challenges like barren plateaus and improving variational algorithm performance requires rigorous benchmarking on quantum hardware. Studies have explored various techniques, ranging from improvements to optimization algorithms to novel ansatz structures and information-sharing protocols.​  

Benchmarking efforts often focus on comparing the performance of proposed methods against established baselines, considering metrics such as convergence speed, solution accuracy, and resource efficiency. For instance, an improved quantum gradient descent algorithm, enhanced via a Linear Combination of Unitaries (LCU) approach, has been benchmarked against previous methods to demonstrate its feasibility on current quantum systems [24]. A key finding from this work is the reduction in quantum gate operations and circuit depth facilitated by the LCU‐based improvement, which serves as a benchmark for its effectiveness in terms of resource overhead [24].  

Similarly, the Variational Generation Optimization Network (VGON) approach has been benchmarked against stochastic gradient descent, showcasing superior performance [10]. The comparison highlights VGON's advantages specifically in terms of achieving faster convergence and higher solution accuracy, two critical factors for the practical utility of VQC algorithms, particularly when navigating potentially flat landscapes [10].  

Beyond algorithm‐specific improvements, the impact of methodological variations within established frameworks has also been benchmarked. For example, different information‐sharing strategies within a Bayesian Optimization (BO) framework for VQC have been systematically evaluated [17]. These strategies include a baseline independent BO (no sharing), independent BO with additional function evaluations, nearest‐neighbor information sharing, and all‐to‐all information sharing [17]. The performance comparison across these strategies, as illustrated in Fig. 2 of the cited work, provides insight into the benefits and potential trade-offs associated with increased communication and coordination overhead in exchange for potentially improved optimization outcomes [17]. While independent BO and independent BO with extra evaluations serve as baselines, the study explores how structured information exchange between different optimization processes might enhance overall efficiency or robustness, implicitly involving a trade-off between communication complexity and performance gain [17].​  

These benchmarking studies collectively underscore the importance of empirical evaluation on hardware to validate the effectiveness and assess the practical costs of proposed mitigation and performance enhancement techniques for VQC. They highlight that successful strategies demonstrate tangible improvements in performance metrics like convergence speed, accuracy, or resource efficiency, often involving careful consideration of the trade-offs introduced by added complexity or communication overhead.​  

# 7. Emerging Trends and Future Directions  

Addressing barren plateaus (BPs) represents a central challenge for the successful implementation and scaling of variational quantum computing algorithms on noisy intermediate-scale quantum (NISQ) devices and beyond. Key open challenges lie in both the theoretical understanding and the practical mitigation of these vanishing gradients. There is a significant need for more comprehensive theoretical frameworks capable of predicting the onset and characteristics of barren plateaus in complex, realistic VQC settings, moving beyond simplified models [9,28]. Such frameworks should aim for a deeper understanding of factors like circuit expressivity, the interplay between symmetry, expressibility, and trainability, and how these properties influence the learning performance guarantees and resource requirements of quantum machine learning models [3,18,22]. Deriving lower bounds for expressivity and exploring the concept of "moderate expressivity" are crucial steps towards this goal [3].​  

Promising research directions for developing robust, scalable, and widely applicable mitigation strategies are actively being explored. These include diverse approaches to preventing, avoiding, or mitigating the effects of barren plateaus [5]. Current strategies, while offering potential, are often heuristic and frequently lack general proofs of effectiveness across different problems and hardware platforms [5]. Research is focused on developing alternative ansätze that inherently avoid the pitfalls associated with random circuits, such as exploring structured quantum circuits or hardware-efficient ansätze designed to mitigate the concentration of measure and vanishing gradient problems [4,13]. Adaptive schemes, where the  

circuit structure or entanglement is dynamically adjusted during training, such as adaptive ansatz schemes or adaptive tensor networks, also show potential for scalability and on-device implementation [4,17,26].  

Other promising avenues include leveraging insights from geometric states to develop new training algorithms [6] and exploring the potential of Group Equivariant Quantum Machine Learning (GQML) for addressing BPs, which requires further investigation into the interplay between symmetry, expressibility, and trainability [18]. Transfer learning frameworks are also being investigated for their potential to improve VQC performance, with future work focusing on extending these methods and understanding the theoretical underpinnings of knowledge transfer [14]. Benchmarking practical VQA algorithms on real devices is being made more feasible through techniques like Bayesian Optimization with Information Sharing (BOIS) [17]. Specific algorithm variants like contextual subspace VQE are demonstrating the ability to achieve required accuracy with fewer quantum resources in certain applications, addressing both accuracy and resource efficiency challenges [11]. The advancement of quantum hardware also underpins the potential of algorithms like quantum gradient descent [24].​  

A critical aspect of future research involves understanding the interplay between barren plateaus and other fundamental limitations of NISQ devices, notably noise and limited connectivity [9,28]. Hardware noise is known to exacerbate training difficulties, cause deviations in optima, and affect final performance [2]. Noise can also lead to exponential concentration phenomena in methods like quantum kernel methods, highlighting the risk of using deep encoding schemes in the near term [16]. Quantum Error Mitigation (QEM) techniques, particularly classical post-processing methods like extrapolation, play a vital role in reducing the impact of physical errors on expectation values, although the synergy between QEM and BP mitigation requires further study [2]. Progress towards fault-tolerant quantum computing (FTQC) remains essential for truly scalable quantum computation, and lowering quantum gate error rates is crucial for both reducing the overhead of quantum error correction and enhancing the capability of NISQ algorithms [28]. Near-term platforms are seen as paving the way for future advancements [28]. In the long term, algorithms like HHL are considered important foundational elements for quantum machine learning, especially for sparse problems, underscoring the need to explore advantages beyond kernel methods [9,22].​  

Interdisciplinary research offers significant opportunities to overcome VQC challenges. Insights from classical machine learning optimization are highly relevant. For instance, supervised learning methods or using machine learning models to dynamically select heuristics can aid in parameter setting for algorithms like QAOA [20]. The integration of QML with traditional machine learning and deep learning techniques is also anticipated [25]. Furthermore, exploring connections to abstract mathematical frameworks, such as leveraging insights from category theory, may unlock new perspectives for quantum information processing and potentially for addressing trainability issues in applications like Quantum Natural Language Processing (QNLP) [1]. Future experiments are needed to demonstrate a quantum advantage in useful tasks for such interdisciplinary applications [1]. Advancements in quantum computing technology, coupled with more theoretical research and solutions to practical challenges like algorithm optimization, data security, and hardware limitations, are expected to drive breakthroughs in diverse fields [25].​  

# 8. Conclusion  

This survey has provided an overview of the barren plateau phenomenon, a critical challenge impeding the scalability and practical application of Variational Quantum Computing (VQC) algorithms, particularly within the realm of near-term intermediate-scale quantum (NISQ) devices and quantum machine learning (QML). Barren plateaus are characterized by the exponential vanishing of gradients in the cost function landscape as the number of qubits increases, making parameter optimization exceedingly difficult [4,13]. This issue directly impacts the trainability, accuracy, and efficiency of VQC algorithms, underscoring its significance for realizing quantum advantage [2,7,12].  

The primary causes of barren plateaus are closely tied to the architecture and initialization of quantum circuits. Specifically, the use of highly entangling random circuits with random parameter initialization is a major culprit, leading to the concentration of cost function gradients near zero in high-dimensional parameter space [4,13]. Beyond this fundamental cause, other factors such as exponential concentration in quantum kernel methods [16] and the inherent complexity of energy landscapes with numerous local minima in specific ansätze like UCC have been identified as contributing challenges to effective optimization [21].​  

Significant research efforts have been directed towards mitigating barren plateaus, leading to a diverse landscape of proposed techniques. These strategies often target different aspects of VQC algorithms. Architectural approaches include incorporating permutation symmetry into quantum neural networks through Group Equivariant QML (GQML) to improve trainability and generalization [18], exploring alternative ansätze structures, and employing structured initial guesses or pretraining methods [4]. Cost function design has also proven effective, with the use of local cost functions demonstrated to mitigate barren plateaus and enable scaling to larger qubit numbers [5]. Specific cost function constructions have also been explored, for instance, in the context of Variational Quantum State Diagonalization (VQSD) [8]. Furthermore, controlling entanglement in tensor network-based machine learning has been shown to mitigate barren plateaus and enhance trainability [26]. Beyond standard unitary evolution, the incorporation of non-unitary dynamics, such as engineered dissipation, presents a promising strategy to overcome trainability barriers by approximating global problems with local ones where barren plateaus are less severe or absent [19]. Advances in optimization strategies, while not solely focused on barren plateaus, contribute to navigating challenging landscapes, including improved gradient descent algorithms [24], unsupervised learning for parameter setting [20], and efficient optimization schemes like BOIS that reduce quantum resource requirements [17]. Novel approaches like Variational Generative Optimization Networks (VGON) have also shown promise in alleviating the barren plateau problem and achieving quick convergence [10]. Additionally, leveraging properties like contextuality in VQE may potentially reduce required quantum resources [11].​  

While substantial progress has been made in understanding the causes and developing initial mitigation strategies for barren plateaus, significant challenges remain. Scaling these mitigation techniques effectively to larger, fault-tolerant quantum computers is still an open question. The impact of hardware noise and the high overhead of quantum error correction in the NISQ era add further complexity [2,28]. The interplay between ansatz design, cost function structure, optimization algorithms, and the specific problem being addressed requires further investigation to develop robust and universally applicable solutions. Overcoming the challenges of trainability, efficiency, and accuracy remains paramount for realizing the full potential of VQAs in various applications [7,12].​  

In conclusion, the barren plateau problem represents a fundamental hurdle for Variational Quantum Computing. Continued theoretical and experimental research into its underlying causes and the development of effective, scalable mitigation techniques is crucial. Addressing this problem is not merely an academic exercise but a necessity for unlocking the full potential of VQC algorithms, driving breakthroughs in quantum machine learning, and paving the way for the transformative applications of quantum computing in the future [2,6,9,25].  

# References  

[1] 量子计算、范畴论与自然语言处理：人工智能的未来 https://mp.weixin.qq.com/s?   
__biz $: =$ MzIzMjQyNzQ5MA $\scriptstyle 1 = =$ &mid=2247661073&idx $: =$ 1&sn=39352b5743a163e58f6e2c729e679e58&chksm $\mid =$ e8992c9cdfeea58a0   
e1f10aba79e4d09c77f8c03266dcfc6939a6065848a5f87d7bb9cffc8ab&scene=27  

[2] 变分量子算法：原理、应用、挑战与展望 https://roll.sohu.com/a/579665101_120762490  

[3] 高效度量变分量子算法表达能力 https://mp.weixin.qq.com/s? biz=MzU0MjU5NjQ3NA $\scriptstyle = =$ &mid $=$ 2247495659&idx $\mathbf { \Psi } : = \mathbf { \Psi }$ 1&sn=ca1fe00438873218740a023c3185b96a&chksm=fb1ae3fecc6d6ae81   
3601a8d1c52d7b5d743ff2c206a456e03cf810196b54df44f490979cd15&scene=27  

[4] 量子神经网络训练中的贫瘠高原问题 https://blog.csdn.net/qq_42415326/article/details/105767942 [5] 量子机器学习新定理：解决“贫瘠高原”难题，助力量子优势 http://baijiahao.baidu.com/s? id=1695193460211503672&wfr=spider&for=pc  

[6] 谷歌量子神经网络新进展：分类与训练策略优化 https://baijiahao.baidu.com/s? id=1620204821486130152&wfr=spider&for=pc  

[7] Variational Quantum Algorithms: A Promising Path t https://www.nature.com/articles/s42254-021-00348-9 [8] VQSD论文笔记2：代价函数构造及其对估计精度的影响 https://blog.csdn.net/qq_43270444/article/details/115526357 [9] 量子人工智能：从近期NISQ到容错计算 https://cfcs.pku.edu.cn/news/241493.htm [10] 基于深度生成网络的量子问题变分优化 http://www.amss.ac.cn/mzxsbg/202504/t20250424_7610990.html [11] Contextuality in Variational Quantum Eigensolvers http://www.psych.purdue.edu/\~ehtibar/QCQMB2021/abstracts/Kirby.html [12] Variational Quantum Algorithms: An Overview and Pr https://www.zhuanzhi.ai/paper/8abc3fe194e677aea0e7d866eda7dd5a  

[13] Quantum Neural Network Training: Barren Plateaus i https://pubmed.ncbi.nlm.nih.gov/30446662/   
[14] Transfer Learning in Variational Quantum Circuits http://www.paperreading.club/page?id $| = \stackrel { \cdot } { }$ 276519​   
[15] Variational Quantum State Diagonalization for Near https://www.nature.com/articles/s41534-019-0167-6​   
[16] Exponential Concentration in Quantum Kernel Method https://link.springer.com/article/10.1038/s41467-024-49287-w   
[17] Information Sharing for Efficient Variational Quan https://www.nature.com/articles/s41534-021-00452-9​   
[18] Theoretical Guarantees for Permutation-Equivariant https://link.springer.com/article/10.1038/s41534-024-00804-1   
[19] Engineered Dissipation Mitigates Barren Plateaus i https://www.nature.com/articles/s41534-024-00875-0​   
[20] Unsupervised Learning for Parameter Setting in QAO http://dx.doi.org/10.1140/epjqt/s40507-022-00131-4​   
[21] UCCSD能量景观研究：LiH和N2的极小值和过渡态 https://pubs.acs.org/doi/10.1021/acs.jctc.4c01667   
[22] Quantum Machine Learning: Beyond Kernels and a Uni https://www.nature.com/articles/s41467-023-36159-y   
[23] Spectral Analysis of Product Formulas for Quantum  https://www.nature.com/articles/s41534-022-00548-w   
[24] Quantum Processor Optimizes Polynomial Problems vi http://en.baqis.ac.cn/news/detail/?cid $=$ 1748​   
[25] 量子机器学习：算法与应用前沿 https://blog.csdn.net/universsky2015/article/details/137310095   
[26] 邓东灵：清华大学交叉信息研究院副教授 - 量子人工智能与信息 https://iiis.tsinghua.edu.cn/zh/dengdl/   
[27] 于立伟 - 理论物理特聘研究员 - 南开大学 http://www.cim.nankai.edu.cn/ylw/list.htm   
[28] 量子计算导论：量子信息科学、物理前沿与量子计算的挑战 https://blog.csdn.net/qq542819222/article/details/122858383   
[29] VQE算法：变分量子本征求解器简介 https://blog.csdn.net/universsky2015/article/details/133565915   
[30] 邓东灵：量子人工智能与金刚石色心量子计算研究 https://sqz.ac.cn/quantum-56​   
[31] YMSC概率论讨论班：近期报告 https://ymsc.tsinghua.edu.cn/info/1053/3116.htm​   
[32] 机器学习速递[9.8]: 图学习、对抗网络、迁移学习等 https://cloud.tencent.com/developer/article/1878408   
[33] Recent Publications https://cfcs.pku.edu.cn/publications/index.htm   
[34] 无法访问 ui.adsabs.harvard.edu：连接超时 https://ui.adsabs.harvard.edu/abs/2025PhRvA.111a2441N/  