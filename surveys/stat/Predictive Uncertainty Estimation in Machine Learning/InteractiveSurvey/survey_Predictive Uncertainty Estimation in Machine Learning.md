# A Survey of Predictive Uncertainty Estimation in Machine Learning

# 1 Abstract


Predictive uncertainty estimation in machine learning is essential for building reliable and trustworthy models, particularly in high-stakes applications such as healthcare, finance, and autonomous systems. This survey paper provides a comprehensive overview of the state-of-the-art techniques for predictive uncertainty estimation, covering ensemble methods, direct learning of uncertainty, consistency and calibration of uncertainty estimates, active learning, concept drift detection, and robust learning frameworks. The main findings include the robustness and interpretability of ensemble methods like Deep Ensembles and Monte Carlo Dropout, the principled approach of direct learning methods such as Evidential Deep Learning and Bayesian Neural Networks, and the importance of consistency and calibration in ensuring reliable uncertainty estimates. The paper also highlights the integration of these methods with advanced techniques like Bayesian inference, physics-informed models, and conformal prediction. Finally, this survey aims to advance the field by providing a clear and structured understanding of the current landscape, identifying key challenges, and offering practical insights for future research and application.

# 2 Introduction
Predictive uncertainty estimation in machine learning is a critical aspect of building reliable and trustworthy models, particularly in high-stakes applications such as healthcare, finance, and autonomous systems. Machine learning models, despite their remarkable performance in various tasks, often lack a principled way to quantify the uncertainty associated with their predictions. This shortcoming can lead to overconfident or unreliable predictions, which can have severe consequences. The ability to estimate and communicate prediction uncertainty is essential for decision-makers to understand the risks and limitations of the model's outputs, enabling more informed and cautious actions. As machine learning continues to permeate critical domains, the need for robust uncertainty quantification methods has become increasingly apparent [1].

This survey paper focuses on the state-of-the-art techniques for predictive uncertainty estimation in machine learning models [2]. We provide a comprehensive overview of the methods and approaches that have been developed to address the challenges of uncertainty quantification [1]. The paper covers a wide range of topics, including ensemble methods, direct learning of uncertainty, consistency and calibration of uncertainty estimates, active learning, concept drift detection, and robust learning frameworks. We also explore the integration of these methods with advanced techniques such as Bayesian inference, physics-informed models, and conformal prediction [3]. The survey aims to provide a clear and structured understanding of the current landscape, highlighting the strengths, limitations, and potential future directions of each approach.

Ensemble methods have emerged as a robust and widely adopted approach for estimating uncertainty in machine learning models [4]. These methods, such as Deep Ensembles and Monte Carlo Dropout, leverage the diversity of multiple models to provide reliable and interpretable measures of prediction uncertainty [5]. Deep Ensembles, for instance, involve training multiple neural networks with different initializations or architectures and aggregating their predictions. This approach not only improves the robustness of predictions but also provides a natural way to quantify uncertainty through the variability of the ensemble's outputs [4]. Monte Carlo Dropout, on the other hand, extends the traditional dropout regularization technique to estimate uncertainty during inference by performing multiple forward passes with different dropout masks [6]. These ensemble methods have been shown to significantly enhance the reliability and trustworthiness of machine learning models by providing more accurate and interpretable uncertainty estimates [4].

Direct learning of uncertainty involves training models to explicitly predict the uncertainty associated with their predictions, rather than inferring it post-hoc. Techniques such as Evidential Deep Learning (EDL) and Bayesian Neural Networks (BNNs) are prominent in this domain [7]. EDL uses the network's output to parameterize a Dirichlet distribution, which captures the uncertainty in the predicted class probabilities. This method provides a principled way to disentangle aleatoric and epistemic uncertainties, offering a more nuanced understanding of the model's confidence [8]. BNNs, which explicitly model the uncertainty in the model parameters, offer a principled way to quantify both types of uncertainty. The direct learning of uncertainty is particularly valuable in applications where the reliability of predictions is critical, such as medical diagnosis and autonomous driving, by providing explicit uncertainty estimates that can help decision-makers understand the risks associated with the predictions [2].

Consistency and calibration of uncertainty estimates are fundamental principles in the evaluation of uncertainty quantification methods [5]. Consistency ensures that the uncertainty estimates are stable and coherent across different data points and model configurations, while calibration ensures that the predicted uncertainties align with the observed errors. Techniques such as temperature scaling, ensemble methods, and Bayesian approaches have been developed to improve calibration. Temperature scaling, for example, involves adjusting the logits of a model to better align the predicted probabilities with the observed accuracies. Ensuring consistency and calibration is crucial for building trustworthy and reliable models, especially in complex and high-dimensional data scenarios.

Active learning and concept drift detection are also critical components of predictive uncertainty estimation. Uncertainty-based active learning (UAL) optimizes the labeling process by focusing on the most informative samples, reducing the number of required labeled instances while maintaining or even improving model performance [9]. Early concept drift detection using prediction uncertainty leverages the inherent variability in model predictions to identify shifts in data distribution, enabling the model to adapt to changing environments. These methods are particularly valuable in dynamic applications where the data distribution can change over time, such as real-time classification tasks.

Finally, this survey paper makes several key contributions to the field of predictive uncertainty estimation [2]. First, it provides a comprehensive and structured overview of the current state-of-the-art techniques, highlighting the strengths and limitations of each approach. Second, it identifies and discusses the key challenges and open research questions in the field, providing a roadmap for future research. Third, it offers practical insights and recommendations for practitioners and researchers looking to implement and improve uncertainty quantification in machine learning models [1]. By addressing these aspects, the survey aims to advance the field and promote the development of more reliable and trustworthy machine learning systems.

# 3 Uncertainty Quantification in Machine Learning Models

## 3.1 Ensemble and Direct Learning Approaches

### 3.1.1 Ensemble Methods for Uncertainty Estimation
Ensemble methods have emerged as a robust approach for estimating uncertainty in machine learning models, particularly in deep learning, where they can provide reliable and interpretable measures of prediction uncertainty [4]. These methods leverage the diversity of multiple models to capture different aspects of the data distribution, thereby offering a more comprehensive understanding of the model's confidence in its predictions. The primary advantage of ensemble methods lies in their ability to decompose uncertainty into its constituent parts, such as aleatoric (data-driven) and epistemic (model-driven) uncertainties, which are crucial for understanding the reliability of predictions in various applications, including classification, hate speech detection, and software issue management [4].

One of the most widely used ensemble methods for uncertainty estimation is Deep Ensembles, which involves training multiple neural networks with different initializations or architectures and aggregating their predictions [4]. This approach not only improves the robustness of predictions but also provides a natural way to quantify uncertainty through the variability of the ensemble's outputs. For instance, the variance of the ensemble's predictions can be used as a measure of epistemic uncertainty, reflecting the model's uncertainty about its parameters. Similarly, the entropy of the ensemble's predicted probability distributions can be used to estimate aleatoric uncertainty, capturing the inherent noise in the data. The combination of these uncertainty measures can provide a more nuanced understanding of the model's confidence, which is particularly useful in scenarios where high-stakes decisions are involved.

Another notable ensemble method is Monte Carlo Dropout (MC Dropout), which extends the traditional dropout regularization technique to estimate uncertainty during inference [6]. By performing multiple forward passes with different dropout masks, MC Dropout generates a distribution of predictions, which can be used to estimate both epistemic and aleatoric uncertainties [6]. This method is computationally efficient and can be applied to any neural network architecture without requiring significant modifications. However, the effectiveness of MC Dropout can be sensitive to the choice of dropout rate and the number of forward passes, which need to be carefully tuned for optimal performance. Despite these challenges, ensemble methods like Deep Ensembles and MC Dropout have been shown to significantly improve the reliability and trustworthiness of machine learning models by providing more accurate and interpretable uncertainty estimates [5].

### 3.1.2 Direct Learning of Uncertainty
Direct learning of uncertainty involves training models to explicitly predict the uncertainty associated with their predictions, rather than inferring it post-hoc. This approach leverages the model's internal representations to provide a direct estimate of the confidence or uncertainty in its predictions. Techniques such as Evidential Deep Learning (EDL) and Bayesian Neural Networks (BNNs) are prominent in this domain [7]. EDL, for instance, uses the network's output to parameterize a Dirichlet distribution, which captures the uncertainty in the predicted class probabilities. This method provides a principled way to disentangle aleatoric and epistemic uncertainties, offering a more nuanced understanding of the model's confidence [8].

The direct learning of uncertainty is particularly valuable in applications where the reliability of predictions is critical, such as medical diagnosis, autonomous driving, and financial forecasting [2]. By providing explicit uncertainty estimates, these models can help decision-makers understand the risks associated with the predictions, enabling more informed and cautious actions. For example, in medical applications, a model that predicts the likelihood of a disease can also indicate its level of uncertainty, allowing doctors to seek additional tests or consult specialists when the model is unsure [10].

However, direct learning of uncertainty faces several challenges. One major challenge is the computational complexity and resource requirements, especially for methods like BNNs, which often require multiple forward passes to estimate uncertainty. Additionally, the choice of loss functions and regularization techniques can significantly impact the quality of uncertainty estimates. Recent advancements have focused on developing more efficient and scalable methods, such as single-pass uncertainty estimation techniques, to address these issues. Despite these challenges, the ability to directly learn and quantify uncertainty remains a crucial aspect of building trustworthy and robust machine learning models [2].

### 3.1.3 Consistency and Calibration of Uncertainty Estimates
Consistency and calibration are fundamental principles in the evaluation of uncertainty estimates, ensuring that the predicted uncertainties align with the observed errors and that the model's confidence is appropriately reflective of its reliability [5]. Consistency refers to the requirement that the uncertainty estimates should be stable and coherent across different data points and model configurations. For instance, a model should not exhibit drastically different uncertainty estimates for similar data points, as this would indicate a lack of robustness in the uncertainty quantification process. Calibration, on the other hand, ensures that the predicted uncertainty matches the empirical frequency of errors. A well-calibrated model will, for example, have a 90% confidence interval that contains the true value 90% of the time.

To assess consistency, several metrics have been proposed, including the variance of uncertainty estimates across similar data points and the stability of these estimates under perturbations. These metrics help in identifying models that produce erratic or unreliable uncertainty estimates. Calibration can be evaluated using reliability diagrams, which plot the predicted probability of correctness against the actual fraction of correct predictions. A perfectly calibrated model would have a reliability diagram that closely follows the diagonal line, indicating that the predicted probabilities are accurate. Additionally, metrics such as the Expected Calibration Error (ECE) and the Maximum Calibration Error (MCE) quantify the deviation from perfect calibration, providing a numerical measure of the model's calibration quality.

In practice, achieving both consistency and calibration is challenging, especially in complex models and high-dimensional data. Techniques such as temperature scaling, ensemble methods, and Bayesian approaches have been developed to improve calibration. Temperature scaling, for example, involves adjusting the logits of a model to better align the predicted probabilities with the observed accuracies. Ensemble methods, by combining multiple models, can provide more robust and consistent uncertainty estimates. Bayesian approaches, which explicitly model the uncertainty in the model parameters, offer a principled way to quantify both aleatoric and epistemic uncertainties [5]. However, these methods often come with increased computational costs and complexity, making them less feasible for large-scale applications. Despite these challenges, ensuring consistency and calibration remains crucial for building trustworthy and reliable models in various domains, including healthcare, autonomous systems, and financial forecasting.

## 3.2 Active Learning and Concept Drift Detection

### 3.2.1 Efficiency of Uncertainty-Based Active Learning
Uncertainty-based active learning (UAL) is a prominent strategy within the realm of active learning, designed to optimize the labeling process by focusing on the most informative samples [11]. The efficiency of UAL primarily stems from its ability to reduce the number of required labeled instances while maintaining or even improving model performance. By leveraging uncertainty measures, UAL algorithms can identify and prioritize samples that the model is least confident about, thereby maximizing the information gain from each labeling iteration. This approach is particularly beneficial in scenarios where labeling is costly or time-consuming, as it ensures that the effort is directed towards the most valuable data points.

The computational efficiency of UAL is influenced by several factors, including the choice of uncertainty metric, the complexity of the model, and the size of the dataset. Common uncertainty metrics include entropy, margin sampling, and least confidence, each with its own trade-offs in terms of computational cost and effectiveness. For instance, entropy-based methods, which measure the entropy of the predicted probability distribution, are computationally efficient but may not always capture the most informative samples. On the other hand, methods like Bayesian neural networks, which provide a more nuanced estimate of uncertainty, can be more effective but are computationally intensive due to the need for multiple forward passes or Monte Carlo sampling. Therefore, the selection of an appropriate uncertainty metric is crucial for balancing efficiency and performance.

Recent advancements in UAL have explored the integration of ensemble methods and deep learning techniques to enhance the efficiency and robustness of uncertainty estimation. Ensemble methods, such as bagging and boosting, can provide more reliable uncertainty estimates by aggregating predictions from multiple models, thereby reducing the risk of overfitting and improving generalization [4]. Additionally, deep learning architectures, particularly those with built-in mechanisms for uncertainty quantification, such as dropout and variational inference, offer a scalable solution for handling large datasets and complex models [12]. These approaches not only improve the accuracy of uncertainty estimates but also streamline the active learning process, making it more feasible for real-world applications.

### 3.2.2 Early Concept Drift Detection Using Prediction Uncertainty
Early concept drift detection using prediction uncertainty leverages the inherent variability in model predictions to identify shifts in data distribution. This approach is particularly valuable in dynamic environments where the underlying data distribution changes over time, such as in real-time classification tasks. By monitoring the uncertainty associated with model predictions, it is possible to detect concept drift before the performance of the model degrades significantly. Prediction uncertainty can be quantified using various metrics, including model confidence, entropy, and variance [2]. These metrics provide insights into the model's confidence in its predictions, which can be indicative of underlying distributional changes.

One of the key advantages of using prediction uncertainty for early concept drift detection is its ability to identify subtle changes in the data that may not be immediately apparent through traditional performance metrics such as accuracy or F1 score. For instance, a sudden increase in prediction uncertainty, even if the overall accuracy remains stable, can signal the onset of concept drift. This is particularly useful in scenarios where the cost of false negatives is high, and early intervention is crucial. Furthermore, prediction uncertainty can help differentiate between different types of concept drift, such as virtual drift (where the relationship between input features and output labels changes) and real drift (where the actual data distribution changes).

However, relying solely on prediction uncertainty for concept drift detection has its limitations. High prediction uncertainty can also arise from noisy data or model overfitting, which may not necessarily indicate a genuine shift in the underlying data distribution. Therefore, it is essential to combine prediction uncertainty with other drift detection methods, such as change point detection algorithms, to ensure robust and accurate detection. Additionally, the choice of uncertainty metric and the threshold for triggering a drift alert are critical parameters that need to be carefully calibrated based on the specific application and the characteristics of the data. Despite these challenges, the integration of prediction uncertainty into concept drift detection frameworks offers a promising approach to maintaining the performance and reliability of machine learning models in dynamic environments.

### 3.2.3 Remedies for Model Mismatch in Active Learning
Model mismatch in active learning (AL) often arises due to the discrepancy between the training data and the target domain, leading to suboptimal model performance. One effective remedy is the incorporation of uncertainty quantification techniques, which enable the model to identify and prioritize samples that are most informative for reducing this mismatch. Techniques such as Bayesian neural networks (BNNs) and Monte Carlo dropout provide robust estimates of epistemic uncertainty, allowing the AL system to focus on regions of the input space where the model is least confident [8]. This not only improves the model's generalization capabilities but also enhances its adaptability to new, unseen data.

Another approach to mitigating model mismatch involves the use of data augmentation and synthetic data generation [13]. By augmenting the training dataset with synthetic samples that mimic the characteristics of the target domain, the model can be better prepared to handle distributional shifts. Generative adversarial networks (GANs) and variational autoencoders (VAEs) are particularly useful in this context, as they can generate realistic samples that cover a broader range of the input space. This strategy helps to bridge the gap between the training and target distributions, thereby reducing the risk of overfitting to the initial training data and improving the model's robustness.

Finally, transfer learning and domain adaptation techniques can be employed to align the source and target domains more closely. Transfer learning leverages pre-trained models on related tasks to initialize the AL process, providing a strong starting point for the model. Domain adaptation methods, such as adversarial domain adaptation and feature alignment, further refine the model by explicitly minimizing the distributional differences between the source and target domains. These techniques ensure that the model remains effective even when faced with significant distributional shifts, thereby enhancing its overall performance and reliability in real-world applications.

## 3.3 Probabilistic and Robust Learning

### 3.3.1 Mitigating Label Noise with Probabilistic ML
Mitigating label noise in machine learning (ML) is a critical challenge, particularly in high-stakes applications such as healthcare, finance, and autonomous systems. Probabilistic ML offers a robust framework to address this issue by explicitly modeling the uncertainty in the data and predictions [14]. Unlike deterministic models, which provide point estimates, probabilistic models output a distribution over possible outcomes, allowing for a more nuanced understanding of the model's confidence. This section explores how probabilistic ML techniques can effectively mitigate label noise, focusing on methods such as Bayesian neural networks, Monte Carlo dropout, and ensemble methods [15].

One of the primary advantages of probabilistic ML in mitigating label noise is its ability to distinguish between different types of uncertainty [16]. Aleatoric uncertainty, which arises from inherent randomness in the data, can be modeled by learning the noise distribution [16]. Epistemic uncertainty, on the other hand, captures the model's lack of knowledge and can be reduced by gathering more data or improving the model [16]. Techniques like Bayesian neural networks (BNNs) provide a principled way to estimate both types of uncertainty. BNNs place priors over the model parameters and use Bayesian inference to compute the posterior distribution, which reflects the uncertainty in the model's predictions. This approach is particularly useful in scenarios where label noise is non-uniform or correlated, as it can adapt to the varying levels of noise in different parts of the data space.

Another effective method for mitigating label noise is Monte Carlo dropout, which extends the standard dropout regularization technique to approximate Bayesian inference. By performing multiple forward passes with dropout enabled, Monte Carlo dropout provides a sample from the approximate posterior distribution of the model's predictions [6]. This method is computationally efficient and can be easily integrated into existing deep learning architectures. Ensemble methods, which combine the predictions of multiple models, also offer a powerful way to reduce the impact of label noise. By averaging the predictions of diverse models, ensembles can smooth out the effects of noisy labels and provide more reliable predictions. These methods have been successfully applied in various domains, including image classification, natural language processing, and time-series forecasting, demonstrating their effectiveness in improving model robustness and reliability.

### 3.3.2 Enhancing Out-of-Distribution Detection with Self-Calibration
Enhancing Out-of-Distribution (OOD) detection through self-calibration involves leveraging the model's internal mechanisms to better estimate and adjust its confidence levels, particularly in scenarios where the input data differs significantly from the training distribution [7]. Traditional OOD detection methods often rely on thresholding the model's confidence scores, but these approaches can be unreliable due to overconfidence in incorrect predictions. Self-calibration addresses this issue by introducing mechanisms that allow the model to adapt its confidence estimates dynamically, thereby improving its ability to detect OOD samples.

One key approach to self-calibration is the use of ensemble methods, such as Monte Carlo Dropout (MCD), which provides a principled way to estimate the uncertainty of model predictions. MCD involves performing multiple forward passes through the network with different dropout masks, generating a distribution of outputs that can be used to compute the predictive uncertainty [6]. This method not only helps in identifying OOD samples but also enhances the model's robustness by reducing the impact of random dropout noise. Additionally, self-calibration techniques can incorporate feedback loops that adjust the model's confidence based on the discrepancy between predicted and actual outcomes, further refining the model's ability to recognize and handle OOD data.

Another promising direction in self-calibration for OOD detection is the integration of adversarial training and active learning. Adversarial training involves generating and incorporating adversarial examples into the training process, which helps the model learn to generalize better to unseen data. Active learning, on the other hand, focuses on selecting the most informative samples for labeling, which can include OOD samples that the model is uncertain about [11]. By combining these techniques, the model can be fine-tuned to better distinguish between in-distribution and OOD samples, leading to improved overall performance and reliability in real-world applications.

### 3.3.3 Robust Learning with Dual-Model Architectures
Robust learning with dual-model architectures represents a significant advancement in addressing the limitations of single-model approaches, particularly in handling the inherent variability and uncertainty in complex domains. Unlike single-model frameworks, which may struggle to generalize effectively across diverse scenarios, dual-model architectures leverage the complementary strengths of two models to enhance robustness and adaptability [17]. By integrating two models, one can focus on capturing stable, transferable features, while the other explores diverse characteristics of the target domain. This dual approach ensures that the models not only converge on a robust representation but also maintain the flexibility to adapt to new and unseen data.

The dual-model architecture typically consists of a primary model and a secondary model, each trained with different objectives or data subsets. The primary model is often designed to learn a general and stable representation of the data, while the secondary model is tasked with capturing more nuanced and specific patterns. This setup can be particularly beneficial in scenarios where the data distribution is highly variable or where the model needs to make decisions under uncertainty. For instance, in autonomous driving systems, the primary model might focus on recognizing common road features, while the secondary model could specialize in detecting rare or anomalous events. By combining the outputs of these two models, the system can achieve a more balanced and reliable decision-making process.

To further enhance the robustness of dual-model architectures, various techniques can be employed. One such technique is the use of uncertainty quantification methods, such as Monte Carlo Dropout or Bayesian Neural Networks, to estimate the confidence of each model's predictions. This allows the system to dynamically weigh the contributions of the primary and secondary models based on their respective uncertainties. Additionally, active learning strategies can be integrated to iteratively refine the models by selectively querying the most uncertain or informative samples. This not only improves the models' performance but also ensures that they remain adaptive to changing conditions. Overall, the dual-model architecture provides a flexible and powerful framework for building more reliable and robust machine learning systems.

# 4 Scalable and Physics-Informed Machine Learning

## 4.1 Deep Learning and Bayesian Inference

### 4.1.1 Spatially Constrained Bayesian Networks for Lithological Mapping
Spatially Constrained Bayesian Networks (SCBNs) represent a significant advancement in the field of predictive lithological mapping (PLM) by integrating spatial constraints into the Bayesian framework. Unlike traditional deterministic neural networks, SCBNs incorporate a probabilistic approach that allows for the estimation of uncertainty in predictions, which is crucial for geological applications where data is often sparse and uncertain. By treating network parameters as random variables, SCBNs can capture both aleatoric and epistemic uncertainties, providing a more robust and reliable mapping of lithological units.

The integration of spatial constraints in SCBNs is achieved through the use of spatially aware priors and likelihood functions. These priors are designed to reflect the spatial continuity and variability of geological features, ensuring that the predictions are not only statistically sound but also geologically plausible. For instance, the spatial prior can be constructed using geostatistical models such as variograms or Gaussian processes, which capture the spatial autocorrelation and spatial heterogeneity of the geological data. This approach helps to regularize the model, preventing overfitting and ensuring that the predictions are smooth and continuous, which is essential for mapping large geological areas.

In practice, SCBNs are trained using a combination of geological data, such as borehole logs, geochemical analyses, and geophysical surveys, alongside auxiliary information like satellite imagery and digital elevation models. The Bayesian framework allows for the incorporation of prior knowledge about the geological setting, which can be updated as new data becomes available. This dynamic updating process is particularly valuable in exploratory geology, where initial models can be refined and improved over time. The resulting maps not only provide a probabilistic estimate of lithological units but also quantify the uncertainty associated with these predictions, enabling geologists to make more informed decisions about resource exploration and environmental management.

### 4.1.2 Post-Hoc Calibration Techniques for Temporal Data
Post-hoc calibration techniques for temporal data aim to improve the reliability of probabilistic predictions by adjusting the output of a trained model to better align with the observed data [5]. Unlike train-time methods that integrate uncertainty quantification into the learning process, post-hoc techniques operate on the model's output, often using a separate calibration dataset [5]. This approach is particularly useful for temporal data, where the distribution of data can shift over time, leading to miscalibrated predictions. Techniques such as Platt scaling, temperature scaling, and isotonic regression are commonly employed to recalibrate the model's outputs, ensuring that the predicted probabilities more accurately reflect the true likelihood of events.

In the context of temporal data, post-hoc calibration techniques must account for the sequential nature of the data and the potential for temporal dependencies. For instance, temperature scaling, which involves adjusting the logits of a model by a learned temperature parameter, can be adapted to consider the temporal context. This adaptation might involve using a time-varying temperature parameter that captures the evolving nature of the data. Similarly, isotonic regression, which is a non-parametric calibration method, can be extended to handle time-series data by incorporating temporal smoothing techniques. These methods ensure that the calibration process respects the temporal structure, leading to more accurate and reliable probabilistic forecasts.

Moreover, recent advancements in post-hoc calibration for temporal data have explored the use of more sophisticated models, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, to capture complex temporal dependencies. These models can be trained on a calibration set to learn a mapping from the model's raw outputs to calibrated probabilities, taking into account the temporal dynamics of the data. Additionally, ensemble methods, where multiple calibration models are combined, have shown promise in improving the robustness of the calibration process. By leveraging these advanced techniques, post-hoc calibration can significantly enhance the reliability of probabilistic predictions in temporal data, making it a valuable tool in applications ranging from weather forecasting to financial market analysis.

### 4.1.3 Martingale Posteriors for Near-Bayes-Optimal Performance
Martingale Posteriors (MPs) represent a novel approach to approximating Bayesian posteriors in complex models, particularly in scenarios where traditional Bayesian inference is computationally infeasible. MPs are constructed by first generating a synthetic dataset using an algorithm \( A \), and then applying \( A \) to a combined sample of real and synthetic data. This process is designed to mimic the properties of a martingale, ensuring that the distribution of parameter estimates from the synthetic data converges to the true posterior distribution [18]. The key advantage of MPs lies in their ability to provide a good approximation of the Bayesian posterior without the need for explicit knowledge of the prior distribution \( \pi \) or the ability to perform approximate inference, which is often a bottleneck in Bayesian neural networks (BNNs).

The theoretical underpinnings of MPs are rooted in the concept of approximate martingales, which satisfy conditions similar to those of traditional martingales but with additional technical constraints. When these conditions are met, the resulting MP provides a close approximation to the Bayesian posterior in terms of Wasserstein distance [18]. This approximation is particularly useful in high-dimensional settings, where the complexity of the posterior distribution can make traditional inference methods impractical. By leveraging the properties of MPs, researchers can achieve near-Bayes-optimal performance in terms of predictive accuracy and uncertainty quantification, while significantly reducing the computational burden associated with full Bayesian inference.

In practical applications, MPs have been shown to enhance the robustness and reliability of predictive models, especially in domains where data is scarce or noisy. For instance, in the context of deep learning, MPs can be used to improve the uncertainty estimates of neural networks, making them more suitable for safety-critical applications such as autonomous driving and medical diagnostics. By providing a principled way to quantify uncertainty, MPs enable models to better handle out-of-distribution (OOD) data and reduce overconfidence, which are common issues in deterministic neural networks. Overall, the use of MPs represents a promising direction for advancing the state-of-the-art in Bayesian machine learning, offering a balance between computational efficiency and statistical rigor.

## 4.2 Physics-Informed and Hybrid Models

### 4.2.1 Probabilistic 3D Direction Prediction with Sparse Convolutions
Probabilistic 3D direction prediction is a critical task in various applications, including robotics, autonomous navigation, and computer vision [19]. Traditional approaches often rely on deterministic models that fail to capture the inherent uncertainty in direction predictions, especially in sparse or noisy data environments. Sparse convolutions, a variant of convolutional neural networks (CNNs), have emerged as a powerful tool for handling sparse data by efficiently processing irregular and non-uniformly distributed inputs. By integrating sparse convolutions with probabilistic modeling, these networks can provide not only accurate direction predictions but also reliable uncertainty estimates, which are essential for robust decision-making.

In this section, we explore the integration of sparse convolutions with probabilistic models to predict 3D directions. Sparse convolutions are designed to operate on sparse tensors, which are efficient representations of data with a high proportion of zero values. This efficiency is particularly beneficial in 3D data, where the input space is vast and often sparsely populated. By leveraging the sparsity-aware nature of these convolutions, the model can focus computational resources on the most relevant features, leading to improved performance and reduced computational costs. When combined with probabilistic frameworks, such as Gaussian processes (GPs) or Bayesian neural networks (BNNs), sparse convolutions enable the model to not only predict the direction but also quantify the uncertainty associated with each prediction.

The key advantage of this approach lies in its ability to handle the complexities of 3D directional data, which often exhibit non-linear and multi-modal distributions. By parameterizing the output distribution using the von Mises-Fisher (vMF) distribution, the model can effectively capture the spherical nature of 3D directions. The vMF distribution is a natural choice for modeling directional data on the unit sphere, providing a principled way to represent and manipulate uncertainties in the predicted directions. Experimental results on various datasets demonstrate that this integrated approach outperforms traditional deterministic models in terms of both accuracy and reliability, making it a promising direction for future research in probabilistic 3D direction prediction [19].

### 4.2.2 Cold Posteriors in Probabilistic Neural Networks
Cold posteriors in probabilistic neural networks (PNNs) refer to a phenomenon where the posterior distribution over the network's weights, obtained through Bayesian inference, is scaled down by a temperature parameter \( T < 1 \). This scaling effectively sharpens the posterior, leading to more confident predictions. The term "cold" is used because a lower temperature corresponds to a more concentrated distribution, akin to a colder physical system. The motivation behind cold posteriors is to address the issue of overfitting and overconfidence in Bayesian neural networks (BNNs), which can arise from the complexity of the posterior landscape and the high dimensionality of the weight space.

The use of cold posteriors has been shown to improve the generalization performance of BNNs, particularly in tasks where the data is limited or noisy. By reducing the temperature, the model becomes less sensitive to the noise in the training data, leading to better-calibrated uncertainty estimates. This is particularly beneficial in applications such as medical diagnosis, autonomous driving, and financial forecasting, where reliable uncertainty quantification is crucial. However, the choice of the temperature parameter \( T \) is critical, as too low a value can lead to underfitting, while too high a value can result in overfitting. Therefore, finding the optimal temperature is an active area of research, often involving empirical validation and cross-validation techniques.

Despite the benefits, the cold posterior phenomenon also raises questions about the validity of the Bayesian framework in deep learning. Critics argue that the need to artificially adjust the posterior distribution suggests that the model may be misspecified or that the prior assumptions are not well-suited to the problem at hand. This has led to a broader discussion on the role of priors in BNNs and the need for more principled approaches to model specification. Recent work has explored the use of more flexible priors, such as hierarchical and non-parametric priors, to better capture the underlying structure of the data. These developments aim to strike a balance between model flexibility and the need for robust uncertainty quantification, ultimately enhancing the practical utility of BNNs in real-world applications.

### 4.2.3 Plug-and-Play Physics-Informed Machine Learning Framework
The Plug-and-Play Physics-Informed Machine Learning (PnPPIML) framework represents a significant advancement in integrating physical laws with machine learning models, particularly for systems characterized by complex dynamics [20]. Unlike traditional machine learning approaches that rely solely on data-driven methods, PnPPIML leverages the strengths of both data and physics to enhance predictive accuracy and robustness. By incorporating physical constraints and conservation laws directly into the learning process, PnPPIML ensures that the model's predictions adhere to known physical principles, thereby reducing the risk of physically implausible outcomes.

One of the key features of PnPPIML is its modular design, which allows for seamless integration of different components, including Gaussian processes (GPs), deep neural networks (DNNs), and physics-based models. This modularity facilitates the customization of the framework to suit specific application domains, such as fluid dynamics, materials science, and robotics. For instance, GPs can be used to model the uncertainty in the system, while DNNs can capture the intricate patterns in the data. The physics-based models, on the other hand, provide the necessary constraints to ensure that the learned representations are consistent with the underlying physical laws [21]. This combination not only improves the model's predictive performance but also enhances its interpretability and reliability.

Moreover, PnPPIML addresses the challenge of data scarcity, which is prevalent in many scientific and engineering domains. By leveraging prior knowledge encoded in physical laws, the framework can effectively utilize limited data to make accurate predictions. This is particularly beneficial in scenarios where experimental data is expensive or difficult to obtain. Additionally, the framework's ability to quantify uncertainty provides valuable insights into the confidence of predictions, which is crucial for decision-making in high-stakes applications [2]. Overall, PnPPIML represents a promising direction in the development of machine learning models that are both data-efficient and physically consistent.

## 4.3 Scalable and Efficient Predictive Models

### 4.3.1 Scalable Spatiotemporal Prediction with Bayesian Neural Fields
Scalable spatiotemporal prediction with Bayesian Neural Fields (BNFs) represents a significant advancement in the integration of Bayesian methods with deep learning for handling complex, high-dimensional data. BNFs leverage the representational power of neural networks to model intricate spatiotemporal dependencies while incorporating Bayesian principles to quantify uncertainty effectively. This combination addresses the limitations of traditional Gaussian Processes (GPs), which, despite their robust uncertainty quantification, often struggle with scalability due to their cubic complexity in the number of data points. By using neural networks to define flexible kernel functions, BNFs can scale to large datasets and high-dimensional input spaces, thus enabling real-world applications in areas such as environmental monitoring, urban planning, and climate science.

The core idea behind BNFs is to use a neural network to parameterize the covariance function of a GP, thereby allowing the model to learn complex, data-driven kernels. This approach, known as deep kernel learning, enhances the flexibility of GPs by enabling them to adapt to the underlying data structure [22]. During training, the neural network learns to map inputs to a latent space where the GP operates, effectively capturing nonlinear relationships and interactions. The Bayesian framework then allows for the estimation of posterior distributions over the model parameters, providing a principled way to handle uncertainty. This is particularly useful in spatiotemporal settings where data may be sparse or noisy, and where understanding the confidence in predictions is crucial for decision-making.

To achieve scalability, BNFs employ various approximation techniques, such as inducing points and sparse approximations, which reduce the computational burden of exact GP inference. These methods enable the model to handle large datasets efficiently while maintaining the benefits of Bayesian uncertainty quantification. Additionally, BNFs can be trained using scalable optimization algorithms, such as stochastic variational inference, which allow for efficient learning even with limited computational resources. The result is a powerful and flexible framework for spatiotemporal prediction that can provide both accurate forecasts and reliable uncertainty estimates, making it a valuable tool for a wide range of applications in data-driven science and engineering.

### 4.3.2 Hybrid Models for Wind Turbine Power Prediction
Hybrid models for wind turbine (WT) power prediction integrate physics-based models with machine learning (ML) techniques to leverage the strengths of both approaches [23]. These models aim to improve the accuracy and robustness of power predictions by combining the physical understanding of the system with the data-driven capabilities of ML. In a typical hybrid model, a physics-based component is used to approximate the power output based on known physical laws and turbine characteristics. This component provides a baseline prediction that captures the deterministic aspects of the turbine's operation. However, real-world conditions often deviate from idealized assumptions, leading to discrepancies between the physics-based predictions and actual measurements. To address these discrepancies, a neural network (NN) is trained to learn the residuals, or the differences between the observed data and the physics-based predictions. This NN component captures the non-linear and stochastic variations that the physics-based model cannot account for, thereby enhancing the overall predictive accuracy.

The integration of the physics-based and ML components in hybrid models is not merely additive; it requires careful design to ensure that the NN does not overfit to the noise in the data and that the physics-based model remains interpretable. One common approach is to use a two-stage training process, where the physics-based model is first calibrated using historical data, and then the residuals are used to train the NN. This approach helps in maintaining the physical interpretability of the model while allowing the NN to capture complex patterns in the data. Additionally, the use of regularization techniques and early stopping can prevent overfitting, ensuring that the model generalizes well to new data. The performance of hybrid models is typically evaluated using metrics such as the root-mean-square error (RMSE) and mean absolute error (MAE), which quantify the accuracy of the predictions. Furthermore, the computational efficiency of these models is crucial, especially for real-time applications where rapid predictions are necessary.

An important aspect of hybrid models is their ability to provide uncertainty estimates, which are essential for decision-making in wind energy management. Uncertainty quantification can be achieved through various methods, such as Bayesian neural networks (BNNs) or conformal prediction (CP) [5]. BNNs incorporate probabilistic layers that allow for the estimation of both aleatoric and epistemic uncertainties, providing a more comprehensive understanding of the prediction reliability [8]. CP, on the other hand, is a model-agnostic technique that constructs prediction intervals with a guaranteed coverage probability, making it particularly useful for ensuring the robustness of the predictions. By combining these uncertainty quantification methods with the hybrid model, it is possible to not only improve the accuracy of power predictions but also to provide reliable confidence intervals, which are critical for optimizing the operation and maintenance of wind turbines.

### 4.3.3 Scalable Deep Kernel Learning with Kolmogorov-Arnold Networks
Kolmogorov-Arnold Networks (KANs) represent a novel approach to deep kernel learning by leveraging the Kolmogorov-Arnold representation theorem, which posits that any multivariate continuous function can be expressed as a finite sum of univariate functions and addition. This theorem provides a theoretical foundation for KANs, enabling them to decompose complex multivariate functions into simpler, univariate components. This decomposition not only simplifies the learning process but also enhances the network's ability to generalize from limited data, making it particularly suitable for high-dimensional and sparse datasets. KANs have shown significant improvements in accuracy and convergence rates compared to traditional multilayer perceptrons (MLPs), especially in tasks involving function approximation and regression.

The scalability of KANs is further enhanced by their modular architecture, which allows for efficient parallelization and distributed training. Each univariate function in the network can be independently optimized, reducing the computational burden and making the training process more efficient. This modular design also facilitates the integration of domain-specific knowledge, such as physical laws or empirical relationships, into the network. By incorporating such knowledge, KANs can achieve better performance in tasks where the underlying data-generating process is well understood, such as in scientific and engineering applications. For example, in materials science, KANs have been used to predict material properties with high accuracy, outperforming conventional shallow kernel learning methods and standard neural networks.

To address the challenges of uncertainty quantification and interpretability, Bayesian Kolmogorov-Arnold Networks (Bayesian-KANs) have been developed [24]. These networks extend the traditional KANs by integrating Bayesian inference techniques, allowing for the probabilistic modeling of network parameters [24]. The use of probabilistic splines in place of deterministic ones enables Bayesian-KANs to capture uncertainty in the predictions, providing a more robust and reliable model [24]. This is particularly valuable in safety-critical applications, such as autonomous systems and medical diagnostics, where understanding the confidence in predictions is crucial. Empirical evaluations have shown that Bayesian-KANs consistently outperform standard ensemble methods, such as deep ensembles and bootstrap, in various tasks, including Gaussian process learning, classification, and conditional density estimation [18]. The ability to provide well-calibrated uncertainty estimates makes Bayesian-KANs a promising direction for future research in scalable deep kernel learning.

# 5 Conformal Prediction and Adaptive Methods

## 5.1 Conformal Prediction Techniques

### 5.1.1 Estimating Conformal Prediction Thresholds from Noisy Labels
Estimating conformal prediction thresholds from noisy labels is a critical challenge in the application of conformal prediction (CP) to real-world datasets, where label noise is often unavoidable [25]. Traditional CP methods assume access to a validation set with clean labels to calibrate the prediction thresholds, ensuring that the prediction sets achieve the desired coverage probability. However, in scenarios where the validation set contains noisy labels, directly applying these methods can lead to overly conservative prediction sets, thereby reducing the practical utility of CP.

To address this issue, recent studies have explored various strategies to estimate noise-free conformal scores from noisy labels. One approach involves ignoring the label noise and applying the standard CP algorithm directly to the noisy validation set [26]. While this method is straightforward, it often results in large prediction sets, particularly in multi-class classification tasks, due to the increased uncertainty introduced by the noisy labels. To mitigate this, another line of research focuses on estimating the noise-free conformal score from its noisy counterpart. This involves developing statistical models or algorithms that can infer the underlying clean labels from the noisy observations, thereby allowing for more accurate threshold estimation.

A notable advancement in this area is the development of a noisy CP algorithm that incorporates conservative coverage guarantee bounds. This method aims to provide a robust coverage guarantee even in the presence of label noise, though it may still produce larger prediction sets compared to scenarios with clean labels. Our proposed approach builds upon these ideas by introducing a novel algorithm that effectively estimates the conformal prediction thresholds from noisy labels while maintaining a balance between coverage and prediction set size [26]. This is achieved through a combination of statistical inference techniques and adaptive thresholding, ensuring that the prediction sets remain both reliable and practical for real-world applications.

### 5.1.2 Accelerating Ensemble Error Bar Prediction with Single Model Fit
Accelerating Ensemble Error Bar Prediction with Single Model Fit involves leveraging the efficiency of single model predictions while maintaining the robustness and reliability of ensemble methods. In traditional ensemble methods, multiple models are trained and their predictions are aggregated to form a final prediction and error bar, which can be computationally expensive and time-consuming. By contrast, the single model fit approach aims to approximate the performance of ensemble methods using a single, well-calibrated model. This is achieved by incorporating uncertainty quantification techniques that can effectively capture the variability and uncertainty inherent in the data and model predictions [1].

One of the key strategies in this approach is the use of conformal prediction (CP) to generate prediction sets that provide statistical coverage guarantees [27]. CP is particularly advantageous because it is non-parametric and model-agnostic, making it a flexible tool for various machine learning models. In the context of single model fit, CP can be applied by first training a single model on the data and then using a validation set to calibrate the prediction intervals. The conformity score, which measures the compatibility of a new observation with the training data, is used to construct these intervals. By carefully selecting the conformity score, it is possible to balance the trade-off between the width of the prediction intervals and the computational efficiency of the single model [3].

To further enhance the performance of the single model fit approach, several techniques can be employed to refine the prediction intervals. For instance, adaptive conformal prediction can dynamically adjust the prediction intervals based on the local data characteristics, leading to more precise and reliable error bars. Additionally, methods such as quantile regression and Bayesian neural networks can be integrated to provide more nuanced uncertainty estimates. These techniques not only improve the accuracy of the error bars but also reduce the computational overhead associated with traditional ensemble methods, making the single model fit approach a viable and efficient solution for real-world applications.

### 5.1.3 Leave-One-Out Stable Conformal Prediction
Leave-One-Out Stable Conformal Prediction (LOO-StabCP) is an advanced technique that builds upon the principles of conformal prediction to enhance the stability and efficiency of prediction sets [3]. Unlike traditional conformal prediction methods that require recalibration for each new prediction, LOO-StabCP leverages the concept of leave-one-out cross-validation to estimate the nonconformity scores [3]. This approach ensures that the prediction model is not overly sensitive to individual data points, thereby improving the robustness of the prediction intervals. By using leave-one-out nonconformity scores, LOO-StabCP effectively mitigates the need for repeated model refitting, which is a significant computational bottleneck in methods like replace-one stable conformal prediction (RO-StabCP).

In LOO-StabCP, the nonconformity score for a test point is calculated by considering the model's performance when each training point is left out. This process involves training the model \( n \) times, where \( n \) is the number of training points, and each time a different training point is excluded. The nonconformity score for a test point \( x_{n+1} \) is then determined by the distribution of these leave-one-out residuals. This method ensures that the prediction sets are stable and reliable, even when the training data contains outliers or is subject to distributional shifts. The computational efficiency of LOO-StabCP is further enhanced by leveraging techniques such as approximate leave-one-out methods, which can significantly reduce the computational burden without compromising the accuracy of the prediction sets.

The stability and efficiency of LOO-StabCP make it particularly suitable for applications where real-time or frequent predictions are required, such as in online learning and adaptive systems. By providing a balance between computational efficiency and predictive accuracy, LOO-StabCP offers a practical solution to the challenges faced by traditional conformal prediction methods [3]. Moreover, the method's ability to handle noisy and non-i.i.d. data makes it a valuable tool in a wide range of machine learning tasks, including classification, regression, and anomaly detection. The robustness of LOO-StabCP in these scenarios is a testament to its potential to improve the reliability and trustworthiness of machine learning models in real-world applications.

## 5.2 Adaptive and Robust Learning

### 5.2.1 Balanced Admission Control for Idiosyncrasy and Delay
Balanced Admission Control for Idiosyncrasy and Delay (BACID) is a method designed to optimize the trade-off between the idiosyncratic costs and delays associated with decision-making in dynamic systems. In such systems, each decision (e.g., admitting or removing a post on a platform) incurs two types of losses: the idiosyncratic loss, which is specific to the individual decision and can be avoided by not making the decision, and the delay loss, which accumulates due to the time required to process the decision. BACID aims to minimize the total expected loss by carefully balancing these two components.

To achieve this balance, BACID first calculates the expected ex-ante per-period loss of each decision using the distributional information of the true cost \( c_j \). For instance, the expected loss of keeping a post on the platform is computed based on the probability distribution of the post's potential negative impact, such as user dissatisfaction or regulatory penalties. Similarly, the expected loss of removing a post is estimated based on the potential loss of user engagement or content value. The method then estimates the delay loss by considering the number of posts of the same type currently waiting for review, which reflects the system's current workload and the potential backlog that could arise from admitting more posts [28].

Finally, BACID makes a decision to admit a post if the weighted idiosyncrasy loss (which can be avoided by not admitting the post) exceeds the estimated delay loss (which is incurred by admitting the post). This approach ensures that the system remains responsive and efficient while minimizing the overall cost. By dynamically adjusting the admission criteria based on the current state of the system, BACID provides a robust framework for managing the complexities of real-time decision-making in dynamic and uncertain environments.

### 5.2.2 Adversarial Reweighting for Domain Adaptation
Adversarial Reweighting for Domain Adaptation (ARDA) addresses the issue of domain shift by dynamically adjusting the importance of different samples during the training process. In traditional domain adaptation, the assumption is that the source and target domains share the same label space, but this often does not hold in practical scenarios, leading to negative transfer and reduced performance. ARDA introduces an adversarial reweighting mechanism that assigns higher weights to target domain samples that are more likely to belong to the common class, while reducing the influence of samples from the source domain that do not align well with the target domain. This is achieved through a min-max optimization problem where the model simultaneously learns to predict labels and to generate weights that minimize the discrepancy between the source and target distributions.

The core of the ARDA framework lies in the adversarial reweighting module, which uses a discriminator to estimate the importance of each sample. The discriminator is trained to distinguish between source and target domain samples, while the reweighting module aims to fool the discriminator by generating weights that make the weighted source samples indistinguishable from the target samples. This process ensures that the model focuses more on the relevant parts of the source domain and less on the parts that might introduce noise or bias. By iteratively updating the weights and the model parameters, ARDA effectively adapts the model to the target domain, even when the target domain contains classes not present in the source domain. This approach is particularly useful in scenarios where labeled target data is scarce or unavailable, as it leverages the structure of the target domain to guide the adaptation process.

To further enhance the robustness of the model, ARDA incorporates a regularization term that encourages the model to maintain a low prediction uncertainty for target domain samples that are confidently assigned high weights. This helps to prevent overfitting to noisy or irrelevant samples and ensures that the model remains reliable and interpretable. Additionally, the reweighting mechanism can be extended to handle open-set and universal domain adaptation by assigning smaller weights to target domain samples that are likely to belong to unseen classes. This flexibility makes ARDA a powerful tool for a wide range of domain adaptation tasks, from image classification to natural language processing, where the distribution of data can vary significantly across domains.

### 5.2.3 Two-Stage Risk Control for Ranked Retrieval Systems
In the realm of ranked retrieval systems, the dual nature of the retrieval process necessitates a nuanced approach to risk management. Each stage—retrieval and ranking—presents unique challenges and requires tailored risk control mechanisms [25]. The retrieval stage focuses on efficiently filtering a vast corpus to a manageable subset, while the ranking stage refines this subset to present the most relevant results. To address these distinct requirements, a two-stage risk control framework is proposed, which identifies and optimizes parameters that simultaneously satisfy the risk constraints for both stages. This framework ensures that the retrieval process not only retrieves a sufficient number of potentially relevant items but also ranks them accurately, thereby enhancing the overall user experience.

The introduction of retrieval risk and ranking risk provides a structured approach to quantifying and managing uncertainties in each stage [25]. Retrieval risk is defined as the probability of missing relevant items during the initial filtering phase, while ranking risk pertains to the likelihood of misordering the retrieved items. By formulating these risks mathematically, the two-stage risk control methods can derive prediction sets that maintain the desired risk levels. These methods are designed to be model-agnostic, allowing seamless integration into various ranked retrieval systems, regardless of the underlying retrieval and ranking algorithms. This flexibility is crucial for adapting to the diverse and evolving landscape of information retrieval.

Empirical evaluations have demonstrated that the proposed two-stage risk control framework achieves competitive performance in terms of both retrieval efficiency and ranking accuracy, while ensuring robust risk management [25]. The ability to control both retrieval and ranking risks at pre-specified levels enhances the reliability and trustworthiness of the retrieval system, making it particularly suitable for applications where precision and recall are critical. Moreover, the framework's adaptability to different models and datasets underscores its potential for widespread adoption in both academic research and industrial applications.

## 5.3 Online and Parameter-Free Methods

### 5.3.1 Adaptive Conformal Predictors with Online Convex Optimization
Adaptive conformal predictors with online convex optimization represent a significant advancement in the field of conformal prediction (CP), addressing the dynamic nature of data streams and the need for real-time adjustments [3]. In this framework, the conformal predictor is designed to adapt to distribution shifts and maintain coverage guarantees by leveraging online convex optimization techniques. These techniques, such as online gradient descent and parameter-free optimization, enable the predictor to dynamically adjust its parameters based on incoming data, ensuring that the model remains robust and accurate over time. The key innovation lies in the integration of algorithmic stability, which ensures that small changes in the data do not lead to large changes in the prediction sets, thus maintaining the reliability of the conformal predictor.

The adaptive conformal predictor with online convex optimization is particularly effective in handling non-stationary environments where the data distribution may change over time. By continuously updating the model parameters, the predictor can adapt to these changes without requiring a complete retraining of the model. This is achieved through the use of regret minimization techniques, which aim to minimize the difference between the performance of the adaptive predictor and an optimal (but unknown) benchmark predictor. The sub-linear regret bounds provide a theoretical guarantee that the adaptive predictor will perform well in the long run, even in the presence of distribution shifts. Empirical evaluations have shown that this approach not only maintains the desired coverage guarantees but also outperforms static conformal prediction methods in terms of the size and accuracy of the prediction sets [27].

To further enhance the adaptability and robustness of the conformal predictor, recent research has explored the use of two-stage conformal prediction methods [3]. These methods involve a learn-then-test framework, where the first stage focuses on learning a model that is robust to distribution shifts, and the second stage applies the conformal prediction algorithm to quantify the uncertainty of the predictions [3]. This two-stage approach has been shown to be particularly effective in scenarios where the data distribution is highly non-stationary or where there is a significant domain shift between the training and test data. By combining the strengths of online convex optimization and two-stage conformal prediction, this framework provides a powerful tool for real-time decision-making in dynamic environments.

### 5.3.2 Adaptive Uncertainty-Guided Knowledge Transfer
Adaptive Uncertainty-Guided Knowledge Transfer (AUKT) represents a significant advancement in addressing the challenges associated with knowledge transfer under data or model uncertainty. Unlike traditional knowledge transfer methods that often assume a static and reliable teacher model, AUKT dynamically adjusts the influence of the teacher based on the quantified uncertainty of its predictions [27]. This dynamic adjustment is crucial in scenarios where the teacher model might encounter out-of-distribution data or experience performance degradation due to domain shifts. By leveraging conformal prediction (CP), AUKT constructs uncertainty quantification wrappers that can be seamlessly integrated with various prediction models, enabling the system to adaptively weigh the teacher's guidance [27].

The core mechanism of AUKT involves the use of CP to generate uncertainty sets for the teacher's predictions. These sets provide a probabilistic guarantee that the true label lies within the predicted interval, thus offering a principled way to measure the reliability of the teacher's outputs. When the uncertainty is high, indicating low confidence in the teacher's predictions, AUKT reduces the reliance on the teacher, thereby preventing the propagation of errors to the student model. Conversely, when the teacher's predictions are highly confident, AUKT increases the influence of the teacher, accelerating the learning process. This adaptive weighting ensures that the student model benefits from the teacher's expertise while mitigating the risks associated with overfitting to unreliable guidance.

In practice, the effectiveness of AUKT is particularly evident in complex and dynamic environments where data distributions can shift unpredictably. For instance, in computer vision tasks, where domain adaptation is critical, AUKT can help the student model generalize better to new, unseen domains by selectively leveraging the teacher's knowledge [27]. Similarly, in medical applications, where accurate and reliable predictions are paramount, AUKT can enhance the robustness of the student model by dynamically adjusting to the varying levels of uncertainty in the teacher's predictions. Overall, AUKT provides a flexible and robust framework for knowledge transfer, making it a valuable tool in a wide range of machine learning applications.

### 5.3.3 Sub-Linear Regret Bounds for Online Quantile Estimation
In the context of online quantile estimation, sub-linear regret bounds play a crucial role in ensuring that the model's performance improves over time, even when faced with non-stationary data streams. The regret, defined as the cumulative difference between the model's predictions and the optimal quantile estimates, is often measured using the pinball loss function. This loss function penalizes underestimates and overestimates differently, depending on the desired quantile level, thereby aligning the model's objectives with practical requirements in various applications such as financial risk management and supply chain optimization.

To achieve sub-linear regret bounds, several advanced techniques have been proposed, including algorithms based on coin betting frameworks [1]. These methods dynamically adjust the learning rate and the weights assigned to different data points, allowing the model to adapt to changes in the data distribution without requiring explicit knowledge of the underlying process. Coin betting algorithms, in particular, leverage the concept of betting on the outcomes of a biased coin, where the bet size is adjusted based on past outcomes. This adaptive mechanism ensures that the model can quickly respond to shifts in the data, leading to improved long-term performance and sub-linear regret growth.

Moreover, the theoretical foundations of these algorithms provide strong guarantees on their convergence properties, making them suitable for real-world applications where data streams are inherently unpredictable. Empirical studies have shown that these methods not only achieve sub-linear regret but also maintain high levels of accuracy and robustness, even in the presence of noise and outliers. The ability to handle large volumes of data efficiently further enhances their practical utility, making them a valuable addition to the toolkit of online learning algorithms for quantile estimation.

# 6 Future Directions


The current landscape of predictive uncertainty estimation in machine learning, despite significant advancements, still faces several limitations and gaps. Ensemble methods, while robust, can be computationally expensive and may not always provide clear insights into the types of uncertainty present in the model. Direct learning of uncertainty, although promising, often requires careful tuning of hyperparameters and can be sensitive to model architecture. Additionally, the consistency and calibration of uncertainty estimates remain challenging, particularly in high-dimensional and complex data scenarios. Active learning and concept drift detection methods, while effective in optimizing the labeling process and adapting to changing data distributions, often struggle with scalability and the need for domain-specific adjustments. Finally, the integration of physics-informed models and scalable deep learning techniques, while enhancing predictive accuracy, can introduce additional complexity and require specialized expertise.

To address these limitations, several promising directions for future research can be explored. First, the development of more efficient and interpretable ensemble methods is essential. Research could focus on optimizing the diversity of ensemble members, exploring lightweight ensemble architectures, and developing more effective aggregation techniques that can handle large-scale and high-dimensional data. Additionally, the integration of ensemble methods with active learning strategies could lead to more efficient and adaptive models, particularly in resource-constrained environments.

Second, advancements in direct learning of uncertainty should aim to reduce computational complexity and improve the robustness of uncertainty estimates. Techniques such as single-pass uncertainty estimation, which can provide reliable uncertainty measures with minimal computational overhead, are particularly promising. Furthermore, the development of hybrid models that combine the strengths of Bayesian approaches with deep learning architectures could offer a principled way to quantify both aleatoric and epistemic uncertainties. Research should also explore the use of advanced loss functions and regularization techniques to improve the quality and reliability of uncertainty estimates.

Third, the consistency and calibration of uncertainty estimates can be enhanced through the development of more sophisticated post-hoc calibration techniques and the integration of uncertainty quantification with model training. Techniques such as temperature scaling and ensemble methods can be further refined to ensure that the predicted uncertainties align more closely with the observed errors. Additionally, the use of adversarial training and domain adaptation methods can help in improving the robustness of uncertainty estimates, particularly in scenarios where the data distribution is non-stationary.

Finally, the integration of physics-informed and hybrid models with machine learning techniques represents a promising direction for future research. The development of more flexible and scalable frameworks for physics-informed learning, such as the Plug-and-Play Physics-Informed Machine Learning (PnPPIML) framework, can enable the model to better capture the underlying physical laws and improve predictive accuracy. Additionally, the application of these models to real-world problems, such as environmental monitoring and autonomous systems, can provide valuable insights and practical benefits.

The potential impact of these proposed future research directions is significant. By addressing the current limitations and gaps, the field of predictive uncertainty estimation can advance towards more reliable and trustworthy machine learning models. Enhanced uncertainty quantification can lead to better decision-making in high-stakes applications, such as healthcare and finance, by providing decision-makers with a clearer understanding of the risks and limitations associated with model predictions. Moreover, the development of more efficient and scalable methods can make uncertainty quantification more accessible and practical for a wider range of applications, ultimately contributing to the broader adoption and impact of machine learning in various domains.

# 7 Conclusion



The survey of predictive uncertainty estimation in machine learning has provided a comprehensive overview of the state-of-the-art techniques and approaches in this critical field. Key findings include the robustness and interpretability of ensemble methods such as Deep Ensembles and Monte Carlo Dropout, which effectively quantify uncertainty through the variability of multiple model predictions. Direct learning of uncertainty, exemplified by Evidential Deep Learning and Bayesian Neural Networks, offers a principled way to disentangle different types of uncertainty and provide explicit estimates that are crucial for high-stakes applications. The importance of consistency and calibration in uncertainty estimates has been highlighted, with techniques like temperature scaling and ensemble methods playing a pivotal role in ensuring that predicted uncertainties align with observed errors. Active learning and concept drift detection have also been discussed, emphasizing their role in optimizing the labeling process and maintaining model performance in dynamic environments. Finally, the integration of probabilistic and physics-informed models, along with scalable and efficient predictive models, has been explored, showcasing the potential for enhancing the reliability and robustness of machine learning systems.

The significance of this survey lies in its contribution to the advancement of predictive uncertainty estimation, a critical aspect of building trustworthy and reliable machine learning models. By providing a structured and detailed overview of the current landscape, the survey serves as a valuable resource for researchers and practitioners. It highlights the strengths and limitations of various techniques, identifies key challenges, and outlines potential future directions. This comprehensive analysis is essential for fostering further research and development in the field, ultimately leading to more robust and trustworthy machine learning systems that can be deployed in critical domains such as healthcare, finance, and autonomous systems.

In conclusion, the field of predictive uncertainty estimation in machine learning is rapidly evolving, with significant progress being made in both theoretical foundations and practical applications. The insights and recommendations provided in this survey aim to guide future research and encourage the development of more reliable and interpretable models. We call upon the research community to continue exploring innovative methods and to address the remaining challenges, such as improving computational efficiency and handling complex, high-dimensional data. By doing so, we can advance the state of the art and contribute to the broader goal of creating machine learning systems that are not only accurate but also trustworthy and safe.

# References
[1] Adaptive Conformal Inference by Betting  
[2] On Information-Theoretic Measures of Predictive Uncertainty  
[3] Leave-One-Out Stable Conformal Prediction  
[4] LTAU-FF  Loss Trajectory Analysis for Uncertainty in Atomistic Force  Fields  
[5] Temporal Distribution Shift in Real-World Pharmaceutical Data   Implications for Uncertainty Quantif  
[6] Enhancing Fairness and Performance in Machine Learning Models  A  Multi-Task Learning Approach with  
[7] Multi-label out-of-distribution detection via evidential learning  
[8] How disentangled are your classification uncertainties   
[9] Querying Easily Flip-flopped Samples for Deep Active Learning  
[10] Uncertainty-aware abstention in medical diagnosis based on medical texts  
[11] Understanding Uncertainty-based Active Learning Under Model Mismatch  
[12] Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty  Fusion in Autonomous Vehicle  
[13] An Investigation on Machine Learning Predictive Accuracy Improvement and  Uncertainty Reduction usin  
[14] Enhancing Lithological Mapping with Spatially Constrained Bayesian  Network (SCB-Net)  An Approach f  
[15] Improving Label Error Detection and Elimination with Uncertainty  Quantification  
[16] Probabilistic Machine Learning for Noisy Labels in Earth Observation  
[17] DRIVE  Dual-Robustness via Information Variability and Entropic  Consistency in Source-Free Unsuperv  
[18] On Uncertainty Quantification for Near-Bayes Optimal Algorithms  
[19] Deep Probabilistic Direction Prediction in 3D with Applications to  Directional Dark Matter Detector  
[20] Plug-and-Play Physics-informed Learning using Uncertainty Quantified  Port-Hamiltonian Models  
[21] Uncertainty Quantification for Data-Driven Machine Learning Models in  Nuclear Engineering Applicati  
[22] DKL-KAN  Scalable Deep Kernel Learning using Kolmogorov-Arnold Networks  
[23] Integrating Physics and Data-Driven Approaches  An Explainable and  Uncertainty-Aware Hybrid Model f  
[24] Bayesian Kolmogorov Arnold Networks (Bayesian KANs)  A Probabilistic  Approach to Enhance Accuracy a  
[25] Two-stage Risk Control with Application to Ranked Retrieval  
[26] Estimating the Conformal Prediction Threshold from Noisy Labels  
[27] AUKT  Adaptive Uncertainty-Guided Knowledge Transfer with Conformal  Prediction  
[28] Learning to Defer in Content Moderation  The Human-AI Interplay  