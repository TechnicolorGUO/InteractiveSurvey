# 5/1/2025, 6:41:28 PM_Predictive Uncertainty Estimation in Machine Learning  

# 0. Predictive Uncertainty Estimation in Machine Learning  

# 1. Introduction  

The increasing integration of machine learning (ML) models across diverse real-world applications necessitates not only high predictive performance but also reliable estimates of prediction uncertainty. As ML models, particularly Deep Neural Networks (DNNs), are deployed in domains ranging from autonomous systems and medical diagnosis to financial forecasting and environmental monitoring, the ability to quantify the confidence or uncertainty associated with their outputs becomes critical [1,4,9,10,11,15,16,17]. Reliable uncertainty estimates are fundamental for effective risk management, informed decision-making, and fostering user trust [2,3,6].  

Predictive uncertainty quantifies the potential variability or doubt in a model's prediction [17]. Unlike traditional approaches that often yield single point estimates [14], a comprehensive uncertainty estimate provides a measure of dispersion around the prediction, indicating how confident the model is in its output [17]. This uncertainty arises from various sources, including inherent noise in the data (aleatoric uncertainty) and limitations or ignorance of the model itself (epistemic uncertainty) [1,2,7,9,10]. Additional sources can include measurement errors, imperfect models, or unmodeled factors specific to a domain, such as in streamflow prediction [16].  

Traditional ML models, and often standard DNNs, exhibit significant limitations in providing well-calibrated uncertainty estimates [2,5,9]. A pervasive issue is the tendency of these models to be overconfident, assigning high confidence scores even to predictions on out-of-distribution (OOD) or unfamiliar samples [1,4,8,15]. This miscalibration means that the predicted probabilities do not align with the true frequency of correctness [2,3,6]. Such issues are particularly pronounced when test data deviates significantly from the training distribution, highlighting a lack of expressiveness and poor generalization capabilities in these models [2,12].​  

The real-world implications of poor uncertainty estimation are severe, particularly in safety-critical domains where erroneous predictions can have catastrophic consequences [1,4,15,17]. In autonomous driving, for instance, a model must not only detect objects but also understand its uncertainty about the detection to avoid unsafe actions [9]. Similarly, in medical diagnosis, overconfident predictions on potentially novel or ambiguous cases can lead to misdiagnosis or delayed treatment [10,11,15]. Building systems that "know what they don't know" and can signal high uncertainty when facing novel or difficult inputs is paramount for creating reliable and safe AI [3,4]. Beyond safety, uncertainty information is valuable for human experts in deciding whether to trust or disregard a model's prediction, and it is essential for advanced ML techniques like active learning and reinforcement learning [2].  

This survey provides a systematic review of methods for predictive uncertainty estimation in machine learning, with a particular focus on deep learning. We delve into the sources and classification of uncertainty, review prominent modeling approaches including Bayesian methods, ensemble techniques, and single-pass deterministic networks, and discuss the crucial aspect of model calibration [1,2]. The survey aims to clarify the distinctions between different uncertainty quantification methods, discuss their respective advantages and disadvantages, and identify key challenges and future directions in the field.​  

# 2. Sources and Types of Uncertainty  

Reliable machine learning necessitates a comprehensive understanding and quantification of the various sources of uncertainty inherent in the modeling process and the data itself.  

![](images/eae3d3cac1905ea9de2e50f5d934644515f2bdcdcdf848f87bc268dedc6673a3.jpg)  

This section defines and explains the principal types of uncertainty encountered in machine learning predictions, primarily focusing on the fundamental distinction between aleatoric and epistemic uncertainty, as presented across the relevant literature [5,7,12].​  

The discussion organizes these sources into a coherent theoretical framework, detailing their characteristics and origins. Aleatoric uncertainty, often termed data uncertainty, is explored as the irreducible component arising from inherent noise, randomness, or variability within the data acquisition process and the observed phenomenon [1,9,12]. Sources include measurement errors, sensor noise, or label inconsistencies [5,16]. In contrast, epistemic uncertainty, also known as model uncertainty, stems from the model's lack of knowledge due to limitations in the training data or structural inadequacies of the model itself [1,9,12]. This type of uncertainty is, in principle, reducible by improving the model or increasing the volume and coverage of the training data [1,9]. Sources contributing to epistemic uncertainty encompass model misspecification, errors in the training process, and insufficient training data distribution [2,12].​  

Beyond these two primary categories, other forms of uncertainty relevant to machine learning reliability are also introduced. These include distributional uncertainty, which arises when the model encounters data from a different distribution than the training data, and meta-level uncertainties like "uncertainty about uncertainty," which concerns the reliability of the uncertainty estimation process itself [2,16,17].​  

Understanding and distinguishing between these sources of uncertainty is paramount for effectively assessing the reliability and trustworthiness of machine learning models and their predictions [12,17]. Identifying the origin of uncertainty is crucial for selecting appropriate mitigation strategies, as approaches effective for reducing epistemic uncertainty (e.g., collecting more data, improving model architecture) differ significantly from those applicable to aleatoric uncertainty (e.g., improving data quality, using more precise instruments) [12,17]. The subsequent sub-sections delve deeper into the characteristics, sources, quantification, and methods for addressing each of these uncertainty types.​  

# 2.1 Aleatoric Uncertainty (Data Uncertainty)  

Aleatoric uncertainty, also referred to as data uncertainty, constitutes the irreducible component of uncertainty inherent in the data itself [7,12]. It originates from inherent noise, stochasticity, or ambiguity within the data acquisition process and the underlying phenomenon being modeled [1,2,5,9,17]. Unlike epistemic uncertainty, aleatoric uncertainty fundamentally cannot be reduced by merely increasing the volume of training data [1,5,9,17]. This irreducibility stems from information loss within the data, making it impossible to represent the real world perfectly or for samples to contain sufficient information for predictions with absolute certainty [2].​  

Real-world scenarios frequently exhibit aleatoric uncertainty. Prominent examples include label noise or inconsistent annotations within datasets, where similar data points may possess significantly different labels [1,4,5]. The presence of higher bias in a dataset is correlated with increased aleatoric uncertainty [5]. Furthermore, sensor noise, stochastic processes in the environment [1,17], and measurement errors, such as the random errors approximated in stage-discharge gaugings for streamflow estimation [16], contribute to this type of uncertainty. Data defects arising from limitations like lowresolution images or imperfections in measurement systems also result in insufficient data, leading to unreliable  

predictions [2]. Even noise present in the test set itself imposes a limit on a model's observed performance, highlighting the data-inherent nature of aleatoric uncertainty [12]. Mitigating aleatoric uncertainty primarily requires improvements to the data collection methodology or utilizing more precise measurement instruments, rather than solely focusing on model enhancements [12,17].  

Quantifying aleatoric uncertainty is essential for robust predictive modeling. Various studies have proposed methods for this quantification. One mentioned technique involves utilizing Monte Carlo dropout [10]. A critical distinction in modeling aleatoric uncertainty lies between homoscedastic and heteroscedastic types [1,9]. Homoscedastic aleatoric uncertainty assumes that the level of noise or variability in the data is constant across all input instances. In contrast, heteroscedastic aleatoric uncertainty allows the noise level to vary depending on the specific input data point. This latter type is particularly relevant when the inherent data variability is known or expected to differ significantly across the input space. Modeling heteroscedastic uncertainty typically involves predicting input-dependent noise parameters, such as the variance or standard deviation of the output distribution, alongside the mean prediction. This approach enables the model to explicitly account for the varying levels of data reliability or randomness associated with different inputs [1].  

# 2.2 Epistemic Uncertainty (Model Uncertainty)  

Epistemic uncertainty, also referred to as model uncertainty or reliability, represents the uncertainty that arises from a model's lack of knowledge [5,9,12,17]. It stems from various shortcomings in the model or the available data, including insufficient training data, limitations in the model structure, errors introduced during the training process, and poor coverage of the training set over the input space [1,2,4,9,17]. Specifically, epistemic uncertainty reflects the imperfect approximation of the true underlying data generation process by the chosen model [16]. Sources contributing to this uncertainty include the selection of model architecture, representation, featurization, and the ambiguity inherent in parameter optimization [12]. Factors such as model structure defects, errors in the stochastic training process, and unknown data interference are considered significant contributors to epistemic uncertainty [2].​  

A key characteristic of epistemic uncertainty is its relationship with the training data and model complexity [1,2,9]. This type of uncertainty can, in principle, be reduced or even theoretically eliminated by improving the model or by acquiring more data [1,2,5,7,9,17]. For example, increasing the amount of training data provides the model with more information about the underlying distribution, thereby reducing the model's lack of knowledge in regions covered by the new data [1,4,7,9,17]. Similarly, refining the model architecture or the learning process can mitigate uncertainty stemming from structural limitations or training imperfections [2,5,17]. Active learning strategies, for instance, are designed to reduce epistemic uncertainty by selectively acquiring more labeled data in informative regions [7].​  

Epistemic uncertainty becomes particularly pronounced when the model is queried with inputs that lie outside the distribution of the training data [4,5,17]. In such extrapolation scenarios, the model's lack of experience leads to higher uncertainty in its predictions [16]. This type of uncertainty essentially measures whether the input data falls within the distribution the model has been trained on, serving as an indicator of the model's ability to generalize [5]. Using appropriate model structures, such as size-extensive model aggregation structures, can be critical for managing epistemic uncertainty in certain prediction tasks [12].  

Methods for estimating and reducing epistemic uncertainty are central to the field of reliable machine learning. Subsequent sections will detail approaches such as Bayesian methods and ensemble techniques, which are specifically designed to capture and quantify this form of uncertainty, often by modeling distributions over model parameters or aggregating predictions from multiple models to explore the model space. For example, Monte Carlo dropout has been utilized to quantify epistemic uncertainty [10].​  

# 2.3 Distinguishing and Addressing Uncertainty Types  

<html><body><table><tr><td>Feature</td><td>Aleatoric Uncertainty (Data Uncertainty)</td><td>Epistemic Uncertainty (Model Uncertainty)</td></tr><tr><td>Definition</td><td>Inherent noise,variability, or ambiguityin data</td><td>Model's lack of knowledge</td></tr><tr><td>Source</td><td>Measurement errors,sensor noise,label noise, data</td><td>Insufficient training data, model misspecification,</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>defects, stochastic processes</td><td>training errors,poor data coverage</td></tr><tr><td>Reducibility</td><td>Fundamentally irreducible by increasing data volume</td><td> increasing data or improving</td></tr><tr><td></td><td>collection/measurement</td><td>model architecture/training</td></tr><tr><td></td><td>Reflects inherent data randomness</td><td>Reflects model's ignorance, often high on OOD data</td></tr></table></body></html>  

The distinction between aleatoric and epistemic uncertainty is crucial for effective uncertainty quantification and subsequent decision-making in machine learning [1,12]. Aleatoric uncertainty captures inherent noise and variability in the data, which cannot be reduced by increasing the training dataset size [7,17]. Conversely, epistemic uncertainty reflects the model's ignorance due to limited training data or model misspecification and can potentially be reduced by acquiring more data or improving the model [7,17]. Understanding the source of uncertainty is paramount for selecting appropriate mitigation techniques and optimizing models effectively [12,17]. Addressing aleatoric uncertainty typically involves improving data collection or measurement processes, while reducing epistemic uncertainty necessitates model improvements or expanding the training dataset [17].​  

Distinguishing these uncertainty types is particularly critical in downstream tasks such as active learning [7]. Active learning strategies specifically target the reduction of epistemic uncertainty by identifying and requesting labels for data points where the model is most uncertain due to lack of knowledge. By reducing epistemic uncertainty in these regions, the model's overall performance is enhanced [7].  

Various methods have been developed to quantify and, in some cases, separate these uncertainty components. The Bayesian framework provides a natural mechanism for formalizing both uncertainty types [2]. Within this framework, model uncertainty (epistemic) is captured by the posterior distribution over parameters, $p ( \boldsymbol { \theta } | D )$ , while data uncertainty (aleatoric) is represented by the likelihood $\boldsymbol { p } ( \boldsymbol { y } ^ { * } | \boldsymbol { x } ^ { * } , \boldsymbol { \theta } )$ [2]. The total predictive uncertainty is given by the integral over the parameter space:​  

$$
p ( y ^ { * } | x ^ { * } , D ) = \int p ( y ^ { * } | x ^ { * } , \theta ) p ( \theta | D ) d \theta
$$  

where  

$$
p ( \theta | D ) = \frac { p ( D | \theta ) p ( \theta ) } { p ( D ) }
$$  

[2]. Due to the intractability of the true posterior, approximate methods such as ensemble approaches are often employed to approximate the integration by averaging predictions from models with different parameter settings [2]. Ensembling is recognized as a reliable tool, particularly effective for addressing model variance, a component of epistemic uncertainty [12].  

Beyond the general Bayesian approach, specific techniques have been proposed to explicitly separate and quantify aleatoric and epistemic uncertainty. For instance, AE-DNN is introduced as a method using a deterministic deep neural network to achieve this separation, leading to a more nuanced understanding of prediction uncertainty sources [9]. Similarly, SDE-Net utilizes stochastic differential equations to model hidden layer dynamics, enabling the estimation of both uncertainty types, with its diffusion network component providing high epistemic uncertainty estimates for out-ofdistribution samples [4]. Another approach, like using MC Dropout, involves keeping dropout layers active during inference to sample from an approximate posterior, capturing epistemic uncertainty; aleatoric uncertainty is then quantified through a specifically designed loss function [5].​  

It is worth noting that not all uncertainty quantification methods explicitly separate these two components. Some methods might assume that the components cannot be separated and include all sources of uncertainty jointly, while others are designed to distinguish them [16]. The choice of method ultimately depends on the specific user requirements and the assumptions made by the method regarding the sources and types of uncertainty [16]. When both types of uncertainty are present, models like those leveraging the Bayesian framework, AE-DNN [9], SDE-Net [4], or MC Dropout [5] are designed to handle them, either by inherently modeling both or through explicit decomposition techniques.  

# 2.4 Other Types of Uncertainty  

Beyond the widely discussed categories of aleatoric and epistemic uncertainty, the literature identifies other forms crucial for comprehensive uncertainty quantification in machine learning. One prominent type is distributional uncertainty, which arises when a model encounters data originating from a distribution different from its training distribution [2,17]. This is also frequently referred to as out-of-distribution (OOD) uncertainty or the uncertainty induced by distribution shift [3,15].​  

Distributional uncertainty stems from examples that fall into regions not adequately represented or covered by the training dataset [2]. It reflects uncertainty regarding the model's output distribution given a particular input [17]. Distribution shift, a specific manifestation of distributional uncertainty, occurs when the data distribution in real-world deployment diverges from the training environment [3]. Such shifts can lead to unexpected model failures, particularly in unforeseen scenarios, and the challenge lies in predicting and accounting for all potential shifts [3]. An illustrative example is the detection of OOD retinal OCT images that do not belong to the specific age-related macular degeneration (AMD) classes used for training [15]. This challenge is amplified in near-OOD cases, where the out-of-distribution data shares pathomorphological characteristics with the in-distribution classes, making detection more difficult [15]. Distributional uncertainty can be conceptualized and, in some frameworks, quantified as a separate component. For instance, predictive uncertainty  

$$
p ( y ^ { * } | x ^ { * } , D ) = \int \int \underbrace { p ( y ^ { * } | \mu ) } _ { \mathrm { D a t a } } \underbrace { p ( \mu | x ^ { * } , \theta ) } _ { \mathrm { D i s t r i b u t i o n a l } } \underbrace { p ( \theta | D ) } _ { \mathrm { M o d e l } } d \mu d \theta
$$  

for a target $y ^ { \ast }$ given input $x ^ { * }$ and training data $D$ can be decomposed. In this formulation, $p ( \mu | x ^ { * } , \theta )$ represents the distributional uncertainty, influenced by the model parameters $\theta$ , while $\mu$ denotes the distribution of the output [2]. This perspective suggests that model (epistemic) uncertainty can influence the estimation of distributional uncertainty, which in turn affects the estimation of data (aleatoric) uncertainty [2]. Methods such as quantifying “Uncertainty Mass” have been proposed to capture distributional uncertainty, particularly for identifying OOD samples [8].​  

Another concept identified is uncertainty about uncertainty, also termed "uncertainty_ $2 "$ [16]. This refers not to the uncertainty in the model's prediction itself, but to the uncertainty inherent in the process of estimating uncertainty. It arises from incomplete information regarding the sources and characteristics of errors, leading to a lack of a single optimal method for uncertainty estimation [16]. This highlights a meta-level of uncertainty related to the reliability and validity of the uncertainty quantification techniques themselves, distinct from the aleatoric or epistemic uncertainties they aim to measure.  

# 3. Methods for Predictive Uncertainty Estimation  

Quantifying predictive uncertainty in machine learning is a critical aspect for ensuring the reliability and safety of models, particularly in high-stakes applications.  

![](images/d35a1ca00e96f1cd6171a38419026d71f6ea479cd5ef0e571ffc86c475b8f1b3.jpg)  

A diverse array of methods has been developed to address this challenge, which can be broadly categorized based on their underlying principles and approaches [1,3]. These categories commonly include Bayesian methods, ensemble techniques, and a range of non-Bayesian or deterministic approaches [1,3]. Furthermore, specialized methods are being developed to handle the unique requirements of specific model architectures, such as Graph Neural Networks [20].​  

Bayesian methods provide a probabilistic framework for uncertainty estimation by placing distributions over model parameters rather than treating them as fixed points [3]. Techniques such as Bayesian Neural Networks (BNNs) aim to capture epistemic uncertainty (model uncertainty) by inferring a posterior distribution over the network weights [1,5]. Predictions are made by integrating over this posterior, resulting in a predictive distribution that inherently reflects uncertainty [3]. While principled, exact Bayesian inference in complex models is often intractable, necessitating approximation methods like Variational Inference (VI), Laplace Approximation, and Markov Chain Monte Carlo (MCMC) [1]. The computational cost of these approximations remains a significant challenge, limiting scalability. Moreover, recent studies suggest that popular Bayesian techniques, including BNNs and MCDropout (often viewed as a Bayesian approximation), may not consistently provide high uncertainty estimates for out-of-distribution samples [19].  

Ensemble methods quantify predictive uncertainty by combining the outputs of multiple individual models [1,3]. By training diverse models (e.g., with different initializations, architectures, or data subsets via bootstrapping), the variability in their predictions for a given input serves as an indicator of uncertainty [1]. A high standard deviation across ensemble member predictions suggests higher uncertainty [3,12]. Ensemble approaches, including Deep Ensembles and Monte Carlo Dropout [5,10], are effective at capturing epistemic uncertainty arising from model choices and data limitations. However, similar to some Bayesian methods, MCDropout may also exhibit limitations in OOD detection [19]. The primary drawback of traditional ensembling is its substantial computational cost and storage requirements due to the need to train and maintain multiple distinct models [1,3].​  

Non-Bayesian and deterministic methods encompass a broad spectrum of techniques that do not rely on Bayesian parameter inference or explicit model ensembles [3,17]. Deterministic models may be trained with objectives designed to increase uncertainty on OOD samples, although anticipating all OOD cases is difficult [3]. Examples include Mean-Variance Estimation and the Delta Method for prediction intervals [12,14], and AE-DNN which attempts to estimate both aleatoric and epistemic uncertainty in a single forward pass [9]. Conformal Prediction offers statistically rigorous prediction sets with guaranteed coverage under the assumption of independent and identically distributed (i.i.d.) data, providing distributionfree guarantees for finite sample sizes [3,11]. However, these guarantees are compromised by distribution shifts [3]. Evidential Deep Learning (EDL) models uncertainty by parameterizing a Dirichlet distribution over class probabilities using evidence derived from network outputs, potentially allowing disentanglement of aleatoric and epistemic uncertainty in a single pass [8]. SDE-Nets model hidden state dynamics via Stochastic Differential Equations (SDEs), where the diffusion term quantifies epistemic uncertainty [4]. kNN-UE is an efficient non-Bayesian method that estimates uncertainty based on distances and label consistency of k-nearest neighbors in the feature space, particularly useful for OOD detection in NLP [6,15]. These methods offer varying trade-offs in terms of theoretical guarantees, computational efficiency, and ability to capture different types of uncertainty or handle OOD data.​  

Beyond general methods, quantifying uncertainty for specific model types like Graph Neural Networks (GNNs) presents unique challenges due to their processing of irregular, relational data [20]. Research focuses on developing frameworks tailored to GNNs to capture both aleatoric and epistemic uncertainty within graph structures [20]. Methods discussed in the context of GNNs include applications of general approaches or model-specific modifications, although detailed comparisons of how different GNN architectures inherently influence uncertainty quantification are still emerging.​  

In summary, the landscape of predictive uncertainty estimation methods is rich and varied. Bayesian methods offer a principled approach but face computational hurdles. Ensemble methods provide empirical robustness at the cost of computational resources. Non-Bayesian/deterministic methods offer diverse solutions, from statistical guarantees (Conformal Prediction) and potential disentanglement (EDL) to efficiency and OOD sensitivity based on feature space properties (kNN-UE). The choice of method often depends on the specific application requirements, including tolerance for computational cost, need for calibrated uncertainty, sensitivity to distribution shifts, and the specific type of uncertainty (aleatoric vs. epistemic) that needs to be quantified. Ongoing research aims to improve the accuracy, calibration, efficiency, and OOD performance of these methods, as well as develop tailored approaches for specialized model architectures.​  

# 3.1 Bayesian Methods  

Bayesian inference offers a principled framework for quantifying uncertainty in machine learning by modeling the distribution over model parameters rather than treating them as fixed values [3,10]. This approach fundamentally views parameters as random variables, updating their probability distributions as more data or evidence becomes available [3]. By representing parameters as prior distributions, such as in Bayesian Neural Networks (BNNs), the model can capture uncertainty stemming from limited data or model misspecification [5]. This formal framework allows for the incorporation of prior information to guide model development [16].  

Within the Bayesian paradigm, specific techniques are employed to estimate uncertainty. Bayesian Neural Networks (BNNs) are a notable example, where the network's weights are assigned prior distributions [1,5]. Predictions are then made by  

averaging over the posterior distribution of these weights, yielding predictive distributions that inherently quantify uncertainty. Other related Bayesian approaches extend existing methods, such as extending the deep image prior framework using Monte Carlo dropout to achieve a Bayesian formulation [10]. Beyond BNNs, Bayesian methods have been applied in various contexts, including reframing posterior approximation as a supervised classification problem with setvalued inputs, as seen in methods approximating posterior probabilities [18], and in tasks like estimating the contributions of different error sources via ensembling [12].​  

While theoretically appealing, exact Bayesian inference on complex, high-dimensional models like modern neural networks is often computationally intractable [1,3]. This computational burden necessitates the use of approximation methods. Common approximation techniques for BNNs include Variational Inference (VI), Laplace Approximation, and Markov Chain Monte Carlo (MCMC) [1]. These methods aim to approximate the true posterior distribution of the model parameters. However, the reliance on approximations means that the model's uncertainty may not always be expressed accurately [3].  

The effectiveness and scalability of these approximation methods present trade-offs. Techniques like VI offer computational efficiency compared to MCMC by transforming the inference problem into an optimization problem, but they rely on simplifying assumptions about the form of the posterior distribution. MCMC methods, while potentially offering better asymptotic approximations, are typically computationally expensive and slow, especially for large models and datasets [1]. Despite these challenges, approximation methods enable BNNs to quantify parameter uncertainty and contribute to avoiding overfitting [1].​  

While BNNs offer advantages in capturing model uncertainty, they also have limitations. Research indicates that BNNs, even those considered popular techniques for uncertainty estimation in deep learning, may not consistently provide high uncertainty estimates for out-of-distribution (OOD) samples [19]. This behavior has been observed in both controlled 2D toy examples and real-world datasets, suggesting a potential weakness of BNNs in effectively identifying and quantifying uncertainty for data points that differ significantly from the training distribution [19]. Furthermore, the implementation details and the amount of required user information can vary significantly across different Bayesian methods, ranging from relying on objective segmentation to building on subjective expert knowledge [16].​  

# 3.2 Ensemble Methods  

Ensemble methods constitute a prominent class of techniques for quantifying predictive uncertainty in machine learning by combining the outputs of multiple individual models [1,3,12]. The fundamental principle involves training several distinct models, often with varying initializations, architectures, or hyperparameters, and aggregating their predictions [1]. The uncertainty in the ensemble's prediction is then typically estimated by measuring the variability or disagreement among the predictions of its constituent models [1,3,12]. A low standard deviation across predictions indicates high confidence or low uncertainty, while a high standard deviation signifies significant disagreement and thus higher uncertainty [3,12].  

Formally, for a given input $\mathsf { \backslash } ( \mathsf { X \_ n } )$ , if an ensemble consists of \(N_{ens}\) models, the ensemble prediction $\mathsf { \backslash } ( \mathsf { \backslash b a r \{ y \} } ( \mathsf { X \_ n } ) \backslash )$ is commonly calculated as the average of the individual model predictions \(\hat{y}_{i,1}(X_n)\):  

The uncertainty associated with this prediction can be quantified by the unbiased standard deviation $\mathsf { \backslash } ( \mathsf { s } ( \mathsf { X \_ n } ) \backslash )$ of the submodel predictions for that data point [12]:  

This standard deviation can be utilized to define confidence intervals and uncertainty bounds [12]. Ensemble methods are particularly effective in capturing model uncertainty, which stems from the model's limitations or mismatch with the data distribution, by observing where different plausible models diverge [3].  

Several strategies exist for constructing ensembles. The Bootstrap Method, for instance, involves resampling the training data to create multiple datasets, on which separate models are trained [1,14]. Another approach involves training models with different random initializations or hyperparameters [1]. Techniques like Deep Ensembles have been shown to provide accurate and well-calibrated uncertainty estimates, demonstrating the effectiveness of this paradigm [3].​  

Monte Carlo Dropout (MCDropout) is recognized as a popular technique for uncertainty estimation in deep learning and can be viewed as an approximation to Bayesian inference through ensembling [5,10,19]. By keeping dropout enabled during testing, MCDropout generates multiple predictions for a single input, effectively simulating an ensemble of models [5]. This method is particularly used to quantify epistemic uncertainty [5]. However, like some other methods, MCDropout may not consistently provide high uncertainty estimates for out-of-distribution (OOD) samples, which can be a limitation in identifying data points significantly different from the training distribution [19].  

The effectiveness of ensemble methods heavily relies on the diversity among the constituent models; different initialization, architectures, or hyperparameters are commonly used to promote this diversity [1]. Diverse models are expected to agree on predictions for inputs similar to the training data but diverge significantly on inputs that differ, reflecting higher uncertainty [3].​  

Despite their strengths in providing robust uncertainty estimates, traditional ensemble methods face significant computational challenges [1,3]. Training and deploying multiple models require substantial computational resources and storage [1,3]. This high cost can limit their applicability in resource-constrained environments or for large-scale models. While methods like SDE-Net share a similarity with ensembles in generating multiple outputs through sampling, they aim to reduce computational cost by requiring only a single training process for the base model, offering potential strategies for mitigating the expense associated with training numerous separate models [4]. Addressing the computational cost remains a key area for research and development in making ensemble methods more widely accessible.​  

# 3.3 Non-Bayesian and Deterministic Methods  

Deterministic methods for uncertainty estimation aim to enable models to express elevated uncertainty, particularly when encountering inputs that differ significantly from the training data [3]. This often involves training procedures designed to increase uncertainty on specific out-of-distribution (OOD) examples [3]. However, a fundamental limitation of this approach is the inherent difficulty in anticipating all possible ways real-world inputs might deviate from the training distribution [3]. Single deterministic networks typically provide predictions via a single forward pass, with uncertainty either directly predicted by the network or derived using external techniques [17]. Examples of such methods include Mean-Variance Estimation and the Delta Method, which are used for estimating prediction intervals [12,14]. Methods like AE-DNN also fall into this category, employing a deterministic deep neural network to estimate both aleatoric and epistemic uncertainty in a single forward pass, promoting computational efficiency [9]. Regression-based approaches, such as the ISO method and the more flexible Bristol method using local nonparametric regression, or GesDyn which tracks temporal variability by updating rating curves, represent other deterministic frameworks for uncertainty quantification [16]. Unlike some popular methods like Gaussian Processes, which have been shown to consistently produce high uncertainty for OOD data [19], the effectiveness of purely deterministic training on OOD data can be limited by the training examples used [3].  

Conformal Prediction offers a statistically rigorous, non-Bayesian approach to uncertainty quantification that provides distribution-free guarantees for prediction sets [1,11]. It guarantees that the prediction set will contain the true label with a user-specified probability, valid even for finite sample sizes [3,11]. The core idea involves defining a nonconformity measure that quantifies how "strange" a new data point is compared to the training data. Based on this measure, a prediction set is constructed. While providing mathematical guarantees, these assurances are strictly contingent on the assumption that the test data originates from the same distribution as the training data [3]. This sensitivity to distribution shift represents a significant limitation in many real-world applications where this assumption is frequently violated [3].  

Evidential Deep Learning (EDL) is another non-Bayesian method that leverages concepts from Subjective Logic and   
Dempster–Shafer Evidence Theory to model uncertainty within deep learning models [8]. In EDL, the ReLU-activated   
outputs of the classifier network are interpreted as evidence $\mathsf { \backslash } ( \mathsf { e \_ k } \backslash )$ supporting each class $\backslash ( \boldsymbol { \kappa } \backslash )$ [8]. This evidence is used   
to parameterize a Dirichlet distribution over the possible class probabilities [1,8]. Specifically, the parameters \( \alpha_k \)   
of the Dirichlet distribution are set as​   
\​   
[8]. The total evidence or Dirichlet strength is​   
\​   
where $\mathsf { \backslash } ( \mathsf { K } \backslash )$ is the number of classes [8]. Belief masses $\mathsf { \backslash } ( \mathsf { b \_ k } )$ for each class and an overall uncertainty mass $\mathsf { \backslash } ( \mathsf { u } \backslash )$ are   
derived from these parameters:​   
\​   
\​   
The expected probability for class $\backslash ( \boldsymbol { \kappa } \backslash )$ is given by  

\​ [8]. By modeling the parameters of a Dirichlet distribution, EDL can capture both aleatoric and epistemic uncertainty in a single forward pass [1,8], and it is discussed in the context of quantifying systematic noise [12].  

SDE-Nets represent a different deterministic approach that models the dynamics of hidden layer activations using Stochastic Differential Equations (SDEs) to capture uncertainty [4]. The hidden state dynamics $\mathsf { \backslash } ( \mathsf { x \_ t } \backslash )$ are governed by an SDE of the form​   
\​   
where $\backslash ( w _ { - } \mathfrak { t } \backslash )$ is standard Brownian motion [4]. Here, $\mathsf { \backslash ( f ( x _ { - } t ) \backslash ) }$ represents the deterministic drift and $\backslash ( \mathrm { g } ( \mathsf { x } _ { - } \mathrm { t } ) \backslash )$ represents the diffusion coefficient, which models the fluctuation in dynamics and serves as an estimate of epistemic uncertainty [4]. The model utilizes two neural networks to parameterize the drift $\backslash ( \textsf { f } \backslash )$ and diffusion $\backslash ( \mathfrak { g } \backslash )$ [4]. Training involves a loss function designed to minimize loss and uncertainty on in-distribution samples while simultaneously increasing uncertainty for OOD samples [4]. SDE-Nets employ parameter sharing across layers and determine the diffusion term's variance solely based on the initial state $\mathsf { \backslash } ( \mathsf { x } _ { - } 0 \mathsf { \backslash } )$ , contributing to ease of training [4].​   
kNN-UE is an efficient, non-Bayesian uncertainty estimation method primarily developed for natural language processing   
tasks, which operates by leveraging the relationship of an input's feature representation to its k-nearest neighbors in the   
training set feature space [6,15]. The process involves extracting a feature vector using the penultimate layer of pre-trained   
language models, finding its $\mathsf { k }$ -nearest neighbors in the training embedding space, and then calculating an uncertainty score   
[6]. This score combines two components: Distance-based Uncertainty (Dist) and Label Existence Ratio (LER) [6]. Dist is the   
average cosine distance to the k nearest neighbors:​   
\​   
where $\backslash ( \mathsf { d } ( \mathsf { f } _ { - } \mathsf { i } , \mathsf { f } _ { - } \mathsf { j } ) \backslash )$ is the cosine distance [6]. LER measures label consistency among neighbors:​   
\​   
where $\backslash ( \mathsf { C } \backslash )$ is the set of labels and $\backslash ( \mathsf { y \_ j } \backslash )$ is the neighbor's label [6]. The final kNN-UE score is a weighted combination:​   
\​   
[6]. This method does not require probabilistic modeling or ensembles [15] and utilizes techniques like PCA for dimension   
reduction and Faiss for approximate nearest neighbor search to improve computational efficiency [6].​  

<html><body><table><tr><td>Method</td><td>Approach Summary</td><td>Uncertainty Types Captured</td><td>OOD Handling</td><td>Efficiency/Cost Notes</td></tr><tr><td>Conformal Prediction</td><td>Provides prediction sets with statistical guarantees based on nonconformity scores</td><td>Implicitly captures both</td><td>Guarantees invalid under shift</td><td>Requires calibration step</td></tr><tr><td>Evidential Deep Learning (EDL)</td><td>Models Dirichlet distribution over class probabilities using evidence</td><td>A&E (potentially disentangled)</td><td>Uses uncertainty mass (u) for detection</td><td>Single forward pass inference after training</td></tr><tr><td>SDE-Nets</td><td>Models hidden state dynamics via SDEs</td><td>Primarily E (diffusion term)</td><td>Specific OOD objective</td><td>Involves SDE solvers</td></tr><tr><td>KNN-UE</td><td>Based on distance & label consistency of k-</td><td>Related to E& Data Density</td><td>Designed for OOD detection</td><td>Efficient with optimized search (Faiss)</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>nearest neighbors in feature space</td><td></td><td></td><td></td></tr></table></body></html>  

Comparing these non-Bayesian and deterministic methods reveals diverse theoretical foundations and practical characteristics. Conformal Prediction provides rigorous statistical guarantees based on distribution-freeness but is highly susceptible to distribution shift, which can invalidate its guarantees [3]. Its computational cost involves a calibration step. EDL grounds uncertainty in evidence theory and Dirichlet distributions, offering a single-pass method to potentially disentangle aleatoric and epistemic uncertainty, making it computationally efficient after training [8]. SDE-Nets model uncertainty through dynamic systems governed by SDEs, specifically linking epistemic uncertainty to the diffusion term; its implementation involves SDE solvers and a specialized training objective [4]. kNN-UE relies on geometric relationships in feature space, using neighbor distance and label entropy to quantify uncertainty related to proximity and local density of training data [6]. While neighbor search can be costly, optimized implementations enhance its efficiency [6]. In terms of uncertainty types, EDL and AE-DNN explicitly aim to model both aleatoric and epistemic components, while SDE-Nets primarily model epistemic uncertainty through the diffusion term. kNN-UE's metrics relate more directly to data density and distance in feature space, which are indicative of epistemic uncertainty and OOD detection [15]. Conformal Prediction's prediction sets implicitly account for both, but its strength lies in guaranteed coverage rather than explicit disentanglement. The ease of implementation varies, with single-pass methods like EDL and AE-DNN requiring specific architectural or loss modifications, SDE-Nets involving SDE solvers, Conformal Prediction needing a calibration procedure, and kNN-UE necessitating feature extraction and nearest neighbor infrastructure. While deterministic training methods aim to induce OOD sensitivity [3], methods like Conformal Prediction are fundamentally limited by distribution shift [3], whereas kNN-UE's design around distance from training data and SDE-Net's OOD objective terms directly address aspects of OOD detection and uncertainty.​  

# 3.4 Methods for Specific Model Types (e.g., GNNs)  

Quantifying predictive uncertainty in machine learning models is crucial for their reliable deployment, and this necessity extends to specialized architectures such as Graph Neural Networks (GNNs) [18,19]. GNNs are increasingly applied in various domains—including chemistry, social science, and biological network analysis—where understanding prediction confidence is vital. The unique structure of graph data, characterized by irregular topology and complex dependencies between nodes and edges, presents distinct challenges and opportunities for uncertainty estimation compared to grid-like data structures processed by convolutional networks [20].  

Research in this area has begun to address the specific requirements for uncertainty quantification within GNNs. A general framework has been proposed that focuses on quantifying both aleatoric and epistemic uncertainty in the context of GNNs [20]. This framework likely details methodologies tailored to the inherent graph structure, aiming to capture uncertainties stemming from data noise (aleatoric) and model ignorance (epistemic) effectively.  

Different GNN architectures are employed across various tasks. For instance, directed message passing neural networks (dMPNNs)—a class of 2-dimensional graph-convolutional neural networks—have been utilized for chemical property predictions, a domain where reliable uncertainty estimates are particularly valuable [12]. Other variants include Graph Convolutional Networks (GCNs) used in federated learning frameworks for tasks like disease prediction, which involve imputing missing graph information using generative models [18]. Meta-learning approaches, such as Meta Propagation Networks (Meta-PN), are applied for graph few-shot semi-supervised learning, involving meta-learned label propagation strategies on unlabeled nodes [18]. Furthermore, GNNs are integral to methods for deep graph-level anomaly detection, which learn normal graph patterns through knowledge distillation [18], and for improving subgraph recognition via variational information bottleneck methods involving noise injection [18].​  

While these examples highlight the diverse applications and architectural variations of GNNs, the provided digests do not offer a direct comparison of the intrinsic capabilities of these different architectures—such as d-MPNNs, GCNs, or Meta-PNs— specifically regarding their ability to quantify uncertainty or detail the uncertainty quantification methods applied within these instances. Nevertheless, the focus on developing tailored frameworks for uncertainty estimation within GNNs [20] indicates an active research direction aimed at equipping these powerful models with robust uncertainty characterization capabilities, acknowledging the unique demands imposed by graph-structured data.  

# 4. Evaluation Metrics and Benchmarks  

Evaluating the quality of predictive uncertainty estimates is crucial for deploying machine learning models reliably, particularly in safety-critical applications.  

<html><body><table><tr><td>Metric</td><td>Purpose / What it</td><td>Aspect Evaluated</td><td>Example Use Case /</td></tr><tr><td>Expected Calibration Error (ECE)</td><td>Measures alignment between predicted</td><td>Calibration</td><td>General Classification UQ</td></tr><tr><td>Brier Score</td><td>Measures accuracy</td><td>Calibration & Sharpness</td><td>Probabilistic Forecasting</td></tr><tr><td>Negative Log- Likelihood (NLL)</td><td>of fit of predictive</td><td>Sharpness</td><td>Models</td></tr><tr><td>AUROC (OOD)</td><td>Operating</td><td>OOD Detection</td><td>distribution task</td></tr><tr><td>AUPR (OOD)</td><td>Precision-Recall curve</td><td></td><td>imbalanced)</td></tr><tr><td>(00D)</td><td>Positive Rate (e.g.,</td><td></td><td></td></tr><tr><td>Interval Coverage</td><td>Percentage of true values within predicted intervals</td><td>Sharpness& Reliability</td><td>Regression Tasks</td></tr></table></body></html>  

This evaluation involves employing various metrics that assess different aspects of the uncertainty quantification, such as calibration, sharpness, and performance on downstream tasks like out-of-distribution (OOD) detection or selective prediction [6].​  

Key metrics for evaluating uncertainty quality include calibration error, Brier score, and Negative Log-Likelihood (NLL). Calibration error, often quantified using the Expected Calibration Error (ECE), measures the agreement between the predicted confidence and the actual accuracy across different confidence levels [6]. A low ECE indicates well-calibrated uncertainty, meaning the model's stated confidence reflects the true probability of correctness. While not explicitly detailed in the digests, the Brier score provides a combined measure of calibration and sharpness for probabilistic predictions, penalizing both miscalibration and lack of confidence. The NLL, or cross-entropy loss, also assesses both calibration and sharpness, favoring models that assign high probability to the correct outcome while providing tight distributions.​  

Beyond these general measures, metrics tailored to specific tasks are frequently employed. For classification tasks, various measures derived from the model's softmax outputs can indicate uncertainty, such as the maximum softmax value, entropy, mutual information, KL divergence, and the mean and variance of softmax probabilities [17]. In regression settings, standard performance metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to evaluate model accuracy [12]. Additionally, the quality of uncertainty intervals can be assessed by computing metrics like the  

percentage of true values falling within predefined confidence intervals, such as $9 5 \%$ and $6 8 \%$ intervals, which correspond to two and one standard deviations of a Gaussian distribution, respectively [16].  

Evaluating uncertainty in downstream tasks provides insight into its practical utility. For selective prediction, evaluation focuses on the risk (error rate) on the most confidently predicted samples, assessing the model's ability to abstain from uncertain predictions to maintain high accuracy on accepted ones [6]. In OOD detection, metrics quantify the model's ability to distinguish between in-distribution and OOD samples based on their uncertainty estimates. Commonly used OOD detection metrics include the Area Under the Receiver Operating Characteristic curve (AUROC) [4,6,15], the Area Under the Precision-Recall Curve (AUPR) [4], the True Negative Rate (TNR) at a specific True Positive Rate (TPR), such as $9 5 \%$ TPR [4], and detection accuracy [4]. The macro-average AUC is also used to assess OOD detection performance, particularly for challenging cases like detecting near-OOD instances [15].  

Benchmarking uncertainty estimation methods involves applying them to various datasets and tasks. Common benchmarks span diverse domains, including natural language processing tasks such as sentiment analysis, natural language inference, and named entity recognition [6]. In computer vision, benchmarks include tasks like depth estimation on datasets such as NYUv2 [5], medical image denoising across different modalities [10], and AMD classification and OOD detection [15]. Regression tasks, such as predicting streamflow [16] or fitting simple functions like a sinusoidal curve [5], also serve as important benchmarks.  

Analyzing performance across these metrics and benchmarks reveals insights into the strengths and weaknesses of different methods. For instance, a method might show well-calibrated uncertainty in experiments, where predictive uncertainty correlates with the predictive error [10]. Performance can be influenced by factors such as noise, bias, and variance in the data and model, which can be analyzed through techniques like examining learning curves (test set error versus dataset size) [12]. It is essential to distinguish between the process of generating uncertainty values (uncertainty estimation) and quantifying the quality or goodness of these estimates using appropriate measures [17].​  

# 5. Calibration of Uncertainty Estimates  

Ensuring the reliability of predictive uncertainty estimates is paramount for their utility in practical decision-making systems [3,17]. Calibration is crucial because it guarantees that the predicted probabilities or uncertainty ranges generated by a machine learning model accurately align with the observed frequencies or true correctness likelihoods of events [3,10,17]. Without proper calibration, a model might be overconfident or underconfident, rendering its uncertainty estimates misleading and potentially detrimental in safety-critical applications [8,10].​  

Miscalibration patterns can be identified using specific metrics designed to quantify the alignment between predicted confidence and empirical accuracy. The Expected Calibration Error (ECE) is a widely used metric for this purpose [3]. ECE assesses the difference between the average confidence and the accuracy within predefined bins of confidence scores, providing a single scalar measure of overall miscalibration. A lower ECE value indicates better calibration, signifying that the model's stated confidence levels correspond more closely to its actual performance [6]. For instance, the kNN-UE method explicitly employs ECE to evaluate its calibration performance, demonstrating better calibration compared to baseline methods in experimental results [6].​  

When a model is found to be miscalibrated, various post-processing techniques can be applied to improve the reliability of its uncertainty estimates. These techniques aim to adjust the model's output probabilities or confidence scores without altering the underlying model architecture or training process. Common post-processing methods include temperature scaling, Platt scaling, and isotonic regression [3]. Temperature scaling, a simple yet effective method for neural networks, involves scaling the logits before the softmax function by a single scalar parameter learned on a validation set. Platt scaling applies a logistic regression model to transform the output scores, typically used for binary classification. Isotonic regression fits a non-decreasing function to the relationship between predicted scores and empirical probabilities, offering a more flexible, non-parametric approach [3]. Each of these methods has its advantages and disadvantages concerning computational cost, flexibility, and performance across different datasets and model types [3].  

Beyond post-processing, some uncertainty quantification methods inherently provide calibrated estimates. Conformal prediction, for example, offers distribution-free guarantees that result in statistically rigorous and valid uncertainty estimates that are inherently calibrated [11]. Its prediction sets come with explicit, non-asymptotic coverage guarantees without making assumptions about the data distribution or the underlying model [11]. Other approaches focus on the problem of over-confidence directly, such as Evidential Deep Learning, which aims to model uncertainty explicitly, thereby implicitly enhancing calibration [8]. In regression tasks, confidence intervals derived from methods like ensembles can also be calibrated. One approach calibrates confidence intervals, defined using the standard deviation of ensemble predictions, by applying a Student‑t factor based on the degrees of freedom, allowing for interval predictions of each test data point n as $\bar { y } ( X _ { n } ) \pm t \cdot s ( X _ { n } )$  

[12]. Furthermore, the reliability of uncertainty estimates across different methods can vary significantly, underscoring the importance of clearly stating the underlying uncertainty assumptions alongside the estimates to ensure transparency and trust [16]. Achieving well‑calibrated uncertainty, where predictive uncertainty correlates directly with predictive error, is a key goal in various applications, such as tackling hallucinations and artifacts in inverse medical imaging [10].​  

# 6. Robustness of Uncertainty Estimation  

![](images/10bde0da6dee63d00d2f5db4fcf4f9eec11f0acf52d6ba8f0fce8cb663f7022a.jpg)  

The reliability of predictive uncertainty estimates in machine learning is significantly challenged by external factors such as distribution shift, adversarial attacks, and the presence of out-of-distribution (OOD) samples [3]. These challenges can compromise the accuracy and calibration of uncertainty estimates, frequently leading to overconfident predictions in regions where the model's performance is unreliable [3]. Evaluating the robustness of uncertainty estimates under these conditions is critical for their practical deployment. This involves assessing model performance and calibration under varying data distributions [5,16], analyzing their susceptibility to adversarial perturbations designed to induce miscalibration [4] and quantifying their effectiveness in distinguishing data points that deviate from the training distribution [4,15].​  

Uncertainty estimates serve a crucial function in detecting out-of-distribution samples, which are instances significantly different from the data encountered during training [8]. The fundamental principle involves leveraging the model’s uncertainty measure as a criterion to differentiate between in-distribution and OOD data points [8]. High uncertainty values, such as a large uncertainty mass in evidential deep learning, or increased epistemic uncertainty, can signal that a test sample lies outside the learned data manifold [5,8]. Various uncertainty quantification methods have been investigated for their performance in OOD detection tasks, including distance-aware approaches [1], Gaussian Process Mixture Models [1], and comparative studies involving methods like GP-based approaches, Bayesian Neural Networks, and Monte Carlo Dropout [19]. Evaluation metrics such as AUROC, TNR, and AUPR are commonly used to assess OOD detection capabilities [4,6]. Factors influencing OOD detection performance include the choice of uncertainty method, the use of distance metrics like Cosine distance, and techniques such as Outlier Exposure, particularly for challenging near-OOD samples [15].  

Addressing the fragility of uncertainty estimates requires developing techniques to improve their robustness against these challenges. For adversarial attacks, defense mechanisms range from standard adversarial training to more advanced methods like Mutual Adversarial Training (MAT), which enhances robustness by leveraging knowledge sharing between models, and the Pairwise Adversarially Robust Loss (PARL) function for ensemble diversity [19]. Novel paradigms even explore reversing adversarial perturbation search to potentially benefit classification confidence [19]. To counter distribution shift, efforts focus on developing methods that maintain performance guarantees or validity under changing data distributions [21]. Conformal prediction offers a framework extendable to scenarios involving distribution shift [11], while domain adaptation techniques aim to build models robust to shifts between source and target domains [21]. Research also explores the influence of noise on uncertainty robustness [12] and robustness in specific contexts like representation learning [21]. These techniques collectively aim to ensure that uncertainty estimates remain dependable even when faced with data distribution challenges or malicious inputs.  

# 6.1 Adversarial Robustness  

Adversarial attacks pose a significant threat to the reliability of machine learning models, including their ability to provide accurate and trustworthy uncertainty estimates. These attacks involve crafting subtly perturbed inputs designed to fool the model, often leading to misclassifications with high confidence, thereby compromising the reliability of predictive uncertainty. Evaluating the robustness of models and their uncertainty estimates is crucial and often involves testing against common adversarial generation methods such as the Fast Gradient-Sign Method (FGSM) and Projected Gradient Descent (PGD) [4]. Research in this domain spans various aspects, including understanding overfitting in robust models and developing provably robust deep learning techniques [21].​  

Developing effective defense mechanisms is essential to improve the adversarial robustness of uncertainty estimation. Adversarial training (AT) serves as a widely recognized baseline defense. However, advanced techniques demonstrate improved performance over standard AT. For instance, Mutual Adversarial Training (MAT) has been shown to significantly enhance model robustness under white-box attacks, achieving notable accuracy gains (approximately $8 \%$ over AT under PGD-100 attacks) [19]. Furthermore, MAT can help mitigate robustness trade-offs across different perturbation types, enabling AT to gain up to $1 3 . 1 \%$ accuracy when facing joint $\ell _ { \infty }$ ​ , $\ell _ { 2 }$ ​ , and $\ell _ { 1 }$ ​ attacks, indicating its effectiveness against a wider range of adversarial perturbations [19].​  

Other defense strategies focus on different attack scenarios or mechanisms. The Pairwise Adversarially Robust Loss (PARL) function aims to enhance the diversity of ensemble networks, demonstrating higher robustness against black-box transfer attacks without negatively impacting the accuracy on clean examples [19]. PARL's effectiveness is also evaluated against white-box attacks, where the adversary has full knowledge of the target model's parameters [19]. Beyond traditional defenses, novel approaches explore leveraging adversarial techniques in reverse. One concept proposes using the opposite direction of the adversarial perturbation search to potentially benefit classification, aiming to increase confidence in correct predictions or even rectify misclassified examples [19]. Robustness in representation learning for specific data types like knowledge graphs also represents an active area of research [21].  

Collectively, these diverse defense mechanisms highlight ongoing efforts to build machine learning models that are not only accurate but also reliable and robust against adversarial manipulation, a prerequisite for trustworthy uncertainty estimation.​  

# 6.2 Out-of-Distribution Detection  

Uncertainty estimates play a crucial role in identifying samples that deviate significantly from the training data distribution, known as out-of-distribution (OOD) samples. The core principle involves leveraging the model's uncertainty measure to distinguish between in-distribution and OOD data [8]. For instance, in evidential deep learning, a high uncertainty mass $( u )$ is utilized as a criterion to classify samples as OOD [8]. Similarly, an increase in epistemic uncertainty has been demonstrated to indicate when test data falls outside the range of the training distribution, as shown in simple regression tasks [5].​  

Various uncertainty quantification methods have been explored and evaluated for their effectiveness in OOD detection tasks. Distance-Aware Neural Networks, for example, employ constraints like bi-Lipschitz conditions on the feature space distance to prevent feature collapse, which can aid in OOD detection [1]. Gaussian Process Mixture Models are also noted as methods applicable to OOD detection [1]. Comparisons among different approaches suggest that some methods may be more suitable than others for OOD detection; specifically, GP-based methods have been posited as potentially more appropriate for this task compared to Bayesian Neural Networks (BNNs) and Monte Carlo Dropout [19].  

The performance of these methods is typically evaluated using metrics such as the Area Under the Receiver Operating Characteristic curve (AUROC), True Negative Rate (TNR), Area Under the Precision-Recall curve (AUPR), and detection accuracy [4]. For instance, kNN-UE has been evaluated using AUROC and shown to outperform baseline methods in OOD detection tasks [6].​  

Factors influencing the effectiveness of OOD detection include the choice of the uncertainty quantification method and supplementary techniques. Research focusing on OOD detection in specific domains, such as retinal OCT images, has explored the effectiveness of distance metrics like Cosine distance and techniques such as Outlier Exposure (OE) [15]. These studies indicate that Cosine distance, particularly when combined with OE, can significantly improve the detection of nearOOD cases [15]. For example, using Cosine distance with only a small number of outliers (e.g., 8 outliers per class) exposed during training improved the near-OOD detection performance of the OE with Reject Bucket method by approximately $1 0 \%$ , achieving an AUC of 0.937 [15]. This highlights that incorporating distance-based measures and leveraging exposure to potential outliers can be critical factors in enhancing OOD detection capabilities, especially in challenging scenarios involving samples close to the training distribution boundary.  

# 6.3 Distribution Shift  

Distribution shift represents a significant challenge for maintaining the accuracy and calibration of predictive uncertainty estimates in machine learning models [3]. This phenomenon occurs when the data distribution encountered during testing or deployment differs from the distribution of the training data. Such shifts are critical factors making it difficult for models to provide reliable uncertainty quantification in real-world scenarios [3]. Specifically, a model that appears well-calibrated on the training distribution may fail to maintain this calibration and produce inaccurate uncertainty estimates when presented with data points that deviate substantially from the training examples [3].  

Evaluating the performance of uncertainty estimation methods under distribution shift is crucial for assessing their realworld applicability. Researchers often employ experimental setups where models trained on a source distribution are tested on data from different target distributions [5]. For instance, experiments utilizing datasets like the sin function dataset and NYUv2 dataset involve testing models on data that lies outside the original training distribution to observe the impact of shifts [5]. In specific domains, such evaluations might involve assessing model performance under varying conditions, such as different hydraulic complexities and temporal stabilities in streamflow prediction, which effectively simulates robustness to shifts encountered in operational environments [16].  

Addressing distribution shift to improve the robustness of uncertainty estimates is an active area of research. Efforts include developing or adapting methods to maintain performance guarantees or validity under changing distributions [21]. Conformal prediction, for example, offers a framework that can be extended to scenarios involving distribution shift, suggesting its potential to maintain validity and reliability even when the test data distribution differs from the training data distribution [11]. Other approaches involve techniques rooted in domain adaptation, specifically designed to build models that perform accurately and robustly when applied to target domains that differ from the source domain [21]. Research in this area aims to mitigate the degradation of uncertainty estimates caused by shifts, ensuring that models provide dependable confidence measures even in non-stationary environments.​  

# 7. Applications of Predictive Uncertainty Estimation  

![](images/ce1fe129b2d22876eef3de7775495bef1f18f7dfcf985d6466ecd0fa655b524c.jpg)  

Predictive uncertainty estimation plays a crucial role across a diverse range of machine learning applications, particularly where reliable decision-making is paramount. The capacity for models to "know what they don't know" is vital in mitigating risks and enhancing trust in model outputs [4]. This capability is especially critical in safety-critical domains.​  

One prominent domain where predictive uncertainty estimation is indispensable is healthcare and medical diagnosis [11]. Applications include medical image diagnosis [1,17] and analysis [10], robust medical image segmentation [21], and secure and robust machine learning for healthcare in general [21]. Specific examples include automated screening and staging of age-related macular degeneration (AMD) using retinal OCT images, where uncertainty estimation is used to improve the reliability and trustworthiness of deep learning systems [15]. Furthermore, it finds application in population-based disease prediction using neuroimaging data [18]. In these high-risk scenarios, accurate uncertainty estimation is critical for informed clinical decision-making [17].  

Autonomous driving represents another safety-critical application area [9,17]. Decision-making in autonomous systems relies heavily on the perceived certainty of the environment and potential actions. Understanding and quantifying predictive uncertainty is therefore crucial for reliable navigation and control [17]. Notably, the ability to separate different types of uncertainty, such as aleatoric and epistemic uncertainty, is particularly important in this domain for making robust and informed decisions [9].  

Beyond these high-stakes areas, uncertainty estimation is also valuable in Natural Language Processing (NLP). It is applied in tasks such as sentiment analysis, natural language inference, and named entity recognition, improving the reliability of model outputs [6]. Furthermore, leveraging uncertainty quantification is increasingly seen as a method to mitigate known weaknesses in large language models, such as their propensity for generating hallucinations [3].​  

In scientific domains like chemistry, predictive uncertainty estimation supports tasks such as chemical property predictions, including molecular enthalpy and HOMO-LUMO gaps [12]. Quantifying uncertainty in these predictions is important for optimizing models and can guide experimental design by indicating regions where model predictions are less certain and thus require empirical validation [12].​  

Environmental management also benefits from uncertainty estimation. For instance, understanding streamflow uncertainty is important for water management, hydrologic modeling, and flood forecasting, enabling better risk assessment and resource allocation [16].  

Other specific applications include depth regression tasks [5] and traffic flow prediction, where parametric models can be used to account for data uncertainty [1].  

Uncertainty estimation also serves as a key component in advanced machine learning paradigms. In active learning, for example, uncertainty metrics are used in acquisition functions to select the most informative samples for labeling. By prioritizing samples with high epistemic uncertainty and low aleatoric noise, the efficiency of the learning process can be significantly improved [4,17]. Similarly, uncertainty is fundamental to out-of-distribution (OOD) detection, which is crucial in safety-critical applications where encountering unknown or corrupted inputs can have severe consequences [8]. This allows systems to recognize when they are operating outside their training distribution, preventing potentially hazardous erroneous predictions.​  

In summary, predictive uncertainty estimation enhances the performance, reliability, and decision-making capabilities of machine learning models across numerous domains [6,10]. Its importance is particularly pronounced in risk-sensitive applications where model confidence must be accurately assessed to prevent undesirable outcomes. The ability of models to express their lack of knowledge is becoming an essential requirement for deploying machine learning systems in the real world.​  

# 8. Challenges and Future Directions  

<html><body><table><tr><td>Area</td><td>Key Challenge</td><td>Promising Future Direction</td></tr><tr><td>Reliability & Accuracy</td><td>Achieving reliable UQ, especially under shift; Understanding method assumptions</td><td>Improve robustness to distribution shifts, OOD, attacks; Standardize evaluation</td></tr><tr><td>Scalability & Efficiency</td><td>Computational cost for large models/datasets</td><td>Optimize methods (e.g., NN search), develop efficient approaches</td></tr><tr><td>New Model Types</td><td>UQ for complex models like LLMs</td><td>Develop methods for high- dimensional outputs, semantic consistency</td></tr><tr><td>Method Development</td><td>Limitations of current methods (e.g.,</td><td>Approximate more robust methods (e.g., GPs); Refine</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>BNN/MCDropout OOD)</td><td>method specifics (e.g., EDL annealing)</td></tr><tr><td>Integration</td><td>Lack of integration with other fields (e.g.,</td><td>interpretability, causal</td></tr><tr><td>Structured Data</td><td>(image inverse, spatio-</td><td>Extend methods to handle structured outputs &</td></tr><tr><td></td><td>applicability across tasks; Near-OOD data cost</td><td>domains (e.g., streamflow); Address near-OOD data</td></tr></table></body></html>  

Despite significant progress, the field of predictive uncertainty estimation in machine learning faces several key challenges that impact the practical deployment and reliability of these methods. A fundamental issue is the difficulty in achieving reliable uncertainty quantification, particularly in the presence of distribution shifts, which remain a core problem [3]. The lack of comprehensive understanding regarding how different methodological decisions and underlying assumptions influence uncertainty estimates further complicates the landscape, making direct comparison of results across various methods challenging, especially in domain-specific applications like streamflow uncertainty estimation [16]. Additionally, model bias can arise from limited data coverage, particularly in high-dimensional spaces, necessitating strategies to detect and mitigate errors stemming from noise, bias, and variance [12]. Specific method limitations include the potential for bias introduced during the construction of out-of-distribution (OOD) datasets used in training certain models like diffusion nets [4]. In applied areas such as medical imaging, detecting near-OOD cases is challenging due to the high cost of data acquisition and labeling, limiting the applicability of large-scale outlier exposure techniques [15]. For evidential deep learning, a potential limitation lies in the optimal scheduling of the annealing coefficient $\lambda _ { t }$ ​ used in the uncertainty loss function $L _ { u n c } = \lambda _ { t } K L$ , which is combined with an accuracy loss $L _ { a c c } = \sum _ { j = 1 } ^ { K } y _ { i j } - { \hat { p } } _ { i j }$ ​ for a total loss ${ \cal L } = { \cal L } _ { a c c } + { \cal L } _ { u n c }$ ​ [8]. Furthermore, calculating prediction intervals, especially for nonlinear models, often requires computationally expensive or specialized techniques [14]. Some methods developed initially for regression face added complexity when applied to classification problems [5]. Effective uncertainty quantification is particularly crucial when test data deviates from the training distribution to confirm prediction certainty [17]. Designing appropriate representations also poses challenges, such as in graph-level anomaly detection where representations must capture both local and global anomalies [18].  

Scaling uncertainty estimation techniques to large-scale datasets and complex models presents significant challenges. Computational cost is a major hurdle, exemplified by methods relying on nearest neighbor search, although dimension reduction and approximate search techniques offer potential mitigation strategies [6]. The need for optimizing such methods for large datasets and complex architectures is a clear requirement for broader applicability [6,9].​  

Several open problems and promising future research directions emerge from the current limitations [1,3]. A key area involves improving efficiency and scalability of existing methods, potentially through further optimization for large-scale data [6]. Enhancing robustness to various shifts and adversarial attacks is critical, building upon the recognition that distribution shifts fundamentally challenge reliable quantification [3]. This includes developing techniques robust to different types of OOD samples [15] and standardizing empirical evaluation setups for validating OOD performance [19].  

Addressing uncertainty in emerging model types, such as large language models (LLMs), is a rapidly growing area, requiring solutions for high-dimensional outputs and ensuring semantic consistency [1,3]. Future work also includes developing methods that approximate robust approaches like Gaussian Processes rather than potentially less reliable methods like BNNs or MCDropout [19].​  

Integrating uncertainty quantification with other fields offers significant potential. This includes combining UQ with interpretability to explain the sources of uncertainty [1]. While not explicitly detailed in the digests, potential exists for integration with causal inference. Advancements in UQ within reinforcement learning show promise [3]. Extending UQ to handle structured outputs, including image inverse problems, spatio-temporal data, and graph data, is crucial [1,11]. Applying UQ in scientific simulations requires incorporating physical constraints and predicting extreme events [1]. Further research is needed to improve the understanding of assumptions and treatment of uncertainties in specific domains like streamflow modeling [16] and to explore the applicability of UQ methods across various tasks and architectures [6]. Specific methods like Bayesian Deep Image Prior could be extended to other inverse problems in medical imaging [10], and AE-DNN could be extended to more complex models and datasets [9]. Exploring alternative optimization schedules for parameters like the annealing coefficient in evidential deep learning is also a specific direction for improvement [8]. Ultimately, while perfect reliability may remain elusive, continued advancements in these directions hold the potential to significantly enhance the robustness and trustworthiness of machine learning systems [3].​  

# 9. Conclusion  

Predictive uncertainty estimation constitutes a critical and rapidly evolving area within machine learning research. The current landscape encompasses a diverse array of methodologies aimed at quantifying the confidence or reliability of model predictions [1]. These methods range from classical approaches like prediction intervals, which face challenges with nonlinear models [14], to modern techniques such as MC dropout [5], Evidential Deep Learning (EDL) for classification and out-of-distribution (OOD) detection [8], AE-DNN for separating aleatoric and epistemic uncertainties [9], Conformal Prediction offering distribution-free validity [11], SDE-Net leveraging stochastic differential equations [4], and efficient methods like kNN-UE for tasks such as confidence calibration and OOD detection in natural language processing [6]. Research indicates that methods specifically designed for uncertainty quantification are generally successful in enhancing system capabilities [3].​  

The significance of calibrated uncertainty estimates cannot be overstated for enabling the trustworthy and responsible deployment of machine learning systems [1,17]. Uncertainty quantification plays a particularly critical role in high-stakes and safety-critical applications [3,9,11], such as medical imaging, where reliable OOD detection and addressing issues like hallucinations are paramount [10,15]. Quantifying uncertainty adds an essential layer of safety to deployed systems [3], allowing models to better correlate uncertainty with predictive error [10].  

Ongoing advancements continue to push the boundaries of what is possible, developing methods that are more efficient [6], better at handling specific challenges like OOD detection [4,8,15], or capable of separating different types of uncertainty [9]. However, practical shortcomings persist [3], and significant challenges remain. These include the difficulty of accurately quantifying uncertainty in complex non-linear models [14], the necessity of addressing distinct sources of error such as noise, bias, and variance during model development [12], and crucially, the risk of developing false confidence, particularly when models encounter data distributions beyond their training range [3]. Future research is poised to tackle these challenges, focusing on areas like uncertainty in large language models, scientific simulations, and enhancing interpretability [1]. Understanding the underlying assumptions of different UQ methods and their suitability for various contexts remains a critical next step [16]. Continued progress in these areas will further improve the reliability and utility of machine learning models across an expanding range of applications.  

# References  

[1] 深度学习不确定性量化方法综述：来源、方法与未来展望 https://blog.csdn.net/weixin_42165798/article/details/14708134   
[2] 深度神经网络不确定性综述：来源、分类与建模方法 https://blog.csdn.net/Rad1ant_up/article/details/139114754   
[3] 人工智能安全：机器学习中可靠的不确定性量化方法 https://baijiahao.baidu.com/s?   
id=1804284994554023919&wfr=spider&for=pc   
[4] SDE-Net：用随机微分方程量化神经网络的不确定性 https://cloud.tencent.com/developer/article/1746536   
[5] 深度学习中的不确定性：偶然与认知 (Aleatoric & Epistemic) https://blog.csdn.net/djfjkj52/article/details/130559019​   
[6] kNN-UE: Efficient Uncertainty Estimation for NLP v http://www.paperreading.club/page?id $\mathbf { \lambda } = \mathbf { \dot { \eta } }$ 237977   
[7] Aleatoric 和 Epistemic 不确定性：主动学习策略 https://blog.csdn.net/DeniuHe/article/details/121544899   
[8] Evidential Deep Learning for Out-of-Distribution D https://blog.csdn.net/beginner1207/article/details/135065172   
[9] AE-DNN: Separating Aleatoric and Epistemic Uncerta https://ieeexplore.ieee.org/document/9412616/   
[10] Bayesian Deep Image Prior for Uncertainty Estimati https://link.springer.com/chapter/10.1007/978-3-030-60365-6_9   
[11] Conformal Prediction: A Gentle Introduction (PKU) https://dics.pku.edu.cn/xzhd/tlb/151389.htm   
[12] Uncertainty Characterization in Machine Learning f https://pubs.acs.org/doi/10.1021/acs.jcim.3c00373   
[13] k-Fold Cross-Validation Ensembles for Uncertainty  https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-   
00709-9​   
[14] 机器学习预测区间 https://machinelearningmastery.com/prediction-intervals-for-machine-learning/   
[15] Few-Shot Out-of-Distribution Detection for AMD Scr https://www.nature.com/articles/s41598-023-43018-9   
[16] Streamflow Uncertainty Estimation: A Comparison of   
http://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018WR022708   
[17] 深度学习：不确定性估计与模型校准 https://blog.csdn.net/xys430381_1/article/details/119531335​   
[18] 机器学习学术速递[12.21]：图学习与Transformer精选 https://cloud.tencent.com/developer/article/1924129   
[19] 机器学习学术速递[12.10] 图学习、Transformer、GAN、半监督学习、强化学习等   
https://cloud.tencent.com/developer/article/1917002   
[20] 2022年5月图神经网络(GNN)相关学术速递 https://www.cnblogs.com/BlairGrowing/articles/16323763.html​   
[21] Awesome Robust Machine Learning: A Curated List https://github.com/monk1337/Awesome-Robust-Machine-Learning/  