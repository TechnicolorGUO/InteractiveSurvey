# A Survey of Recent Developments in Measures of Association

# 1 Abstract


The field of measures of association has seen significant advancements, driven by the increasing complexity of data and the need for robust statistical methods to analyze and interpret it. This survey paper focuses on recent developments in measures of association, with a particular emphasis on the integration of advanced computational methods in genomics and machine learning. The paper explores the application of singular value decomposition (SVD) in reduced-order modeling (ROM) and its impact on efficient parametric state estimation, the SHallow REcurrent Decoder (SHRED) architecture, and the use of invertible neural networks (INNs) for amortized inference. It also examines the integration of large language models (LLMs) into system-theoretic process analysis (STPA) for generating and validating unsafe control actions (UCAs) and loss scenarios. Additionally, the paper covers event stream processing and mobile sensing, focusing on the representation and processing of event-based data, hardware and software acceleration techniques, and end-to-end learning for task optimization. The contributions of this survey paper include a comprehensive overview of recent advancements, a detailed analysis of methodologies and their practical implications, and the identification of key challenges and future research directions. By bridging the gap between theoretical advancements and real-world applications, this survey aims to serve as a valuable resource for researchers, practitioners, and students interested in the evolving landscape of measures of association.

# 2 Introduction
The field of measures of association has seen significant advancements in recent years, driven by the increasing complexity of data and the need for robust statistical methods to analyze and interpret it. Measures of association, which quantify the strength and direction of relationships between variables, are fundamental in various scientific disciplines, including genomics, machine learning, and complex systems analysis. These measures are essential for understanding the intricate dependencies and interactions within high-dimensional data, enabling researchers to uncover hidden patterns and make informed decisions. The development of advanced computational techniques and the availability of large-scale datasets have further propelled the field, leading to the emergence of new methods and frameworks that can handle the challenges of modern data analysis.

This survey paper focuses on recent developments in measures of association, with a particular emphasis on the integration of advanced computational methods in genomics and machine learning. The paper explores the application of singular value decomposition (SVD) in reduced-order modeling (ROM) and its impact on efficient parametric state estimation. It delves into the SHallow REcurrent Decoder (SHRED) architecture, which combines SVD with recurrent neural networks (RNNs) to map sensor data to the full state of a system. Additionally, the paper examines the use of invertible neural networks (INNs) for amortized inference, particularly in scenarios where the likelihood function is intractable [1]. The discussion also covers the integration of large language models (LLMs) into system-theoretic process analysis (STPA) for generating and validating unsafe control actions (UCAs) and loss scenarios. Furthermore, the paper explores the application of event stream processing and mobile sensing, focusing on the representation and processing of event-based data, hardware and software acceleration techniques, and end-to-end learning for task optimization [2].

The paper also investigates the theoretical and experimental approaches in quantum physics and materials science, including the Schwinger-Dyson and Bethe-Salpeter equations, lattice simulations, and the Pinch Technique (PT) combined with the Background Field Method (BFM). It examines the dynamics of phase-space holes in plasmas and the behavior of quantum phenomena in materials, such as density functional theory (DFT) and scanning tunneling microscopy (STM), the exotic quantum states in twisted bilayer graphene (TBG), and quantum imaging with entangled photon pairs. The mathematical and theoretical frameworks in complex systems, such as mean-field optimal control and the convergence rates of particle approximations, are also discussed [3]. The paper further explores computational and theoretical models, including generalized Feynman diagrams, the factorization of amplitudes under split kinematics, and the role of the tropical Grassmannian in matroid amplitudes [4]. Finally, the paper delves into the study of complex manifolds and index theory, focusing on heat kernel techniques, the analytical and geometric properties of open complex manifolds, and the index theorem in non-compact spaces.

The contributions of this survey paper are multifaceted. It provides a comprehensive overview of recent developments in measures of association, highlighting the integration of advanced computational methods and their applications in various fields. The paper synthesizes the latest research findings, offering a detailed analysis of the methodologies and their practical implications. By bridging the gap between theoretical advancements and real-world applications, this survey aims to serve as a valuable resource for researchers, practitioners, and students interested in the evolving landscape of measures of association. The paper also identifies key challenges and future research directions, providing a roadmap for further investigation and innovation in this dynamic field.

# 3 Advanced Computational Methods in Genomics and Machine Learning

## 3.1 Data Compression and Efficient Parametric State Estimation

### 3.1.1 Reduced-Order Modeling with SVD
Reduced-Order Modeling (ROM) with Singular Value Decomposition (SVD) is a powerful technique for dimensionality reduction in the context of solving Partial Differential Equations (PDEs) and other high-dimensional systems [5]. SVD, a fundamental linear algebra method, decomposes a matrix into a product of three matrices, \( U \Sigma V^T \), where \( U \) and \( V \) are orthogonal matrices, and \( \Sigma \) is a diagonal matrix containing the singular values. This decomposition provides a compact representation of the original data, enabling the extraction of the most significant features while discarding less important information. In the context of ROM, SVD is used to identify a low-dimensional subspace that captures the essential dynamics of the system, thereby reducing the computational complexity of solving the PDEs.

The application of SVD in ROM involves constructing a snapshot matrix from the solution data of the Full Order Model (FOM). This matrix is then subjected to SVD to obtain the singular values and singular vectors. The singular vectors corresponding to the largest singular values form the basis of the reduced-order space. By projecting the original high-dimensional system onto this reduced space, the dimensionality of the problem is significantly reduced, leading to a more efficient and computationally tractable model. This approach not only accelerates the solution process but also maintains a high level of accuracy, as the most important modes of the system are retained. The reduced-order model can then be used for real-time simulations, optimization, and control, making it particularly useful in applications where rapid and accurate solutions are required.

Moreover, the integration of SVD with machine learning (ML) techniques has further enhanced the capabilities of ROM. ML algorithms, such as neural networks, can be trained on the reduced-order data to predict system behavior with high accuracy and efficiency. The data compression achieved through SVD reduces the training cost and improves the generalization performance of these models. Additionally, the reduced-order space can be used to initialize and guide the training process, leading to faster convergence and better model performance. This combination of SVD and ML has opened new avenues for developing fully data-driven frameworks that can handle complex engineering systems with a balance of computational efficiency and predictive accuracy.

### 3.1.2 SHallow REcurrent Decoder (SHRED) Architecture
The SHallow REcurrent Decoder (SHRED) architecture is a specialized machine learning model designed to efficiently map the trajectories of measured observable quantities to the full state of a system, particularly in the context of complex and safety-critical environments such as Molten Salt Fast Reactors (MSFRs). SHRED leverages a combination of Singular Value Decomposition (SVD) and recurrent neural networks (RNNs) to achieve this mapping. The architecture begins by applying SVD to the sensor data, which compresses the high-dimensional data into a lower-dimensional latent space. This compression step is crucial for reducing the computational complexity and making the training process more efficient, even on modest hardware.

In the SHRED architecture, the compressed data is then fed into a shallow RNN, typically a Long Short-Term Memory (LSTM) network, which is responsible for capturing the temporal dependencies in the sensor measurements. The LSTM network is trained to predict the full state of the system from the compressed sensor data. The shallow nature of the RNN ensures that the model remains computationally lightweight, allowing for rapid training and inference times. This is particularly advantageous in real-time applications where quick and accurate state estimation is critical. The training process is further optimized by performing it in the compressed space, which significantly reduces the amount of data that needs to be processed and stored.

One of the key strengths of the SHRED architecture is its ability to handle sparse and randomly placed sensors [5]. Unlike other machine learning models that may require a dense and structured sensor network, SHRED can effectively learn from a minimal number of sensors, making it highly adaptable to various deployment scenarios [5]. This flexibility, combined with the efficiency of the SVD and RNN components, makes SHRED a promising approach for state estimation in complex systems where sensor placement and data collection can be challenging. The architecture's robustness and computational efficiency also make it suitable for real-time monitoring and control applications, where timely and accurate state information is essential for ensuring system safety and performance.

### 3.1.3 Amortized Inference with Invertible Neural Networks (INNs)
Amortized inference with Invertible Neural Networks (INNs) represents a powerful approach to Bayesian inference, particularly in scenarios where the likelihood function is intractable or computationally expensive. INNs achieve this by learning a bijective mapping between the observed data and the latent parameters, enabling efficient sampling from the posterior distribution. Unlike traditional Markov Chain Monte Carlo (MCMC) methods, which require repeated sampling to approximate the posterior, INNs perform a single forward pass through the network to obtain a posterior sample, significantly reducing computational overhead.

The key advantage of INNs lies in their ability to model complex, high-dimensional posterior distributions through a series of invertible transformations. These transformations are typically constructed using normalizing flows, which are a class of deep generative models that allow for exact likelihood computation and efficient sampling. During the training phase, INNs are optimized to minimize the discrepancy between the transformed prior and the empirical distribution of the data. This is often achieved using maximum likelihood estimation or variational inference, depending on the specific problem and available data. Once trained, the INN can be used to perform rapid and accurate posterior inference, making it particularly suitable for real-time applications and large-scale datasets.

In the context of Whole Exome Sequencing (WES) analyses, INNs can be leveraged to infer the posterior distribution of genetic variants associated with cognitive aging and neurodegenerative disorders [6]. By modeling the complex relationships between genetic markers and phenotypic outcomes, INNs can provide insights into the underlying genetic architecture of these conditions. This approach not only enhances the interpretability of the inferred models but also improves the robustness of the predictions by accounting for the uncertainty in the parameter estimates. Furthermore, the ability of INNs to handle high-dimensional data and complex dependencies makes them a promising tool for integrating functional annotations and other auxiliary information, thereby increasing the power and reliability of WES analyses.

## 3.2 Large Language Models and System-Theoretic Process Analysis

### 3.2.1 Automated Generation of Unsafe Control Actions
Automated Generation of Unsafe Control Actions (UCAs) is a critical component in the System-Theoretic Process Analysis (STPA) methodology, designed to systematically identify potential hazards in complex systems. The process begins with the creation of a control structure diagram, which maps out the interactions between controllers and the system under analysis. This diagram serves as the foundation for identifying UCAs, which are defined as actions that, if taken by a controller, could lead to a hazardous situation. The identification of UCAs is not a trivial task, as it requires a deep understanding of the system's operational context, the control logic, and the potential for human error or system malfunction.

The automation of UCA generation leverages advanced algorithms and computational models to enhance the efficiency and comprehensiveness of the hazard analysis process. These algorithms can systematically explore the control structure to identify all possible combinations of control actions that could lead to unsafe states. Techniques such as constraint satisfaction, graph theory, and formal verification are often employed to ensure that the analysis is thorough and that no potential hazards are overlooked. The use of automation in this context not only accelerates the hazard analysis but also reduces the risk of human oversight, thereby improving the overall safety of the system.

However, the automated generation of UCAs also presents several challenges. One of the primary challenges is the potential for generating a large number of false positives, which can overwhelm the analysis and make it difficult to prioritize and address the most critical hazards. To mitigate this issue, advanced filtering and prioritization techniques are necessary to refine the list of identified UCAs. Additionally, the complexity of modern systems can lead to a combinatorial explosion in the number of possible control actions, making the computational requirements for automated UCA generation significant. Despite these challenges, the integration of automated tools in the STPA process represents a significant advancement in the field of safety engineering, offering a more systematic and rigorous approach to hazard identification and risk management [7].

### 3.2.2 Evaluation of LLM-Enhanced STPA Framework
The evaluation of the LLM-enhanced System-Theoretic Process Analysis (STPA) framework focuses on assessing the effectiveness and accuracy of integrating Large Language Models (LLMs) into the traditional STPA process. Specifically, the evaluation aims to determine how well LLMs can assist in generating Unintended Control Actions (UCAs), loss scenarios, and links between various artifacts such as hazards and controls. The primary hypothesis is that LLMs can significantly enhance the STPA process by providing more comprehensive and nuanced outputs compared to traditional methods, thereby improving the identification and mitigation of potential safety issues.

To validate this hypothesis, a case study from the STPA Handbook is used as a benchmark. The evaluation is structured around two main research questions: (1) How many of the LLM-generated UCAs and loss scenarios are deemed valid by a requirement engineer, and (2) How accurately does the LLM link related artifacts together. The case study involves a detailed analysis of a complex system, where the LLM is tasked with generating UCAs and loss scenarios based on the provided control structure diagram and other relevant artifacts. The outputs are then reviewed by a requirement engineer who assesses their validity and relevance to the system and its operational environment.

The results of the evaluation indicate that the LLM-enhanced STPA framework can indeed generate a substantial number of valid UCAs and loss scenarios, with a high degree of accuracy in linking related artifacts [7]. This suggests that LLMs can effectively augment the traditional STPA process, potentially reducing the cognitive load on requirement engineers and enhancing the thoroughness of safety analyses [7]. Additionally, the use of LLMs helps mitigate confirmation bias, a common issue in requirement engineering, by providing a broader and more diverse set of potential failure scenarios [7]. Overall, the evaluation demonstrates the potential of LLMs to significantly improve the efficiency and effectiveness of the STPA framework, paving the way for more robust and comprehensive safety assessments in complex systems.

### 3.2.3 Case Study on Valid UCAs and Loss Scenarios
In the context of system safety analysis, the identification and validation of Unintended Control Actions (UCAs) and loss scenarios are critical for ensuring the robustness and reliability of complex systems [7]. The process of generating UCAs involves systematically considering all control actions and their potential to fail in four distinct ways: too soon, too late, too much, and too little. Each of these failure types can lead to a unique set of loss scenarios, which must be carefully analyzed to identify potential hazards. The case study presented in this section focuses on a small system with a limited number of control actions to illustrate the complexity and depth of this analysis.

The challenge in validating UCAs and loss scenarios lies in the exponential growth of artifacts that must be considered. For instance, a system with only five control actions can generate up to 20 UCAs (5 control actions × 4 failure types), and each UCA can be linked to four different loss scenario types, leading to a total of 80 potential loss scenarios [7]. This combinatorial explosion necessitates a structured approach to prioritize and validate the most critical scenarios. The requirement engineer plays a pivotal role in this process by applying domain expertise to filter and evaluate the generated UCAs and loss scenarios. This involves assessing the likelihood and severity of each scenario to determine which ones pose the greatest risk to system safety.

To enhance the validity of the identified UCAs and loss scenarios, the case study employs a combination of qualitative and quantitative methods. Qualitative methods, such as expert reviews and scenario workshops, help in refining the initial set of UCAs and loss scenarios by incorporating insights from experienced practitioners. Quantitative methods, including fault tree analysis and event tree analysis, are used to model the causal relationships between UCAs and potential hazards [7]. These methods provide a systematic way to evaluate the robustness of the system's safety mechanisms and identify any gaps that need to be addressed. The integration of these methods ensures a comprehensive and rigorous validation process, ultimately contributing to the development of safer and more reliable systems.

## 3.3 Event Stream Processing and Mobile Sensing

### 3.3.1 Event Representation and Data Processing Algorithms
Event representation and data processing algorithms are crucial components in the analysis of event-based data, particularly in applications involving event cameras [2]. These algorithms must address the unique challenges posed by the high temporal resolution and sparse nature of event data. The first step in processing event data is the representation of events, which involves encoding the asynchronous and sparse nature of the data into a format suitable for further processing. Common representations include event frames, which aggregate events over a fixed time window, and event volumes, which incorporate a time dimension to capture the temporal dynamics of the events. Each representation has its advantages and trade-offs, with event frames being computationally efficient but potentially losing temporal information, and event volumes preserving more temporal detail but at a higher computational cost.

Once the events are represented, the next step is to apply data processing algorithms to extract meaningful features and reduce noise. Event-based denoising techniques are essential due to the sensitivity of event cameras to illumination changes, which can introduce significant noise into the data [2]. Techniques such as temporal filtering and spatial filtering are commonly used to remove noise while preserving the relevant features. Additionally, event-based filtering and feature extraction algorithms are designed to identify and extract salient features from the event stream. These features can include edges, corners, and motion patterns, which are crucial for tasks such as object detection and tracking. Advanced methods, such as deep learning-based approaches, have shown promise in enhancing the accuracy and robustness of feature extraction.

Finally, event-based matching and mapping algorithms are used to align and integrate the extracted features across different views or time instances. These algorithms are essential for tasks such as 3D reconstruction and simultaneous localization and mapping (SLAM). Event-based hardware and software acceleration techniques are also critical for real-time processing of event data. These techniques include specialized hardware architectures and optimized software algorithms that can handle the high data rates and low latency requirements of event-based systems. The integration of these algorithms into end-to-end learning frameworks has further improved the performance and efficiency of event-based data processing, making it a promising area for future research and development [2].

### 3.3.2 Hardware and Software Acceleration Techniques
Hardware and software acceleration techniques play a crucial role in enhancing the performance and efficiency of computational tasks, particularly in resource-constrained environments such as mobile devices and edge computing. In the context of event-based systems, hardware acceleration involves the use of specialized hardware components, such as event cameras and custom processors, to offload computationally intensive tasks from the main CPU. Event cameras, with their μs-level temporal resolution and low-latency sensing capabilities, are particularly well-suited for high-speed and high-dynamic range applications. These cameras can capture rapid changes in the environment without motion blur, making them ideal for real-time applications such as robotics and augmented reality.

Software acceleration, on the other hand, focuses on optimizing the algorithms and software frameworks that process event data. Techniques such as parallel processing, efficient data structures, and algorithmic optimizations can significantly reduce the computational overhead and improve the responsiveness of event-based systems. For instance, parallel processing frameworks like OpenCL and CUDA can leverage the parallel processing power of GPUs to accelerate tasks such as event filtering, feature extraction, and object recognition. Additionally, the use of lightweight and optimized libraries, such as those designed for embedded systems, can further enhance performance by minimizing memory usage and reducing execution times.

Recent advancements in both hardware and software acceleration have led to the development of hybrid approaches that combine the strengths of both domains. For example, custom hardware accelerators can be designed to work in tandem with optimized software algorithms to achieve optimal performance. These hybrid systems often involve the integration of FPGA-based accelerators and specialized ASICs, which can be tailored to specific computational tasks. Furthermore, the use of machine learning techniques, such as neural networks, can be accelerated through the use of dedicated hardware like TPUs and NPUs, which are specifically designed to handle the high computational demands of deep learning models. This integration of hardware and software acceleration techniques is essential for enabling the deployment of complex and resource-intensive applications on mobile and edge devices.

### 3.3.3 End-to-End Learning for Task Optimization
End-to-end learning for task optimization has emerged as a powerful paradigm, particularly in the context of Whole Exome Sequencing (WES) analyses for cognitive aging and neurodegenerative disorders [6]. This approach integrates the entire data processing pipeline, from raw sequence data to variant annotation and association testing, into a single, unified model. By doing so, it aims to optimize the overall performance and efficiency of the analysis, reducing the need for manual intervention and minimizing the risk of errors that can arise from disjointed processing steps. The key advantage of end-to-end learning lies in its ability to adaptively learn the most relevant features and parameters directly from the data, thereby enhancing the detection power and interpretability of genetic associations.

In the context of WES, end-to-end learning models can incorporate advanced techniques such as dynamic window scanning and functional annotations, similar to the SCANG and STAAR frameworks, to enhance the detection of rare variant associations [6]. These models can be trained to weigh the importance of different functional annotations, thereby improving the robustness and accuracy of the association tests. Moreover, the integration of multiple annotation-weighted methods into a single unified test, as seen in the STAAR-O test, can further boost the power of detecting genetic factors contributing to cognitive aging and neurodegenerative disorders. However, the effectiveness of these models is highly dependent on the quality and relevance of the functional annotations, which can introduce biases and computational challenges.

Despite these challenges, end-to-end learning models offer significant advantages in terms of scalability and adaptability. They can handle large and complex datasets efficiently, making them suitable for large-scale WES studies. Additionally, these models can be fine-tuned to specific tasks and datasets, allowing for the incorporation of domain-specific knowledge and the optimization of performance metrics relevant to the study. Future research in this area should focus on developing more robust and interpretable models, addressing the computational and resource demands, and exploring the integration of additional data types, such as epigenetic and transcriptomic data, to further enhance the understanding of genetic factors in cognitive aging and neurodegenerative disorders.

# 4 Theoretical and Experimental Approaches in Quantum Physics and Materials Science

## 4.1 Quantum Dynamics and Field Theory

### 4.1.1 Schwinger-Dyson and Bethe-Salpeter Equations
The Schwinger-Dyson equations (SDEs) and Bethe-Salpeter equations (BSEs) are fundamental tools in the study of nonperturbative quantum field theories, particularly in the context of Quantum Chromodynamics (QCD) [8]. These equations provide a systematic framework for calculating the Green's functions and vertex functions of the theory, which are essential for understanding the dynamics of strongly interacting systems. The SDEs are derived from the generating functional of the theory and represent a set of coupled integral equations that describe the behavior of Green's functions at all orders in perturbation theory. They are particularly useful for exploring the nonperturbative regime, where traditional perturbative methods fail due to the strong coupling between the fields.

In the context of QCD, the SDEs are used to study the properties of quarks and gluons, which are confined within hadrons. The equations allow for the inclusion of dynamical effects such as chiral symmetry breaking and the generation of quark masses, which are crucial for understanding the structure of hadrons. The BSEs, on the other hand, are derived from the SDEs and are used to study the bound states of quarks, such as mesons and baryons. These equations describe the interaction between quarks and gluons and provide a way to calculate the masses and wave functions of hadrons. The BSEs are particularly important for understanding the spectrum of hadrons and their decay properties, as they account for the nonperturbative nature of the strong interaction.

The interplay between the SDEs and BSEs is essential for a comprehensive understanding of the nonperturbative aspects of QCD. The SDEs provide the necessary input for the BSEs, such as the quark and gluon propagators and the interaction vertices, which are then used to solve for the bound states. This coupled system of equations is often solved numerically, using techniques such as the truncation of the infinite hierarchy of equations and the use of approximations like the rainbow-ladder approximation. These methods have been successful in reproducing experimental data for hadron masses and other observables, thus validating the nonperturbative approach based on the SDEs and BSEs.

### 4.1.2 Lattice Simulations and Theoretical Predictions
Lattice simulations serve as a cornerstone in the theoretical exploration of quantum chromodynamics (QCD) and other strongly interacting systems, offering a non-perturbative approach to understanding the behavior of quarks and gluons. These simulations discretize spacetime on a lattice, allowing for the numerical evaluation of path integrals that are otherwise intractable. The primary advantage of lattice QCD lies in its ability to compute quantities such as the masses of hadrons, the structure of the QCD vacuum, and the properties of quark-gluon plasma, which are essential for validating theoretical predictions against experimental data. The implementation of advanced algorithms, such as the Hybrid Monte Carlo (HMC) method, has significantly enhanced the efficiency and accuracy of these simulations, enabling the exploration of a broader range of physical scenarios.

In the context of this study, lattice simulations were employed to investigate the impact of higher-order many-body interactions on the energy of the system, particularly focusing on the role of exchange effects between \(^4\)He atoms [9]. The Path-Integral Monte Carlo (PIMC) algorithm, which accounts for these exchange effects, was utilized to simulate the system's behavior at various temperatures and densities. The choice of the PIMC algorithm is crucial as it allows for the accurate sampling of configurations that would otherwise be computationally prohibitive. The pair product approximation, a perturbative method, was also employed to evaluate the many-body effects, providing a benchmark for the more complex PIMC results. This approach has been validated through its successful application in studying the properties of hydrogen, where it has demonstrated high accuracy and reliability.

The theoretical predictions derived from these lattice simulations were compared against experimental data to assess the validity of the underlying models. Key observables, such as the energy spectrum and correlation functions, were computed and analyzed to identify the contributions of higher-order interactions. The results indicate that these interactions play a significant role in determining the system's energetics, particularly at low temperatures where quantum effects dominate. The agreement between the lattice simulations and experimental measurements provides strong support for the theoretical framework and highlights the importance of considering many-body effects in the study of condensed quantum systems. Furthermore, the insights gained from these simulations contribute to a deeper understanding of the fundamental interactions governing the behavior of particles at the quantum scale.

### 4.1.3 Pinch Technique and Background Field Method
The Pinch Technique (PT) and Background Field Method (BFM) are powerful tools in the realm of quantum field theory, particularly when dealing with gauge theories such as Quantum Chromodynamics (QCD) [8]. The PT, initially developed to address issues of gauge dependence in perturbative calculations, ensures that the resulting Green functions are gauge-invariant and satisfy the Slavnov-Taylor identities (STIs) in the Abelian limit. This is achieved by reorganizing the perturbative series in a way that the pinch contributions, which are gauge-dependent, are systematically absorbed into the self-energies and vertices, thereby ensuring that the physical amplitudes remain gauge-independent.

When combined with the BFM, the PT-BFM framework provides a robust and consistent approach to non-perturbative studies of gauge theories. The BFM introduces a background gauge field, which is treated as a classical external field, while the quantum fluctuations are expanded around this background. This method naturally respects the gauge symmetry of the theory and allows for a clear separation between the background and quantum fields. The combination of PT and BFM ensures that the resulting Green functions are not only gauge-invariant but also satisfy the STIs, which are crucial for maintaining the consistency of the theory. This framework is particularly useful in the context of non-perturbative studies, where the gauge dependence of the Green functions can significantly affect the physical predictions.

The PT-BFM framework has been successfully applied to a variety of problems, including the study of the gluon propagator and the three-gluon vertex in QCD. By ensuring that the Green functions satisfy the STIs, this approach provides a transparent and systematic way to analyze the non-perturbative behavior of these quantities. For instance, the four-gluon kernel, which is a central input in the study of the gluon propagator, can be modeled using the one-loop exchange approximation, leading to results that are consistent with the saturation point of the gluon propagator [8]. This framework also allows for a detailed numerical analysis of the soft-gluon limit, where the displacement of the form factor can be described in terms of the residue function, providing a clear and consistent picture of the non-perturbative dynamics [8].

## 4.2 Plasma Physics and Phase-Space Dynamics

### 4.2.1 Vlasov-Poisson Equations for Phase-Space Holes
The Vlasov-Poisson equations are fundamental in describing the evolution of collisionless plasmas and self-gravitating systems, and they play a crucial role in the formation and dynamics of phase-space holes. These holes, which are regions of depleted particle density in phase space, exhibit behavior analogous to vortices in fluid dynamics. The Vlasov equation governs the time evolution of the distribution function \( f(\mathbf{x}, \mathbf{v}, t) \) in phase space, while the Poisson equation relates the self-consistent electric field to the density fluctuations caused by the particles. Together, these equations capture the nonlinear interactions that lead to the formation of phase-space structures.

Phase-space holes are characterized by localized regions where the distribution function \( f \) is significantly reduced compared to the surrounding phase space. This depletion is often associated with the presence of a strong electric field that expels particles from the region, creating a void. The analogy with fluid vortices is particularly evident in the vorticity field, which can be defined in phase space and used to identify and analyze these structures. Techniques from fluid dynamics, such as the identification of vortices through vorticity criteria, have been adapted to phase-space analysis, allowing for a deeper understanding of the dynamics of phase-space holes.

The mathematical framework provided by the Vlasov-Poisson equations enables a detailed study of the stability and evolution of phase-space holes. Linear stability analysis reveals that small perturbations in the distribution function can grow exponentially, leading to the formation of phase-space holes. Nonlinear effects, captured by the full Vlasov-Poisson system, then determine the subsequent evolution and interaction of these structures. Numerical simulations and analytical methods, such as the fluid-like phase-space hydrodynamic approach, have been instrumental in elucidating the complex dynamics of phase-space holes, highlighting their importance in various physical systems, including plasma physics and astrophysics [10].

### 4.2.2 Stability and Coalescence in Electron-Ion Plasmas
In electron-ion plasmas, the stability and coalescence of phase-space holes are critical phenomena that influence the dynamics and structure of the plasma [10]. Phase-space holes, or regions of depleted electron density, form as a result of nonlinear wave-particle interactions, particularly in the context of two-stream instabilities. These instabilities lead to the formation of multiple electron phase-space holes, which can subsequently coalesce into larger, more stable structures. The coalescence process is driven by the attractive forces between the holes, which are mediated by the plasma's electric field and the collective behavior of the electrons and ions.

The coalescence of phase-space holes in electron-ion plasmas has been extensively studied using both theoretical models and numerical simulations [10]. Theoretical approaches often employ fluid-analogous descriptions of phase-space dynamics, where the holes are treated as fluid elements that can merge and form larger structures. These models predict that the coalescence process is an inelastic collision, resulting in the formation of a single, stable hole that persists over time. Numerical simulations, particularly those using particle-in-cell (PIC) methods, have provided detailed insights into the mechanisms of hole coalescence, including the role of nonlinear wave-particle interactions and the influence of plasma parameters such as temperature and density.

Understanding the stability and coalescence of phase-space holes is crucial for a wide range of applications, from laboratory plasmas to astrophysical environments [10]. In laboratory settings, these phenomena are relevant to the operation of plasma-based accelerators and the performance of fusion devices. In astrophysics, the coalescence of phase-space holes may play a role in the dynamics of relativistic jets and the formation of large-scale structures in the interstellar medium. The study of these processes not only enhances our fundamental understanding of plasma physics but also provides practical insights into the control and optimization of plasma-based technologies.

### 4.2.3 Cylindrical Wave-Guided Plasma Setups
Cylindrical wave-guided plasma setups represent a unique class of experimental configurations that leverage the geometric properties of cylindrical symmetry to study wave propagation and nonlinear phenomena in plasma. These setups are particularly useful for investigating the behavior of plasma waves under confined conditions, where the cylindrical geometry can significantly influence the dispersion relations and the stability of the waves. The cylindrical geometry also facilitates the application of external magnetic fields, which can further modify the wave dynamics and introduce new instabilities or nonlinear effects. The confinement provided by the cylindrical waveguide can lead to the formation of localized structures, such as solitons and vortices, which are of great interest in both fundamental plasma physics and applied technologies, such as plasma-based accelerators and waveguides for high-frequency communication.

In these setups, the cylindrical waveguide typically consists of a plasma column confined by either a magnetic field or a physical boundary, such as a dielectric tube. The wave propagation is governed by the interplay between the plasma density, the magnetic field, and the wave frequency. Theoretical models of cylindrical wave-guided plasma setups often employ the cold plasma approximation, where the plasma is considered to be collisionless and the thermal effects are negligible. This approximation simplifies the analysis while still capturing the essential physics of wave propagation and nonlinear interactions. Numerical simulations, such as particle-in-cell (PIC) and fluid simulations, are crucial for understanding the complex dynamics of these systems, especially in the presence of nonlinear effects and instabilities. These simulations can provide detailed insights into the formation and evolution of coherent structures, such as solitons and vortices, and the conditions under which they can be stabilized or destabilized.

Recent experimental and theoretical studies have focused on the specific phenomena of ion hole coalescence and the formation of nonlinear structures in cylindrical wave-guided plasmas [10]. Ion hole coalescence, where multiple ion holes merge into a single larger structure, has been observed in both simulations and experiments [10]. The relative velocities of the ion holes play a critical role in this process; at low relative velocities, the holes tend to merge, while at high velocities, they pass through each other without significant interaction. The cylindrical geometry of the waveguide can enhance or inhibit this coalescence process, depending on the boundary conditions and the strength of the external magnetic field. These studies not only advance our understanding of fundamental plasma physics but also have practical implications for the design of plasma-based devices and the control of plasma waves in technological applications.

## 4.3 Quantum Phenomena in Materials

### 4.3.1 Density Functional Theory and Scanning Tunneling Microscopy
Density Functional Theory (DFT) and Scanning Tunneling Microscopy (STM) are powerful tools that have significantly advanced our understanding of surface science and nanotechnology. DFT provides a theoretical framework for calculating the electronic structure of materials, enabling the prediction of various physical and chemical properties. By solving the Kohn-Sham equations, DFT can accurately describe the electronic density and energy landscape of complex systems, making it indispensable for studying the behavior of electrons at surfaces and interfaces. This capability is particularly crucial for understanding the electronic properties of nanostructures, where quantum effects dominate.

STM, on the other hand, offers a complementary experimental approach by allowing direct visualization of atomic-scale structures and the manipulation of individual atoms and molecules. The operation of STM relies on the quantum tunneling effect, where a sharp metallic tip is brought close to a conducting surface, creating a small tunneling current that is sensitive to the local density of states. This technique has revolutionized surface science by providing real-space images of surfaces with atomic resolution, enabling researchers to observe and manipulate individual atoms and molecules. The combination of DFT and STM has led to a deeper understanding of surface phenomena, such as adsorption, diffusion, and catalytic reactions, by bridging the gap between theoretical predictions and experimental observations.

Moreover, the integration of DFT and STM has facilitated the exploration of novel materials and devices at the nanoscale. For instance, DFT can predict the stability and electronic properties of specific atomic configurations, which can then be verified and further investigated using STM. This synergy has been particularly useful in the development of molecular electronics, where the precise control of atomic arrangements is essential for achieving desired functionalities. Additionally, the ability to correlate theoretical models with experimental data has enhanced our understanding of fundamental processes such as charge transfer and spin-polarized transport, paving the way for the design of advanced nanoelectronic and spintronic devices.

### 4.3.2 Exotic Quantum States in Twisted Bilayer Graphene
Twisted bilayer graphene (TBG) has emerged as a platform for exploring a wide range of exotic quantum states, driven by the interplay between the moiré superlattice and the electronic structure. When two graphene layers are twisted at a small angle, the resulting moiré pattern creates a periodic potential that strongly modifies the electronic band structure, leading to the formation of flat bands near the Fermi level. These flat bands are characterized by a vanishing density of states and a large effective mass, which can stabilize various correlated electronic states, including superconductivity, magnetism, and topological insulating phases.

One of the most striking phenomena observed in TBG is the emergence of superconductivity at specific twist angles, particularly around the "magic angle" of approximately 1.1 degrees. The superconducting state in TBG is believed to be driven by strong electron-electron interactions, which are enhanced by the flatness of the bands. The nature of the superconducting pairing mechanism remains a topic of intense debate, with proposals ranging from conventional phonon-mediated pairing to more exotic mechanisms involving spin fluctuations or topological excitations. The coexistence of superconductivity with other electronic phases, such as Mott insulating states, further complicates the phase diagram and suggests a rich interplay between different orders.

Beyond superconductivity, TBG also hosts a variety of magnetic and topological states. The flat bands in TBG can support ferromagnetic and antiferromagnetic order, which are stabilized by the strong Coulomb interactions and the unique band topology. Additionally, the presence of a strong spin-orbit coupling can lead to the formation of topological insulating phases, characterized by protected edge states and non-trivial bulk band topology. These exotic quantum states in TBG not only provide a fertile ground for fundamental research but also hold potential for applications in quantum computing and spintronics. The tunability of the electronic properties through the twist angle and external gating offers a versatile platform for exploring and engineering these states.

### 4.3.3 Quantum Imaging with Entangled Photon Pairs
Quantum imaging with entangled photon pairs leverages the unique properties of quantum mechanics to achieve imaging capabilities beyond classical limits. Entangled photon pairs, typically generated via spontaneous parametric down-conversion (SPDC), exhibit correlations that can be exploited for various imaging tasks. In SPDC, a pump photon is converted into a pair of lower-energy photons, often referred to as the signal and idler photons, which are inherently entangled in multiple degrees of freedom, including polarization, spatial mode, and frequency. This entanglement allows for the creation of non-classical states of light that can be used to enhance the resolution, sensitivity, and contrast of imaging systems.

One of the key applications of entangled photon pairs in quantum imaging is ghost imaging, where the image is reconstructed using the correlations between the signal and idler photons. In a typical ghost imaging setup, the signal photon is directed towards the object to be imaged, while the idler photon is detected without ever interacting with the object. The coincidence detection of the signal and idler photons provides the necessary correlation to reconstruct the image. This technique has been shown to offer improved signal-to-noise ratios and reduced exposure times compared to classical imaging methods, making it particularly useful in low-light conditions or for imaging sensitive samples.

Recent advancements in quantum imaging have also explored the use of entangled photon pairs for quantum lithography and quantum metrology. In quantum lithography, the sub-wavelength spatial correlations of entangled photons can be used to create patterns with resolutions finer than the classical diffraction limit. Similarly, in quantum metrology, the enhanced precision of measurements derived from entangled states can lead to significant improvements in the accuracy of various sensing applications, such as gravitational wave detection and magnetic field sensing. These developments underscore the potential of entangled photon pairs to revolutionize imaging technologies by pushing the boundaries of what is possible with classical light.

# 5 Mathematical and Theoretical Frameworks in Complex Systems

## 5.1 Mean-Field Optimal Control and Convergence Rates

### 5.1.1 Particle Approximation and Projection Argument
The particle approximation and projection argument form a crucial component in the analysis of systems involving a large number of interacting particles, particularly in the context of mean-field games and optimal control problems [3]. The particle approximation involves representing the system by a finite number of particles, each governed by a stochastic differential equation (SDE), and then analyzing the behavior of these particles as the number of particles tends to infinity. This approach allows for the derivation of a limiting equation that describes the macroscopic behavior of the system. The projection argument is subsequently used to establish the convergence of the finite-particle system to the solution of the limiting equation.

In the projection argument, the value function of the limiting problem, which is often formulated over a space of probability measures, is projected onto a finite-dimensional space [3]. This projection is necessary because the value function in the limit may lack the necessary smoothness for a direct application of comparison principles or other analytical tools. By regularizing the limiting value function, one can apply the projection argument to this smoothed version, thereby facilitating the derivation of convergence rates. The mollification procedure, which involves convolving the value function with a smooth kernel, is a common technique used to achieve this regularization. This process ensures that the projected value function satisfies a PDE that is similar to the one governing the finite-particle system.

The main estimates in the paper are derived by comparing the solutions of the PDEs governing the finite-particle system and the regularized limiting value function. These estimates provide bounds on the difference between the two value functions, which in turn yield convergence rates. The analysis often involves intricate calculations and the use of advanced techniques from stochastic analysis and partial differential equations. The projection argument, combined with the mollification procedure, thus serves as a powerful tool for bridging the gap between the finite-particle approximation and the limiting behavior of the system.

### 5.1.2 Mollification and Comparison of Value Functions
Mollification plays a crucial role in the analysis of value functions, particularly in contexts where the original value function lacks the necessary smoothness for rigorous mathematical treatment. In the setting of (A∞, 2)-flow categories, the mollification process involves constructing a smooth approximation of the value function, which facilitates the application of various analytical tools. This technique is essential when dealing with the convergence of discrete approximations to continuous models, as it allows for a more precise characterization of the limiting behavior of these approximations.

The mollification procedure typically involves convolving the original value function with a smooth kernel, effectively smoothing out irregularities while preserving key properties. In the context of (A∞, 2)-categories, this process is particularly useful for establishing the consistency between the discrete and continuous formulations of the problem. By ensuring that the mollified value function retains the essential features of the original, such as its critical points and asymptotic behavior, one can rigorously compare the solutions obtained from different models. This comparison is vital for validating the accuracy and reliability of numerical methods used in the discretization process.

Moreover, the mollification technique provides a bridge between the theoretical analysis and practical implementation of algorithms designed to solve complex optimization problems. By regularizing the value function, one can derive explicit error bounds and convergence rates, which are crucial for understanding the performance of these algorithms. This approach not only enhances the theoretical foundations but also guides the development of more efficient and robust computational methods, thereby advancing the field of Floer homotopy theory and related areas.

### 5.1.3 Smoothness of Limiting Value Function
The smoothness properties of the limiting value function play a crucial role in the analysis of optimal control problems, particularly in establishing the rates of convergence and the validity of asymptotic approximations. In the context of (A∞, 2)-flow categories, the limiting value function often lacks the necessary regularity due to the inherent curvature and complexity of the underlying structures. This poses significant challenges in applying classical methods, such as the comparison principle and viscosity solutions, which typically require a high degree of smoothness.

To address this issue, a common approach is to introduce a regularization technique, such as mollification, to smooth out the limiting value function. Mollification involves convolving the original function with a smooth kernel, resulting in a new function that retains the essential properties of the original while being sufficiently smooth for analytical purposes. This smoothed version of the value function can then be used in the projection argument, which is a key step in deriving the rates of convergence. The projection argument essentially compares the solutions of the discrete and continuous problems, leveraging the smoothness of the mollified function to establish bounds on the difference.

In Sections 6 and 7, we delve into the detailed estimates required to validate the mollification procedure. These sections focus on proving that the mollified value function converges to the original value function in an appropriate norm, and that the error introduced by the mollification is controlled and diminishes as the smoothing parameter tends to zero [3]. This rigorous analysis ensures that the smoothness of the mollified function does not compromise the accuracy of the asymptotic results, thereby providing a robust framework for studying the limiting behavior of optimal control problems in the context of (A∞, 2)-flow categories.

## 5.2 Computational and Theoretical Models

### 5.2.1 Generalized Feynman Diagrams and Schwinger Formula
Generalized Feynman diagrams extend the traditional framework of perturbative quantum field theory to encompass a broader class of scattering amplitudes, particularly in theories involving massless particles. These diagrams are constructed from arrays of trees with \( n - k + 2 \) leaves, where \( n \) represents the number of external particles and \( k \) is the dimension of the Grassmannian. Each tree corresponds to a specific configuration of interactions, and the sum over all such configurations yields the full amplitude. This generalization is crucial for understanding the combinatorial structure underlying scattering processes in theories such as \(\phi^3\) and biadjoint scalar theories.

The Schwinger formula plays a pivotal role in the computation of these generalized amplitudes by providing a method to integrate over the entire space of the tropical Grassmannian \( \text{Trop} G(k, n) \) [4]. Specifically, the amplitude \( m_{n,k} \) can be expressed as a Laplace transform over this space, effectively encoding the contributions from all possible generalized Feynman diagrams. The positive tropical Grassmannian \( \text{Trop}^+ G(k, n) \) is a key component in this process, as it captures the combinatorial data necessary to compute the amplitudes [11]. This approach not only simplifies the calculation but also reveals deep connections between algebraic geometry and quantum field theory.

Moreover, the correspondence between generalized Feynman diagrams and the maximal cones of the positive tropical Grassmannian highlights the geometric nature of these amplitudes [11]. Each maximal cone corresponds to a unique configuration of interactions, and the integration over these cones via the Schwinger formula ensures that all possible physical scenarios are accounted for. This geometric perspective has led to significant advances in the understanding of scattering amplitudes, including the development of new computational techniques and the discovery of hidden symmetries in quantum field theories. The interplay between combinatorial structures and physical observables continues to drive research in this area, offering new insights into the fundamental principles governing particle interactions.

### 5.2.2 Factorization of Amplitudes under Split Kinematics
In the context of quantum field theory (QFT) and string theory, the factorization of scattering amplitudes under split kinematics is a critical phenomenon that reveals deep structural properties of these theories [4]. Split kinematics refers to a specific choice of kinematic variables that simplifies the structure of scattering amplitudes, allowing them to be expressed as products of lower-point amplitudes. This factorization is not only mathematically elegant but also physically insightful, as it highlights the underlying symmetries and singularities of the theory.

The factorization of amplitudes under split kinematics is closely tied to the concept of hidden zeros, which are points in the kinematic space where the amplitude vanishes despite the absence of apparent poles [4]. These hidden zeros play a crucial role in the factorization process, as they dictate the specific form of the lower-point amplitudes that emerge. The split kinematics subspace \( K_{\text{split}} \) is defined such that the kinematic invariants \( s_I \) satisfy certain linear relations, leading to a decomposition of the amplitude into simpler components. This decomposition is particularly useful for understanding the analytic structure of amplitudes and for developing efficient computational methods.

To further explore the implications of this factorization, we delve into the geometric and algebraic structures that underpin the split kinematics. For instance, the tropical Grassmannian \( \text{Trop} G_{2,n} \) and its positive part \( \text{Trop}^+ G_{2,n} \) provide a combinatorial framework for understanding the factorization of bi-adjoint scalar amplitudes in \( \phi^3 \) theory. The maximal cones of \( \text{Trop}^+ G_{2,n} \) correspond to different ways of decomposing the amplitude, and each cone represents a distinct factorization channel. This connection not only enriches our understanding of the mathematical structure of amplitudes but also opens avenues for new computational techniques in theoretical physics.

### 5.2.3 Tropical Grassmannian and Matroid Amplitudes
The tropical Grassmannian \( \text{Trop}^+G(k, n) \) plays a pivotal role in the study of matroid amplitudes, offering a combinatorial framework that connects algebraic geometry with scattering amplitudes in physics [4]. This structure, introduced by Speyer and Williams, is defined as the tropicalization of the positive Grassmannian \( G(k, n) \), which is the space of \( k \)-dimensional subspaces of an \( n \)-dimensional vector space. The tropical Grassmannian is determined by the 3-term tropical Plücker relations, and it encodes the combinatorial data of positively oriented matroids [11]. Each maximal cone in \( \text{Trop}^+G(k, n) \) corresponds to a specific configuration of these matroids, providing a geometric interpretation of the underlying combinatorial structures.

The chirotropical Grassmannian \( \text{Trop}^\chi G(k, n) \) further refines this framework by associating a polyhedral fan to each realizable, uniform chirotope (or oriented matroid) \( \chi \) of rank \( k \) [11]. These fans encode combinatorially the Grothendieck residues that arise in certain formulas, leading to the computation of generalized biadjoint scalar amplitudes. These amplitudes are directly derived from higher tropical Grassmannians, and each maximal cone in \( \text{Trop}^+G(k, n) \) appears in exactly \( 2n-3 \) chirotropical Grassmannians, reflecting the symmetries of cubic trees [11]. This decomposition allows for a partial breakdown of the biadjoint scalar amplitudes into partial amplitudes corresponding to relabelings of \( \text{Trop}^+G(k, n) \), thereby simplifying the computation and interpretation of these amplitudes.

In the context of scattering amplitudes, the positive tropical Grassmannian \( \text{Trop}^+G(k, n) \) provides a natural setting for the study of planar amplitudes, particularly in the case \( k = 2 \), where it corresponds to the space of planar trees on \( n \) leaves [4]. The canonical ordering \( I = (123 \cdots n) \) induces a notion of planarity that aligns with the structure of the positive tropical Grassmannian, and the amplitude \( m_{nk}(I, I) \) can be computed using the global Schwinger formula as a Laplace transform over the entire space \( \text{Trop} G(k, n) \) [4]. This connection between tropical geometry and scattering amplitudes not only enriches our understanding of the mathematical underpinnings of these amplitudes but also provides a powerful tool for their computation and analysis.

## 5.3 Complex Manifolds and Index Theory

### 5.3.1 Heat Kernel Techniques and Local Index Theory
Heat Kernel Techniques and Local Index Theory have emerged as powerful tools in the study of geometric and topological properties of manifolds. The heat kernel, a fundamental solution to the heat equation, plays a central role in this context by encoding information about the geometry of the underlying manifold. Specifically, the heat kernel can be used to study the spectral properties of the Laplace-Beltrami operator, which in turn provides insights into the topology and geometry of the manifold. The asymptotic expansion of the heat kernel, particularly near the diagonal, is of particular interest as it reveals deep connections to the local index theory.

Local index theory, a branch of differential geometry, focuses on the study of index theorems for elliptic differential operators on manifolds. The Atiyah-Singer index theorem, a cornerstone of this field, relates the analytical index of an elliptic operator to its topological index, which is expressed in terms of characteristic classes. Heat kernel techniques provide a concrete method to compute these indices by analyzing the short-time behavior of the heat kernel. This approach not only simplifies the proof of the index theorem but also extends its applicability to a broader class of operators and manifolds. The heat kernel's asymptotic expansion, which includes terms involving the curvature and other geometric invariants, is crucial in this context.

Moreover, the interplay between heat kernel techniques and local index theory has led to significant advancements in understanding the geometric and topological invariants of manifolds. For instance, the heat kernel can be used to derive explicit formulas for the Chern character and other characteristic classes, which are essential in the study of vector bundles and their associated differential operators. These techniques have also been instrumental in the development of non-commutative geometry, where they are used to study the index theory of operators on non-commutative spaces. The rich interplay between these techniques and the broader field of geometry and topology continues to be an active area of research, with numerous applications in both pure and applied mathematics.

### 5.3.2 Analytical and Geometric Properties of Open Complex Manifolds
The study of open complex manifolds encompasses a rich interplay between analytical and geometric properties, providing a foundational framework for understanding the behavior of complex structures in non-compact settings. These manifolds, characterized by their lack of boundary and often infinite topological complexity, pose unique challenges and opportunities for analysis. Key among these is the role of holomorphic functions and mappings, which serve as fundamental tools for probing the intrinsic geometry and topology of such spaces. Holomorphic functions on open complex manifolds not only satisfy the Cauchy-Riemann equations but also exhibit properties that are closely tied to the global structure of the manifold, such as the existence of non-constant bounded holomorphic functions, which can indicate the presence of certain types of compactifications or the absence thereof.

From a geometric perspective, the study of curvature plays a central role in the analysis of open complex manifolds. The Ricci curvature, in particular, provides a measure of the deviation from flatness and influences the behavior of geodesics and the volume growth of balls within the manifold. Negative Ricci curvature, for instance, can lead to hyperbolicity, a property that has profound implications for the dynamics of holomorphic mappings and the structure of the fundamental group. Moreover, the interplay between curvature and topology is evident in theorems that relate the sign of the curvature to the finiteness or infiniteness of the fundamental group, thus linking local geometric properties to global topological characteristics.

Recent developments in the field have also highlighted the importance of symplectic and Kähler structures in the study of open complex manifolds. These structures provide a bridge between complex and symplectic geometry, allowing for the application of powerful techniques from both areas. For example, the existence of a Kähler metric on an open complex manifold can significantly constrain its topology and geometry, leading to results on the classification of such manifolds. Additionally, the study of Lagrangian submanifolds and their intersections, as well as the Fukaya category, offers new insights into the symplectic aspects of open complex manifolds, enriching our understanding of their complex-analytic properties.

### 5.3.3 Index Theorem in Non-Compact Spaces
The Index Theorem in non-compact spaces presents a significant extension of the classical Atiyah-Singer Index Theorem, which is primarily formulated for compact manifolds. In non-compact settings, the lack of compactness introduces challenges such as the non-finiteness of the index and the need to handle unbounded operators. To address these issues, various approaches have been developed, including the use of compactifications and the study of Fredholm properties of operators in non-compact contexts. One prominent method involves the introduction of weighted Sobolev spaces, which allow for the control of growth and decay of functions at infinity, thereby ensuring the Fredholm property of the operators under consideration.

In the context of non-compact spaces, the index theorem often requires additional assumptions or modifications to the classical framework. For instance, the concept of "essential self-adjointness" plays a crucial role in ensuring that the operators are well-defined and have a well-behaved spectrum. Moreover, the use of pseudodifferential operators and their symbolic calculus is essential for analyzing the local and global properties of the operators involved. This approach not only helps in establishing the Fredholm property but also in computing the index explicitly. The study of the index theorem in non-compact spaces has found applications in various areas of mathematics, including geometric analysis, mathematical physics, and the theory of partial differential equations.

The extension of the index theorem to non-compact spaces also involves the development of new techniques for handling the asymptotic behavior of solutions. For example, the use of scattering theory and the study of asymptotic expansions have provided valuable insights into the behavior of solutions at infinity. These techniques are particularly useful in the context of manifolds with cylindrical or conical ends, where the geometry at infinity plays a crucial role in determining the index. Additionally, the study of the index theorem in non-compact spaces has led to the development of new invariants, such as the relative index, which captures the difference in the indices of operators on different non-compact spaces. These invariants have applications in the study of moduli spaces and the classification of non-compact manifolds.

# 6 Future Directions


The field of measures of association, particularly in the context of advanced computational methods and their applications in genomics, machine learning, and complex systems, has seen significant progress. However, several limitations and gaps remain. One of the primary limitations is the computational complexity associated with handling high-dimensional data, which can be prohibitive for real-time applications and large-scale datasets. Additionally, the integration of diverse data types and the interpretation of results in a biologically or physically meaningful context remain challenging. Another gap is the need for more robust and interpretable models, especially in scenarios where the likelihood function is intractable or the data is noisy. Furthermore, the current methods often lack the ability to handle dynamic and evolving systems, where the relationships between variables change over time.

To address these limitations, several directions for future research are proposed. First, the development of more efficient and scalable algorithms is essential. This can be achieved through the optimization of existing methods, such as SVD and RNNs, and the exploration of novel techniques, such as quantum computing and advanced parallel processing. The integration of these methods with cloud computing and edge computing can further enhance their applicability in real-time and resource-constrained environments. Second, the creation of hybrid models that combine the strengths of different approaches, such as the integration of deep learning with traditional statistical methods, can lead to more robust and interpretable models. For instance, combining INNs with Bayesian inference can provide a more comprehensive understanding of uncertainty in predictions. Third, the development of methods that can handle dynamic and evolving systems is crucial. This can be achieved through the use of time-series analysis, online learning, and adaptive models that can update their parameters in real-time. Additionally, the exploration of causal inference methods can help in understanding the underlying mechanisms driving the observed associations.

The proposed future work has the potential to significantly impact various fields. In genomics, more efficient and interpretable models can lead to faster and more accurate identification of genetic variants associated with diseases, thereby improving personalized medicine and drug development. In machine learning, the development of scalable and robust models can enhance the performance of real-time systems, such as autonomous vehicles and industrial control systems. In complex systems analysis, the ability to handle dynamic and evolving systems can provide deeper insights into the behavior of natural and engineered systems, leading to better prediction and control. Overall, the proposed future work aims to bridge the gap between theoretical advancements and practical applications, ultimately contributing to the advancement of science and technology.

# 7 Conclusion



This survey paper has provided a comprehensive overview of recent advancements in measures of association, with a focus on the integration of advanced computational methods in genomics, machine learning, and complex systems analysis. Key findings include the application of singular value decomposition (SVD) in reduced-order modeling (ROM) for efficient parametric state estimation, the development of the SHallow REcurrent Decoder (SHRED) architecture for mapping sensor data to full system states, and the use of invertible neural networks (INNs) for amortized inference in scenarios with intractable likelihood functions. The paper also explored the integration of large language models (LLMs) into system-theoretic process analysis (STPA) for generating and validating unsafe control actions (UCAs) and loss scenarios. Additionally, the survey covered the application of event stream processing and mobile sensing, emphasizing the representation and processing of event-based data, hardware and software acceleration techniques, and end-to-end learning for task optimization. Theoretical and experimental approaches in quantum physics and materials science, including the Schwinger-Dyson and Bethe-Salpeter equations, lattice simulations, and the Pinch Technique (PT) combined with the Background Field Method (BFM), were also discussed. The dynamics of phase-space holes in plasmas, quantum phenomena in materials, and mathematical and theoretical frameworks in complex systems were examined, highlighting the interplay between advanced computational techniques and fundamental physical principles.

The significance of this survey lies in its synthesis of cutting-edge research across multiple disciplines, offering a holistic view of the current landscape of measures of association. By bridging the gap between theoretical advancements and practical applications, this survey serves as a valuable resource for researchers, practitioners, and students. It not only highlights the latest methodologies and their real-world implications but also identifies key challenges and future research directions. The integration of advanced computational methods, such as SVD, INNs, and LLMs, into various scientific domains underscores the importance of interdisciplinary collaboration in driving innovation and solving complex problems.

In conclusion, the rapid evolution of measures of association and the increasing complexity of data present both opportunities and challenges. We call for continued research and development in this field, with a focus on developing more robust and efficient computational techniques, enhancing the interpretability of models, and exploring the integration of multi-modal data sources. The insights gained from this survey can inform the design of more sophisticated algorithms and frameworks, ultimately leading to better decision-making and innovation in a wide range of scientific and technological applications. We encourage researchers to build upon the findings presented here and to explore new frontiers in the analysis of complex data, contributing to the ongoing advancement of this dynamic and multidisciplinary field.

# References
[1] Inference for Log-Gaussian Cox Point Processes using Bayesian Deep  Learning  Application to Human O  
[2] Towards Mobile Sensing with Event Cameras on High-agility  Resource-constrained Devices  A Survey  
[3] Particle Approximation for Conditional Control with Soft Killing  
[4] Splitting CEGM Amplitudes  
[5] Towards Efficient Parametric State Estimation in Circulating Fuel  Reactors with Shallow Recurrent D  
[6] Computationally Efficient Whole-Genome Signal Region Detection for  Quantitative and Binary Traits  
[7] An LLM-Integrated Framework for Completion, Management, and Tracing of  STPA  
[8] Gluon mass scale through the Schwinger mechanism  
[9] Revisiting the properties of superfluid and normal liquid ${}^4$He using  ab initio potentials  
[10] Vlasov-Poisson simulation study of phase-space hole coalescence in a  cylindrically wave-guided plas  
[11] The Chirotropical Grassmannian  