# A Survey of ChatGPT Research in Accounting and Finance

# 1 Abstract


The advent of Large Language Models (LLMs) has revolutionized artificial intelligence, particularly in their applications across domains such as accounting and finance. This survey paper provides a comprehensive overview of the current state of research on the application of ChatGPT and other LLMs in these fields, delving into empirical evaluations, methodologies, and application domains. The paper identifies key trends, challenges, and opportunities, highlighting the methodological frameworks used to assess the performance and reliability of LLMs. It also explores the interdisciplinary impacts of LLMs, covering areas such as software development, educational settings, and legal and social implications. By synthesizing the existing literature, this survey aims to offer a foundational resource for future research and practical applications, fostering a deeper understanding of the role of LLMs in accounting and finance.

# 2 Introduction
The advent of Large Language Models (LLMs) has revolutionized the landscape of artificial intelligence, particularly in their applications across various domains such as accounting and finance [1]. These models, exemplified by ChatGPT, have demonstrated remarkable capabilities in generating human-like text, performing complex tasks, and providing valuable insights [2]. The integration of LLMs into professional fields like accounting and finance holds significant promise, offering the potential to automate routine tasks, enhance decision-making processes, and improve overall operational efficiency. However, this integration also introduces a range of challenges, including issues of data privacy, model interpretability, and ethical considerations. As the use of LLMs continues to grow, it is imperative to critically evaluate their performance, applications, and implications in these domains.

This survey paper aims to provide a comprehensive overview of the current state of research on the application of ChatGPT and other LLMs in accounting and finance [3]. The paper delves into the empirical evaluation of these models, examining the methodologies and frameworks used to assess their performance and reliability. It also explores the various application domains where LLMs have been deployed, including model-based systems engineering, vision-language models in food nutrition, and writing tasks. Additionally, the paper discusses the interdisciplinary impacts of LLMs, covering areas such as software development workflows, educational settings, and legal and social implications. By synthesizing the existing literature, this survey aims to identify key trends, challenges, and opportunities in the field, providing a foundation for future research and practical applications.

The empirical evaluation of LLMs is a critical component of this survey, focusing on the frameworks and methodologies used to assess their performance. This section explores unlearning frameworks, which are essential for addressing data privacy and model adaptability. These frameworks enable the removal of specific data points or knowledge from a trained model, ensuring that sensitive information can be deleted or updated as needed. The evaluation of unlearning frameworks is crucial, as it helps in maintaining the integrity and performance of LLMs while ensuring compliance with data protection regulations. Another key aspect is systematic querying and configuration, which involves optimizing the performance and reliability of LLMs through the interaction between retrieval and generation modules. This section also examines stereotype content analysis, a methodological approach that assesses the presence and implications of stereotypes within the outputs of LLMs, particularly in the context of food labeling and nutritional databases.

The application domains of LLMs in accounting and finance are explored in detail, highlighting their diverse uses and impacts [3]. In model-based systems engineering, LLMs are leveraged to manage the complexity and dynamic nature of systems, facilitating better communication and decision-making. Vision-language models have emerged as powerful tools in food nutrition, integrating visual and textual data to provide comprehensive analyses of food-related information [4]. These models enhance the accuracy of food recognition and nutrient estimation, contributing to personalized dietary recommendations and improved public health outcomes. In writing tasks, LLMs have transformed content creation, documentation, and technical writing by generating coherent and contextually relevant text. While these applications offer significant benefits, they also present challenges related to bias, interpretability, and ethical considerations.

The interdisciplinary analysis of AI impacts covers a broad range of topics, including software development workflows, educational settings, and legal and social implications. In software development, AI tools have enhanced productivity, error reduction, and collaboration, but also raise ethical concerns related to data privacy and algorithmic bias. In educational settings, the use of AI in integration and orientation services for newcomers highlights the need for trustworthy and reliable systems that can provide accurate and culturally sensitive information. The legal and social implications of AI-generated content, including issues of attribution, digital border governance, and political preferences, are also examined [5]. These sections underscore the importance of developing robust legal frameworks and ethical guidelines to ensure that AI applications are used responsibly and equitably [5].

The contributions of this survey paper are manifold. First, it provides a comprehensive and structured overview of the current state of research on the application of ChatGPT and other LLMs in accounting and finance [3]. By synthesizing the existing literature, the paper identifies key trends, challenges, and opportunities in the field, offering a valuable resource for researchers, practitioners, and policymakers. Second, the paper highlights the methodological frameworks and empirical evaluations used to assess the performance and reliability of LLMs, providing insights into best practices and areas for improvement. Finally, the paper emphasizes the interdisciplinary impacts of LLMs, discussing their applications and implications across various domains, and advocating for a balanced approach that leverages the strengths of both open and closed models. By addressing these aspects, the survey aims to foster a deeper understanding of the role of LLMs in accounting and finance and to guide future research and practical applications in this rapidly evolving field.

# 3 Empirical Evaluation of LLMs

## 3.1 Frameworks and Methodologies

### 3.1.1 Unlearning Frameworks for LLMs
Unlearning frameworks for Large Language Models (LLMs) aim to address the critical issue of data privacy and model adaptability by enabling the removal of specific data points or knowledge from a trained model. These frameworks are essential in scenarios where sensitive information must be deleted or when a model needs to be updated to reflect new policies or regulations. The primary challenge in unlearning is to ensure that the removal of specific data does not degrade the overall performance of the model, while also maintaining the integrity of the remaining knowledge. Various approaches have been proposed, ranging from retraining the entire model from scratch to more efficient methods that selectively update the model parameters.

One notable approach is the use of Retrieval-Augmented Generation (RAG) technology, which combines retrieval-based and generative models to create a hybrid system that can dynamically incorporate and remove information. RAG-based unlearning frameworks leverage external knowledge sources to augment the model's capabilities, allowing for more targeted and efficient unlearning [6]. This method has shown promise in achieving high-quality unlearning of both specific samples and broader concepts, without significant loss in performance [6]. Additionally, RAG-based systems can adapt to new data and contexts more flexibly, making them suitable for dynamic environments where data requirements frequently change.

Another key aspect of unlearning frameworks is their ability to handle diverse scenarios and data types. State-of-the-art unlearning schemes have been evaluated across various metrics, including the effectiveness of unlearning, computational efficiency, and the impact on model performance [6]. These evaluations have highlighted the strengths and weaknesses of different approaches, providing valuable insights for future research. For instance, some methods excel in removing specific data points but may struggle with broader concept unlearning, while others offer a balanced trade-off between unlearning effectiveness and computational cost. Overall, the development of robust unlearning frameworks remains an active area of research, driven by the increasing demand for more flexible and privacy-aware LLMs.

### 3.1.2 Systematic Querying and Configuration
Systematic querying and configuration play a pivotal role in optimizing the performance and reliability of large language models (LLMs) in various applications. In this section, we delve into the methodologies and frameworks designed to systematically evaluate and configure LLMs, particularly focusing on the interaction between the retrieval and generation modules [6]. The retrieval module is crucial for extracting relevant information from a knowledge base, which is then used by the LLM to generate more accurate and contextually appropriate responses [6]. This dual-module approach not only enhances the quality of outputs but also mitigates common issues such as misinformation and hallucinations.

To explore the effectiveness of systematic querying and configuration, we conducted an extensive study using a corpus of 548 methods, each implemented to query GPT-4, the most recent version of the GPT series at the time of this research. Each query was submitted multiple times with varying configurations, resulting in a total of 27,400 requests. The configurations included different parameters such as temperature, top-k sampling, and context length, which are known to influence the model's output. The results of this study revealed that the choice of configuration parameters significantly impacts the performance of LLMs, with certain settings leading to more coherent and contextually relevant responses.

Furthermore, the integration of retrieval-augmented generation (RAG) techniques has emerged as a promising approach to enhance the capabilities of LLMs. RAG leverages an information retrieval module to fetch relevant data from a knowledge base, which is then used by the LLM to generate responses. This method not only improves the accuracy and reliability of the generated text but also allows for more dynamic and context-aware interactions. The systematic evaluation of RAG configurations has shown that it can effectively reduce the likelihood of generating incorrect or misleading information, making it a valuable tool for applications where precision and reliability are critical.

### 3.1.3 Stereotype Content Analysis
Stereotype Content Analysis (SCA) is a methodological approach that systematically examines the presence, nature, and implications of stereotypes within textual, visual, or multimedia content. This analysis is crucial in understanding how stereotypes are constructed, perpetuated, and potentially deconstructed through various forms of media. In the context of food labeling and nutritional databases, SCA can provide insights into how cultural and regional stereotypes influence the annotation of food items. For instance, certain ingredients or preparation methods may be associated with specific cultural stereotypes, which can affect the consistency and accuracy of annotations across different regions or cultural contexts.

The FoodNExTDB, with its diverse set of annotations from seven different nutrition experts, offers a rich dataset for conducting SCA. By analyzing the labels provided by these experts, researchers can identify patterns and variations that may be attributed to cultural or regional biases. For example, a food item labeled as "healthy" in one region might be labeled differently in another due to varying cultural perceptions of health and nutrition. SCA can help uncover these biases and provide a more nuanced understanding of how cultural stereotypes impact the interpretation and annotation of food data. This, in turn, can inform the development of more culturally sensitive and accurate food labeling systems.

Moreover, SCA can also be applied to the outputs of language models and other AI systems used in food labeling and nutritional analysis. These models, while powerful, may inadvertently perpetuate stereotypes due to the data they are trained on or the way they process and generate information. By conducting SCA on the outputs of these models, researchers can identify and mitigate biases, ensuring that AI-generated labels are both accurate and culturally appropriate. This is particularly important in the context of open-access language models like Chat GPT, which may lack the depth of knowledge required to handle the complexities of cultural and regional variations in food labeling.

## 3.2 Application Domains

### 3.2.1 Model-Based Systems Engineering
Model-Based Systems Engineering (MBSE) represents a significant shift in the traditional practices of systems engineering, emphasizing the use of models to support the development, analysis, and management of complex systems. MBSE leverages formalized modeling languages and tools to create a comprehensive and consistent representation of a system throughout its lifecycle. This approach facilitates better communication among stakeholders, enhances traceability, and improves the ability to manage complexity and change. By integrating models into the engineering process, MBSE aims to reduce ambiguity, ensure alignment with stakeholder needs, and enable more efficient and effective decision-making.

In the context of Systems-of-Systems (SoS), MBSE plays a crucial role in managing the inherent complexity and dynamic nature of these systems. SoS often consist of multiple independent systems that operate collaboratively to achieve a common goal, but each system may have its own lifecycle, governance, and operational constraints. MBSE provides a structured framework for defining the interfaces, interactions, and dependencies between these systems, ensuring that they can be integrated and managed effectively. Moreover, MBSE supports the simulation and analysis of SoS behavior under various scenarios, allowing engineers to predict and optimize system performance before deployment.

The integration of artificial intelligence (AI) techniques into MBSE further enhances its capabilities, particularly in the areas of problem formulation and solution synthesis. AI can assist in automating the creation and refinement of system models, reducing the time and effort required for manual modeling. Machine learning algorithms can analyze large datasets to identify patterns and insights that inform the design and optimization of systems. Additionally, AI-driven simulations can explore a vast space of possible configurations and scenarios, helping to identify optimal solutions and potential risks. This synergy between MBSE and AI is essential for addressing the increasing complexity and dynamism of modern systems, enabling more robust and adaptive engineering practices.

### 3.2.2 Vision-Language Models in Food Nutrition
Vision-Language Models (VLMs) have emerged as a powerful tool in the domain of food nutrition, offering a more holistic approach to understanding and analyzing food-related information [4]. These models integrate visual and textual data to provide a comprehensive analysis, which is particularly useful in tasks such as food recognition, nutrient estimation, and dietary recommendation. By leveraging the strengths of both computer vision and natural language processing, VLMs can interpret complex food images and extract meaningful textual information, such as ingredient lists and cooking methods, to generate detailed nutritional profiles. This integration not only enhances the accuracy of food recognition but also provides a deeper understanding of the nutritional content and potential health impacts of different foods.

Despite their potential, VLMs in food nutrition face several challenges that need to be addressed to improve their reliability and interpretability [4]. One of the primary challenges is the variability in food images, which can significantly affect the model's performance. Factors such as lighting, angle, and background can introduce noise and make it difficult for the model to accurately recognize and classify food items. Additionally, the interpretability of VLMs remains a critical issue, as the complex interactions between visual and textual data can make it difficult to understand how the model arrives at its predictions. This lack of transparency can be particularly problematic in sensitive applications like dietary recommendations, where the reliability of the model's output is crucial.

Another key challenge is the need for large, high-quality datasets that are specifically annotated for food nutrition tasks. While existing datasets like FoodNExTDB provide a valuable resource, they often lack the depth and breadth required to train VLMs effectively. The creation of such datasets is a labor-intensive process that requires the expertise of nutritionists and dietitians to ensure accurate and consistent labeling. Furthermore, the dynamic nature of food and dietary trends means that these datasets must be continually updated to remain relevant. Despite these challenges, the potential benefits of VLMs in food nutrition, such as personalized dietary advice and improved public health outcomes, make them a promising area of research and development [4].

### 3.2.3 Adoption and Impact in Writing Tasks
The adoption of Large Language Models (LLMs) in writing tasks has been transformative, particularly in areas such as content creation, documentation, and technical writing [1]. These models have shown significant promise in generating coherent and contextually relevant text, thereby reducing the time and effort required for manual writing. For instance, in technical documentation, LLMs can automatically generate user manuals, API documentation, and help guides, ensuring consistency and accuracy. This not only accelerates the development process but also enhances the quality of the final product, making it more accessible and user-friendly.

Moreover, LLMs have been increasingly utilized in creative writing, such as generating stories, articles, and even poetry. These models can produce high-quality content that closely mimics human writing styles, making them valuable tools for content creators and journalists. The ability of LLMs to understand and generate text based on specific prompts allows writers to explore new ideas and perspectives, enhancing the creativity and diversity of their work. Additionally, LLMs can assist in editing and proofreading, identifying grammatical errors, suggesting stylistic improvements, and providing feedback on the coherence and flow of the text.

However, the integration of LLMs in writing tasks also presents several challenges and ethical considerations. One major concern is the potential for these models to generate biased or harmful content, reflecting the biases present in their training data. This necessitates careful monitoring and the implementation of robust filtering mechanisms to ensure that the generated content is safe and inclusive. Furthermore, there is a risk of over-reliance on LLMs, which could undermine the development of human writing skills. Balancing the benefits of automation with the need for human oversight remains a critical issue in the adoption of LLMs for writing tasks.

# 4 Interdisciplinary Analysis of AI Impacts

## 4.1 Software Development Workflows

### 4.1.1 Productivity and Error Reduction
In the realm of software development, AI-driven tools have significantly enhanced productivity by automating a wide array of repetitive and time-consuming tasks [2]. These tools can generate boilerplate code, manage dependencies, and perform routine testing, thereby freeing developers to concentrate on more strategic and innovative aspects of their projects. For instance, code generation tools can automatically create scaffolding for applications, reducing the initial setup time and ensuring that best practices are followed from the outset. Similarly, automated testing frameworks can run tests continuously, providing immediate feedback and reducing the need for manual testing cycles. This shift not only accelerates the development process but also improves the overall quality of the software by maintaining consistency and adherence to coding standards.

Error reduction is another critical area where AI tools have made substantial contributions. These tools can analyze code in real-time, identifying potential bugs, security vulnerabilities, and performance bottlenecks before they become problematic. Static code analysis tools, for example, can detect common programming errors such as null pointer exceptions, memory leaks, and incorrect data types, which are often overlooked during manual reviews. Moreover, AI-powered debugging assistants can suggest fixes and optimizations, guiding developers through the resolution process. This proactive approach to error detection and correction not only enhances the reliability of the software but also reduces the time and resources required for debugging and maintenance, leading to more robust and efficient applications.

Enhanced collaboration is a third key benefit of AI tools in software development, particularly in large, distributed teams [2]. These tools can facilitate communication and coordination by providing real-time insights and recommendations, ensuring that all team members are aligned and working towards common goals. For example, AI-driven project management tools can predict project timelines and resource needs based on historical data, helping teams to plan more effectively and avoid delays. Additionally, code review tools can provide automated feedback and suggestions, streamlining the review process and fostering a culture of continuous improvement. By integrating these AI tools into the development workflow, organizations can achieve higher levels of productivity, error reduction, and collaboration, ultimately leading to more successful and sustainable software projects [2].

### 4.1.2 Collaboration and Ethical Implications
Collaboration among Service Provider Organizations (SPOs) is essential for optimizing the delivery of information and orientation (I&O) services to newcomers. Centralized online platforms, such as those piloted in Calgary, have demonstrated significant potential in streamlining access to information and fostering cooperation between SPOs [7]. These platforms not only enhance the efficiency of service delivery but also improve the overall user experience by providing a single point of access to a wide range of resources. However, despite these advancements, challenges persist, particularly in terms of service awareness and accessibility. Many newcomers, especially women, remain unaware of available services, highlighting the need for more robust outreach and communication strategies.

Ethical implications arise when considering the integration of technology in I&O services. While AI-driven tools can significantly enhance the capabilities of SPOs, they also introduce concerns related to data privacy, algorithmic bias, and transparency. Ensuring that these technologies are developed and deployed ethically requires a multidisciplinary approach, involving input from technologists, ethicists, and community stakeholders. Adopting clear ethical standards and enhancing public awareness through stakeholder engagement are crucial steps in addressing these issues [5]. This collaborative approach not only helps in building trust among users but also ensures that the benefits of technological advancements are equitably distributed.

Moreover, the development of legal frameworks and ethical guidelines is essential to govern the use of AI in I&O services [5]. These frameworks should address issues such as accountability, transparency, and fairness, ensuring that AI systems are used responsibly and effectively. By bridging the gap between legal and technical domains, policymakers can create an environment that supports innovation while safeguarding the rights and interests of all stakeholders. Effective collaboration between SPOs, policymakers, and the broader community is therefore vital to ensure that I&O services remain accessible, inclusive, and responsive to the needs of newcomers.

### 4.1.3 Case Studies and User Feedback
In the realm of integration and orientation (I&O) services for newcomers, case studies and user feedback provide critical insights into the effectiveness and accessibility of these programs. The Newcomer Outcomes Survey 2020-2021 revealed significant challenges faced by clients, despite substantial funding and resources dedicated to I&O services [7]. Many respondents reported difficulties in accessing services, citing a lack of awareness, inadequate service quality, capacity constraints, and inconvenient operating hours. These findings underscore the gap between the availability of resources and their effective utilization, particularly among vulnerable groups such as women, who were disproportionately unaware of the services available to them.

To further explore these issues, several case studies have been conducted to examine the experiences of newcomers in various contexts. These studies highlight the importance of tailored and culturally sensitive services, which can significantly enhance user engagement and satisfaction. For instance, one case study in a major urban center found that the establishment of community-based outreach programs, which included multilingual support and flexible scheduling, led to a marked increase in service utilization among immigrant populations. This approach not only improved access but also fostered a sense of community and belonging, which is crucial for the successful integration of newcomers.

User feedback also revealed the potential risks and ethical considerations associated with the use of AI tools in the I&O sector. Hypothetical scenarios illustrate how the ad hoc deployment of general-purpose generative algorithms could inadvertently harm newcomers, such as by providing inaccurate or culturally insensitive information. These scenarios emphasize the need for rigorous evaluation and mitigation strategies when integrating AI technologies. Recommendations for responsible AI design in this context include the development of ethical standards, enhanced stakeholder engagement, and the creation of clear legal frameworks to ensure accountability and transparency [5]. By addressing these challenges, stakeholders can better support the integration and well-being of newcomers, ensuring that AI tools are used to augment, rather than undermine, the effectiveness of I&O services.

## 4.2 Educational Settings

### 4.2.1 Perceptions and Experiences
In the context of integration and orientation (I&O) services, perceptions and experiences of newcomers play a critical role in the effectiveness and utilization of these services. The Newcomer Outcomes Survey 2020-2021 highlighted significant barriers to accessing I&O services, with respondents frequently citing a lack of awareness and understanding of how to navigate the available resources [7]. This issue is particularly pronounced among women, who are less likely to be aware of the services and thus less likely to benefit from them. The findings suggest a gap between the substantial investment in I&O services and the actual reach and impact on the target population, indicating a need for more targeted outreach and information dissemination strategies.

Furthermore, the adequacy and capacity of I&O services were frequently questioned by survey respondents. Many reported that the services, while available, did not meet their specific needs or were limited in scope and capacity. This inadequacy can be attributed to a mismatch between the services provided and the diverse and evolving needs of newcomers, as well as constraints such as limited funding and staffing. Inconvenient service hours and location were also cited as significant barriers, suggesting that service providers need to consider the practical challenges faced by newcomers, such as work schedules and transportation availability, when designing and delivering services.

Despite these challenges, the survey also revealed that when newcomers were aware of and able to access I&O services, they generally reported positive experiences [7]. These positive outcomes highlight the potential of well-designed and well-implemented I&O services to significantly enhance the integration and orientation process. However, the discrepancies in awareness and access underscore the importance of continuous evaluation and adaptation of service models to better align with the needs and experiences of the newcomer population. This includes developing more inclusive and accessible communication strategies, expanding service capacity, and ensuring that services are flexible and responsive to the diverse needs of different demographic groups.

### 4.2.2 Trustworthiness and Reliability
Trustworthiness and reliability are paramount in the deployment of AI systems, particularly in sectors such as immigration and settlement services where the stakes are high for vulnerable populations. In the context of the Canadian settlement sector, the challenges of accessing services, as highlighted in the Newcomer Outcomes Survey 2020-2021, underscore the need for AI solutions that are both trustworthy and reliable. Trustworthiness in AI systems is multifaceted, encompassing aspects such as transparency, explainability, and accountability. For AI tools to be trusted by newcomers, they must provide clear and understandable explanations of their decisions and actions, ensuring that users can verify the logic behind the recommendations or services provided. This transparency is crucial in building confidence among users who may already be navigating unfamiliar and complex systems.

Reliability, on the other hand, is about the consistent performance of AI systems under various conditions and over time. In the settlement sector, AI tools must be robust enough to handle a diverse range of user needs and scenarios, from language barriers to varying levels of digital literacy. Ensuring that these systems can operate effectively and without frequent errors is essential for maintaining user trust and confidence. Moreover, reliability extends to the security and privacy of user data, which must be protected against unauthorized access and breaches. As the sector faces increasing operational pressures due to rising immigration targets, the reliability of AI solutions will be crucial in scaling services to meet the growing demand without compromising on quality or user safety.

To achieve both trustworthiness and reliability, it is essential to integrate user feedback and continuous improvement mechanisms into the development and deployment of AI systems. This involves not only technical validation but also regular assessments of user satisfaction and the identification of any ethical concerns. By fostering a feedback loop, developers can refine AI tools to better align with the needs and expectations of newcomers, thereby enhancing the overall user experience. Additionally, establishing clear guidelines and standards for AI deployment in the settlement sector can help ensure that these systems are designed and implemented with the highest levels of integrity and accountability.

### 4.2.3 Qualitative Analysis and Interviews
Qualitative analysis and interviews serve as crucial methodologies for understanding the nuanced experiences and perceptions of newcomers regarding their access to Integration and Orientation (I&O) services. These methods allow researchers to delve deeper into the subjective experiences of individuals, providing rich, detailed insights that quantitative data alone cannot capture. Through semi-structured interviews, researchers can explore the specific barriers that newcomers face, such as unfamiliarity with service locations, inadequate service offerings, and limited operational hours. This approach also helps identify the unique challenges faced by different demographic groups, such as women, who may be less aware of available services due to cultural or linguistic barriers.

In conducting qualitative analyses, researchers often employ thematic coding to identify common themes and patterns in the data. For instance, recurring themes might include the lack of awareness about I&O services, the complexity of navigating bureaucratic systems, and the need for more culturally sensitive support. These themes can then be used to inform the development of more targeted interventions and policy recommendations. Additionally, qualitative data can highlight the emotional and psychological impacts of these barriers, which are critical for understanding the full scope of the challenges faced by newcomers. By capturing these personal narratives, researchers can better advocate for systemic changes that address the root causes of service access issues.

Furthermore, qualitative interviews can reveal the effectiveness of existing I&O services from the perspective of those who use them. Participants' feedback can provide valuable insights into what aspects of the services are working well and which areas require improvement. For example, interviews might uncover that while some services are well-received, others are perceived as insufficient or irrelevant to the needs of newcomers. This information can guide service providers in tailoring their offerings to better meet the diverse needs of their clientele. Ultimately, qualitative analysis and interviews are indispensable tools for creating a more inclusive and supportive environment for newcomers, ensuring that they have the resources and support necessary to successfully integrate into their new communities.

## 4.3 Legal and Social Implications

### 4.3.1 Attribution Mechanisms and Fair Use
Attribution mechanisms in AI-generated content are critical for ensuring transparency, accountability, and fairness [5]. These mechanisms involve identifying the sources and contributors of the data used by AI models, as well as the entities responsible for the generation and dissemination of the content. The complexity arises from the multifaceted nature of AI systems, which often draw from diverse and extensive datasets, making it difficult to trace the origins of specific pieces of content. This challenge is exacerbated by the black-box nature of many AI algorithms, which can obscure the decision-making processes that lead to the final output. As a result, establishing clear and reliable attribution is essential for addressing issues such as copyright infringement, plagiarism, and the misattribution of creative works.

To support fair use and accountability, a robust legal framework is necessary to complement technical attribution mechanisms [5]. This framework should delineate the rights and responsibilities of content creators, AI developers, and users. It must also provide guidelines for the ethical use of AI-generated content, particularly in contexts where the content may be used without proper attribution or with minimal oversight. For instance, in the realm of settlement services for immigrants and refugees, the lack of clear attribution can lead to the dissemination of inaccurate or misleading information, which can have severe consequences for vulnerable populations. A well-defined legal framework would help mitigate these risks by setting standards for transparency and accuracy in AI-generated content.

Technical methods for attribution include watermarking, digital signatures, and blockchain-based solutions, each with its own strengths and limitations [5]. Watermarking involves embedding imperceptible identifiers within the content, which can be detected to verify its source. Digital signatures provide cryptographic proof of the content's origin and integrity, ensuring that any alterations are detectable. Blockchain technology offers a decentralized and tamper-resistant ledger for recording the provenance of AI-generated content, enhancing trust and accountability. However, these methods must be rigorously tested and benchmarked to ensure their effectiveness in real-world applications. By integrating these technical solutions with a supportive legal framework, we can foster an environment where AI-generated content is both innovative and ethically sound.

### 4.3.2 Digital Border Governance and Newcomer Services
The integration of digital technologies, particularly generative AI, into digital border governance and newcomer services represents a significant shift in how migration and settlement processes are managed [7]. As noted by the Office of the United Nations High Commissioner for Human Rights, the international migration sector is increasingly leveraging AI to enhance service delivery and streamline administrative functions. In Canada, the rapid growth in immigration targets has exacerbated operational challenges within the settlement sector, leading to a critical need for more efficient and scalable solutions. Generative AI, such as ChatGPT, offers a promising avenue to address these issues by providing real-time, personalized information and support to newcomers, thereby reducing the burden on overstrained human resources [2].

However, the adoption of AI in newcomer services is not without its challenges. The Canadian settlement sector must navigate complex regulatory and ethical landscapes to ensure that AI applications are both effective and responsible. Temporary protection mechanisms, such as provisional patents, can play a crucial role in fostering innovation while protecting intellectual property. Concurrently, empirical research is essential to understand how local regulations align with national legal frameworks, enabling the development of adaptable and responsive policies. Clear regulatory guidelines for AI development, coupled with the adoption of ethical standards, are necessary to build trust and ensure that AI systems are used in a manner that respects the rights and dignity of newcomers [5].

To fully realize the potential of AI in digital border governance and newcomer services, stakeholders must engage in collaborative efforts to promote innovation and address emerging challenges. Enhancing public awareness through stakeholder engagement is vital to ensure that the benefits of AI are widely understood and accepted [5]. By fostering a collaborative environment, policymakers, service providers, and technology developers can work together to create AI solutions that not only improve the efficiency of newcomer services but also enhance the overall integration experience. This holistic approach will be crucial in ensuring that the digital transformation of border governance and newcomer services is both sustainable and equitable.

### 4.3.3 Political Preferences and Attitudes
In the realm of political preferences and attitudes, machine learning models have increasingly been employed to analyze and predict voter behavior, policy preferences, and public sentiment. These models leverage large datasets, often derived from social media, surveys, and historical voting records, to identify patterns and correlations that can inform political strategies and policy-making. However, the diversity of political beliefs and the complexity of individual attitudes pose significant challenges. The models must navigate a spectrum of ideologies, ranging from conservative to liberal, and account for factors such as socioeconomic status, education, and cultural background, which can profoundly influence political preferences.

Despite the potential of these models to provide valuable insights, there is a critical need to address issues of bias and representation. Training data often reflects existing societal biases, which can lead to skewed predictions and reinforce inequalities. For instance, underrepresented groups, such as women, resettled refugees, and protected minorities, may not be adequately captured in the data, leading to models that fail to accurately represent their political preferences and attitudes. This can result in policies and strategies that inadvertently marginalize these groups, further entrenching existing disparities.

To mitigate these challenges, it is essential to adopt a multi-faceted approach that includes the development of more inclusive datasets, the implementation of ethical standards, and enhanced public awareness through stakeholder engagement. Collaborative efforts between policymakers, technologists, and community leaders can help ensure that the models used in political analysis are transparent, fair, and reflective of the diverse population they aim to serve. By fostering a more inclusive and participatory process, these models can better support democratic processes and contribute to more equitable outcomes in political decision-making.

# 5 Transparency and Ethical Considerations in AI

## 5.1 Mitigating Negative Societal Effects

### 5.1.1 Data Curation and Model Interpretability
Data curation plays a pivotal role in enhancing the performance and reliability of large deep learning models, particularly in addressing issues of bias and trustworthiness. The process involves the systematic selection, organization, and management of data to ensure that the training datasets are representative, balanced, and free from inherent biases. High-quality, curated datasets are essential for mitigating the risk of model-induced biases, which can arise from skewed or incomplete data sources. By carefully curating data, developers can ensure that the models are trained on diverse and inclusive datasets, thereby reducing the likelihood of perpetuating or amplifying existing social inequalities.

Model interpretability is another critical aspect that complements data curation in improving the trustworthiness and usability of deep learning models. Interpretability refers to the ability to understand and explain the decision-making processes of a model, making it easier for users and stakeholders to trust and validate its outputs. Techniques such as feature importance analysis, attention mechanisms, and local interpretable model-agnostic explanations (LIME) have been developed to enhance the transparency of complex models. These methods help in identifying which input features are most influential in the model's predictions, thereby providing insights into how the model makes decisions. This transparency is crucial for applications where the consequences of model errors can be significant, such as in healthcare, finance, and legal domains.

Integrating data curation with model interpretability can lead to more robust and trustworthy AI systems. For instance, by using high-quality, curated datasets, models can be trained to produce more accurate and fair outcomes. Subsequently, interpretability techniques can be applied to these models to provide clear and understandable explanations of their decisions. This combination not only helps in building user trust but also facilitates the identification and correction of potential biases and errors. Furthermore, it supports the development of more accountable and ethically aligned AI systems, which are essential for their widespread adoption and integration into various sectors of society.

### 5.1.2 Adversarial Testing and Bias Reduction
Adversarial testing has emerged as a critical technique in the evaluation and improvement of deep learning models, particularly in the context of large language models (LLMs). This method involves the deliberate introduction of adversarial examples—inputs designed to cause the model to make errors—to assess and enhance the robustness of these models. By identifying and addressing these vulnerabilities, adversarial testing not only improves the model's performance but also helps in uncovering hidden biases that may have been encoded during the training process. These biases can arise from various sources, including imbalanced training datasets, skewed data distributions, and even the inherent biases of the model's architecture.

To effectively reduce bias, adversarial testing must be complemented by a suite of techniques that address the root causes of these issues. One such approach is data curation, which involves carefully selecting and balancing the training data to ensure that it represents a diverse and inclusive range of perspectives and demographics. This can help mitigate the risk of the model learning and perpetuating biases that are present in the training data. Additionally, model interpretability techniques, such as explainable AI (XAI) methods, can provide insights into how the model makes decisions, allowing developers to identify and correct biased behavior. These methods can highlight the specific features and data points that the model relies on, enabling a more granular analysis of its decision-making process.

Furthermore, the integration of fairness metrics and constraints into the training process can help ensure that the model's outputs are equitable across different groups. This can involve the use of fairness-aware algorithms that explicitly account for and mitigate bias during training. Adversarial testing, in this context, serves as a continuous evaluation mechanism to monitor and validate the effectiveness of these fairness measures. By combining these approaches, researchers and practitioners can work towards developing LLMs that are not only more robust and accurate but also fair and trustworthy, thereby promoting the responsible and ethical deployment of AI technologies [8].

### 5.1.3 Promoting Fair and Ethical AI
Promoting fair and ethical AI is a critical component of the broader discourse on the responsible development and deployment of AI systems [5]. As AI models, particularly large language models (LLMs), become more sophisticated and ubiquitous, the potential for these systems to perpetuate or even exacerbate existing biases and inequalities increases [8]. Ensuring that AI systems are fair and ethical requires a multifaceted approach that addresses both technical and social dimensions. This includes the development of robust methodologies for identifying and mitigating biases in training data and model outputs, as well as the implementation of transparent and accountable practices throughout the AI lifecycle.

One key aspect of promoting fair and ethical AI is the need for greater transparency and reproducibility in AI development [8]. True open-source AI, which involves not only the sharing of model weights but also the underlying training data and development processes, is essential for fostering trust and enabling external scrutiny. This transparency can help identify and rectify issues such as data biases, algorithmic flaws, and unintended consequences. Additionally, establishing clear standards and guidelines for AI development can set realistic expectations and promote responsible innovation [5]. These standards should encompass not only technical best practices but also ethical considerations, such as the protection of privacy and the prevention of misuse.

Another critical area of focus is the development of tools and techniques to detect and mitigate the unethical use of AI. For example, in the educational domain, the rise of generative AI tools has raised concerns about academic integrity. Developing robust models to detect AI-generated content can help educators ensure that student work is genuinely human-authored, thereby promoting fairness and maintaining the integrity of the educational process [9]. Similarly, in other domains, such as social media and content creation, the ability to identify and counteract deepfakes and other forms of AI-generated misinformation is crucial. Techniques such as explainable AI (XAI) can provide transparency and explainability, helping to build trust and ensure that AI systems are used ethically and responsibly.

## 5.2 Detection and Classification Techniques

### 5.2.1 ML and DL for AI Content Classification
Machine Learning (ML) and Deep Learning (DL) have emerged as pivotal tools in the realm of AI content classification, particularly in distinguishing between human-generated and AI-generated content [9]. Traditional ML algorithms, such as XGBoost and Random Forest, have demonstrated robust performance in classifying AI-generated content, achieving high accuracy and low misclassification rates [9]. These models excel in identifying subtle patterns and features that differentiate human and AI-generated content, making them valuable for applications requiring high precision. However, their performance can vary depending on the size and complexity of the content being classified, with smaller content pieces (e.g., paragraphs) presenting more challenges compared to larger documents (e.g., articles).

Deep Learning (DL) techniques, especially those involving neural networks, have further advanced the state-of-the-art in AI content classification. Generative Adversarial Networks (GANs) and diffusion models have been instrumental in creating highly realistic synthetic content, but they also pose significant challenges in terms of detection and classification. DL models, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), are adept at capturing complex temporal and spatial features in text and multimedia content. These models can be fine-tuned to detect specific characteristics of AI-generated content, such as language patterns, image artifacts, and video anomalies. The integration of Explainable AI (XAI) techniques with DL models enhances transparency and interpretability, allowing users to understand the reasoning behind classification decisions.

Despite the advancements in ML and DL for AI content classification, several challenges remain. The rapid evolution of AI content generation techniques necessitates continuous updates and retraining of classification models to maintain their effectiveness. Adversarial attacks and the creation of increasingly sophisticated deepfakes pose ongoing threats that require robust defense mechanisms. Techniques such as adversarial training and ensemble methods can improve model resilience, but they also introduce additional computational costs. Moreover, the ethical implications of AI content classification, including privacy concerns and potential misuse, highlight the need for interdisciplinary collaboration to develop fair, ethical, and trustworthy AI systems.

### 5.2.2 Audiovisual Deepfake Detection
Audiovisual deepfake detection represents a critical challenge in the realm of multimedia security and integrity, particularly as deep learning models continue to advance in their ability to generate highly realistic forgeries. Unlike unimodal forgery detection methods, which are typically specialized in either video or audio analysis, audiovisual deepfake detection requires a holistic approach capable of integrating and analyzing both modalities simultaneously. This integration is essential because deepfakes often exploit the interplay between visual and auditory cues to enhance their deceptive capabilities. Consequently, effective detection systems must be able to identify inconsistencies or anomalies that might not be apparent when the modalities are analyzed in isolation.

Recent advancements in deep learning have led to the development of multimodal neural networks that can process and correlate information from both audio and video streams. These networks often employ convolutional neural networks (CNNs) for visual feature extraction and recurrent neural networks (RNNs) or long short-term memory (LSTM) units for temporal and auditory feature analysis. By combining these features, the models can detect subtle discrepancies in lip movements, facial expressions, and speech patterns that are indicative of deepfake manipulation. Additionally, attention mechanisms and transformers have been integrated into these architectures to improve the model's ability to focus on relevant features and maintain coherence across different time frames and modalities.

Despite these advancements, several challenges remain in the field of audiovisual deepfake detection. One significant issue is the scarcity of large, well-labeled datasets that encompass a diverse range of deepfake types and qualities. This lack of data hinders the training and generalization capabilities of deep learning models, making them less robust against novel or high-quality forgeries. Furthermore, the rapid evolution of deepfake generation techniques necessitates continuous updates and improvements to detection algorithms. To address these challenges, researchers are exploring the use of synthetic data generation, transfer learning, and ensemble methods to enhance the adaptability and performance of detection systems. Additionally, the integration of explainable AI (XAI) techniques is crucial for providing transparency and interpretability, enabling users to understand the decision-making processes of these complex models [9].

### 5.2.3 Explainable AI in Vulnerability Detection
Explainable AI (XAI) has emerged as a critical component in enhancing the transparency and trustworthiness of AI-driven systems, particularly in the domain of vulnerability detection. In this context, vulnerability detection is a complex task that requires deep understanding of code structures and behaviors to identify potential security weaknesses [10]. Traditional approaches often rely on static and dynamic analysis techniques, which can be limited by their inability to provide clear and interpretable insights into why certain code segments are flagged as vulnerable. XAI methods, such as SHAP (SHapley Additive exPlanations), offer a promising solution by generating local explanations for individual predictions, thereby enabling security analysts to better understand the reasoning behind the detection of vulnerabilities.

By leveraging SHAP values, researchers and practitioners can uncover underlying patterns and features that contribute to the identification of vulnerabilities in software [10]. This not only enhances the interpretability of the models but also aids in the development of more robust and reliable vulnerability detection systems. For instance, SHAP values can highlight specific code constructs or programming practices that are most indicative of vulnerabilities, allowing developers to focus their efforts on high-risk areas. Moreover, the ability to provide clear explanations for AI-driven decisions can foster greater trust among stakeholders, including developers, security teams, and end-users, by making the decision-making process more transparent and understandable.

In summary, the integration of XAI techniques into vulnerability detection systems represents a significant step forward in the field of software security. By providing actionable insights and enhancing the interpretability of AI models, XAI can significantly improve the effectiveness of vulnerability detection, ultimately leading to more secure software systems. Future research should focus on developing and refining XAI methods that are specifically tailored to the unique challenges of vulnerability detection, such as the need for high precision and the ability to handle large and complex codebases.

## 5.3 Openness and Reproducibility

### 5.3.1 Transparency in Model Availability
Transparency in model availability is a critical aspect of ensuring the ethical and responsible deployment of large language models (LLMs). This section examines the distinctions between open-weight and fully open-source models, focusing on how transparency practices differ and impact the broader research and application landscape [11]. Open-weight models, such as Meta's LLama series, offer downloadable model weights under non-proprietary licenses, allowing researchers and developers to fine-tune and deploy these models in specialized environments [11]. This approach contrasts with proprietary models like OpenAI's GPT series, which are primarily accessible through API-based interactions, limiting direct access to model internals and weights [11].

The availability of model weights in open-weight LLMs facilitates a higher degree of transparency and accountability. Researchers can inspect and analyze the model's architecture, training data, and performance metrics, which is crucial for identifying and mitigating issues such as bias and hallucination. This level of access also enables the development of specialized applications and the fine-tuning of models for specific tasks, enhancing their utility and adaptability. For instance, the ability to fine-tune models on domain-specific data can lead to more accurate and contextually relevant outputs, which is particularly important in fields such as healthcare, finance, and education.

However, the transparency provided by open-weight models is not without challenges. While the availability of model weights promotes transparency, it also raises concerns about the potential misuse of these models, such as in the creation of deepfakes or other malicious applications. Additionally, the complexity of large models can make it difficult for non-expert users to fully understand and interpret the model's behavior. Therefore, alongside the provision of model weights, there is a need for comprehensive documentation, interpretability tools, and community guidelines to ensure that the benefits of transparency are realized while mitigating potential risks. This balanced approach is essential for fostering trust and responsible innovation in the development and deployment of LLMs.

### 5.3.2 Community Engagement and Ethical Dimensions
In the realm of AI development, community engagement and ethical considerations have emerged as critical factors that influence the trajectory of technological advancement. The distinction between truly open-source models and those labeled as "open-weight" is particularly pertinent, as it directly impacts the level of transparency and community involvement [11]. Truly open-source models provide complete access to the source code, training data, and model parameters, fostering a collaborative environment where developers can contribute to and learn from each other [11]. In contrast, "open-weight" models, while offering access to pre-trained model weights, often lack the comprehensive transparency of open-source models, thereby limiting the extent of community engagement and scrutiny [11]. This partial transparency can hinder reproducibility and the ability to identify and address biases, trustworthiness, and interpretability issues, which are crucial for ethical AI development.

The implications of partial transparency on community engagement are multifaceted. Limited access to model details can stifle innovation and collaboration, as developers are unable to fully understand the inner workings of the models they are working with. This can lead to a fragmented community, where only a select few have the necessary insights to contribute meaningfully. Moreover, the lack of transparency can erode trust among users and stakeholders, as the ethical implications of AI systems become opaque. Ethical AI development requires a robust community that can collectively identify and mitigate potential risks, such as bias and unfairness. By fostering a more inclusive and transparent environment, the AI community can better address these challenges and promote the development of responsible AI systems.

Promoting responsible AI development through enhanced community engagement and ethical considerations is essential for the long-term sustainability of AI technologies [5]. Ethical dimensions, such as fairness, accountability, and transparency, must be integrated into the development process from the outset. This involves not only technical solutions, such as data curation and model interpretability, but also broader societal engagement to ensure that AI systems align with human values and societal norms. By prioritizing these ethical dimensions, the AI community can build systems that are not only technically advanced but also socially responsible, ultimately leading to more equitable and beneficial outcomes for all.

### 5.3.3 Trends in Open vs Closed Models
The distinction between open and closed large language models (LLMs) has become a focal point in recent years, reflecting broader trends in the machine learning community [1]. Closed models, characterized by proprietary ownership and limited access to their architecture and training data, have historically dominated the landscape due to their superior performance and the substantial resources required for their development. However, the emergence of open models, which offer transparency in terms of code, weights, and training methodologies, has sparked a significant shift. This trend is exemplified by the rapid adoption of open-source models like Meta’s Llama, which have seen an exponential increase in usage and research interest, contrasting sharply with the linear growth of closed models such as GPT-3 [11].

The surge in popularity of open LLMs can be attributed to several factors, including the democratization of AI research, the fostering of collaborative innovation, and the reduction of barriers to entry for developers and researchers. Open models enable a wider range of stakeholders to contribute to and benefit from advancements in natural language processing (NLP), thereby accelerating the pace of innovation. Moreover, the transparency provided by open models facilitates a deeper understanding of their inner workings, which is crucial for addressing issues such as bias, trustworthiness, and interpretability. These concerns, which have long been roadblocks in the development of large deep learning (DL) models, are more effectively tackled in an open environment where diverse perspectives and methodologies can be applied [10].

Despite the advantages of open models, the debate over open versus closed approaches remains complex. Closed models continue to hold a significant edge in terms of performance and security, as they are often developed and maintained by well-funded organizations with access to vast amounts of data and computational resources. However, the growing emphasis on ethical AI and the need for models that are transparent and accountable is shifting the balance. As the limitations of sheer model scaling become apparent, the community is increasingly recognizing the importance of a more balanced approach that leverages the strengths of both open and closed models. This hybrid approach may represent a promising path forward, combining the performance and security of closed models with the transparency and collaborative potential of open models.

# 6 Future Directions


The current landscape of research on the application of Large Language Models (LLMs) in accounting and finance, while promising, is not without its limitations and gaps. One of the primary challenges is the lack of robust unlearning frameworks that can efficiently and effectively remove specific data points or knowledge from trained models without degrading their overall performance. This is particularly critical in domains where data privacy and compliance with regulations are paramount. Additionally, the interpretability of LLMs remains a significant issue, as these models often operate as black boxes, making it difficult to understand and trust their outputs, especially in high-stakes applications such as financial decision-making. Another gap is the need for more comprehensive and diverse datasets, particularly in the context of food nutrition and vision-language models, to ensure that the models are trained on a wide range of scenarios and are less prone to biases and errors. Lastly, the ethical implications of AI-generated content, including issues of bias, accountability, and transparency, require further exploration and the development of clear legal and ethical frameworks.

To address these limitations, several directions for future research are proposed. Firstly, the development of more advanced unlearning frameworks is essential. Research should focus on creating methods that can selectively and efficiently remove specific data points or knowledge from LLMs while maintaining or even improving their performance. This could involve the integration of more sophisticated retrieval-augmented generation techniques and the exploration of hybrid models that combine the strengths of different unlearning approaches. Secondly, enhancing the interpretability of LLMs is crucial. Future work should investigate the use of explainable AI (XAI) techniques, such as attention mechanisms and local interpretable model-agnostic explanations (LIME), to provide clearer insights into how these models make decisions. This will be particularly important in domains like finance, where transparency and accountability are paramount. Thirdly, the creation of more comprehensive and diverse datasets is necessary to improve the reliability and fairness of LLMs. This could involve the development of large-scale, annotated datasets that cover a wide range of scenarios and are representative of diverse populations. Additionally, there is a need for more interdisciplinary research that combines insights from fields such as psychology, sociology, and ethics to better understand the social and ethical implications of AI-generated content. This could lead to the development of more robust legal and ethical frameworks that ensure the responsible and equitable use of AI in various applications.

The potential impact of the proposed future work is significant. By developing more advanced unlearning frameworks, we can enhance the privacy and adaptability of LLMs, making them more suitable for use in regulated industries such as finance and healthcare. Improving the interpretability of these models will increase trust and acceptance among users, particularly in high-stakes applications where the consequences of errors can be severe. Creating more comprehensive and diverse datasets will help reduce biases and improve the accuracy and fairness of LLMs, leading to better outcomes in areas such as food nutrition and personalized dietary recommendations. Finally, the development of clear legal and ethical frameworks will ensure that AI-generated content is used responsibly and ethically, fostering a more inclusive and trustworthy AI ecosystem. These advancements will not only drive innovation in the field but also contribute to the broader goal of creating AI systems that are beneficial and equitable for all.

# 7 Conclusion



This survey paper has provided a comprehensive overview of the current state of research on the application of ChatGPT and other large language models (LLMs) in accounting and finance. The paper has explored the empirical evaluation of these models, focusing on unlearning frameworks, systematic querying and configuration, and stereotype content analysis. It has also delved into the diverse application domains of LLMs, including model-based systems engineering, vision-language models in food nutrition, and writing tasks. The interdisciplinary impacts of LLMs have been discussed in detail, covering areas such as software development workflows, educational settings, and legal and social implications. By synthesizing the existing literature, the paper has identified key trends, challenges, and opportunities in the field, offering a foundation for future research and practical applications.

The significance of this survey lies in its comprehensive and structured approach to understanding the multifaceted impacts of LLMs in accounting and finance. By providing a detailed analysis of the methodologies and frameworks used to assess the performance and reliability of these models, the paper offers valuable insights into best practices and areas for improvement. The interdisciplinary analysis further underscores the broad and far-reaching implications of LLMs, highlighting the need for a balanced approach that leverages the strengths of both open and closed models. This survey serves as a valuable resource for researchers, practitioners, and policymakers, facilitating a deeper understanding of the role of LLMs in these critical domains and guiding the responsible and effective integration of AI technologies.

In conclusion, the rapid advancement of LLMs presents both significant opportunities and formidable challenges in the fields of accounting and finance. As these models continue to evolve, it is imperative to address issues of data privacy, model interpretability, and ethical considerations to ensure their responsible and beneficial use. Future research should focus on developing robust unlearning frameworks, optimizing querying and configuration methodologies, and enhancing the interpretability and fairness of LLMs. Additionally, fostering collaboration between different stakeholders, including technologists, ethicists, and policymakers, will be crucial in creating a supportive and inclusive environment for the continued development and deployment of LLMs. By embracing a holistic and interdisciplinary approach, we can harness the full potential of LLMs to drive innovation and improve operational efficiency while safeguarding the interests and rights of all stakeholders.

# References
[1] How Good is ChatGPT at Audiovisual Deepfake Detection  A Comparative  Study of ChatGPT, AI Models an  
[2] Balancing Innovation and Ethics in AI-Driven Software Development  
[3] A Scoping Review of ChatGPT Research in Accounting and Finance  
[4] Are Vision-Language Models Ready for Dietary Assessment  Exploring the  Next Frontier in AI-Powered  
[5] Who Owns the Output  Bridging Law and Technology in LLMs Attribution  
[6] When Machine Unlearning Meets Retrieval-Augmented Generation (RAG)  Keep  Secret or Forget Knowledge  
[7] Social and Ethical Risks Posed by General-Purpose LLMs for Settling  Newcomers in Canada  
[8] Stars, Stripes, and Silicon  Unravelling the ChatGPT's All-American,  Monochrome, Cis-centric Bias  
[9] Detecting AI-Generated Text in Educational Content  Leveraging Machine  Learning and Explainable AI  
[10] Toward Neurosymbolic Program Comprehension  
[11] Comprehensive Analysis of Transparency and Accessibility of ChatGPT,  DeepSeek, And other SoTA Large  