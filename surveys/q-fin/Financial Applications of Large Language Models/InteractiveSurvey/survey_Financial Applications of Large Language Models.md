# A Survey of Financial Applications of Large Language Models

# 1 Abstract


The financial sector has been at the forefront of technological innovation, driven by the need for sophisticated tools to manage and analyze complex data. This survey paper provides a comprehensive overview of the current state and future directions of large language models (LLMs) in financial applications. The paper explores various techniques and frameworks designed to enhance the performance of LLMs in the financial domain, addressing issues such as data augmentation, fine-tuning, and domain adaptation. Key findings include the development of bilingual financial language models, the integration of temporal dependencies in financial data, and the use of zero-shot and few-shot learning approaches. The paper also examines interdisciplinary applications, such as multi-agent systems for trading and portfolio management, and the role of benchmark suites in evaluating financial LLMs. Finally, this survey contributes to the field by synthesizing the latest research findings and practical insights, providing a roadmap for future research and development in the application of LLMs to financial tasks, ultimately aiming to foster innovation and drive the adoption of LLMs in the financial sector.

# 2 Introduction
The financial sector has long been at the forefront of technological innovation, driven by the need for sophisticated tools to manage and analyze vast amounts of complex data. In recent years, the advent of large language models (LLMs) has opened new avenues for enhancing financial applications, from risk assessment and portfolio management to regulatory compliance and market analysis [1]. LLMs, with their ability to process and understand natural language, have the potential to transform the way financial institutions operate, making them more efficient, accurate, and responsive to market dynamics [2]. However, the integration of LLMs into financial systems also presents unique challenges, including the need for specialized data, domain-specific knowledge, and robust evaluation frameworks [1].

This survey paper focuses on the financial applications of large language models, providing a comprehensive overview of the current state of research and practice. We explore various techniques and frameworks designed to enhance the performance of LLMs in the financial domain, addressing issues such as data augmentation, fine-tuning, and domain adaptation [3]. The paper also delves into the development of bilingual financial language models, which are essential for handling multilingual financial data in a globalized market [4]. Additionally, we examine the role of temporal dependencies in financial data and the construction of high-quality training datasets, which are crucial for building reliable and accurate financial models [5]. The survey further discusses zero-shot and few-shot learning approaches, which are particularly valuable in scenarios where labeled data is limited or rapidly changing.

A significant portion of the paper is dedicated to the interdisciplinary applications of financial LLMs, including the use of multi-agent systems for trading and portfolio management, the integration of multimodal data in cryptocurrency analysis, and the implementation of verbal reinforcement mechanisms for risk control [6]. We also present a detailed examination of benchmark suites and evaluation frameworks, which are essential for assessing the performance and capabilities of financial LLMs [3]. These benchmarks, such as the SuperCLUE-Fin and KFinEval-Pilot, provide a standardized and transparent evaluation process, ensuring that LLMs can be effectively compared and refined.

The paper further reviews the state-of-the-art knowledge in financial LLMs, comparing their performance with human analysts in tasks such as earnings forecasting and risk assessment [7]. We highlight the importance of interdisciplinary collaboration in developing predictive models that combine the strengths of computer science and financial expertise. This collaboration is crucial for creating robust and interpretable models that can be trusted by financial practitioners and regulators.

Finally, this survey paper contributes to the field by synthesizing the latest research findings and practical insights, providing a roadmap for future research and development in the application of LLMs to financial tasks [1]. The contributions of this survey include a detailed overview of the current landscape, a critical analysis of existing techniques and frameworks, and a set of recommendations for advancing the field. By addressing the challenges and opportunities in this rapidly evolving domain, we aim to foster innovation and drive the adoption of LLMs in the financial sector, ultimately leading to more informed and effective decision-making processes [3].

# 3 Enhancement Techniques for Financial LLMs

## 3.1 Data Augmentation and Fine-Tuning Strategies

### 3.1.1 Retrieval-Augmented Instruction Data Generation
Retrieval-Augmented Instruction Data Generation (RAIDG) is a sophisticated approach designed to enhance the performance of large language models (LLMs) in domain-specific tasks, particularly in the financial sector [8]. RAIDG leverages external knowledge sources to augment the training data, ensuring that the generated instructions are contextually rich and relevant. This method involves the integration of a retrieval system that fetches domain-specific documents, such as financial reports, market analyses, and regulatory filings, which are then used to create context-aware instructions for the LLM. By doing so, RAIDG not only enriches the training data with up-to-date and accurate information but also helps mitigate the risk of generating irrelevant or incorrect outputs.

The core mechanism of RAIDG involves a two-step process: retrieval and instruction generation. In the retrieval phase, a specialized search engine or database query system is employed to identify and extract relevant documents or passages that pertain to the financial task at hand. These documents are typically selected based on their relevance to the specific financial context, such as the type of financial instrument, market conditions, or regulatory environment. Once the relevant documents are retrieved, the instruction generation phase begins. Here, the LLM is prompted to generate instructions or questions that are grounded in the retrieved context. This ensures that the generated instructions are not only coherent and meaningful but also aligned with the financial domain's nuances and requirements.

To further enhance the effectiveness of RAIDG, the generated instructions are often refined through a feedback loop involving human annotators or domain experts. This iterative process helps to refine the quality of the generated instructions, ensuring they are both accurate and useful for the intended financial tasks. Additionally, RAIDG can be integrated with other techniques, such as chain-of-thought (CoT) reasoning and reinforcement learning (RL), to further improve the LLM's performance. By combining these approaches, RAIDG not only enhances the LLM's ability to generate contextually appropriate instructions but also improves its overall robustness and reliability in financial applications.

### 3.1.2 Model-Agnostic Frameworks for Text-to-SQL
Model-agnostic frameworks for Text-to-SQL aim to leverage the strengths of Large Language Models (LLMs) while minimizing dependency on proprietary APIs and ensuring cost-effectiveness [9]. One prominent example is FinSQL, a framework designed to develop Text-to-SQL models using any open-source LLM [9]. FinSQL addresses the challenges of supporting large input context lengths and strong context understanding, which are crucial for accurately translating natural language queries into SQL commands. The framework comprises three key components: prompt construction, parameter-efficient fine-tuning, and output calibration. Prompt construction involves crafting effective prompts that guide the LLM to generate accurate SQL queries, while parameter-efficient fine-tuning ensures that the model can be adapted to specific financial datasets with minimal computational resources. Output calibration refines the generated SQL queries to improve their syntactic correctness and execution efficiency.

FinSQL's model-agnostic nature allows it to be applied to a wide range of LLMs, making it a versatile solution for financial institutions and developers. This flexibility is particularly important in the financial sector, where data privacy and regulatory compliance are paramount. By avoiding reliance on expensive proprietary APIs, FinSQL reduces operational costs and enhances the accessibility of Text-to-SQL capabilities [9]. The framework's ability to handle large input contexts and maintain strong context understanding is achieved through advanced prompt engineering techniques and fine-tuning strategies that leverage domain-specific financial data. These features are essential for generating accurate and reliable SQL queries, which are critical for financial analysis and decision-making processes.

To further enhance the performance of Text-to-SQL models, FinSQL incorporates retrieval-augmented generation (RAG) methods, which provide the LLM with relevant financial context during the query generation process. This approach ensures that the generated SQL queries are grounded in the specific financial domain, reducing the likelihood of errors and improving the relevance of the results. Additionally, FinSQL supports a hybrid training approach that combines top-k selected instruction data with the original dataset, thereby improving model robustness and generalization. This method not only mitigates common issues such as hallucinations and knowledge forgetting but also enhances the model's ability to handle diverse and complex financial queries, making it a powerful tool for financial data analysis and management.

### 3.1.3 Bilingual Financial Language Models
Bilingual Financial Language Models (BFLMs) represent a significant advancement in the application of Large Language Models (LLMs) to the financial sector, addressing the unique challenges posed by multilingual financial data. These models are designed to handle and process financial information in multiple languages, thereby broadening their applicability and utility in a globalized financial market. The development of BFLMs involves not only the technical challenges of training models on bilingual data but also the need to ensure that the models can accurately capture the nuances and complexities of financial terminology and contexts in both languages. This section explores the current state of BFLMs, focusing on their architecture, training methodologies, and specific applications in financial analysis and decision-making.

One of the key contributions in this area is the creation of the FinMA-ES model, which is the first LLM optimized for processing and understanding bilingual financial data in both Spanish and English [4]. FinMA-ES is derived from the LLaMA2 7B model and is fine-tuned using a novel bilingual and multi-task instruction tuning dataset called FIT-ES. This dataset encompasses 21 datasets covering 9 tasks, ensuring that the model is well-equipped to handle a wide range of financial NLP tasks. The training regimen for FinMA-ES includes a two-stage process: continued pre-training on a large corpus of bilingual financial texts, followed by supervised fine-tuning on task-specific datasets [4]. This approach not only enhances the model's zero-shot and few-shot capabilities but also ensures that it can generalize well to unseen financial data and tasks.

The evaluation of BFLMs, such as FinMA-ES, is crucial for validating their effectiveness in real-world financial applications. To this end, the FLAREES benchmark has been developed, which includes validation and test sets from FIT-ES, as well as six additional unseen datasets and two unseen tasks. This benchmark is designed to rigorously assess the model's performance across various financial domains, including sentiment analysis, named entity recognition, and earnings report analysis. The results from these evaluations highlight the potential of BFLMs to significantly improve the accuracy and reliability of financial analytics, particularly in scenarios where data is available in multiple languages. Furthermore, the open-source nature of FinMA-ES and the FLAREES benchmark promotes transparency and fosters further research and innovation in the field of bilingual financial NLP [4].

## 3.2 Data Pruning and Distillation Methods

### 3.2.1 Temporal Dependencies in Financial Data
Temporal dependencies in financial data are critical for understanding and predicting market behaviors, as financial markets are inherently dynamic and influenced by a multitude of time-varying factors. These dependencies encompass a wide range of temporal scales, from high-frequency trading data, which captures millisecond-level price movements, to long-term trends observed over years in economic indicators. The accurate modeling of these dependencies is essential for tasks such as stock price prediction, risk management, and portfolio optimization. Traditional methods, such as autoregressive models and moving averages, have been widely used but often struggle to capture the complex and non-linear relationships present in financial time series.

Recent advances in deep learning, particularly with the use of recurrent neural networks (RNNs) and their variants like long short-term memory (LSTM) networks and gated recurrent units (GRUs), have significantly improved the ability to model temporal dependencies in financial data. These models are capable of retaining long-term memory and handling non-linear relationships, making them well-suited for financial time series analysis. However, the high dimensionality and noise in financial data pose challenges for these models, necessitating careful data preprocessing and model tuning. Techniques such as attention mechanisms and transformer models have further enhanced the ability to focus on relevant temporal features and improve predictive accuracy.

Despite these advancements, the dynamic nature of financial markets and the presence of regime shifts, where market conditions change abruptly, remain significant challenges [10]. Models must be adaptable to these shifts and capable of incorporating new information in real-time. This has led to the development of hybrid models that combine deep learning with traditional econometric techniques, as well as the use of reinforcement learning to dynamically adjust model parameters based on market conditions. The integration of temporal dependencies in financial data with these advanced techniques is crucial for building robust and reliable financial models that can navigate the complexities of modern financial markets.

### 3.2.2 High-Quality Training Data Construction
High-quality training data construction is a critical component in the development of specialized large language models (LLMs) for financial misinformation detection (FMD). The quality and relevance of the training data significantly influence the model's ability to accurately classify financial claims and generate justifications. In this section, we discuss the methodologies and techniques employed to construct high-quality training datasets for FMD, focusing on data collection, data augmentation, and data pruning.

Data collection for FMD involves gathering a diverse and representative set of financial claims and their corresponding justifications. This process typically starts with identifying reliable sources of financial data, such as financial news articles, company reports, and regulatory filings. To ensure the dataset is comprehensive and covers a wide range of financial contexts, multiple sources are often combined. Additionally, the data is annotated by domain experts to provide accurate labels and justifications for each claim. This step is crucial for supervised fine-tuning (SFT) and ensures that the model learns to recognize and justify the classification of financial claims effectively.

To further enhance the quality and diversity of the training data, data augmentation techniques are employed. These techniques include chain-of-thought (CoT) prompting, where the model is provided with a sequence of logical steps to guide its reasoning, and reinforcement learning (RL), which helps in generating more robust and contextually relevant data. CoT prompting, in particular, is effective in improving the model's ability to generate coherent and detailed explanations for its classifications. Data pruning is another important technique used to refine the training dataset. By utilizing an agent model to score and select the top-K training samples, the dataset is optimized to reduce hallucinations and improve model robustness. This hybrid training approach, which combines top-K selected instruct data with the original dataset, ensures that the model remains grounded in the financial domain while maintaining high performance across various financial tasks.

### 3.2.3 Low-Rank Adaptation and Quantization
Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that leverages the low-rank structure of the linear layers in transformer models to reduce redundancy and improve computational efficiency [11]. By decomposing the weight matrices of these layers into a low-rank approximation, LoRA significantly reduces the number of trainable parameters, thereby minimizing the GPU memory footprint and training time [11]. This approach is particularly beneficial for large language models (LLMs) where the sheer size of the model can be a significant barrier to fine-tuning and deployment [11].

Quantized LoRA (QLoRA) extends the concept of LoRA by converting the precision of the model parameters from half-precision (16-bit) to lower precision formats such as INT8 (8-bit) or INT4 (4-bit). This further reduces the memory consumption and computational requirements, making it feasible to fine-tune and deploy LLMs on resource-constrained devices. The combination of low-rank adaptation and quantization not only enhances the efficiency of the models but also ensures that they can be effectively used in real-world applications, including financial analysis and decision-making, where resource optimization is crucial.

In our work, we employ both LoRA and QLoRA to fine-tune open-source LLMs for financial misinformation detection (FMD). By integrating these techniques, we achieve a significant reduction in the number of trainable parameters and a substantial improvement in the model's performance across various financial tasks. The reduced memory footprint and faster inference times enable us to deploy these models in real-time applications, such as automated financial news analysis and risk assessment, without compromising on accuracy or reliability. This approach not only optimizes the computational resources but also enhances the scalability and practicality of LLMs in the financial domain [1].

## 3.3 Zero-Shot and Few-Shot Learning Approaches

### 3.3.1 Zero-Shot Prompting for Sentiment Analysis
Zero-shot prompting in the context of sentiment analysis leverages the inherent capabilities of large language models (LLMs) to perform tasks without explicit training on labeled data. This approach is particularly valuable in the financial domain, where the rapid evolution of market conditions and the emergence of new financial instruments can outpace the availability of annotated datasets. By formulating appropriate prompts, LLMs can infer the sentiment of financial texts, such as news articles, social media posts, and analyst reports, with a degree of accuracy that rivals or even surpasses traditional supervised models [10].

In zero-shot prompting, the effectiveness of the prompt design is crucial. Prompts must be carefully crafted to guide the LLM towards the desired output while minimizing ambiguity. For sentiment analysis, this often involves providing clear instructions and examples that illustrate the range of possible sentiments (positive, negative, neutral) and the context in which they might appear. For instance, a well-designed prompt might include a brief description of the task, followed by a few annotated examples that highlight the nuances of financial language, such as the use of industry-specific jargon or the impact of market events on sentiment.

Despite the promising results, zero-shot prompting for sentiment analysis in the financial domain faces several challenges. Financial texts are often characterized by their complexity, including the presence of sarcasm, irony, and subtle implications that can be difficult for LLMs to interpret accurately. Additionally, the dynamic nature of financial markets means that the relevance and interpretation of certain terms can change rapidly [10]. To mitigate these challenges, researchers are exploring advanced prompt engineering techniques, such as chain-of-thought prompting and few-shot learning, which can provide additional context and improve the robustness of the model's predictions. These approaches aim to enhance the LLM's ability to generalize from limited examples and adapt to the evolving landscape of financial discourse [12].

### 3.3.2 Continued Pre-Training and Bias Mitigation
Continued pre-training of large language models (LLMs) is essential for enhancing their performance on specific tasks, particularly in specialized domains such as finance [11]. In the context of financial misinformation detection (FMD), continued pre-training involves fine-tuning pre-existing models on domain-specific datasets to improve their accuracy and relevance. Techniques such as supervised fine-tuning (SFT) and direct preference optimization (DPO) have been employed to refine the models' capabilities. SFT involves training the model on labeled data, while DPO uses human feedback to guide the optimization process, ensuring that the model aligns with human preferences and ethical standards. Additionally, reinforcement learning (RL) and chain-of-thought (CoT) prompting are utilized to synthesize data and enhance the model's reasoning abilities, making it more adept at handling complex financial claims and justifications.

Despite the advancements in LLMs, they often exhibit biases that can undermine their effectiveness and fairness. These biases can manifest in various forms, including gender, racial, and socioeconomic disparities, which can significantly impact the reliability of the model's outputs [13]. To address these issues, researchers have developed benchmark datasets like StereoSet and GenderCare to identify and mitigate stereotypes and biases [13]. Techniques such as Low-Rank Adaptation (LoRA) have been applied to fine-tune models in a parameter-efficient manner, allowing for targeted adjustments that reduce bias without compromising performance. Moreover, the integration of temporal dependencies in financial data through methods like TracSeq helps to preserve long-term knowledge and reduce catastrophic forgetting, further enhancing the model's robustness and reliability [5].

In summary, continued pre-training and bias mitigation are critical components in the development of LLMs for financial applications [1]. By leveraging advanced techniques such as SFT, DPO, RL, and CoT, along with the use of benchmark datasets and parameter-efficient fine-tuning methods, researchers can significantly improve the performance and fairness of these models. This not only enhances their utility in financial tasks but also ensures that they operate ethically and responsibly, contributing to the overall integrity and stability of the financial sector.

### 3.3.3 Sequential Fine-Tuning for Misinformation Detection
Sequential fine-tuning represents a critical advancement in the application of Large Language Models (LLMs) for financial misinformation detection [2]. This approach involves a two-step process where the initial pre-trained LLM is first fine-tuned on a general financial dataset to enhance its domain-specific understanding [10]. Subsequently, the model undergoes a second round of fine-tuning on a specialized dataset designed to detect and classify financial misinformation. This sequential strategy leverages the strengths of pre-existing financial knowledge while honing the model's ability to identify misleading claims and provide accurate justifications.

In our study, we experimented with various open-source LLMs, including Qwen2.5 and Deepseek-R1, employing techniques such as supervised fine-tuning (SFT) and direct preference optimization (DPO) [14]. SFT involved training the models on a labeled dataset of financial claims, categorized into true, false, and misleading, along with corresponding justifications. DPO, on the other hand, utilized a reward function to optimize the model's outputs based on user preferences, ensuring that the generated explanations were not only accurate but also coherent and persuasive. The results showed significant improvements in both classification accuracy and the quality of generated explanations, highlighting the effectiveness of sequential fine-tuning in enhancing the model's performance.

To further refine the capabilities of the LLMs, we incorporated reinforcement learning (RL) and chain-of-thought (CoT) prompting. RL was used to fine-tune the models on synthetic data generated through CoT, which helped simulate complex reasoning processes required for detecting nuanced forms of financial misinformation. This approach not only improved the model's ability to handle intricate financial scenarios but also reduced the risk of overfitting to the training data. Additionally, the use of CoT prompting facilitated the generation of step-by-step reasoning, making the model's decision-making process more transparent and interpretable. These enhancements collectively contribute to a robust system capable of accurately detecting and explaining financial misinformation, thereby supporting informed decision-making in the financial sector [15].

# 4 Interdisciplinary Applications of Financial LLMs

## 4.1 Multi-Agent Systems for Financial Tasks

### 4.1.1 Hierarchical Investment Roles in Trading
Hierarchical investment roles in trading are critical for the effective management of financial portfolios and the execution of trading strategies. In traditional financial institutions, these roles are often structured in a hierarchical manner, with different levels of decision-making and oversight. For instance, data analysts process and analyze raw market data, risk analysts assess and manage potential risks, and portfolio managers make final investment decisions. This hierarchical structure ensures that each step in the trading process is carefully managed and that decisions are informed by a comprehensive analysis of available data.

In the context of LLM-driven trading systems, this hierarchical structure can be mirrored to enhance the capabilities of AI models. For example, a data analyst LLM can be responsible for preprocessing and analyzing large volumes of financial data, identifying patterns, and generating insights that are then passed to a risk analyst LLM. The risk analyst LLM evaluates these insights for potential risks and formulates risk management strategies. Finally, a portfolio manager LLM integrates these inputs to make informed investment decisions [12]. This hierarchical approach not only leverages the strengths of each LLM but also ensures that the decision-making process is robust and aligned with the goals of the trading firm.

Moreover, the hierarchical structure in LLM-driven trading systems can be further enhanced by incorporating feedback loops and continuous learning mechanisms. For instance, the performance of each LLM in the hierarchy can be monitored, and the models can be fine-tuned based on real-world outcomes. This iterative process ensures that the LLMs remain accurate and relevant in a dynamic market environment. Additionally, the hierarchical structure can be adapted to different investment strategies and market conditions, allowing for flexible and responsive trading systems that can adapt to changing market dynamics and investor preferences.

### 4.1.2 Multimodal Data in Cryptocurrency Portfolio Management
In the realm of cryptocurrency portfolio management, the integration of multimodal data has emerged as a critical component for enhancing decision-making processes [6]. Cryptocurrency markets are characterized by high volatility and rapid information dissemination, making it essential to leverage diverse data sources, including textual, numerical, and visual data. Textual data, such as news articles, social media posts, and regulatory announcements, provide valuable insights into market sentiment and emerging trends. Numerical data, including historical price movements, trading volumes, and blockchain metrics, offer quantitative foundations for predictive models. Visual data, such as charts and graphs, can reveal patterns and anomalies that are not immediately apparent from raw numerical data. The integration of these modalities allows for a more comprehensive understanding of market dynamics, enabling more accurate and timely investment decisions.

Recent advancements in large language models (LLMs) have significantly enhanced the capability to process and analyze multimodal data in cryptocurrency portfolio management [6]. LLMs, with their sophisticated natural language understanding and generation capabilities, can effectively parse and synthesize textual information, providing nuanced insights into market sentiment and investor behavior [10]. When combined with numerical and visual data, LLMs can generate predictive models that not only forecast price movements but also provide actionable investment recommendations. For instance, LLMs can be fine-tuned on historical market data to identify patterns and correlations that inform trading strategies. Additionally, these models can be trained to recognize and interpret visual data, such as candlestick charts and technical indicators, further enriching the decision-making process.

However, the effective integration of multimodal data in cryptocurrency portfolio management also presents several challenges [6]. One of the primary challenges is the need for robust data preprocessing and alignment techniques to ensure that data from different modalities are consistent and compatible. This involves developing methods to handle missing or inconsistent data, as well as techniques to align temporal and spatial data across modalities. Another challenge is the computational complexity associated with processing large volumes of multimodal data in real-time. Efficient data management and processing frameworks are essential to ensure that the models can operate at the speed required for high-frequency trading. Finally, the interpretability of LLMs remains a concern, as the black-box nature of these models can make it difficult to understand the rationale behind their predictions. Addressing these challenges is crucial for the successful adoption of multimodal data in cryptocurrency portfolio management.

### 4.1.3 Verbal Reinforcement Mechanisms for Risk Control
Verbal reinforcement mechanisms in the context of risk control for financial large language models (LLMs) involve the dynamic updating of investment beliefs and risk assessments based on the outcomes of decision-making processes [16]. These mechanisms are designed to enhance the adaptability and robustness of LLMs in managing financial risks by integrating feedback from both the reasoning trajectories and the profit-and-loss (PnL) trends. The process begins with the LLM generating a series of investment recommendations, which are then executed in a simulated or real market environment. The outcomes of these recommendations, whether positive or negative, are analyzed to extract key insights and conceptual perspectives.

These insights are selectively back-propagated from the portfolio manager to the relevant analyst agents, allowing for a more nuanced and informed adjustment of future investment strategies. The verbal reinforcement mechanism ensures that the LLM not only learns from its own decisions but also from the broader context of market dynamics and external factors. This approach is particularly valuable in managing market risks, as it enables the LLM to adapt its risk assessments in real-time, thereby reducing the likelihood of significant financial losses.

To further enhance risk control, a dual-level risk management component is implemented. At the intra-episode level, risk is supervised using the Conditional Value at Risk (CVaR), a quantile-based risk measure that focuses on the tail risk of the portfolio [16]. This ensures that the LLM is not only optimizing for average returns but is also vigilant about potential extreme losses. At the inter-episode level, the verbal reinforcement mechanism updates the risk assessments based on the cumulative performance over multiple episodes, allowing the LLM to refine its risk management strategies over time. This dual-level approach provides a comprehensive framework for managing financial risks, making the LLM more resilient to market volatility and more effective in long-term portfolio management.

## 4.2 Benchmark Suites and Evaluation Frameworks

### 4.2.1 Comprehensive Benchmarks for Chinese Financial Models
The development of large language models (LLMs) in the financial domain has been rapid, particularly in the Chinese market, where a growing number of models are being tailored to address specific financial tasks [12]. To evaluate the performance and capabilities of these models, comprehensive benchmarks are essential. The SuperCLUE-Fin (SC-Fin) is a notable example of such a benchmark, designed to provide a rigorous and multi-faceted evaluation of Chinese native financial LLMs [17]. SC-Fin evaluates models across various task types, including sentiment analysis, stock return prediction, corporate disclosure summarization, and risk assessment, ensuring a holistic assessment of their financial applications.

The SC-Fin benchmark is structured to align with the stringent regulatory environment and the unique characteristics of the Chinese financial market. It incorporates a diverse set of financial data, ranging from textual reports and news articles to structured financial statements and market indicators. This comprehensive dataset allows for a detailed analysis of how well LLMs can process and interpret complex financial information [3]. The benchmark also includes a variety of evaluation metrics, such as precision, recall, F1-score, and Sharpe ratio, to provide a balanced view of model performance. These metrics are crucial for understanding the models' strengths and weaknesses in different financial contexts.

To ensure the benchmark remains relevant and effective, it is continuously updated to reflect the evolving nature of the financial market and the advancements in LLM technology. The SC-Fin benchmark also emphasizes the importance of interpretability and transparency, which are critical in the financial sector due to regulatory requirements and the need for explainable AI. By providing a standardized and transparent evaluation framework, SC-Fin facilitates the comparison of different financial LLMs and supports the development of more accurate and reliable models. This benchmark is expected to play a pivotal role in guiding future research and innovation in the application of LLMs in Chinese finance [3].

### 4.2.2 Task-Oriented Evaluation for Korean Financial Industry
In the Korean financial industry, the adoption of large language models (LLMs) for task-oriented evaluation represents a significant advancement, addressing the need for sophisticated tools capable of handling the complex and dynamic nature of financial operations [12]. The KFinEval-Pilot benchmark, specifically designed for the Korean market, focuses on evaluating LLMs across three critical domains: financial knowledge, financial reasoning, and financial toxicity [18]. These domains are crucial for ensuring that LLMs can effectively support tasks such as compliance monitoring, risk assessment, and customer service, which are essential for maintaining the integrity and efficiency of financial institutions [1].

The KFinEval-Pilot benchmark is structured to reflect the unique regulatory and operational landscape of Korea, incorporating tasks that simulate real-world financial scenarios [18]. For instance, the financial knowledge domain assesses the LLM's ability to understand and apply local financial regulations, while the financial reasoning domain evaluates its capacity to analyze financial statements and market trends. The financial toxicity domain, on the other hand, tests the model's ability to detect and mitigate harmful content in financial communications, which is vital for maintaining trust and compliance. By focusing on these specific areas, the benchmark provides a comprehensive evaluation framework that goes beyond generic financial literacy and numeric reasoning, ensuring that LLMs are well-suited for the nuanced demands of the Korean financial sector [18].

To further enhance the practical utility of LLMs in the Korean financial industry, the KFinEval-Pilot benchmark also incorporates a multi-agent system approach, where specialized agents are trained to handle different aspects of financial tasks. This method leverages the strengths of individual agents to perform subtasks, such as data analysis, risk assessment, and decision-making, before integrating their outputs to form a cohesive solution. This collaborative approach not only improves the accuracy and reliability of financial predictions but also enhances the transparency and explainability of LLM-driven decisions, which are critical for regulatory compliance and stakeholder trust. Through this structured evaluation and deployment strategy, the Korean financial industry can harness the full potential of LLMs to drive innovation and operational excellence.

### 4.2.3 Ensemble Approaches for Price Movement Prediction
Ensemble approaches in price movement prediction leverage the combination of multiple models to improve the robustness and accuracy of financial forecasts. These methods integrate the strengths of individual models, such as deep learning, traditional statistical models, and large language models (LLMs), to mitigate the weaknesses of any single approach. By combining diverse models, ensemble methods can capture a broader range of market dynamics and reduce the impact of overfitting, which is a common issue in financial time series analysis. For instance, an ensemble might include a recurrent neural network (RNN) for capturing temporal dependencies, a support vector machine (SVM) for handling non-linear relationships, and an LLM for extracting insights from textual data such as news articles and social media.

One of the key advantages of ensemble approaches is their ability to adapt to changing market conditions. Financial markets are highly volatile and influenced by a multitude of factors, including macroeconomic indicators, company-specific news, and investor sentiment. By integrating models that specialize in different aspects of the market, ensembles can provide more reliable predictions even in uncertain environments. For example, during periods of economic uncertainty, an ensemble might weigh the predictions of a macroeconomic model more heavily, while in stable periods, it might rely more on technical analysis models. This flexibility allows ensemble methods to maintain consistent performance across different market regimes.

Moreover, ensemble approaches can enhance the interpretability of complex models, which is crucial in the financial domain where transparency and explainability are paramount. By breaking down the prediction process into contributions from individual models, ensembles can provide insights into which factors are driving the forecast. This is particularly valuable for regulatory compliance and risk management, where stakeholders need to understand the rationale behind investment decisions. Additionally, ensemble methods can be designed to incorporate expert knowledge through the use of weighted averaging or stacking, where the weights are determined based on historical performance or domain expertise. This hybrid approach ensures that both data-driven and human insights are leveraged effectively in the prediction process.

## 4.3 Literature Reviews and Practical Guidance

### 4.3.1 State-of-the-Art Knowledge in Financial LLMs
The state-of-the-art in financial LLMs has seen significant advancements, driven by the increasing complexity and volume of financial data [3]. Modern LLMs, particularly those based on transformer architectures, have demonstrated exceptional capabilities in handling unstructured financial text, such as earnings reports, news articles, and regulatory filings. These models leverage their self-attention mechanisms to capture long-range dependencies and contextual nuances, enabling them to perform tasks like sentiment analysis, earnings prediction, and risk assessment with high accuracy. For instance, recent studies have shown that fine-tuned LLMs can outperform traditional machine learning models and even human analysts in predicting stock prices and identifying market trends. This is largely attributed to their ability to integrate vast amounts of historical and real-time data, providing a more comprehensive view of market dynamics.

However, the application of LLMs in finance is not without challenges. One of the primary issues is the domain-specific nature of financial data, which often requires specialized knowledge and understanding of complex financial instruments and regulations. To address this, researchers have explored various strategies, including fine-tuning pre-trained LLMs on financial datasets and developing domain-specific LLMs from scratch [1]. Fine-tuning has proven effective in adapting general-purpose LLMs to financial tasks, while domain-specific models offer deeper expertise but require substantial labeled data and computational resources [2]. Additionally, the interpretability of LLMs remains a concern, as their black-box nature can make it difficult to understand the rationale behind their predictions, which is crucial for regulatory compliance and stakeholder trust.

Another area of active research is the integration of LLMs with multi-agent systems to enhance decision-making in complex financial environments [2]. Multi-agent frameworks, such as FINCON, leverage the strengths of multiple LLMs to perform specialized tasks, such as market analysis, risk assessment, and portfolio management [16]. These systems often incorporate a hierarchical structure, where different agents handle specific aspects of the financial process, and communicate to form a cohesive strategy. This approach not only improves the robustness and adaptability of financial models but also addresses the limitations of single LLMs in handling diverse and dynamic market conditions [3]. Despite these advancements, ongoing efforts are needed to optimize communication efficiency and ensure that these systems remain aligned with financial regulations and ethical standards.

### 4.3.2 Human Analyst Comparison in Earnings Forecasting
In the realm of earnings forecasting, the integration of large language models (LLMs) represents a significant advancement over traditional methods, particularly in terms of processing and interpreting complex financial disclosures [10]. Earnings press releases, which are rich in both narrative and quantitative data, serve as a critical source of information for financial analysts [7]. However, the cognitive biases, information overload, and potential conflicts of interest that human analysts face can often lead to suboptimal predictions. LLMs, such as GPT-4, have the potential to mitigate these issues by leveraging their advanced natural language processing capabilities to extract and synthesize relevant information more efficiently and objectively [13].

To evaluate the performance of LLMs in earnings forecasting, this section compares the accuracy of GPT-generated forecasts with those produced by human analysts. Initial findings suggest that LLMs can indeed outperform human analysts in certain scenarios, particularly when dealing with large volumes of textual data and complex financial statements [7]. The ability of LLMs to process and analyze vast amounts of information quickly and without the cognitive biases that can affect human judgment is a key advantage. However, the quality of LLM-generated forecasts also depends on the model's ability to accurately interpret and contextualize the information, a task that remains challenging and requires further refinement.

Furthermore, the performance of LLMs in earnings forecasting is closely tied to their approach to processing both textual and quantitative information. While LLMs excel at extracting and summarizing textual data, their performance in handling quantitative data, such as financial ratios and historical earnings figures, is an area that requires additional attention [19]. The integration of specialized financial knowledge and the development of more sophisticated data processing techniques are essential to enhance the accuracy and reliability of LLM-generated forecasts. This section also explores the potential for hybrid models that combine the strengths of LLMs with the domain expertise of human analysts, aiming to achieve a balance between predictive accuracy and interpretability.

### 4.3.3 Interdisciplinary Collaboration for Predictive Models
Interdisciplinary collaboration is essential for the development of predictive models in the financial domain, as it bridges the gap between the theoretical foundations of computer science and the practical applications in finance. By integrating the expertise of computer scientists, who focus on optimizing algorithms for predictive accuracy and efficiency, with the domain knowledge of financial experts, who prioritize understanding the underlying mechanisms and causal relationships, collaborative efforts can lead to the creation of more robust and interpretable models. This synergy not only enhances the accuracy of predictions but also ensures that the models align with the real-world objectives and constraints of the financial industry.

One of the key areas where interdisciplinary collaboration has shown significant promise is in the application of large language models (LLMs) to financial analysis [10]. LLMs, with their ability to process and understand large volumes of textual data, can provide valuable insights into financial disclosures, market sentiment, and other qualitative factors that are crucial for accurate forecasting [7]. Through joint research initiatives, financial experts can guide the development and fine-tuning of these models to ensure they capture the nuances of financial language and context. This collaboration has led to the creation of models that can rival or even surpass human analysts in tasks such as earnings forecasting, thereby opening new avenues for innovation in financial decision-making.

Moreover, the integration of interdisciplinary approaches facilitates the development of hybrid models that combine the strengths of both quantitative and qualitative analysis. For instance, while LLMs excel at processing unstructured data, they can be complemented by traditional statistical and econometric models that are better suited for handling structured data and establishing causal relationships. This hybrid approach not only improves the overall predictive power of the models but also enhances their interpretability, making them more trustworthy and actionable for financial practitioners. Through such collaborative efforts, the financial industry can leverage the latest advancements in artificial intelligence to drive more informed and effective decision-making processes.

# 5 Evaluation Frameworks for Financial LLMs

## 5.1 Reasoning-Enhanced LLMs and Benchmarks

### 5.1.1 Systematic Evaluation of Complex Financial Tasks
In the systematic evaluation of complex financial tasks, the focus is on assessing the performance of advanced models and frameworks designed to handle the intricate and multifaceted nature of financial data. This section delves into the methodologies used to evaluate these systems, particularly in their ability to process and interpret financial text, tabular data, and equations. The evaluation criteria encompass several key capabilities, including numerical reasoning, tabular interpretation, financial terminology comprehension, and long-context understanding. These capabilities are crucial for accurately extracting and synthesizing information from financial documents, which often contain dense and technical content [4].

To ensure a comprehensive evaluation, we employ a variety of datasets that reflect the diversity and complexity of real-world financial scenarios. These datasets include annual reports, regulatory filings, financial news articles, and market analysis reports. Each dataset is carefully curated to test specific aspects of the models' performance, such as their ability to perform accurate numerical calculations, understand financial jargon, and interpret complex tabular data. The evaluation metrics used include precision, recall, F1-score, and mean absolute error (MAE) for numerical predictions. Additionally, we assess the models' ability to generate coherent and contextually relevant responses to user queries, which is essential for practical applications in financial analysis and decision-making [19].

Our findings indicate that while high-quality data and extensive pretraining are foundational for strong performance, they alone are insufficient to achieve optimal results in complex financial tasks. General techniques such as chain-of-thought (CoT) fine-tuning, while beneficial, provide limited gains in specific areas like long-context understanding and equation-based problem solving. To address these limitations, we explore specialized techniques tailored to the financial domain, such as domain-adaptive pretraining and task-specific retrieval methods [15]. These methods leverage the unique language and structural features of financial information, leading to significant improvements in retrieval performance and overall system accuracy [15]. The results highlight the importance of domain-specific optimizations in achieving reliable and accurate financial reasoning [20].

### 5.1.2 Domain-Specific Reasoning Paths and Fine-Tuning
Domain-specific reasoning paths and fine-tuning represent a significant advancement in enhancing the performance of Large Language Models (LLMs) in specialized fields such as finance [20]. Traditional LLMs, while powerful in general natural language processing tasks, often struggle with domain-specific nuances and the risk of generating inaccurate or irrelevant responses. To address these challenges, researchers have developed methods that integrate domain-specific reasoning paths into the fine-tuning process. These paths are essentially curated sequences of logical steps and relevant data points that guide the model towards more accurate and contextually appropriate outputs.

One notable approach involves the use of Retrieval-Augmented Generation (RAG), which combines the strengths of LLMs with the precision of external knowledge sources [15]. In this method, the model first retrieves domain-specific information from external databases or documents, ensuring that it has access to the most current and relevant data. This retrieved information is then integrated into the generation process, allowing the model to produce responses that are both informed and contextually accurate. For instance, in the financial domain, RAG can help LLMs avoid common pitfalls such as outdated market data or misinterpretation of financial jargon, thereby reducing the likelihood of errors and improving overall reliability.

Another key aspect of domain-specific reasoning paths is the curation and filtering of training data [20]. Recent advancements have shown that carefully selected and high-quality data, particularly those that include detailed reasoning cases rather than generic question-answer pairs, can significantly enhance the model's performance. This targeted training strategy ensures that the model learns to reason through complex scenarios and make informed decisions based on domain-specific knowledge. For example, the Fino1 suite of models, which are specifically designed for financial applications, demonstrates superior performance compared to general-domain models and even outperforms larger, closed-source models. These results highlight the importance of domain-specific reasoning paths and fine-tuning in achieving state-of-the-art performance in specialized domains.

### 5.1.3 Performance on Financial Text and Equation Reasoning
In the domain of financial text and equation reasoning, the performance of large language models (LLMs) is critically evaluated across several key capabilities, including numerical reasoning, tabular interpretation, financial terminology comprehension, long-context understanding, and equation-based problem solving [10]. These tasks are essential for accurate financial analysis, where the ability to interpret complex financial statements, regulatory filings, and market data is paramount. The evaluation framework leverages a diverse set of financial documents and user queries, ensuring that the models are tested under conditions that closely mimic real-world scenarios. The results indicate that while general LLMs demonstrate robust performance in basic financial text understanding, they often struggle with more complex tasks that require deep domain-specific knowledge and the ability to reason with numerical data and equations [10].

To enhance performance in financial reasoning, we explore the impact of specialized data preparation and fine-tuning techniques [20]. Our analysis reveals that high-quality, domain-specific data and tailored preprocessing methods significantly improve the models' ability to capture and utilize context from complex financial documents [10]. For instance, the use of task-specific retrieval methods that leverage state-of-the-art models to identify and extract relevant information from financial texts and tables leads to more accurate and contextually appropriate responses [15]. Additionally, the integration of a temporal weighting mechanism in the retrieval process ensures that the models are responsive to the time-sensitive nature of financial information, which is crucial for making informed investment decisions and compliance [19].

Despite these advancements, the study also highlights the limitations of general techniques such as chain-of-thought (CoT) fine-tuning, which, while effective in other domains, offer limited gains in financial reasoning tasks. This suggests that further domain-specific adaptations and fine-tuning strategies are necessary to fully leverage the capabilities of LLMs in the financial sector [2]. The development of FinSearchBench-24, a benchmark comprising 1,500 multiple-choice questions covering a wide range of financial topics, provides a valuable resource for evaluating and improving the performance of LLMs in financial reasoning [20]. The benchmark not only assesses the models' ability to handle complex financial text and equations but also evaluates their temporal awareness and responsiveness to emerging financial information.

## 5.2 Specialized Search Agents and Information Retrieval

### 5.2.1 Multi-Step Search Pre-Planners
In the realm of financial information retrieval and query processing, the multi-step search pre-planner plays a crucial role in enhancing the efficiency and accuracy of search agents [19]. This component is designed to decompose complex user queries into structured sub-queries, each of which can be efficiently mapped to specific financial data sources. By leveraging graph representations, the pre-planner ensures that each sub-query is precisely formulated to target the relevant data, thereby reducing the computational overhead and improving the overall performance of the search process. The use of a graph-based approach allows for the dynamic adaptation of queries based on the structure and content of the available data sources, making it highly flexible and adaptable to the diverse and evolving nature of financial data.

The multi-step search pre-planner in FinSearch is built on a foundation of large language models (LLMs), which are fine-tuned to understand the nuances of financial language and the specific requirements of financial queries. This fine-tuning process involves extensive training on domain-specific datasets, ensuring that the LLM can accurately parse and interpret complex financial documents, tables, and other structured data [10]. The pre-planner then uses this understanding to generate sub-queries that are not only syntactically correct but also semantically rich, capturing the intent and context of the original user query. This approach significantly enhances the relevance and precision of the retrieved information, making it easier for users to find the exact data they need.

Furthermore, the multi-step search pre-planner in FinSearch incorporates a dynamic query adaptation mechanism that allows it to refine and adjust sub-queries in real-time based on the feedback from the initial search results [19]. This mechanism ensures that the search process is iterative and self-correcting, continuously improving the quality of the retrieved information. By integrating this dynamic adaptation with the structured sub-query generation, the pre-planner can handle a wide range of financial queries, from simple lookups to complex multi-faceted analyses, thereby providing a robust and versatile solution for financial information retrieval [19].

### 5.2.2 Dynamic Query Adaptation and Temporal Weighting
Dynamic query adaptation is a critical component of the proposed RAG pipeline, designed to enhance the retrieval of relevant financial information in a rapidly evolving market environment [15]. The system incorporates an LLM-based adaptive query rewriter that dynamically refines sub-queries based on intermediate search results [19]. This mechanism allows the search process to remain agile and responsive to emerging financial data, ensuring that the final information retrieved is not only accurate but also up-to-date. The adaptive query rewriter continuously evaluates the relevance of search results and adjusts the query parameters to focus on the most pertinent information, thereby improving the overall efficiency and effectiveness of the retrieval process.

In addition to dynamic query adaptation, the system introduces a temporal weighting mechanism to further enhance the relevance of the retrieved information. This mechanism assigns higher weights to information that is more recent or closely aligned with the time context deduced from user queries. Financial data is inherently time-sensitive, and the temporal weighting ensures that the system prioritizes recent market trends, news, and reports. This is particularly important in financial analysis, where the timeliness of information can significantly impact decision-making. By integrating temporal weighting, the system can provide users with more contextually relevant and actionable insights, thereby addressing a key limitation of traditional information retrieval systems.

The combination of dynamic query adaptation and temporal weighting mechanisms is particularly well-suited for the financial domain, where the relevance and timeliness of information are paramount [19]. These features enable the system to handle complex and evolving user queries more effectively, ensuring that the final results are both accurate and timely. The adaptive nature of the query rewriter, coupled with the temporal weighting, allows the system to maintain a high level of performance even in scenarios with limited training data, making it a robust solution for financial information retrieval [19].

### 5.2.3 Handling Complex Financial Queries
Handling complex financial queries requires a sophisticated approach that integrates advanced preprocessing, retrieval, and reasoning capabilities [15]. In this section, we delve into the methodologies and frameworks designed to address the unique challenges of financial data. Financial queries often involve intricate relationships, temporal dependencies, and the need for high accuracy, which traditional search engines and natural language processing (NLP) models struggle to handle effectively. To overcome these limitations, we propose a specialized framework, FinSearch, which employs a multi-step search pre-planner driven by large language models (LLMs). This pre-planner decomposes complex financial queries into structured subqueries, each targeting specific financial data sources, thereby enhancing the precision and relevance of the retrieved information [19].

FinSearch's dynamic query adaptation mechanism further refines the search process by adjusting the query structure based on the context and user intent. This mechanism is crucial in financial applications, where user queries can be ambiguous or require multiple iterations to refine. By dynamically adapting queries, the system can better capture the nuanced relationships and temporal dependencies inherent in financial data, leading to more accurate and timely information retrieval [19]. Additionally, the framework leverages state-of-the-art (SOTA) models to improve the retrieval performance, ensuring that the system can effectively handle the unique language and structural features of financial documents, such as financial statements, regulatory filings, and market reports [15].

To ensure the robustness and reliability of the system, we conduct a comprehensive evaluation of existing reasoning models, focusing on their transferability from general domains to financial tasks. Our analysis reveals that while general-purpose LLMs exhibit strong reasoning capabilities, they often lack the domain-specific knowledge required for financial applications [20]. Therefore, we introduce Fino1, a reasoning-enhanced LLM specifically tailored for financial tasks [20]. Fino1 is trained on a diverse set of financial data, including documents, tables, equations, and structured XBRL texts, to improve its performance on financial reasoning benchmarks. This specialized model, combined with the multi-step search pre-planner and dynamic query adaptation, forms a powerful framework for handling complex financial queries, thereby enhancing decision-making and compliance processes in the financial sector [19].

## 5.3 Robust Data Preparation and Domain Adaptation

### 5.3.1 Tailored Preprocessing for User Inputs
Tailored preprocessing of user inputs is a critical component in enhancing the effectiveness and accuracy of financial search systems. In the financial domain, user queries often contain complex terminologies, numerical values, and specific contextual requirements that traditional preprocessing techniques may not adequately handle. To address these challenges, we have developed a series of preprocessing methods that are specifically designed to parse and interpret user inputs in a way that aligns with the unique characteristics of financial data. These methods include tokenization strategies that recognize financial terms and symbols, normalization techniques that standardize numerical values and dates, and context-aware parsing that identifies and extracts relevant entities and relationships from user queries.

One of the key aspects of our tailored preprocessing approach is the integration of domain-specific knowledge into the preprocessing pipeline. This involves leveraging financial ontologies and taxonomies to enhance the semantic understanding of user inputs. For example, recognizing that "EPS" refers to "Earnings Per Share" and not "Encapsulated PostScript" is crucial for accurate query interpretation. Additionally, we employ advanced natural language processing (NLP) techniques to identify and disambiguate financial jargon, acronyms, and idiomatic expressions. This ensures that the system can accurately map user queries to the appropriate financial concepts and data sources, thereby improving the relevance and precision of the search results.

Furthermore, our preprocessing methods incorporate dynamic query adaptation, which allows the system to refine and expand user queries based on intermediate results and user feedback. This iterative process helps to capture the evolving nature of financial inquiries, where users often need to refine their queries as they gain more insights from the initial results [19]. By continuously learning from user interactions and adapting the preprocessing steps accordingly, our system can better handle the complexity and variability of financial queries, ultimately leading to more accurate and useful information retrieval [19].

### 5.3.2 Task-Specific Retrieval Methods
Task-specific retrieval methods play a crucial role in enhancing the performance of financial information systems by leveraging advanced models tailored to the unique characteristics of financial data [15]. These methods focus on optimizing the retrieval process to ensure that the system can efficiently access and utilize the most relevant and accurate information. By incorporating state-of-the-art (SOTA) models, these methods can better understand the complex and often nuanced language used in financial documents, such as annual reports, market analyses, and regulatory filings. This understanding is critical for tasks that require deep contextual awareness, such as long-context question answering and complex reasoning over financial documents.

To achieve this, task-specific retrieval methods often employ a combination of preprocessing techniques and advanced search algorithms. Preprocessing steps include tokenization, normalization, and entity recognition, which help in breaking down the text into manageable units and identifying key financial entities. These steps are crucial for ensuring that the system can effectively capture and utilize the context from diverse and complex data sources. Additionally, the use of SOTA models, such as transformer-based architectures, allows for the extraction of semantic features that are specific to the financial domain. This enables the system to perform more accurate and contextually relevant retrievals, thereby improving the overall quality of the generated responses.

Furthermore, task-specific retrieval methods often incorporate dynamic query adaptation mechanisms to refine search queries based on intermediate results. This is particularly important in the financial domain, where market conditions and regulatory requirements can change rapidly. The search executor in these systems typically includes an LLM-based adaptive query rewriter that continuously refines sub-queries to ensure that the search process remains responsive to emerging information [19]. Additionally, a temporal weighting mechanism is employed to prioritize information relevance based on the deduced time context from user queries [19]. This ensures that the system not only retrieves the most accurate data but also the most timely and contextually appropriate information, enhancing the overall effectiveness of the retrieval process.

### 5.3.3 Grounded Responses with Document Selection
In the realm of financial data processing, ensuring that generated responses are grounded in accurate and relevant information is paramount. To achieve this, we implemented a novel method known as document selection, which operates in conjunction with a reranking mechanism [15]. This approach begins by leveraging state-of-the-art retrieval models to identify a set of candidate documents that are most likely to contain the necessary information for generating a response. However, simply retrieving documents is insufficient; the quality and relevance of these documents must be further refined to ensure that the final output is both precise and contextually appropriate.

The document selection process involves a multi-step evaluation of the retrieved documents. Initially, a tailored preprocessing step is applied to both user inputs and financial documents to enhance the system's ability to capture and utilize context from complex and diverse data sources. This preprocessing includes tokenization, normalization, and the extraction of key entities and relationships. Subsequently, a task-specific retrieval method is employed, which leverages advanced models to recognize the unique language and structural features of financial information [15]. This ensures that the retrieved documents are not only relevant but also aligned with the specific requirements of the financial domain.

Finally, the selected documents undergo a reranking process to prioritize those that are most likely to provide the ground truth for the response. This reranking is informed by a combination of factors, including the document's recency, the relevance of its content to the user query, and the presence of key financial terms and concepts [15]. By integrating this document selection and reranking approach, the system can generate responses that are not only accurate but also grounded in the most pertinent and up-to-date information, thereby mitigating the risk of hallucinations and misinterpretations that are common in specialized domains like finance.

# 6 Future Directions


The current landscape of large language models (LLMs) in the financial sector, while promising, is not without its limitations and gaps. One significant limitation is the lack of comprehensive, high-quality, and domain-specific training data, which is essential for fine-tuning LLMs to perform financial tasks accurately. The financial domain is characterized by complex, rapidly changing, and highly regulated data, which poses unique challenges for data collection and annotation. Additionally, the black-box nature of LLMs and the difficulty in interpreting their outputs remain barriers to their widespread adoption in financial institutions, where transparency and explainability are paramount. Furthermore, the integration of LLMs into existing financial systems often requires specialized knowledge and infrastructure, which can be a significant barrier for smaller firms and organizations.

To address these limitations, several directions for future research are proposed. First, the development of more robust and scalable methods for collecting, annotating, and curating financial data is essential. This includes the creation of large, diverse, and high-quality datasets that cover a wide range of financial tasks and contexts. Collaborative efforts between financial institutions, regulatory bodies, and academic researchers can help in creating benchmark datasets that are representative of the real-world financial landscape. Second, there is a need for the development of explainable AI (XAI) techniques that can provide transparent and interpretable outputs from LLMs. This is crucial for building trust and ensuring that the models can be effectively audited and regulated. Techniques such as attention mechanisms, feature importance analysis, and model distillation can be explored to enhance the interpretability of LLMs in financial applications.

Third, the integration of LLMs with other advanced technologies, such as reinforcement learning (RL) and multi-agent systems, can significantly enhance their capabilities in dynamic and complex financial environments. For example, RL can be used to develop adaptive models that can learn and adjust their strategies based on real-time market conditions. Multi-agent systems can be employed to handle specialized tasks, such as risk management and portfolio optimization, by leveraging the strengths of multiple LLMs working in concert. Additionally, the development of hybrid models that combine the strengths of LLMs with traditional econometric and statistical methods can lead to more robust and accurate financial models.

The potential impact of the proposed future work is substantial. By addressing the current limitations and gaps, the proposed research directions can lead to the development of more reliable, transparent, and efficient financial LLMs. These advancements can significantly enhance the decision-making processes in financial institutions, leading to more informed and effective strategies for risk management, portfolio optimization, and regulatory compliance. Moreover, the integration of LLMs with other advanced technologies can drive innovation and foster the development of new financial products and services, ultimately contributing to the stability and growth of the financial sector.

# 7 Conclusion



This survey paper has provided a comprehensive overview of the current state of research and practice in the application of large language models (LLMs) to financial tasks. Key findings include the development of specialized techniques and frameworks to enhance the performance of LLMs in the financial domain, such as data augmentation, fine-tuning, and domain adaptation. The paper also highlights the importance of bilingual financial language models, which are essential for handling multilingual financial data in a globalized market. Additionally, the survey explores the role of temporal dependencies in financial data and the construction of high-quality training datasets, which are crucial for building reliable and accurate financial models. The paper further examines zero-shot and few-shot learning approaches, which are particularly valuable in scenarios where labeled data is limited or rapidly changing. Interdisciplinary applications, such as the use of multi-agent systems for trading and portfolio management, the integration of multimodal data in cryptocurrency analysis, and the implementation of verbal reinforcement mechanisms for risk control, are also discussed. Benchmark suites and evaluation frameworks, such as SuperCLUE-Fin and KFinEval-Pilot, are essential for assessing the performance and capabilities of financial LLMs, ensuring that they can be effectively compared and refined.

The significance of this survey lies in its contribution to the growing body of knowledge on the application of LLMs in the financial sector. By synthesizing the latest research findings and practical insights, the paper provides a roadmap for future research and development in this rapidly evolving domain. The comprehensive overview of current techniques and frameworks, along with a critical analysis of existing challenges, offers valuable guidance for researchers, practitioners, and policymakers. The paper underscores the potential of LLMs to transform financial practices, making them more efficient, accurate, and responsive to market dynamics. However, it also highlights the need for continued innovation and collaboration to address the unique challenges posed by the financial domain, such as the need for specialized data, domain-specific knowledge, and robust evaluation frameworks.

In conclusion, this survey paper calls for increased interdisciplinary collaboration between computer scientists and financial experts to develop predictive models that combine the strengths of both fields. Such collaboration is crucial for creating robust and interpretable models that can be trusted by financial practitioners and regulators. The paper also emphasizes the importance of ongoing research to optimize communication efficiency and ensure that these systems remain aligned with financial regulations and ethical standards. By fostering innovation and driving the adoption of LLMs in the financial sector, we can ultimately lead to more informed and effective decision-making processes, contributing to the overall stability and growth of the financial industry.

# References
[1] Large Language Models in Finance  A Survey  
[2] SeQwen at the Financial Misinformation Detection Challenge Task   Sequential Learning for Claim Veri  
[3] Bridging Language Models and Financial Analysis  
[4] Dólares or Dollars  Unraveling the Bilingual Prowess of Financial LLMs  Between Spanish and English  
[5] ZiGong 1.0  A Large Language Model for Financial Credit  
[6] LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management  
[7] The Promise and Peril of Generative AI  Evidence from GPT-4 as Sell-Side  Analysts  
[8] Auto-Generating Earnings Report Analysis via a Financial-Augmented LLM  
[9] FinSQL  Model-Agnostic LLMs-based Text-to-SQL Framework for Financial  Analysis  
[10] HybridRAG  Integrating Knowledge Graphs and Vector Retrieval Augmented  Generation for Efficient Inf  
[11] FinGPT-HPC  Efficient Pretraining and Finetuning Large Language Models  for Financial Applications w  
[12] Leveraging Large Language Models for Institutional Portfolio Management   Persona-Based Ensembles  
[13] Are LLMs Rational Investors  A Study on Detecting and Reducing the  Financial Bias in LLMs  
[14] LLMs Meet Finance  Fine-Tuning Foundation Models for the Open FinLLM  Leaderboard  
[15] Optimizing Retrieval Strategies for Financial Question Answering  Documents in Retrieval-Augmented G  
[16] FinCon  A Synthesized LLM Multi-Agent System with Conceptual Verbal  Reinforcement for Enhanced Fina  
[17] SuperCLUE-Fin  Graded Fine-Grained Analysis of Chinese LLMs on Diverse  Financial Tasks and Applicat  
[18] KFinEval-Pilot  A Comprehensive Benchmark Suite for Korean Financial  Language Understanding  
[19] An Agent Framework for Real-Time Financial Information Searching with  Large Language Models  
[20] Fino1  On the Transferability of Reasoning Enhanced LLMs to Finance  