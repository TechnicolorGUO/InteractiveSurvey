# A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation  

Xiangru $\mathrm { T a n g ^ { 1 , * } } ^ { \mathbb { \oplus } }$ , Howard Dai1,∗, Elizabeth Knight2,∗, Fang Wu3, Yunyang Li1, Tianxiao Li4 and Mark Gerstein1,4,5,6,7,†  

1Department of Computer Science, Yale University, New Haven, CT 06520, 2School of Medicine, Yale University, New Haven, CT 06520, 3Computer Science Department, Stanford University, CA 94305, 4Program in Computational Biology & Bioinformatics, Yale University, New Haven, CT 06520, 7Department of Molecular Biophysics & Biochemistry, Yale University, New Haven, CT 06520 , 5Department of Statistics & Data Science, Yale University, New Haven, CT 06520 and 6Department of Biomedical Informatics & Data Science, Yale University, New Haven, CT 06520 ∗Contributed equally to this work.†Corresponding author: Mark Gerstein. Email: mark@gersteinlab.org.  

# Abstract  

Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AIdriven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level observations across different fields. We discuss parallel challenges and approaches between the two applications and highlight future directions for AI-driven de novo drug design as a whole. An organized repository of all covered sources is available at https://github.com/gersteinlab/GenAI4Drug.  

Key words: Generative Model, Drug Design, Molecule Generation, Protein Generation  

# Introduction  

During the drug design process, ligands must be created, selected, and tested for their interactions and chemical effects conditioned on specific targets [1]. These ligands range from small molecules with tens of atoms to large proteins such as monoclonal antibodies [2, 3]. While methods exist to optimize the selection and testing of probable molecules, traditional discovery methods across all fields are computationally expensive [4]. Recent artificial intelligence (AI) models have demonstrated competitive performance [5, 6, 7] in improving various tasks in the drug design process. Methods such as machine learning (ML)-driven quantitative structure-activity relationship (QSAR) approaches [8, 9] have significantly improved virtual screening (VS) in molecule design [10, 11], while ML-assisted directed evolution techniques for protein engineering [12, 13] have proven to be reliable and widely used tools. However, an emerging and even more powerful task for ML is the generation of entirely new biological compounds in de novo drug design [14, 15, 16].  

In contrast to applications in VS and directed evolution, which seek to expedite and optimize tasks within an existing framework, de novo drug design focuses on generating entirely new biological entities not found in nature [14]. While other ML-driven methods search within existing chemical libraries for drug-like candidates, thereby facing inherent constraints, de novo design circumvents this limitation by exploring unknown chemical space and generating drug-like candidates from scratch [24, 25, 26].  

In this paper, we explore the impacts and developments of ML-driven de novo drug design in two primary areas of research: molecule and protein generation. Within protein generation, we additionally explore antibody and peptide generation, given their high research activity and relevance. Although the types of pharmaceuticals and the associated chemical nuances differ across fields, the overarching goal of exploring chemical space through de novo design remains constant. Both fields are rapidly growing industries with traditionally high research and development costs [27, 28, 29], and current improvements are driven by active developments in ML-based de novo methods.  

Molecule design specifically refers to the development of novel molecular compounds, often with the aim of smallmolecule drug design. The generated molecules must satisfy a complex and often abstract array of chemical constraints that determine both their validity and “drug-likeness” [30, 31]. This, combined with the vast space of potential drug-like compounds (up to $1 0 ^ { 2 3 } \textrm { - } 1 0 ^ { 6 0 }$ [32]), renders traditional smalldrug design time-consuming and expensive. Using traditional methods, preclinical trials can cost hundreds of millions of dollars [33] and take between 3 and 6 years [4]. In recent years, AI-driven methods have gained traction in drug design. AI-focused biotechnology companies have initiated over 150 small-molecule drugs in the discovery phase and 15 in clinical trials, with the usage of this AI-fueled process expanding by almost $4 0 \%$ each year [34].  

An equally promising field, protein design, refers to the artificial generation or modification of proteins (protein engineering) for various biological uses. Native proteins have adapted and evolved over millions of years, so the rapid progression of human society in recent years poses challenges for naturally occurring proteins [35]. Protein design has an even more versatile range of applications, finding utility in immune signaling, targeted therapeutics, and various other fields. When executed efficiently, protein design has the potential to transform synthetic biology [36, 37, 28]. Like molecule generation, proteins must adhere to abstract biological constraints, yet the inherently more complex structure of a protein presents a nuanced generative objective, requiring a more direct application of chemical knowledge in the process [38, 39, 40]. Traditional methods such as directed evolution are confined to specific evolutionary trajectories for existing proteins; the de novo generation of proteins would add an entirely new dimension for researchers to explore [41, 37, 42].  

![](images/7537fa264d1745cfaa2ce5acdfb3d4fdbe84adf5ab3763fa97ef12572c3456fd.jpg)  
Fig. 1. An overview of the topics covered in this survey. In particular, we explore the intersection between generative AI model architectures and real-world applications, organized into two main categories: small molecule and protein generation tasks. Note that diffusion and flow-based models are often paired with GNNs for processing 2D/3D-based input, while VAEs and GANs are typically used for 1D input [17, 18, 19, 20, 21, 22, 23].  

Structurally, we aim to provide an organized introduction to the two fields mentioned above. We begin with a technical overview of relevant deep learning architectures employed in both small molecule and protein design. We then explore their applications in molecule and protein design, dividing our analysis into a variety of subfields highlighted in Figure 1. For each subfield, we provide (1) a general background/task definition, (2) common datasets used for training and testing, (3) common evaluation metrics, (4) an overview of past and current ML approaches, and (5) a comparative analysis of the performance of state-of-the-art (SOTA) models. A detailed overview of this structure is shown in Figure 2. Finally, we integrate concepts within each subfield into a broad analysis of de novo drug design as a whole, providing a comprehensive summary of the field in terms of current trends, top-performing models, and future directions. Our overall objective is to provide a systematic overview of ML in drug design, capturing recent advancements in this rapidly evolving area of research.  

# Related Surveys  

Several survey papers delve into specific aspects of generative AI in drug design, with some focusing on molecule generation [24, 43, 44], protein generation [35, 36], or antibody generation [45, 46, 47, 48]. Other survey papers are organized based on model architecture rather than application, with recent papers by Zhang et al. [49] and Guo et al. [50] reviewing diffusion models in particular. While each of the above surveys provides an in-depth analysis of a specific application or type of model, the level of specialization may limit their scope. Our approach is a macro-level analysis of small molecule and protein generation, tailored for those seeking a high-level introduction to the emerging field of generative AI in chemical innovation. This broad perspective enables us to highlight relationships across fields, such as parallel shifts in methods of input representation, the common emergence of architectures like equivariant graph neural networks (EGNNs), and similar challenges faced in both molecule and protein design.  

# Preliminary: Generative AI Models  

Generative AI uses various statistical modeling, iterative training, and random sampling to generate new data samples resembling the input data. Historically, prominent approaches include generative adversarial networks (GANs) [51], variational autoencoders (VAEs) [52], and flow-based models [53]. More recently, diffusion models [54] have emerged as promising alternatives. In our survey, we begin by providing a concise mathematical and computational overview of these architectures.  

# Variational Autoencoders  

A VAE [52] is a type of generative model that extends upon the typical encoder-decoder framework by representing each latent attribute using a distribution rather than a single value. This approach creates a more dynamic representation for the underlying properties of the training data and enables the sampling of new data points from scratch.  

![](images/aad8412010785f4c9de82d38c74df305bf9b6d835b5946b2206a3a5135d1f65e.jpg)  
Fig. 2. A structured layout for all terms and papers covered in our survey, including datasets, models, and metrics for each task. Sections contained in the main text are highlighted in blue, while sections expanded upon in the appendix are highlighted in purple.  

Formally, we can express the encoder as:  

$$
q _ { \phi } ( z | x ) = \mathcal { N } ( z ; \mu _ { \phi } ( x ) , \sigma _ { \phi } ^ { 2 } ( x ) I )
$$  

Intuitively, each $x$ will be mapped to some mean $\mu _ { \phi } ( x )$ and variance $\sigma _ { \phi } ^ { 2 } ( x )$ , which describe a corresponding normal distribution. We express the decoder as $p _ { \theta } ( x | z )$ , where $z$ is a randomly sampled point from the latent distribution $\mathcal { N } ( \mu _ { \phi } ( x ) , \sigma _ { \phi } ^ { 2 } ( x ) I )$ and is mapped into a point $x$ in the decoding process.  

VAE loss is computed using two balancing ideas: reconstruction loss and Kullback–Leibler (KL) divergence loss. Reconstruction loss measures the difference between the ground truth and the reconstructed decoder output, often expressed using cross-entropy loss:  

$$
\mathcal { L } _ { \mathrm { r e c o n } } = - 1 * \int q _ { \phi } ( z | x ) \log p _ { \theta } ( x | z ) d z
$$  

KL divergence measures the difference between two probability distributions [55]. For VAEs, KL divergence is computed between the encoded distribution and the standard normal distribution. This can be seen as “regularization,” as it encourages the encoder to map elements to a more central region with overlapping distributions, thus improving continuity across the latent space. Formally, the KL loss can be expressed as follows, with $k$ representing the $k$ th dimension in the latent space and $K$ representing the dimension of $\boldsymbol { \mathbf { z } }$ :  

$$
\begin{array} { c } { { \mathcal { L } _ { \mathrm { K L } } = D _ { \mathrm { K L } } \big ( q _ { \phi } ( z | x ^ { ( i ) } ) | | \mathcal { N } ( 0 , I ) \big ) } } \\ { { { } } } \\ { { = - \displaystyle \frac { 1 } { 2 } \sum _ { k } \Big ( 1 + \log ( \sigma _ { k } ^ { ( i ) ^ { 2 } } ) - \mu _ { k } ^ { ( i ) ^ { 2 } } - \sigma _ { k } ^ { ( i ) ^ { 2 } } \Big ) } } \end{array}
$$  

Here, µk and σ(i) represent the mean and variance of the kth component of the latent space, respectively, for datapoint $\boldsymbol { x } ^ { ( i ) }$ . Then, the overall loss function can be expressed as follows, where $\beta$ can be adjusted to balance the reconstruction loss and KL loss:  

$$
\mathscr { L } = \mathscr { L } _ { \mathrm { r e c o n } } + \beta \mathscr { L } _ { \mathrm { K L } }
$$  

# Generative Adversarial Networks  

GANs [51] utilize “competing” neural networks for mutual improvement. The two neural networks — the generator and the discriminator — compete in a zero-sum game. The generator $G _ { \epsilon }$ ) creates instances (e.g., chemical structures of potential drugs) from random noise $( z )$ sampled from a prior distribution $p _ { z } ( z )$ to mimic the training samples, while the discriminator ${ \bf \Pi } ( D )$ aims to distinguish between the synthetic data and the training samples.  

The learning process involves the optimization of the following loss function:  

$$
\operatorname* { m i n } _ { G } \operatorname* { m a x } _ { D } \mathbb { E } _ { x } [ l o g D ( x ; \theta _ { d } ) ] + \mathbb { E } _ { z \sim p ( z ) } [ l o g ( 1 - D ( G ( z ; \theta _ { g } ) ; \theta _ { d } ) ) ]
$$  

Here, $\mathbb { E } _ { x } \left[ l o g D ( x ; \theta _ { d } ) \right]$ represents the likelihood applied by the discriminator to a correct sample, while $\mathbb { E } _ { z \sim p ( z ) } [ l o g ( 1 -$ $D ( G ( z ; \theta _ { g } ) ; \theta _ { d } ) ) ]$ represents the negative likelihood applied by the discriminator to an incorrect sample. This function returns a higher value when the discriminator accurately categorizes samples; thus, the discriminator aims to maximize this function, while the generator aims to minimize it.  

# Flow-Based Models  

Flow-based generative models [53] generate data according to a target distribution $\begin{array} { r } {  { \boldsymbol { { x } } } \ \sim \ p (  { \boldsymbol { { x } } } ) } \end{array}$ by applying a chain of transformations to a simple latent distribution, often Gaussian, denoted $z _ { 0 } \sim p _ { 0 } ( z _ { 0 } )$ . This transformation applies an invertible function $f : z _ { 0 } \mapsto x$ , such that:  

$$
x = f ( z ; \theta ) \Rightarrow z = f ^ { - 1 } ( x ; \theta )
$$  

where the trained model learns parameters $\theta$ . Since $f$ is invertible and thus the learned map is bijective, $z$ has the same dimensionality as $x$ . Often, $f$ is a composite function where $f ( x ) = f _ { K } \circ f _ { k - 1 } \circ \ldots \circ f _ { 1 } ( x )$ ; this allows for more complex probability distributions to be modeled. Because each function is invertible, the posterior can be easily computed — the loglikelihood of a single point $x$ can be written in terms of its latent variable $z$ :  

$$
\log p ( x ) = \log p _ { 0 } ( z ) + \log \left| \operatorname* { d e t } \frac { \partial f } { \partial z } \right|
$$  

This function is used to train parameters $\theta$ to maximize the probability of observing the data. Various models build upon this premise to represent complex data distributions and capture relationships within sequential data.  

# Diffusion Models  

Diffusion models [54] perform a fixed learning procedure, gradually adding Gaussian noise to data over a series of time steps. We define two stages of the model: the noise-adding (forward) and the noise-removing (reverse) process.  

In the forward process, each step $x _ { t + 1 }$ can be represented as a Markov chain and is comprised of $x _ { t - 1 }$ and a small amount of Gaussian noise. We represent this mathematically as follows:  

$$
x _ { t + 1 } = \sqrt { 1 - \beta _ { t } } x _ { t } + \sqrt { \beta _ { t } } \epsilon , \quad \epsilon \sim \mathcal { N } ( 0 , I )
$$  

Here, $\scriptstyle { x _ { t } }$ is the data at time $ { \mathrm { ~  ~ t ~ } }$ , and $\beta _ { t }$ denotes the noise schedule. The variance $\beta _ { t }$ decreases in the forward process so that after many steps, we have $p ( x _ { t } | x _ { 0 } ) \approx N ( 0 , 1 )$ .  

In the reverse process, our aim is to reconstruct the data from the noise. In this process, a denoising function is learned, often modeled by a neural network:  

$$
x _ { t - 1 } = f _ { \theta } ( x _ { t } , t ) + \sqrt { \beta _ { t } } \epsilon , \quad \epsilon \sim \mathcal { N } ( 0 , I )
$$  

where $f _ { \theta } ( x _ { t } , t )$ is the denoising function parameterized by $\theta$ .  

To train a diffusion model, we approximate the added noise at each step; the loss function minimizes the difference between the true noise and the model-predicted noise:  

$$
L _ { t } = \mathbb { E } _ { t \sim [ 1 , T ] , x _ { 0 } , \epsilon _ { t } } [ | | \epsilon _ { t } - \epsilon _ { \theta } \left( x _ { t } , t \right) | | ^ { 2 } ]
$$  

Here, $t \ \sim \ U \{ 1 , T \}$ means that the time step $t$ is drawn uniformly at random from the set $\{ 1 , 2 , . . . , T \}$ , and $\boldsymbol { \epsilon } _ { \boldsymbol { \theta } } ( x _ { t } , t )$ represents the noise predicted by the model parameterized by $\theta$ . $T$ represents the last time step of the model. Once the neural network has been trained, we can sample from the noise distribution and iterate through the reverse process to generate new data.  

While the mathematical framework for the diffusion process is generally based on continuous data, adaptations made by Austin et al. [56] allow for smoother implementation with discrete data forms like molecular graphs.  

# Other Models  

While we cover the main generative methods used in these fields, a variety of other models also appear in specific applications and tasks in our paper, such as transformers, energy-based models (EBMs), BERT, and more [57, 58, 59, 60, 61, 62, 63]. While not generative models on their own, graph neural networks (GNNs) [64] are often paired with the above generative methods to capture the graph-like structure of molecules. A wide variety of GNN variations exist, including equivariant graph neural networks (EGNNs) [65], messagepassing neural networks (MPNNs) [66], graph convolutional networks (GCNs) [67], graph isomorphism networks (GINs) [68], and convolutional neural networks (CNNs) [69, 70, 71]. We discuss GNNs and EGNNs in more detail in the appendix on page 23.  

# Applications  

# Molecule  

# Task Background  

Molecule generation focuses on the creation of novel molecular compounds for drug design. These generated molecules are intended to be (1) valid, (2) stable, and (3) unique, with an overall goal of pharmaceutical applicability. “Pharmaceutical applicability” is a broad term for a molecule’s binding affinity to various biological targets. While the first three tasks may seem trivial, there are a variety of challenges with simply generating valid and stable molecules. Thus, the field of target-agnostic molecule generation is focused on generating valid sets of molecules without consideration for any biological target. Target-aware molecule generation (or ligand generation) focuses on the generation of molecules for specific protein structures and therefore focuses more on the pharmaceutical component. Finally, 3D conformation generation involves generating various 3D conformations given 2D connectivity graphs.  

For training and testing, molecule inputs can be formatted in a variety of ways, depending on the available information or desired output. Molecules can be expressed in 1D format through the simplified molecular-input line-entry system (SMILES), in 2D using connectivity graphs to represent bonds, or in 3D using point cloud embeddings on graph nodes [72].  

# Target-Agnostic Molecule Design  

# Overview  

While the task of target-agnostic molecule design may seem simplistically open-ended, there are a vast array of chemical properties and rules that generated molecules must align with to be considered “valid” and “stable.” The determination of “validity” includes a complex combination of considerations, such as electromagnetic forces, energy levels, and geometric constraints, and a well-defined “formula” does not yet exist for predicting the feasibility of molecular compounds. This, combined with the vast space of potential drug-like compounds (up to $1 0 ^ { 2 3 } \textrm { - } 1 0 ^ { 6 0 }$ ), makes brute-force experimentation quite time-consuming [32]. Deep learning can assist in learning abstract features for existing valid compounds and efficiently generate new molecules with a higher likelihood of validity.  

# Task  

The target-agnostic molecule design task is as follows: given no input, generate a set of novel, valid, and stable molecules.  

# Datasets  

To learn these abstract constraints, models must learn from large sets of existing valid, stable molecules. The following datasets are most commonly used for this task:  

QM9 [73] - Quantum Machines 9, contains small stable molecules pulled from the larger chemical universe database GDB-17 GEOM-Drug [74] - Geometric Ensemble of Molecules, contains more complex, drug-like molecules, often used to test scalability beyond the simpler molecules of QM9  

# Metrics  

The most general task within this field is unconditional molecule generation, where models aim to generate a new set of valid, stable molecules with no input. All of these metrics can be evaluated using either QM9 or GEOM-Drug as testing sets.  

Atom Stability - The percentage of atoms with the correct valency   
Molecule Stability - The percentage of molecules whose atoms are all stable   
Validity - The percentage of stable molecules that are considered valid, often evaluated by RDKit   
Uniqueness - The percentage of valid molecules that are unique (not duplicates)   
Novelty - The percentage of molecules not contained within the training dataset   
QED [30] - Quantitative Estimate of Drug-Likeness, a formulaic combination of a variety of molecular properties that collectively estimate how likely a molecule is to be used for pharmaceutical purposes  

Note that the novelty metric is sometimes omitted, as argued by Vignac et al. [75], who contend that QM9 is an exhaustive set of all molecules with up to nine heavy atoms following a predefined set of constraints. Therefore, any “novel” molecule would have to break one of these constraints, making novelty a poor indicator of performance. While QED is a wellestablished metric and may see expanded usage in the future, many current models have focused solely on generating valid molecules and do not report performance on QED.  

Models are often evaluated on conditional molecule generation, aiming to generate models which fit desired chemical properties. For evaluation, a property classifier network $\phi _ { c }$ is trained on half of the QM9 dataset, while the model is trained on the other half. $\phi _ { c }$ is then evaluated on the model’s generated molecules, and the mean absolute error between the target property value and the evaluated property value is calculated. Below are the six molecular properties considered:  

α - Polarizability, or the tendency of a molecule to acquire an electric dipole moment when subjected to an external electric field, measured in cubic Bohr radius $( B o h r ^ { 3 } )$   
εHOMO - Highest Occupied Molecular Orbit Energy, measured in millielectron volts $( m e V )$   
εLUMO - Lowest Unoccupied Molecular Orbit Energy, measured in millielectron volts $( m e V )$   
$\Delta _ { \epsilon }$ - Difference between $\varepsilon _ { H O M O }$ and $\varepsilon _ { L U M O }$ , measured in millielectron volts $( m e V )$   
µ - Dipole moment, measured in debyes $( D )$   
$C _ { v }$ - Molar heat capacity at $2 9 8 . 1 5 \ K$ , measured in calories per Kelvin per mole ${ \frac { c a l } { m o l } } K$  

<html><body><table><tr><td>Model</td><td>Type of Model</td><td>Dataset</td><td>At Stb. (%,↑)</td><td>Mol Stb. (%, ↑)</td><td>Valid (%,↑)</td><td>Val/Uniq. (%,↑)</td></tr><tr><td>G-SchNet [76]</td><td>SchNet</td><td>QM9</td><td>95.7[77]</td><td>68.1 [77]</td><td>85.5[77]</td><td>80.3[77]</td></tr><tr><td>E-NF[78]</td><td>EGNN,Flow</td><td>QM9</td><td>85 [77]</td><td>4.9 [77]</td><td>40.2 [77]</td><td>39.4[77]</td></tr><tr><td>EDM[77]</td><td>EGNN,Diffusion</td><td>QM9, GEOM-Drugs</td><td>98.7</td><td>82.0</td><td>91.9</td><td>90.7</td></tr><tr><td>GCDM[79]</td><td>EGNN,Diffusion</td><td>QM9</td><td>98.7</td><td>85.7</td><td>94.8</td><td>93.3</td></tr><tr><td>MDM[80]</td><td>EGNN,VAE,Diffusion</td><td>QM9,GEOM-Drugs</td><td>99.2 [81]</td><td>89.6 [81]</td><td>98.6</td><td>94.6</td></tr><tr><td>JODO [81]</td><td>EGNN,Diffusion</td><td>QM9,GEOM-Drugs</td><td>99.2</td><td>93.4</td><td>99.0</td><td>96.0</td></tr><tr><td>MiDi** [82]</td><td>EGNN,Diffusion</td><td>QM9,GEOM-Drugs</td><td>99.8</td><td>97.5</td><td>97.9</td><td>97.6</td></tr><tr><td>GeoLDM** [77]</td><td>VAE,Diffusion</td><td>QM9, GEOM-Drugs</td><td>98.9</td><td>89.4</td><td>93.8</td><td>92.7</td></tr></table></body></html>  

Table 1. An overview of relevant molecular generation models. All benchmarking metrics are self-reported unless otherwise noted. All metrics are evaluated with the QM9 dataset. For models with multiple variations, the highest-performing version was selected. $[ { ^ { * * } } ]$ represents the current SOTA. As MiDi and MDM use slightly different evaluation conditions, their results are not fully comparable.   


<html><body><table><tr><td>Model</td><td>Atom Stb.(%,↑)</td><td>Mol Stb. (%,↑)</td><td>Valid (%,↑)</td><td>Val/Uniq. (%,↑)</td></tr><tr><td>EDM[83]</td><td>81.30</td><td></td><td></td><td></td></tr><tr><td>MDM[80]</td><td></td><td>62.20</td><td>99.50</td><td>99.00</td></tr><tr><td>MiDi [82]</td><td>99.80</td><td>91.60</td><td>77.80</td><td>77.80</td></tr><tr><td>GeoLDM[77]</td><td>84.40</td><td></td><td>99.30</td><td></td></tr></table></body></html>

Table 2. Molecular generation molecules evaluated on the larger GEOM-Drugs dataset. All metrics are self-reported. As MiDi uses slight different evaluation conditions, its results are not fully comparable.  

<html><body><table><tr><td>Task (↓)</td><td>α</td><td>△</td><td>EHOMO</td><td>ELUMO</td><td>μ</td><td>Cu</td></tr><tr><td>Units</td><td>Bohr3</td><td>meV</td><td>meV</td><td>meV</td><td>D</td><td>mK</td></tr><tr><td>EDM [83]</td><td>2.76</td><td>655</td><td>356</td><td>584</td><td>1.111</td><td>1.101</td></tr><tr><td>GCDM[79]</td><td>1.97</td><td>602</td><td>344</td><td>479</td><td>0.844</td><td>0.689</td></tr><tr><td>MDM[80]</td><td>1.591</td><td>44</td><td>19</td><td>40</td><td>1.177</td><td>1.647</td></tr><tr><td>GeoLDM[77]</td><td>2.37</td><td>587</td><td>340</td><td>522</td><td>1.108</td><td>1.025</td></tr></table></body></html>

Table 3. Molecular generation models evaluated on the conditional molecule generation task. All metrics are self-reported. All metrics are evaluated with the QM9 dataset.  

# Models  

Approaches to the molecular generation task have seen significant shifts over the past few years, transitioning from 1D SMILES strings to 2D connectivity graphs, then to 3D geometric structures, and finally to incorporating both 2D and 3D information.  

Early methods like Character VAE (CVAE) [84], Grammar VAE (GVAE) [85], and Syntax-Directed VAE (SD-VAE) [86] apply VAEs to 1D SMILES string representations of molecule graphs. While 1D SMILES strings can be deterministically mapped to molecular graphs, SMILES falls short in quality of representation – two graphs with similar chemical structures may end up with very different SMILES strings, making it harder for models to learn these similarities and patterns [87].  

Junction Tree VAE (JTVAE) [87] was the first model to address this issue by generating 2D graph structures directly. JTVAE generates a tree-structured scaffold and then converges this scaffold into a molecule using a graph message passing network. This approach allows JTVAE to iteratively expand its molecule and check for validity at each step, resulting in considerable performance improvement over previous SMILESbased methods.  

2D graph methods like JTVAE still fall short due to the lack of 3D input; because binding and interaction with other molecules/proteins rely heavily on 3D conformations, models that do not consider 3D information cannot properly represent and optimize properties like binding affinity. Thus, more recent developments include models that incorporate 3D information. Earlier 3D-based methods like E-NF [78] and G-SchNet [76] approached the molecular generation problem with flow-based methods or autoregressive methods (in particular, G-SchNet uses the SchNet architecture developed by Schutt et al. [61]). More recently, a wave of diffusion-based models operate on  

3D point clouds, taking advantage of E(3) equivariance and demonstrating superior performance.  

EDM [83] provided an initial baseline for the application of diffusion, applying a standard diffusion process to an equivariant GNN with atoms represented as nodes with variables for both scalar features and 3D coordinates. While autoregressive models require an arbitrary ordering of the atoms, diffusion-based methods like EDM are not sequential and do not need such ordering, reducing a dimension of complexity and thus improving efficiency.  

Many subsequent models compared themselves with EDM as a baseline on the molecule generation task, seeking to improve upon its performance by adding additional considerations and adjustments. GCDM [79] implements a crossover between geometric deep learning and diffusion, using a geometrycomplete perceptron network to introduce attention-based geometric message-passing. While both EDM and GCDM have already demonstrated massive performance improvements, both models still struggle with both large-molecule scalability and diversity in the generated molecules. MDM [80] addressed the scalability issue by pointing out the lack of consideration for interatomic relations in EDM and GCDM. MDM separately defines graph edges for covalent bonds and for Van der Waals forces (dependent on a physical distance threshold $\tau$ ) to allow for thorough consideration of interatomic forces and local constraints. In addition, MDM addressed the diversity issue by introducing an additional distribution-controlling noise variable in each diffusion step. While previous diffusion models operated directly in the complex atomic feature space, GeoLDM [77] applies VAEs to map molecule structures to a lowerdimensional latent space for its diffusion. This latent space has a smoother distribution and lower dimensionality, leading to higher efficiency and scalability for large molecules. In addition, conditional generation is improved, as specified chemical properties are more clearly defined within latent spaces than they are in raw format.  

![](images/deafdb10c5d5183aac26016ac047290f6795f2ded6a5146c71c235fb4c707eca.jpg)  
Fig. 3. An overview of the progress in target-agnostic molecule design over time. Shortcomings of previous models are shown in the corresponding pink boxes, with subsequent models solving these shortcomings through novel design choices [84, 83, 87, 77, 80, 81].  

While previous models learned exclusively from either 2D or 3D representations, a new wave of models recognize the need for both: a molecule’s 2D connectivity structure is necessary to determine bond types and gather information about chemical properties and synthesis, while the 3D conformation is crucial for its interaction and binding affinity with other molecules. By jointly learning and generating both representations, models can maximize the amount of chemically relevant information and produce higher-quality molecular samples. The Joint 2D and 3D Diffusion Model (JODO) [81] uses a geometric graph representation to capture both 3D spatial information and connectivity information, applying score SDEs to this joint representation while proposing a diffusion graph transformer to parameterize the data prediction model and avoid the loss of correlation after noise is independently added to each separate channel. MiDi [82] uses a similar graph representation but instead applies a DDPM. It proposes a “relaxed” EGNN, which improves upon the classical EGNN architecture by exploiting the observation that translational invariance is not needed in the zero center-of-mass subspace. An full overview of the developments described in this section can be seen in Figure 3.  

As shown in Table 1, diffusion-based methods demonstrate significant improvements over previous methods, all achieving over $9 8 . 5 \%$ in atom stability. However, some models fall behind when extended to the larger GEOM-Drugs dataset, as shown in Table 2, where MiDi distinguishes itself for its capability to generate more stable complex molecules, albeit at the expense of validity. Table 3 illustrates that MDM and GCDM excel at conditional generation tasks, with the former model achieving the best performance in four out of six tasks and the latter outperforming the remaining two. Overall, current models demonstrate high performance on the QM9 dataset, but there is room for improvement when dealing with the more complex molecules found in the GEOM-Drugs dataset.  

# Target-Aware Molecule Design  

# Overview  

Contrasting target-agnostic molecule design, target-aware design involves generating molecules based on specific biological targets. Within target-aware design, two primary approaches exist: ligand-based drug design (LBDD) and structure-based drug design (SBDD). LBDD models often utilize the amino acid sequences of target proteins, leveraging the characteristics and features of known ligands to build new molecules with similar properties. By contrast, SBDD models use the 3D structure of the target protein to design a corresponding molecular structure. LBDD models are most useful when the 3D structure is not experimentally available, but are limited in novelty because they only learn from existing bindings [44]. When the 3D structure of the target protein is available, SBDD models are generally preferred, as they consider crucial 3D information.  

# Task  

Given input target information, typically in the form of protein amino acid sequences in LBDD and protein 3D structure in SBDD, these approaches generate molecules that exhibit high binding affinity and potential interactions with this target.  

# Datasets  

The following datasets are used for target-aware molecule design. CrossDocked2020 [88] is currently the most heavily used, as the cross-docking technique allows for the generation of combinatorially large quantities of data by “mixing and matching” similar ligand-protein pairs (22.5M compared to 40K in Binding MOAD [89]).  

CrossDocked2020 [88] - Contains ligand-protein complexes generated by cross-docking within clusters of similar binding sites called ”pockets”   
ZINC20 [90] - Fully enumerated dataset of possible ligands Binding MOAD [89] - Binding Mother of All Databases, a subset of PDB [91] containing experimentally determined protein-ligand pairings  

# Metrics  

Target-aware molecule design utilizes the following metrics. Beyond affinity/applicability metrics, an additional consideration for diversity is considered, as a diverse array of potential options for a given target can provide more flexibility in the drug development process.  

Vina Score [92] - Scoring function supported by the Vina platform that returns a weighted sum of atomic interactions and is useful for docking Vina Energy [92] - Energy prediction by the Vina platform that is used to measure binding affinity High Affinity Percentage - Percentage of molecules with lower Vina energy than the reference (ground-truth) molecule when binding to the target protein QED [30] - Quantitative Estimate of Drug-Likeness, which is also used in target-agnostic generation (see page 5)  

<html><body><table><tr><td>Model</td><td>Type of Model</td><td>Dataset</td><td>Vina (↑)</td><td>Affinity (%,↑)</td><td>QED (↑)</td><td>SA (↑)</td><td>Diversity (↑)</td></tr><tr><td>Luo et al. [98]</td><td>SchNet</td><td>CrossDocked2020</td><td>-6.344</td><td>29.09</td><td>0.525</td><td>0.657</td><td>0.720</td></tr><tr><td>LiGAN[96]</td><td>CNN, VAE</td><td>CrossDocked2020</td><td>-6.144 [98]</td><td>21.1 [99]</td><td>0.39 [99]</td><td>0.59 [99]</td><td>0.66 [99]</td></tr><tr><td>Pocket2Mol** [97]</td><td>EGNN,MLP</td><td>CrossDocked2020</td><td>-5.14 [99]</td><td>48.4 [99]</td><td>0.56 [99]</td><td>0.74 [99]</td><td>0.69 [99]</td></tr><tr><td>TargetDiff** [99]</td><td>EGNN,Diffusion</td><td>CrossDocked2020</td><td>-6.3</td><td>58.1</td><td>0.48</td><td>0.58</td><td>0.72</td></tr><tr><td>DiffSBDD** [100]</td><td>EGNN,Diffusion</td><td>CrossDocked,MOAD</td><td>-7.333</td><td></td><td>0.467</td><td>0.554</td><td>0.758</td></tr></table></body></html>

Table 4. An overview of relevant target-aware molecular generation models. All benchmarking metrics are self-reported unless otherwis noted. $[ { ^ { * * } } ]$ represents the current SOTA. As each paper uses slightly different benchmarking methods, their results may not be fully comparable. All metrics are evaluated with the CrossDocked2020 dataset.  

SAscore [93] - Synthetic Accessibility Score, a formulaic combination of molecular properties that determine how easy a molecule is to create in a real lab setting Diversity - The diversity of generated molecules for each specific binding site, measured by Tanimoto similarities [94] between pairs of Morgan fingerprints  

# Models  

LBDD models incorporate transformer architectures to generalize properties of learned ligands. For example, DrugGPT [95] is a recent autoregressive model that uses transformers to train numerous protein-ligand pairs. In their model, the ligand SMILES and protein amino acid sequence are tokenized for training, and the model produces viable SMILES ligand outputs. Generally, with improving protein structure prediction methods (see page 9) and increasing access to structural information as a whole, SBDD methods have become more prevalent than LBDD methods; thus, we explore SBDD methods below in more detail.  

LiGAN [96] introduces the idea of a 3D target-aware molecule output, fitting molecules into grid formats for learning with a CNN and training the model under a VAE framework. Pocket2Mol [97] places more emphasis on the specific “pockets” on the target protein to which the molecules bind, using an EGNN and geometric vector MLP layers to train on graph-structured input. Luo et al. [98] directly model the probability of atoms occurring at a certain position in the binding site, taking advantage of invariance through the SchNet [61] architecture.  

Recent SBDD models have also popularized the use of diffusion models. TargetDiff [99] performs diffusion on an EGNN (with many similarities to EDM [83]) to learn the conditional distribution. To optimize the binding affinity, Guan et al. note that the flexibility of atom types should be low, which is reflected by the entropy of the atom embedding. Schneuing et al. [100] propose DiffSBDD, which includes two sub-models: DiffSBDD-cond and DiffSBDDinpaint. DiffSBDD-cond is a conditional DDPM that learns a conditional distribution in a similar way as TargetDiff. In our benchmarking, we focus on the higher performing model, DiffSBDD-inpaint, which applies the inpainting approach (traditionally applied to filling parts of images) by masking and replacing segments of the ligand-protein complex.  

As shown in Table 4, DiffSBDD leads in Vina score and diversity, while TargetDiff leads in high affinity. Interestingly, diffusion-based methods seem to be outperformed by the MLP used in Pocket2Mol when it comes to drug-like metrics like QED and SA. However, Guan et al. [99] note that adjustments to TargetDiff, such as switching to fragment-based generation or predicting atom bonds, could improve performance on QED and SA.  

# Protein  

# Task Background  

Proteins are large biomolecules that contain one or more long chains of amino acids. Each amino acid is a molecular compound that contains both an amino (-NH2) and a carboxylic acid (-COOH) [101]. While there are over 500 naturally occurring amino acids, only 22 are proteinogenic (“proteinbuilding”) and thus relevant to the protein generation task [102]. Thus, in addition to 3D structural representations, proteins can be represented through their amino acid sequences, assigning each amino acid a letter label and representing each long chain as a string of labels. This sequential representation for amino acids mirrors the sequence structure of human language, allowing for natural language models to be applied in ways that would not be possible for the previously discussed molecule generation task. Within protein generation, several generative subtasks can be defined. Representation learning involves creating meaningful embeddings for protein inputs, which improves the data space for other models to train on. Structure prediction involves the generation of a protein structure for its corresponding amino acid sequence, historically a challenging task due to the vast conformational space. Sequence generation describes the inverse, creating a protein sequence for its corresponding structure. Finally, backbone design refers to creating protein structures from scratch, which forms the core of the de novo design task.  

We also briefly discuss antibody generation due to its high relevance within the protein generation field. In particular, antibodies are Y-shaped proteins used by the immune system to identify and bind to bacteria known as antigens. While many protein design models use multiple sequence alignment (MSA) to map evolutionary relationships between related sequences, MSAs are not always available for antibody sequences, and antibody-specific models cannot rely on this input. Additionally, binding regions are specifically defined for antibodies, contained within six complementarity-determining regions (CDRs). The CDR-H3 region is particularly diverse and complex, leading to a specialized task for the reconstruction of this region, known as CDR-H3 generation. We discuss antibody-specific methods within each corresponding subtask, and we include more detailed discussion of antibody generation in the appendix on page 26.  

Finally, we provide an additional section on peptide generation due to its high relevance and more specialized applications. Advances in drug delivery and synthesis technology have broadened its therapeutic potential, and recent innovations in peptide drug discovery have significantly improved treatments for type 2 diabetes with the creation of glucagon-like peptide-1 receptor agonists (e.g., liraglutide and semaglutide). Protein generation, while related, differs from peptide generation in length and complexity. Peptides are shorter (often no more than 50 amino acids) and have greater flexibility; thus, distinct computational models are needed to capture this differentiation.  

# Protein Representation Learning  

# Overview  

Protein representation learning involves learning embeddings to convert raw protein data into latent space representations, thereby extracting meaningful features and chemical attributes. Specifically, given a protein $x = [ o _ { 1 } , o _ { 2 } , . . . o _ { L } ]$ , where each $\scriptstyle { o _ { 1 } }$ represents an amino acid (sequence-based) or atom coordinate (structure-based), the goal is to learn an embedding $z \ =$ $[ h _ { 1 } , h _ { 2 } , \ldots h _ { L } ]$ , where each $h _ { i } \in \mathbb { R } ^ { d }$ represents a $d -$ dimensional token representation for amino acid $o _ { i }$ . Although representation learning is not a generative task on its own, these embeddings create “richer” data spaces for other generative models to train on. Hence, we briefly discuss them here. For a more in-depth analysis of protein representation learning models, please refer to the appendix on page 23.  

# Structure Prediction  

# Overview  

Generating 3D structures of proteins from their amino acid sequence is a challenging and important task in drug design. Historically, techniques like protein threading [103] and homology modeling [104] have been used to predict structures; however, these methods have fallen short due to a lack of computational power and the difficulty of finding structures in the vast conformational space. New research in computational modeling has used various deep learning architectures to discover information from the amino acid sequences to generate accurate 3D structures. Current models have achieved impressive accuracy in structure prediction, but there is room for improvement in terms of speed and scale.  

# Task  

Given a protein amino acid sequence, generate a set of 3D point coordinates for each amino acid residue, aiming to replicate a target ground-truth structure as closely as possible.  

# Datasets  

Unlike many of the other fields mentioned previously, the field of protein structure prediction benefits from a widely standardized benchmarking task through the Critical Assessment of Protein Structure Prediction (CASP). CASP conducts biennial testing on models using solved protein structures that have not been released to PDB.  

PDB [91] - Protein Data Bank, a central archive for all experimentally determined protein structures, widely used in almost all protein structure-related tasks CASP14 [105] - 14th Critical Assessment of Protein Structure Prediction, a set of unreleased PDB structures used to create a standardized blind testing environment CAMEO [106] - Continuous Automated Model Evaluation, a complement to CASP that conducts weekly blind tests using around 20 pre-released PDB targets (to provide more continuous feedback, as CASP is biennial)  

# Metrics  

Models are evaluated by comparing each protein’s groundtruth structure with the generated structure. Three different approaches are taken to evaluate structural similarity:  

RMSD - Root-Mean-Square Deviation, directly compares ground-truth positions with generated positions for each amino acid. If each $\delta _ { i }$ denotes the distance between each ground truth and generated amino acid position, with $N$ total amino acids, we have:  

$$
\mathrm { R M S D } = \sqrt { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \delta _ { i } ^ { 2 } }
$$  

GDT-TS [107] - Global Distance Test-Total Score, finds the most optimal superposition between two structures, searching for the highest number of corresponding residues that are within a distance threshold from each other. The GDT-TS aims to represent global fold similarities.   
TM-score [108] - Template Modeling score, a similarity scoring formula that adjusts the GDT metric, normalizing for protein sequence length to avoid dependency on protein size, evaluating all residues beyond just those within the proposed cutoff for a more cohesive score. The TM-score aims to represent both global fold and local structural similarities.   
LDDT [109] - Local-Distance Difference Test, a superpos.- free metric based on local distances between atoms. For each atom pair, its local distance is “preserved” if the generated local distance is within a given threshold of the groundtruth distance, and the proportion of preserved distances is calculated. The LDDT can protect against artificially unfavorable scores when considering flexible proteins with multiple domains.  

# Models  

AlphaFold2 [110] is a landmark model that uses deep learning techniques to compete with experimental methods. AlphaFold2 integrates numerous layers of transformers in an end-to-end approach. The transformers incorporate information from the MSA and pair representations to explore the folding space, potential orientations of amino acids, and overall structure based on pairwise distances. The MSA aligns multiple related protein sequences to create a 2D representation that informs the transformer architecture. Additionally, AlphaFold2 employs invariant point attention (IPA) for spatial attention, while the transformer captures interactions along the chain structure. Notably, AlphaFold2 introduces novel constraints from experimental data, which record probable distances between residues, preferred orientations between residues, and likely dihedral angles for the covalent bonds in the backbone.  

Proposed in 2020, trRosetta [114], i.e., transform-restrained Rosetta, is another model that uses a deep residual network with attention mechanisms. Upon inputting MSA information, trRosetta predicts distances and orientations for residue pairs, which are then utilized to construct the 3D structure using a Rosetta protocol. Despite their advancements, both trRosetta and AlphaFold2 face several challenges, including their reliance on the MSA representation, limitations to natural proteins, and high computational requirements. Recently, RoseTTAFold [113], which replaced trRosetta [114], demonstrated performance comparable to AlphaFold2 based on CASP14 test data. Importantly, RoseTTAFold can generate samples within 10 minutes, which is around 100 times faster than AlphaFold2. RoseTTAFold employs a three-track neural network that simultaneously learns from 1D sequence-level, 2D distance map-level, and 3D backbone coordinate-level information with attention mechanisms integrated throughout. RoseTTAFold exhibits robust performance in predicting protein complexes, whereas AlphaFold2 excels primarily for single protein structure prediction.  

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Type of Model</td><td rowspan="2">Dataset</td><td colspan="4">CAMEO</td><td rowspan="2">CASP14 TMScore (↑)</td></tr><tr><td>RMSD (A,↓)</td><td>TMScore (↑)</td><td>GDT-TS (↑)</td><td>IDDT (↑)</td></tr><tr><td>AlphaFold2** [110]</td><td>Transformer</td><td>CASP14</td><td>3.30 [111]</td><td>0.87 [111]</td><td>0.86 [111]</td><td>0.90 [111]</td><td>0.38[112]</td></tr><tr><td>RoseTTAFold [113]</td><td>Transformer</td><td>CAMEO,CASP14</td><td>5.72 [111]</td><td>0.77 [111]</td><td>0.71 [111]</td><td>0.79 [111]</td><td>0.37 [112]</td></tr><tr><td>ESMFold [112]</td><td>Transformer</td><td>CAMEO,CASP14</td><td>3.99 [111]</td><td>0.85 [111]</td><td>0.83 [111]</td><td>0.87 [111]</td><td>0.68</td></tr><tr><td>EigenFold[111]</td><td>Diffusion</td><td>CAMEO</td><td>7.37</td><td>0.75</td><td>0.71</td><td>0.78</td><td></td></tr></table></body></html>

Table 5. An overview of relevant protein structure prediction models. All metrics are self-reported unless otherwise noted. Scores are provided as mean performance on single-structure tasks. lDDT represents the lDDT metric computed specifically using coordinates for the alpha carbon of each residue. $[ { ^ { * * } } ]$ denotes the current SOTA.  

Building on these techniques, ESMFold [112] is a language model that predicts protein structure by leveraging ESM2 representations. The output embeddings from ESM-2 are passed to a series of self-attending “folding blocks.” A structure module, featuring an SE(3) transformer architecture, generates the final structure predictions. Since ESMFold uses ESM-2 representations instead of MSA representations, this model offers faster processing with competitive scores on CAMEO and CASP14. EigenFold [111] is another model that applies diffusion models to generate protein structures. The model represents the protein as a system of harmonic oscillators. Then, the structure can be projected onto the eigenmodes of that system during the forward process. In the reverse process, the rough global structure is sampled before refining local details. As a score-based model, EigenFold is not as computationally intensive but still underperforms in accuracy and range compared to other models.  

# Antibody Structure Prediction  

A class of models has also been developed specifically catering to antibody structure prediction. As discussed previously, MSA alignment cannot be used for antibody input, rendering general models like AlphaFold highly inefficient and slow in the context of antibody prediction. IgFold [115] uses sequence embeddings from AntiBERTy [116] and invariant point attention to predict antibody structures, achieving SOTA generation speed with comparable accuracy to other models in the field. tFold-Ab [117] performs comparably to IgFold, generating full-atom structures more efficiently by reducing reliance on external tools like Rosetta energy functions. For a more in-depth analysis of the datasets, task definition, metrics, and performance, refer to the appendix on page 27.  

# Sequence Generation  

# Overview  

Sequence generation, also known as inverse folding or fixed-backbone design, entails the inverse task of structure prediction. Generating amino acid sequences that can fold into target structures is crucial for designing proteins with desired structural and functional properties. As with molecules and protein structures, the space of valid sequences is vast; this figure has been estimated to lie between ${ 1 0 } ^ { 6 5 }$ and $1 0 ^ { 1 3 0 }$ [118]. In addition, the process of protein folding is naturally complex and difficult to predict.  

To address these challenges, a variety of deep learning methods have been applied to represent the distribution of sequences with respect to structural information. While we briefly discuss some preliminary methods that are structureagnostic, we place the highest focus on models that target specific protein structures.  

# Task  

Given a fixed protein backbone structure, generate a corresponding amino acid sequence that will fold into the given structure.  

# Datasets  

Models in this field utilize the following datasets. Models primarily use CATH for training, with some using various augmentation methods utilizing UniRef and UniParc. CATH and TS500 are most frequently used for evaluation. To produce a standardized benchmark, Yu et al. [119] created a set of 14 known de novo protein structures that do not exist in CATH to avoid data contamination.  

PDB [91] - Protein Data Bank, a comprehensive protein structure dataset (see page 9)   
UniRef [120] - A clustered version of the Unified Protein KnowledgeBase (UniProtKB), part of the central resource UniProt, which is a curated and labeled set of protein sequences and their functions   
UniParc [120] - A larger dataset of protein sequences, part of the central resource Uniprot, which includes UniProtKB and adds proteins from a variety of other sources   
CATH [121] - A classification of protein domains (subsequences that can fold independently) into a hierarchical scheme with four levels: class (C), architecture (A), topology (T), and homologous superfamily (H). Using their classification, the authors also provide a diverse set of proteins that have minimal overlap and sequence similarity. TS500 [122] - A subset of 500 proteins from PDB [123] filtered by sequence identity using the PISCES network. Li et al. also created a smaller subset, TS50, with additional filters to control for sequence length and fraction of surface residue.  

# Metrics  

While many models perform their own testing, we use results from the independent benchmark created by Yu et al. for fair comparison. We list the evaluated metrics below. Note that while Yu et al. did not evaluate on perplexity, we include it here due to its frequent use in sequence design method evaluations.  

AAR - Amino Acid Recovery, the proportion of matching amino acids between the generated and native sequences Diversity - The average difference between pairs of generated sequences, measured using Clustalw2 [124] RMSD - Root-Mean-Square Deviation, a structural comparison between two structures (see page 9). In the context of sequence generation, the proposed sequences  

are folded into structures before comparison with native backbone structures.   
Nonpolar Loss - A metric measuring the rationality of polar amino acid types within the folded structure, where higher presence of nonpolar amino acids on the surface results in higher loss   
PPL - Perplexity, an exponentiation of cross-entropy loss, representing the inverse likelihood of a native sequence appearing in the predicted sequence distribution. For a series of $N$ amino acids $x _ { 1 } , x _ { 2 } , \dotsc x _ { N }$ , we can express perplexity as:  

$$
{ \mathrm { P P L } } = \exp \left( { \frac { 1 } { N } } \sum _ { i = 1 } ^ { N } \log P ( x _ { i } | x _ { 1 } , x _ { 2 } , \ldots x _ { i - 1 } ) \right)
$$  

Perplexity is calculated individually for each protein in the test set and averaged to produce a final PPL value.  

# Models  

A preliminary class of models generates novel protein sequences without considering a fixed backbone target, aiming to capture the unconditional distribution of amino acid sequence space. ProteinVAE [125] utilizes ProtBERT [126] to reduce raw input sequences into latent representations, employing an encoder-decoder framework with position-wise multi-head self-attention to capture long-range dependencies in these sequences. ProT-VAE [127] uses a different pre-trained language model ProtT5NV [128]. It includes an inner familyspecific encoder-decoder layer to learn parameters relevant to specific protein families. Conversely, ProteinGAN [129] uses a GAN architecture to generate protein sequences. The model’s efficacy is exemplified through the example of malate dehydrogenase, demonstrating its potential to generate fully functional enzyme proteins. While these approaches demonstrate relative success in generating valid and diverse sets of protein sequences, models that operate entirely in sequence space cannot consider crucial structural information. This limitation restricts their ability to capture the full range of constraints and dependencies between amino acid residues.  

The primary class of models in this field receive fixed backbone targets as input, generating corresponding amino acid sequences. ProteinSolver [130] draws connections between generating backbone structures and solving Sudoku puzzles, arguing that both are forms of constraint satisfaction problems (CSP) where positional information imposes constraints on the labels that can be assigned to each “node.” After finding a GNN architecture that can effectively solve Sudoku puzzles, Strokach et al. apply a similar architecture to the task of protein sequence design. In this design, node attributes encode amino acid types, and edge attributes encode relative distances between pairs of residues. PiFold [131] extends this approach by introducing more comprehensive feature representations, including explicit distance, angle, and direction information in its node and edge features. Anand et al. [132] design a 3D CNN that directly learns conditional distributions for each residue given previous amino acid types, relative distances for heavy atoms, and torsional angles for side chains. Using these learned distributions, the 3D CNN autoregressively generates potential sequences. ABACUS-R [133] incorporates a pretrained transformer to infer a residue’s amino acid types from nearby residues. To generate valid sequences, the model iteratively updates subsets of residues based on their environments, gradually constructing self-consistent sequences. ProRefiner [134] improves upon this design by introducing entropy scores for each prediction. While ABACUS-R uses every residue in the neighborhood for refinement, ProRefiner masks out high-entropy (low-confidence) predictions. By filtering out low-quality predictions, ProRefiner mitigates error accumulation from incorrect predictions.  

To better model input protein structures, GPD [138] uses the Graphormer [139] architecture, which is a modified transformer for graph-structured data. GPD also uses Gaussian noise and random masks to improve diversity and recovery. GVP-GNN [135] uses a simple yet novel geometric representation for all nodes in the system. Rather than individually encoding vector features (like relative node orientations) for each node, these features are directly represented as geometric vectors that transform accordingly alongside global transformations. This approach defines a global geometric orientation rather than independent features of each node. ESM-IF1 [136] extends upon the representations in GVP-GNN by attaching a generic transformer block and training on an expanded dataset. To generate additional training examples, ESM-IF1 uses MSA Transformer [140], a representation learning model, to rank the sequences in the UniRef50 dataset by predicted LDDT scores. The top 12 million of these sequences are assigned corresponding structures through predictions made by AlphaFold2, producing a collection of sequence-structure pairs much larger than that of experimentally determined pairs. While previous methods require a fixed decoding order, ProteinMPNN [137] implements an order-agnostic autoregressive approach, which allows for a flexible choice of decoding order based on each specific task.  

We report the benchmark results measured by Yu et al. in Table 6. ProteinMPNN generates the most accurate sequences, leading all methods in sequence recovery, RMSD, and nonpolar loss. GPD remains the most time-efficient method, generating sequences around three times faster than ProteinMPNN. Performance on diversity varies, but this can often be artificially controlled by adjusting a noise hyperparameter during testing to increase variation. Note that ProRefiner is not listed; ProRefiner primarily acts as an addon module for existing methods and reports 2-7 percentage points of improvement on AAR when used to refine sequences for GVP-GNN, ESM-IF1, and ProteinMPNN.  

In general, sequence generation remains a challenging field, as current SOTA models only recover fewer than half of the target amino acid residues.  

# Backbone Design  

# Overview  

Like molecule generation, generating novel proteins from scratch can directly expand the library of available proteins capable of performing highly complex and versatile functions. While other areas such as structure prediction and sequence generation contribute to the overall drug design process, backbone design lies at the core of de novo design, where new protein structures can be created entirely from scratch.  

As seen in molecule design, protein design contains a similar distinction between structure and sequence. Some models generate 1D amino acid sequences, while others directly generate 3D structures, with some co-designing both 1D sequences and 3D structures.  

# Datasets  

Models in this field utilize the following datasets:  

<html><body><table><tr><td>Model</td><td>Type of Model</td><td>Dataset</td><td>AAR(%,↑)</td><td>Div. (↑)</td><td>RMSD (A,↓)</td><td>Non. (↓)</td><td>Time (s,↓)</td></tr><tr><td>ProteinSolver [130]</td><td>GNN</td><td>UniParc</td><td>24.6</td><td>0.186</td><td>5.354</td><td>1.389</td><td>180</td></tr><tr><td>3D CNN [132]</td><td>CNN</td><td>CATH</td><td>44.5</td><td>0.272</td><td>1.62</td><td>1.027</td><td>536544</td></tr><tr><td>ABACUS-R [133]</td><td>Transformer</td><td>CATH</td><td>45.7</td><td>0.124</td><td>1.482</td><td>0.968</td><td>233280</td></tr><tr><td>PiFold [131]</td><td>GNN</td><td>CATH,TS50/TS500</td><td>42.8</td><td>0.141</td><td>1.592</td><td>1.464</td><td>221</td></tr><tr><td>GVP-GNN [135]</td><td>GNN</td><td>CATH,TS50</td><td>44.9* [135]</td><td></td><td></td><td></td><td></td></tr><tr><td>ESM-IF1** [136]</td><td>Transformer</td><td>CATH,UniRef+</td><td>47.7</td><td>0.184</td><td>1.265</td><td>1.201</td><td>1980</td></tr><tr><td>ProteinMPNN** [137]</td><td>MPNN</td><td>CATH</td><td>48.7</td><td>0.168</td><td>1.019</td><td>1.061</td><td>112</td></tr><tr><td>GPD[138]</td><td>Transformer</td><td>CATH</td><td>46.2</td><td>0.219</td><td>1.758</td><td>1.333</td><td>35</td></tr></table></body></html>

Table 6. An overview of relevant sequence design methods. Div. and Non. refer to diversity and nonpolar loss, respectively. Results are reported by Yu et al. [119]. For GVP-GNN, we report self-evaluated AAR on a CATH test split. $[ { ^ { * * } } ]$ denotes the current SOTA.  

PDB [91] - Protein Data Bank, a comprehensive protein structure dataset (see page 9)   
AlphaFoldDB [141] - AlphaFold Database, an expanded protein structural dataset created by using AlphaFold2 to predict structures for corresponding sequences in the UniRef dataset   
SCOP [142] - Structural Classification of Proteins, a classification of proteins by homology and structural similarity. SCOP has been updated several times to include additional categorizations and features, with many recent models using the extended SCOPe [143] database.   
CATH [121] - A classification of protein domains into a hierarchical scheme with four levels, also used in sequence generation (see page 10)  

# Tasks  

The backbone design task involves designing a protein backbone structure either from scratch or based on existing context. This involves generating coordinates for the backbone atoms for each amino acid (nitrogen, alpha-carbon, carbonyl, and oxygen atom). External tools like Rosetta [144] can be used for side-chain packing, generating the remaining atoms.  

Context-Free Generation - Given no input, the goal is to generate a diverse set of protein structures. This task is evaluated using the self-consistency TM (scTM) score. Context-Given Generation - This is an inpainting task for proteins. Given a motif (a set of existing amino acid residues for a native protein), the goal is to accurately fill in the missing residues according to the native protein, which is evaluated using a variety of similarity metrics like AAR, PPL, and RMSD.  

# Metrics  

Generated backbones should be highly designable. High designability is generally determined by the ability of a structure to be generated by a corresponding amino acid sequence. While lab testing is optimal, this is not always feasible, so the folding process is simulated by other generative models. Thus, Trippe et al. [145] proposed the scTM approach, which includes the following steps: a proposed structure is fed into a sequence-prediction model (typically ProteinMPNN [137]) to produce a corresponding amino acid sequence, and then fed back into a structure prediction model (typically AlphaFold2 [110]) to produce a sample structure. The TM-score (see page 9) between the generated structure and this sample structure is calculated.  

scTM [145] - Self-consistency TM-score, an approach proposed by Trippe et al. to simulate the folding process, described in detail above. Scores of scTM $> 0 . 5$ are typically  

considered designable, so the percentage of generated structures with $\mathrm { s c T M } > 0 . 5$ is often reported.   
scRMSD - Self-consistency RMSD, identical to the scTM but uses RMSD instead of the TM-score for evaluation. A score of scRMSD $< 2$ is typically used as a cutoff.   
AAR - Amino acid recovery, a comparison between the ground truth and the generated amino acid sequences. The AAR is also measured in antibody representation learning (see page 26).   
RMSD - Root-mean-square deviation, measures distances between the ground truth and the generated residue coordinates. The RMSD is also used in protein structure prediction (see page 9).  

# Models  

ProtDiff [145] represents each residue with 3D Cartesian coordinates and uses a particle filtering diffusion approach. However, 3D Cartesian point clouds do not mirror the folding process to create protein structures— FoldingDiff [147] instead uses an angular representation for the protein structure, which more closely mirrors the rotational energy-optimizing protein folding process. FoldingDiff treats the protein backbone structure as a sequence of six angles representing orientations of consecutive residues. It denoises from a random, unfolded state to a folded structure using a DDPM and a BERT architecture. LatentDiff [146] initially uses an equivariant protein autoencoder with GNNs to embed proteins into a latent space. Subsequently, it uses an equivariant diffusion model to learn the latent distribution. This process is analogous to GeoLDM [77] for molecule design. Notably, LatentDiff’s sampling on its latent space is ten times faster than sampling on raw protein space.  

The above models have shown relatively high performance in generating shorter proteins (up to 128 residues in length) but struggle with larger and more complex proteins [149]. To address longer protein structures, other methods use frame-based construction methods. This representation was initially demonstrated in the architecture of AlphaFold2 [110] in structure prediction, known as IPA. In this paradigm, each residue is represented by orientation-preserving rigid body transformations (reference frames), which can be consistently defined regardless of global orientation. This allows for a more generalized representation than a series of 3D point clouds. Genie [149] performs discrete-time diffusion using a cloud of frames determined by a translation and rotation element to generate backbone structures. During each diffusion step, Genie computes the Frenet-Senet frames and uses paired residue representations and IPA for noise prediction. FrameDiff [148] also parameterizes the backbone structures based on the frame manifold, using a score-based generative model. This approach establishes a diffusion process on  

Table 7. An overview of relevant backbone design methods. “AFDB” refers to AlphaFoldDB. “Design” refers to designability, defined by Lin et al. [149], where any proteins with scRMSD $< 2$ and pLDDT $> 7 0$ (a local distan metric used in AlphaFold2 [110]) are considered designable. All metrics are evaluated with the PDB dataset, while [\*] denotes results tested only on $\beta$ -lactamase metalloproteins extracted from PDB. $[ { ^ { * * } } ]$ denotes the current SOTA.   


<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Type of Model</td><td rowspan="2">Dataset</td><td colspan="2">Context-Free</td><td colspan="3">Context-Given</td></tr><tr><td>scTM (%，↑)</td><td>Design. (%,↑)</td><td>PPL (↓)</td><td>AAR (%,↑)</td><td>RMSD (A,↓)</td></tr><tr><td>LatentDiff [146]</td><td>EGNN,Diffusion</td><td>PDB,AFDB</td><td>31.6</td><td></td><td></td><td></td><td></td></tr><tr><td>FoldingDiff[147]</td><td>Diffusion</td><td>CATH</td><td>14.2 [146]</td><td></td><td></td><td></td><td></td></tr><tr><td>FrameDiff [148]</td><td>Diffusion</td><td>PDB</td><td>84</td><td>48.3 [149]</td><td></td><td></td><td></td></tr><tr><td>Genie [149]</td><td>Diffusion</td><td>SCOPe,AFDB</td><td>81.5</td><td>79.0</td><td></td><td></td><td></td></tr><tr><td>RFDiffusion** [150]</td><td>Diffusion</td><td>PDB</td><td></td><td>95.1 [149]</td><td></td><td></td><td></td></tr><tr><td>ProtDiff [145]</td><td>EGNN,Diffusion</td><td>PDB</td><td>11.8 [146]</td><td></td><td></td><td>12.47* [151]</td><td>8.01* [151]</td></tr><tr><td>GeoPro [151]</td><td>EGNN</td><td>PDB</td><td></td><td></td><td>一</td><td>43.41*</td><td>2.98*</td></tr><tr><td>ProtSeed [152]</td><td>MLP</td><td>CATH</td><td></td><td></td><td>5.6</td><td>43.8</td><td></td></tr><tr><td>Protpardelle [153]</td><td>Diffusion</td><td>CATH</td><td>85</td><td></td><td></td><td></td><td></td></tr></table></body></html>  

$S E ( 3 ) ^ { N }$ , the manifold of frames, invariant to translations and rotations. Then, the neural network predicts the denoised frame and torsion angle using IPA and a transformer model. Finally, RFDiffusion [150] combines the powerful structure prediction methods from RoseTTAFold with diffusion models to generate protein structures. RFDiffusion fine-tunes the RoseTTAFold weights and inputs a masked input sequence and random noise coordinates to iteratively generate the backbone structure. RFDiffusion also “self-conditions” on the predicted final structure, leading to improved performance. RFDiffusion is a large pre-trained model with significantly more parameters than the other two frame-based models, enabling it to outperform the other frame-based models. GPDL [154] utilizes a similar technique to RFDiffusion, using ESMFold instead of RoseTTAFold as its base structure prediction model. Additionally, it incorporates the ESM2 language model to extract important evolutionary information from input sequences to ESMFold. Due to ESMFold’s superior efficiency in structure prediction, GPDL generates backbone structures 10-20 times faster than RFDiffusion.  

Another class of models aim to co-design both protein sequence and structure simultaneously. GeoPro [151] uses an EGNN to encode and predict 3D protein structures, designing a separate decoder to decode protein sequences from these learned representations. Protpardelle [155] creates a “superposition” over the possible sidechain states and collapses them during each iterative update step in the reverse diffusion process. The backbone is updated in each iterative step, while the sidechains are chosen probabilistically by another network to update. ProtSeed [152] uses a trigonometry-aware encoder that computes constraints and interactions from the context features and uses an equivariant decoder to translate proteins into their desired state, updating the sequence and structure in a one-shot manner. Anand et al. [132] use IPA as mentioned above, performing diffusion in frame space to efficiently generate protein sequences and structures.  

For an overview of developments described in this section, see Figure 4. Note that as seen in molecule generation, we observe a progression from 1D-based (amino acid) models to 3D structure-based models to 1D/3D co-design; in addition, the field of protein design faces analogous questions of complexity scaling and latent space regularization.  

# Antibody CDR-H3 Generation  

As mentioned previously, antibody generation primarily focuses on the generation of a particular region known as the CDRH3 region. Similar to protein generation, models in CDR-H3 generation have transitioned from sequence-based methods like the LSTM used by Akbar et al. [156] to sequence-structure co-design pioneered by RefineGNN [157] through iterative refinement. Notably, some models extend beyond the CDRH3 generation task, aiming to tackle multiple parts of the antibody pipeline at once. dyMEAN [158] is an end-to-end method incorporating structure prediction, docking, and CDRH3 generation into a singular model. For a more in-depth analysis of datasets, task definition, metrics, and performance, refer to the appendix on page 28.  

# Peptide Design  

# Overview  

While we have discussed the monumental and robust models developed for protein generation, it is necessary to have models tailored for peptide-specific needs due to the inherently intricate and context-dependent nature of peptide structure, as well as the highly diverse array of downstream applications [159]. This section briefly explores four different applications for AI in peptide generation, focusing on four state-of-the-art models: MMCD (Multi-Modal Contrastive Diffusion Model), PepGB (Peptide-protein interaction via Graph neural network for Binary prediction), PepHarmony, and AdaNovo.  

# Peptide Generation  

In peptide generation, like protein backbone design, models aim to generate novel peptides from scratch. MMCD [160] is a diffusion-based model for therapeutic peptide generation that co-designs peptide sequences and structures (backbone coordinates). It employs a transformer encoder for sequences and an EGNN for structures, along with contrastive learning strategies to align sequence and structure embeddings and differentiate therapeutic and non-therapeutic peptide embeddings. MMCD outperforms baselines in both sequence and structure generation tasks, as demonstrated by testing on datasets of antimicrobial peptides and anticancer peptides.  

# Peptide-Protein Interaction  

For peptide-protein interactions, models aim to predict the physical binding site for a proposed peptide-protein pair. PepGB [161] is a GNN-based model for facilitating peptide drug discovery. It predicts peptide-protein interactions and leverages graph attention neural networks to learn interactions between peptides and proteins. PepGB was trained on a binary interaction benchmark dataset of protein-peptide and protein-protein interactions. A mutation dataset of peptide analogs targeting MDM2 was used for validating PepGB, and a large-scale peptide sequence dataset from UniProt was used for pre-training. PepGB consistently outperforms baselines in predicting peptide-protein interactions for novel peptides and proteins, with an increase of at least $9 \%$ , $9 \%$ , and $2 7 \%$ in AUCprecision scores and $1 9 \%$ , $6 \%$ , and $4 \%$ in AUC-recall scores under novel protein, peptide, and pair settings, respectively.  

![](images/a577de0976dcc690b1589ee8e5c8946fc0cbc95ee2a90cd1f8b6f10e6f407346.jpg)  
Fig. 4. An overview of the progress in protein generation over time. Shortcomings of previous models are shown in the corresponding pink boxes, with subsequent models solving these shortcomings through novel design choices [127, 145, 147, 146, 148, 152]. For consistency, only methods that generate proteins from scratch (without fixed backbone or sequence input) are depicted.  

# Peptide Representation Learning  

As with protein representation learning, models in peptide representation learning aim to convert raw peptide sequences into latent representations that capture valuable information. PepHarmony [162] is a multi-view contrastive learning model that integrates both sequence and structural information for enhanced peptide representation learning. It employs a sequence encoder (ESM) and a structure encoder (GearNet), which are trained together using contrastive or generative learning. PepHarmony utilizes data from conventional datasets like AlphaFoldDB and PDB while also employing a cellpenetrating peptide dataset (compiled from a variety of existing datasets), a solubility dataset (PROSOS-II [163]), and an affinity dataset (DrugBank [164]). Zhang et al. report that PepHarmony demonstrates superior performance in downstream tasks such as cell-penetrating peptide prediction, peptide solubility prediction, peptide-protein affinity prediction, and self-contact map prediction. When compared to general protein representation learning methods like ESM2 and GearNet, PepHarmony outperforms baseline and fine-tuned versions of these models in most evaluation metrics, including accuracy, F1 score, area under the receiver operating characteristic curve, and correlation coefficients.  

# Peptide Sequencing  

Mass spectrometry has played a crucial role in analyzing protein compositions from physical samples, but various forms of noise have posed challenges in extracting information from these reports. In peptide sequencing, models aim to address this challenge by predicting amino acid sequences given mass spectra data. AdaNovo [165] is a state-of-the-art model for de novo peptide sequencing, composed of a mass spectrum encoder (MS Encoder) and two peptide decoders inspired by the transformer architecture. AdaNovo significantly improves upon previous models like DeepNovo [166], PointNovo [167], and Casanovo [168] in terms of peptide-level and amino acidlevel precision across various species. For example, in a human dataset, AdaNovo achieves a peptide-level precision of 0.373 and an amino acid-level precision of 0.618, outperforming DeepNovo (0.293 and 0.610), PointNovo (0.351 and 0.606), and Casanovo (0.343 and 0.585). AdaNovo’s success is attributed to its innovative use of conditional mutual information and adaptive training strategies, which enhance its ability to identify post-translational modifications and handle noisy data typically associated with mass spectrometry.  

# Current Trends  

The drug design process, marked by a history of high complexity and cost, is poised for a transformative shift fueled by generative AI. AI-based methods are driving faster development and reducing costs, resulting in more effective and accessible pharmaceuticals for the public. Within the realm of generative AI, a notable shift has occurred; the emergence of GNNs and graph-based methods has fueled the transition from sequence-based approaches to structure-based approaches, ultimately leading to the integration of both sequence and structure in generation tasks.  

Within the field of molecular generation, we are witnessing the recent dominance of graph-based diffusion models. These models take advantage of E(3) equivariance to achieve SOTA performance, with leaders like GeoLDM and MiDi excelling in target-agnostic design, and TargetDiff, Pocket2Mol, and DiffSBDD excelling in target-aware design. Finally, Torsional Diffusion outperforms all counterparts in molecular conformation generation. Additionally, we observe a shift from sequence to structural approaches in target-aware molecule design, where SBDD approaches demonstrate clear advantages over LBDD approaches, which operate with amino acid sequences.  

Within protein generation, a shift from sequence to structure is evident, as exemplified by the emergence of structure-based representation learning models like GearNET. These models leverage established sequence-based representation models such as ESM-1B and UniRep, recognizing the importance of 3D structural information in the protein generation process. AlphaFold2 remains a clear SOTA model for structure prediction. Similar to molecule generation, a wave of diffusion models are now tackling the protein scaffolding task, with RFDiffusion emerging as the top-performing model.  

# Challenges and Future Directions  

While the prospects for generative AI in drug design are promising, several issues must be addressed before we can embrace ML-driven de novo drug design. The main areas for improvement include increasing performance in existing tasks, defining more applicable tasks within molecule, protein, and antibody generation, and exploring entirely new areas of research.  

Current generative models still struggle with a variety of tasks and benchmarks. Within molecule generation, we face the following challenges:  

Complexity - Models generate high frequencies of valid and   
stable molecules when trained on the simple QM9 dataset but struggle when trained on the more complex GEOMDrugs dataset.   
Applicability - More applicable tasks like protein-molecule   
binding are especially challenging, and current models   
still struggle with generating molecules with high binding affinity for targets.   
Explainability - All methods discussed are fairly blackbox and abstract; existing models do not reveal aspects   
like “important” atoms or structures, and explainable AI in molecule generation is undeveloped as a whole.  

Within protein generation, we encounter the following challenges:  

Benchmarking - While most models in molecule generation use standardized benchmarking procedures, generative tasks in protein design lack a standard evaluative procedure, with variance between each model’s own metrics and testing conditions, making it hard to objectively evaluate the quality of designed proteins. Performance - As the tasks in protein generation are generally more complex than those in molecule generation, SOTA models still struggle in several key areas like fold classification, gene ontology, and antibody CDR H3 generation, leaving room for future improvement.  

While our paper focuses on generative models and applications, it is important to note that many current tasks are evaluated with predictive models, such as the affinity optimization task in antibody generation or the conditional generation task for molecules. In these cases, classifier networks are used to predict binding affinity or molecular properties, and improvements to these classification methods would naturally lead to more precise alignment with real-world biological applications.  

# Conclusion  

The survey has provided an overview of the current landscape of generative AI in de novo drug design, focusing on molecule and protein generation. It has discussed important advancements in these fields, detailing the key datasets, model architectures, and evaluation metrics used. The paper also highlights key challenges and future directions, including improvements to benchmarking methods, improving explainability, and further alignment with real-world tasks to increase applicability. Overall, generative AI has shown great promise in the field of drug design, and continued research in this field can lead to exciting advancements in the future.  

# Key Points  

Our survey examines the advancements and applications of Generative AI within de novo drug design, particularly focusing on the generation of novel small molecules and proteins. We explore the intricacies of generating biologically plausible and pharmaceutically potential compounds from scratch, providing a comprehensive yet approachable digest  

of formal task definitions, datasets, benchmarks, and model types in each field.   
The work captures the progression of AI model architectures in drug design, highlighting the emergence of equivariant graph neural networks and diffusion models as key drivers in recent work.   
We highlight remaining challenges in applicability, performance, and scalability, delineating future research trajectories.   
Through our organized repository, we aim to facilitate further collaboration in the rapidly evolving intersection of computational biology and artificial intelligence.  

# Funding  

Xiangru Tang and Mark Gerstein are supported by Schmidt Futures.  

# References  

1. Jurgen Drews. Drug discovery: a historical perspective. science, 287(5460):1960–1964, 2000.   
2. Soma Mandal, Sanat K Mandal, et al. Rational drug design. European journal of pharmacology, 625(1-3):90– 100, 2009.   
3. Lucy J Colwell. Statistical and machine learning approaches to predicting protein–ligand interactions. Current opinion in structural biology, 49:123–128, 2018.   
4. Christopher Horvath. Comparison of preclinical development programs for small molecules (drugs/pharmaceuticals) and large molecules (biologics/biopharmaceuticals): studies, timing, materials, and costs. Pharmaceutical Sciences Encyclopedia: Drug Discovery, Development, and Manufacturing, pages 1–35, 2010.   
5. Gregory Sliwoski, Sandeepkumar Kothiwale, Jens Meiler, and Edward W Lowe. Computational methods in drug discovery. Pharmacological reviews, 66(1):334–395, 2014.   
6. Petra Schneider, W Patrick Walters, Alleyn T Plowright, Norman Sieroka, Jennifer Listgarten, Robert A Goodnow Jr, Jasmin Fisher, Johanna M Jansen, Jose´ S Duca, Thomas S Rush, et al. Rethinking drug design in the artificial intelligence era. Nature Reviews Drug Discovery, 19(5):353–364, 2020.   
7. Yankang Jing, Yuemin Bian, Ziheng Hu, Lirong Wang, and Xiang-Qun Sean Xie. Deep learning for drug design: an artificial intelligence paradigm for drug discovery in the big data era. The AAPS journal, 20:1–10, 2018.   
8. Pavel Polishchuk. Interpretation of quantitative structure–activity relationship models: past, present, and future. Journal of Chemical Information and Modeling, 57(11):2618–2639, 2017.   
9. Chartchalerm Isarankura-Na-Ayudhya, Thanakorn Naenna, Chanin Nantasenamat, and Virapong Prachayasittikul. A practical overview of quantitative structure-activity relationship. 2009.   
10. Zheng Li, Siwen Wang, Wei Shan Chin, Luke E Achenie, and Hongliang Xin. High-throughput screening of bimetallic catalysts enabled by machine learning. Journal of Materials Chemistry A, 5(46):24131–24138, 2017.   
11. Hongjian Li, Kam-Heung Sze, Gang Lu, and Pedro J Ballester. Machine-learning scoring functions for structure-based virtual screening. Wiley Interdisciplinary Reviews: Computational Molecular Science, 11(1):e1478, 2021.   
12. Kevin K Yang, Zachary Wu, and Frances H Arnold. Machine-learning-guided directed evolution for protein engineering. Nature methods, 16(8):687–694, 2019.   
13. Zachary Wu, SB Jennifer Kan, Russell D Lewis, Bruce J Wittmann, and Frances H Arnold. Machine learningassisted directed protein evolution with combinatorial libraries. Proceedings of the National Academy of Sciences, 116(18):8852–8858, 2019.   
14. Markus Hartenfeller and Gisbert Schneider. De novo drug design. Chemoinformatics and computational chemical biology, pages 299–323, 2011.   
15. Varnavas D Mouchlis, Antreas Afantitis, Angela Serra, Michele Fratello, Anastasios G Papadiamantis, Vassilis Aidinis, Iseult Lynch, Dario Greco, and Georgia Melagraki. Advances in de novo drug design: from conventional to machine learning methods. International journal of molecular sciences, 22(4):1676, 2021.   
16. Angelica Nakagawa Lima, Eric Allison Philot, Gustavo Henrique Goulart Trossini, Luis Paulo Barbour Scott, Vin´ıcius Gon¸calves Maltarollo, and Kathia Maria Honorio. Use of machine learning approaches for novel drug discovery. Expert opinion on drug discovery, 11(3):225– 239, 2016.   
17. Meiqiu Dong, Xinrui Miao, Romain Brisse, Wenli Deng, Bruno Jousselme, and Fabien Silly. Molecular trapping in two-dimensional chiral organic kagom´e nanoarchitectures composed of baravelle spiral triangle enantiomers. NPG Asia Materials, 12(1):20, 2020.   
18. Lilian Weng. What are diffusion models?, Jul 2021.   
19. Jagadeesan Ganapathy, Kannan Damodharan, Bakthadoss Manickam, and Aravindhan Sanmargam. Crystal and molecular structure of 4, 6-dimethyl-9- phenyl-8, 12-dioxa-4, 6-diazatetracyclo [8.8. 0.02, 7.013, 18] octadeca-2 (7), 13, 15, 17-tetraene-3, 5, 11-trione 2-ethoxyphenyl (2e)-but-2-enoate. Journal of the Chosun Natural Science, 6(4):197–204, 2013.   
20. Thalles Santos Silva. A few words on representation learning, Apr 2021.   
21. Protein structure prediction.   
22. Embl-Ebi. Levels of protein structure – quaternary.   
23. Jun Zhao, Ruth Nussinov, Wen-Jin Wu, and Buyong Ma. In silico methods in antibody design. Antibodies, 7(3):22, 2018.   
24. Mingyang Wang, Zhe Wang, Huiyong Sun, Jike Wang, Chao Shen, Gaoqi Weng, Xin Chai, Honglin Li, Dongsheng Cao, and Tingjun Hou. Deep learning approaches for de novo drug design: An overview. Current Opinion in Structural Biology, 72:135–144, 2022.   
25. Peter S Kutchukian and Eugene I Shakhnovich. De novo design: balancing novelty and confined chemical space. Expert opinion on drug discovery, 5(8):789–812, 2010.   
26. Xuhan Liu, Adriaan P IJzerman, and Gerard JP van Westen. Computational approaches for de novo drug design: past, present, and future. Artificial neural networks, pages 139–165, 2020.   
27. Joseph A DiMasi, Henry G Grabowski, and Ronald W Hansen. The cost of drug development. New England Journal of Medicine, 372(20):1972–1972, 2015.   
28. Shaun M Lippow and Bruce Tidor. Progress in computational protein design. Current opinion in 18(4):305-311 2007   
29. Wei Zhou, Yonghua Wang, Aiping Lu, and Ge Zhang. Systems pharmacology in small molecular drug discovery. International journal of molecular sciences, 17(2):246, 2016.   
30. G Richard Bickerton, Gaia V Paolini, Je´re´my Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012.   
31. Oleg Ursu, Anwar Rayan, Amiram Goldblum, and Tudor I Oprea. Understanding drug-likeness. Wiley Interdisciplinary Reviews: Computational Molecular Science, 1(5):760–781, 2011.   
32. Pavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of drug-like chemical space based on gdb-17 data. Journal of computer-aided molecular design, 27:675–679, 2013.   
33. Joseph A DiMasi, Henry G Grabowski, and Ronald W Hansen. Innovation in the pharmaceutical industry: new estimates of r&d costs. Journal of health economics, 47:20–33, 2016.   
34. Madura KP Jayatunga, Wen Xie, Ludwig Ruder, Ulrik Schulze, and Christoph Meier. Ai in small-molecule drug discovery: A coming wave. Nat. Rev. Drug Discov, 21:175–176, 2022.   
35. Wenze Ding, Kenta Nakai, and Haipeng Gong. Protein design via deep learning. Briefings in bioinformatics, 23(3):bbac102, 2022.   
36. Wenhao Gao, Sai Pooja Mahajan, Jeremias Sulam, and Jeffrey J Gray. Deep learning in protein structural modeling and design. Patterns, 1(9), 2020.   
37. Po-Ssu Huang, Scott E Boyken, and David Baker. The coming of age of de novo protein design. Nature, 537(7620):320–327, 2016.   
38. Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Jiazhang Lian, Qiang Zhang, and Huajun Chen. Ontoprotein: Protein pretraining with gene ontology embedding. arXiv preprint arXiv:2201.11147, 2022.   
39. Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Cheng Bian, and Yizhou Yu. Protein representation learning via knowledge enhanced primary structure modeling. bioRxiv, pages 2023–01, 2023.   
40. Chang Ma, Haiteng Zhao, Lin Zheng, Jiayi Xin, Qintong Li, Lijun Wu, Zhihong Deng, Yang Lu, Qi Liu, and Lingpeng Kong. Retrieved sequence augmentation for protein representation learning. bioRxiv, pages 2023–02, 2023.   
41. Philip A Romero and Frances H Arnold. Exploring protein fitness landscapes by directed evolution. Nature reviews Molecular cell biology, 10(12):866–876, 2009.   
42. Bassil I Dahiyat and Stephen L Mayo. De novo protein design: fully automated sequence selection. Science, 278(5335):82–87, 1997.   
43. Zaixi Zhang, Jiaxian Yan, Qi Liu, and Enhong Che. A systematic survey in geometric deep learning for structurebased drug design. arXiv preprint arXiv:2306.11768, 2023.   
44. Morgan Thomas, Andreas Bender, and Chris de Graaf. Integrating structure-based approaches in generative molecular design. Current Opinion in Structural Biology, 79:102559, 2023.   
45. Rahmad Akbar, Habib Bashour, Puneet Rawat, Philippe A Robert, Eva Smorodina, Tudor-Stefan Cotet, Karine Flem-Karlsen, Robert Frank, Brij Bhushan Mehta, Mai Ha Vu, et al. Progress and challenges for the machine learning-based design of fit-for-purpose monoclonal antibodies. In MAbs, volume 14, page   
2008790. Taylor & Francis, 2022.   
46. Alissa M Hummer, Brennan Abanades, and Charlotte M Deane. Advances in computational structure-based antibody design. Current Opinion in Structural Biology,   
74:102379, 2022.   
47. Michael Chungyoun and Jeffrey J Gray. Ai models for protein design are driving antibody engineering. Current Opinion in Biomedical Engineering, page 100473, 2023.   
48. Jisun Kim, Matthew McFee, Qiao Fang, Osama Abdin, and Philip M Kim. Computational and artificial intelligence-based methods for antibody development. Trends in Pharmacological Sciences, 2023.   
49. Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, and Chaoning Zhang. A survey on graph diffusion models: Generative ai in science for molecule, protein and material. arXiv preprint arXiv:2304.01565, 2023.   
50. Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, and Jianlin Cheng. Diffusion models in bioinformatics: A new wave of deep learning revolution in action. arXiv preprint arXiv:2302.10907, 2023.   
51. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   
52. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   
53. Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530–1538. PMLR, 2015.   
54. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796, 2022.   
55. Tim Van Erven and Peter Harremos. Re´nyi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60(7):3797–3820, 2014.   
56. Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981–17993,   
2021.   
57. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   
58. Marius-Constantin Popescu, Valentina E Balas, Liliana Perescu-Popescu, and Nikos Mastorakis. Multilayer perceptron and neural networks. WSEAS Transactions on Circuits and Systems, 8(7):579–588, 2009.   
59. Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.   
60. Jiquan Ngiam, Zhenghao Chen, Pang W Koh, and Andrew Y Ng. Learning deep energy models. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 1105–1112, 2011.   
61. Kristof Sch¨”utt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M¨”uller. Schnet: A continuousfilter convolutional neural network for modeling quantum interactions. Advances in neural information processing systems, 30, 2017.   
62. Sepp Hochreiter and Ju¨rgen Schmidhuber. Long shortterm memory. Neural computation, 9(8):1735–1780, 1997.   
63. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   
64. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008.   
65. Vıctor Garcia Satorras0, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323–9332. PMLR, 2021.   
66. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263–1272. PMLR, 2017.   
67. Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   
68. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.   
69. Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.   
70. Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang, Jianfei Cai, et al. Recent advances in convolutional neural networks. Pattern recognition, 77:354–377, 2018.   
71. Keiron O’Shea and Ryan Nash. An introduction to convolutional neural networks. arXiv preprint arXiv:1511.08458, 2015.   
72. Xiangru Tang, Andrew Tran, Jeffrey Tan, and Mark B Gerstein. Mollm: A unified language model for integrating biomedical text with 2d and 3d molecular representations. bioRxiv, pages 2023–11, 2023.   
73. Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1–7, 2014.   
74. Simon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data, 9(1):185, 2022.   
75. Clement Vignac and Pascal Frossard. Top-n: Equivariant set and graph generation without exchangeability. arXiv preprint arXiv:2110.02096, 2021.   
76. Niklas Gebauer, Michael Gastegger, and Kristof Sch¨”utt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. Advances in neural information processing systems, 32, 2019.   
77. Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. arXiv preprint arXiv:2305.01140, 2023.   
78. Victor Garcia Satorras, Emiel Hoogeboom, Fabian Fuchs, Ingmar Posner, and Max Welling. E (n) equivariant normalizing flows. Advances in Neural Information Processing Systems, 34:4181–4192, 2021.   
79. Alex Morehead and Jianlin Cheng. Geometry-complete diffusion for 3d molecule generation. arXiv preprint arXiv:2302.04313, 2023.   
80. Lei Huang, Hengtong Zhang, Tingyang Xu, and Ka-Chun Wong. Mdm: Molecular diffusion model for 3d molecule generation. arXiv preprint arXiv:2209.05710, 2022.   
81. Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. Learning joint 2d & 3d diffusion models for complete molecule generation. arXiv preprint arXiv:2305.12347,   
2023.   
82. Clement Vignac, Nagham Osman, Laura Toni, and Pascal Frossard. Midi: Mixed graph and 3d denoising diffusion for molecule generation. arXiv preprint arXiv:2302.09048,   
2023.   
83. Emiel Hoogeboom, Vıctor Garcia Satorras, Cle´ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International Conference on Machine Learning, pages 8867–8887. PMLR, 2022.   
84. Rafael G´omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose´ Miguel Herna´ndez-Lobato, Benjamı´n S´anchez-Lengeling, Dennis Sheberla, Jorge AguileraIparraguirre, Timothy D Hirzel, Ryan P Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018.   
85. Matt J Kusner, Brooks Paige, and Jose´ Miguel Hern´andezLobato. Grammar variational autoencoder. In International conference on machine learning, pages   
1945–1954. PMLR, 2017.   
86. Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoencoder for structured data. arXiv preprint arXiv:1802.08786, 2018.   
87. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pages 2323–2332. PMLR, 2018.   
88. Paul G. Francoeur, Tomohide Masuda, Jocelyn Sunseri, Andrew Jia, Richard B. Iovanisci, Ian Snyder, and David R. Koes. Three-dimensional convolutional neural networks and a cross-docked data set for structure-based drug design. Journal of Chemical Information and Modeling, 60(9):4200–4215, 2020. PMID: 32865404.   
89. Liegi Hu, Mark L Benson, Richard D Smith, Michael G Lerner, and Heather A Carlson. Binding moad (mother of all databases). Proteins: Structure, Function, and Bioinformatics, 60(3):333–340, 2005.   
90. John J. Irwin, Khanh G. Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R. Wong, Munkhzul Khurelbaatar, Yurii S. Moroz, John Mayfield, and Roger A. Sayle. Zinc20—a free ultralarge-scale chemical database for ligand discovery. Journal of Chemical Information and Modeling, 60(12):6065–6073, 2020. PMID: 33118813.   
91. Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1):235–242, 2000.   
92. Oleg Trott and Arthur J. Olson. Autodock vina: Improving the speed and accuracy of docking with new scoring function, efficient optimization, and multithreading. Journal of Computational Chemistry, 31(2):455–461, 2010.   
93. Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1:1–11, 2009.   
94. Taffee T Tanimoto. Elementary mathematical theory of classification and prediction. 1958.   
95. Yuesen Li, Chengyi Gao, Xin Song, Xiangyu Wang, Yungang Xu, and Suxia Han. Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins. bioRxiv, 2023.   
96. Tomohide Masuda, Matthew Ragoza, and David Ryan Koes. Generating 3d molecular structures conditional on a receptor binding site with deep generative models. arXiv preprint arXiv:2010.14442, 2020.   
97. Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In International Conference on Machine Learning, pages 17644–17655. PMLR, 2022.   
98. Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3d generative model for structure-based drug design. Advances in Neural Information Processing Systems, 34:6229–6239, 2021.   
99. Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. arXiv preprint arXiv:2303.03543, 2023.   
100. Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Li´o, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695, 2022.   
101. Michael J Lopez and Shamim S Mohiuddin. Biochemistry, essential amino acids. 2020.   
102. Areski Flissi, Emma Ricart, Cle´mentine Campart, Mickael Chevalier, Yoann Dufresne, Juraj Michalik, Philippe Jacques, Christophe Flahaut, Frederique Lisacek, Vale´rie Lecle\`re, et al. Norine: Update of the nonribosomal peptide resource. Nucleic acids research, 48(D1):D465–D469, 2020.   
103. Christian M-R Lemer, Marianne J Rooman, and Shoshana J Wodak. Protein structure prediction by threading methods: evaluation of current techniques. Proteins: Structure, Function, and Bioinformatics, 23(3):337–355, 1995.   
104. Elmar Krieger, Sander B Nabuurs, and Gert Vriend. Homology modeling. Structural bioinformatics, 44:509– 523, 2003.   
105. Andriy Kryshtafovych, Torsten Schwede, Maya Topf, Krzysztof Fidelis, and John Moult. Critical assessment of methods of protein structure prediction (casp)—round xiv. Proteins: Structure, Function, and Bioinformatics, 89(12):1607–1617, 2021.   
106. J¨”urgen Haas, Alessandro Barbato, Dario Behringer, Gabriel Studer, Steven Roth, Martino Bertoni, Khaled Mostaguir, Rafal Gumienny, and Torsten Schwede. Continuous automated model evaluation (cameo) complementing the critical assessment of structure prediction in casp12. Proteins: Structure, Function, and Bioinformatics, 86:387–398, 2018.   
107. Adam Zemla. Lga: a method for finding 3d similarities in protein structures. Nucleic acids research, 31(13):3370– 3374, 2003.   
108. Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template quality. Proteins: Structure, Function, and Bioinformatics, 57(4):702–710, 2004.   
109. Valerio Mariani, Marco Biasini, Alessandro Barbato, and Torsten Schwede. lddt: a local superposition-free score for comparing protein structures and models using distance difference tests. Bioinformatics, 29(21):2722–2728, 2013.   
110. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zˇ´ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.   
111. Bowen Jing, Ezra Erives, Peter Pao-Huang, Gabriele Corso, Bonnie Berger, and Tommi Jaakkola. Eigenfold: Generative protein structure prediction with diffusion models. arXiv preprint arXiv:2304.02198, 2023.   
112. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123–1130, 2023.   
113. Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, Claudia Mill´an, Hahnbeom Park, Carson Adams, Caleb R. Glassman, Andy DeGiovanni, Jose H. Pereira, Andria V. Rodrigues, Alberdina A. van Dijk, Ana C. Ebrecht, Diederik J. Opperman, Theo Sagmeister, Christoph Buhlheller, Tea Pavkov-Keller, Manoj K. Rathinaswamy, Udit Dalwadi, Calvin K. Yip, John E. Burke, K. Christopher Garcia, Nick V. Grishin, Paul D. Adams, Randy J. Read, and David Baker. Accurate prediction of protein structures and interactions using a three-track neural network. Science, 373(6557):871–876, 2021.   
114. Zongyang Du, Hong Su, Wenkai Wang, Lisha Ye, Hong Wei, Zhenling Peng, Ivan Anishchenko, David Baker, and Jianyi Yang. The trrosetta server for fast and accurate protein structure prediction. Nature News, Nov 2021.   
115. Jeffrey A Ruffolo, Lee-Shin Chu, Sai Pooja Mahajan, and Jeffrey J Gray. Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies. Nature communications, 14(1):2389, 2023.   
116. Jeffrey A Ruffolo, Jeffrey J Gray, and Jeremias Sulam. Deciphering antibody affinity maturation with language models and weakly supervised learning. arXiv preprint arXiv:2112.07782, 2021.   
117. Jiaxiang Wu, Fandi Wu, Biaobin Jiang, Wei Liu, and Peilin Zhao. tfold-ab: Fast and accurate antibody structure prediction without sequence homologs. bioRxiv, pages 2022–11, 2022.   
118. David TF Dryden, Andrew R Thomson, and John H White. How much of protein sequence space has been explored by life on earth? Journal of The Royal Society Interface, 5(25):953–956, 2008.   
119. Jinyu Yu, Junxi Mu, Ting Wei, and Hai-Feng Chen. Multi-indicator comparative evaluation for deep learningbased protein sequence design methods. Bioinformatics, 40(2):btae037, 2024.   
120. Rolf Apweiler, Amos Bairoch, Cathy H Wu, Winona C Barker, Brigitte Boeckmann, Serenella Ferro, Elisabeth Gasteiger, Hongzhan Huang, Rodrigo Lopez, Michele Magrane, et al. Uniprot: the universal protein knowledgebase. Nucleic acids research, 32(suppl 1):D115–D119, 2004.   
121. Ian Sillitoe, Tony E Lewis, Alison Cuff, Sayoni Das, Paul Ashford, Natalie L Dawson, Nicholas Furnham, Roman A Laskowski, David Lee, Jonathan G Lees, et al. Cath: comprehensive structural and functional annotations for genome sequences. Nucleic acids research, 43(D1):D376– D381, 2015.   
122. Zhixiu Li, Yuedong Yang, Eshel Faraggi, Jian Zhan, and Yaoqi Zhou. Direct prediction of profiles of sequences compatible with a protein structure by neural networks with fragment-based local and energy-based nonlocal profiles. Proteins: Structure, Function, and Bioinformatics, 82(10):2565–2573, 2014.   
123. Guoli Wang and Roland L Dunbrack Jr. Pisces: a protein sequence culling server. Bioinformatics, 19(12):1589– 1591, 2003.   
124. Mark A Larkin, Gordon Blackshields, Nigel P Brown, R Chenna, Paul A McGettigan, Hamish McWilliam, Franck Valentin, Iain M Wallace, Andreas Wilm, Rodrigo Lopez, et al. Clustal w and clustal $\mathbf { x }$ version 2.0. bioinformatics, 23(21):2947–2948, 2007.   
125. Suyue Lyu, Shahin Sowlati-Hashjin, and Michael Garton. Proteinvae: Variational autoencoder for translational protein design. bioRxiv, pages 2023–03, 2023.   
126. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112– 7127, 2021.   
127. Emre Sevgen, Joshua Moller, Adrian Lange, John Parker, Sean Quigley, Jeff Mayer, Poonam Srivastava, Sitaram Gayatri, David Hosfield, Maria Korshunova, et al. Protvae: Protein transformer variational autoencoder for functional protein design. bioRxiv, pages 2023–01, 2023.   
128. Bionemo.   
129. Donatas Repecka, Vykintas Jauniskis, Laurynas Karpus, Elzbieta Rembeza, Irmantas Rokaitis, Jan Zrimec, Simona Poviloniene, Audrius Laurynenas, Sandra Viknander, Wissam Abuajwa, et al. Expanding functional protein sequence spaces using generative adversarial networks. Nature Machine Intelligence, 3(4):324–333, 2021.   
130. Alexey Strokach, David Becerra, Carles Corbi-Verge, Albert Perez-Riba, and Philip M Kim. Fast and flexible protein design using deep graph neural networks. Cell systems, 11(4):402–411, 2020.   
131. Zhangyang Gao, Cheng Tan, Pablo Chac´on, and Stan Z Li. Pifold: Toward effective and efficient protein inverse folding. arXiv preprint arXiv:2209.12643, 2022.   
132. Namrata Anand, Raphael Eguchi, Irimpan I Mathews, Carla P Perez, Alexander Derry, Russ B Altman, and Po-Ssu Huang. Protein sequence design with a learned 13(1):746   
133. Yufeng Liu, Lu Zhang, Weilun Wang, Min Zhu, Chenchen Wang, Fudong Li, Jiahai Zhang, Houqiang Li, Quan Chen, and Haiyan Liu. Rotamer-free protein sequence design based on deep learning and self-consistency. Nature Computational Science, 2(7):451–462, 2022.   
134. Xinyi Zhou, Guangyong Chen, Junjie Ye, Ercheng Wang, Jun Zhang, Cong Mao, Zhanwei Li, Jianye Hao, Xingxu Huang, Jin Tang, et al. Prorefiner: an entropy-based refining strategy for inverse protein folding with global graph attention. Nature Communications, 14(1):7434,   
2023.   
135. Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.   
136. Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. In International conference on machine learning, pages 8946–8970. PMLR, 2022.   
137. Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning–based protein sequence design using proteinmpnn. Science, 378(6615):49–56, 2022.   
138. Junxi Mu, Zhengxin Li, Bo Zhang, Qi Zhang, Jamshed Iqbal, Abdul Wadood, Ting Wei, Yan Feng, and HaiFeng Chen. Graphormer supervised de novo protein design method and function validation. Briefings in Bioinformatics, 25(3):bbae135, 2024.   
139. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and TieYan Liu. Do transformers really perform badly for graph representation? Advances in neural information processing systems, 34:28877–28888, 2021.   
140. Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844–8856. PMLR, 2021.   
141. Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, et al. Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic acids research,   
50(D1):D439–D444, 2022.   
142. Alexey G Murzin, Steven E Brenner, Tim Hubbard, and Cyrus Chothia. Scop: a structural classification of proteins database for the investigation of sequences and structures. Journal of molecular biology, 247(4):536–540, 1995.   
143. John-Marc Chandonia, Lindsey Guan, Shiangyi Lin, Changhua Yu, Naomi K Fox, and Steven E Brenner. Scope: improvements to the structural classification of proteins–extended database to facilitate variant interpretation and machine learning. Nucleic acids research, 50(D1):D553–D559, 2022.   
144.   
145. Brian L Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in   
3d for the motif-scaffolding problem. arXiv preprint arXiv:2206.04119, 2022.   
146. Cong Fu, Keqiang Yan, Limei Wang, Wing Yee Au, Michael McThrow, Tao Komikado, Koji Maruhashi, Kanji Uchino, Xiaoning Qian, and Shuiwang Ji. A latent diffusion model for protein structure generation. arXiv preprint arXiv:2305.04120, 2023.   
147. Kevin E Wu, Kevin K Yang, Rianne van den Berg, James Y Zou, Alex X Lu, and Ava P Amini. Protein structure generation via folding diffusion. arXiv preprint arXiv:2209.15611, 2022.   
148. Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint arXiv:2302.02277, 2023.   
149. Yeqing Lin and Mohammed AlQuraishi. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds. arXiv preprint arXiv:2301.12485, 2023.   
150. Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089–1100, 2023.   
151. Zhenqiao Song, Yunlong Zhao, Yufei Song, Wenxian Shi, Yang Yang, and Lei Li. Joint design of protein sequence and structure based on motifs. arXiv preprint arXiv:2310.02546, 2023.   
152. Chence Shi, Chuanrui Wang, Jiarui Lu, Bozitao Zhong, and Jian Tang. Protein sequence and structure codesign with equivariant translation. arXiv preprint arXiv:2210.08761, 2022.   
153. Alexander E Chu, Lucy Cheng, Gina El Nesr, Minkai Xu, and Po-Ssu Huang. An all-atom protein generative model. bioRxiv, pages 2023–05, 2023.   
154. Bo Zhang, Kexin Liu, Zhuoqi Zheng, Yunfeiyang Liu, Junxi Mu, Ting Wei, and Haifeng Chen. Protein language model supervised precise and efficient protein backbone design method. bioRxiv, pages 2023–10, 2023.   
155. Alexander E Chu, Lucy Cheng, Gina El Nesr, Minkai Xu, and Po-Ssu Huang. An all-atom protein generative model. bioRxiv, pages 2023–05, 2023.   
156. Rahmad Akbar, Philippe A Robert, Ce´dric R Weber, Michael Widrich, Robert Frank, Milena Pavlovic´, Lonneke Scheffer, Maria Chernigovskaya, Igor Snapkov, Andrei Slabodkin, et al. In silico proof of principle of machine learning-based antibody design at unconstrained scale. In MAbs, volume 14, page 2031482. Taylor & Francis, 2022.   
157. Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi Jaakkola. Iterative refinement graph neural network for antibody sequence-structure co-design. arXiv preprint arXiv:2110.04624, 2021.   
158. Xiangzhe Kong, Wenbing Huang, and Yang Liu. Endto-end full-atom antibody design. arXiv preprint arXiv:2302.00203, 2023.   
159. Markus Muttenthaler, Glenn F. King, David J. Adams, et al. Trends in peptide drug discovery. Nat Rev Drug Discov, 20:309–325, 2021.   
160. Yongkang Wang, Xuan Liu, Feng Huang, Zhankun Xiong, and Wen Zhang. A multi-modal contrastive diffusion model for therapeutic peptide generation, 2024.   
161. Yipin Lei, Xu Wang, Meng Fang, Han Li, Xiang Li, and Jianyang Zeng. Pepgb: Facilitating peptide drug discovery via graph neural networks, 2024.   
162. Ruochi Zhang, Haoran Wu, Chang Liu, Huaping Li, Yuqian Wu, Kewei Li, Yifan Wang, Yifan Deng, Jiahui Chen, Fengfeng Zhou, and Xin Gao. Pepharmony: A multi-view contrastive learning framework for integrated sequence and structure-based peptide encoding, 2024.   
163. Pawel Smialowski, Gero Doose, Phillipp Torkler, Stefanie Kaufmann, and Dmitrij Frishman. Proso ii–a new method for protein solubility prediction. The FEBS journal, 279(12):2192–2200, 2012.   
164. David S Wishart, Craig Knox, An Chi Guo, Dean Cheng, Savita Shrivastava, Dan Tzur, Bijaya Gautam, and Murtaza Hassanali. Drugbank: a knowledgebase for drugs, drug actions and drug targets. Nucleic acids research, 36(suppl 1):D901–D906, 2008.   
165. Jun Xia, Shaorong Chen, Jingbo Zhou, Tianze Ling, Wenjie Du, Sizhe Liu, and Stan Z. Li. Adanovo: Adaptive De Novo peptide sequencing with conditional mutual information, 2024.   
166. Ngoc Hieu Tran, Xianglilan Zhang, Lei Xin, Baozhen Shan, and Ming Li. De novo peptide sequencing by deep learning. Proceedings of the National Academy of Sciences, 114(31):8247–8252, 2017.   
167. Rui Qiao, Ngoc Hieu Tran, Lei Xin, Xin Chen, Ming Li, Baozhen Shan, and Ali Ghodsi. Computationally instrument-resolution-independent de novo peptide sequencing for high-resolution devices. Nature Machine Intelligence, 3(5):420–425, 2021.   
168. Melih Yilmaz, William Fondrie, Wout Bittremieux, Sewoong Oh, and William S Noble. De novo mass spectrometry peptide sequencing with a transformer model. In International Conference on Machine Learning, pages 25514–25522. PMLR, 2022.   
169. Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative dynamics for molecular conformation generation. arXiv preprint arXiv:2102.10240, 2021.   
170. Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry prediction using a deep generative graph neural network. Scientific reports, 9(1):20381, 2019.   
171. Gregor NC Simm and Jose´ Miguel Hern´andez-Lobato. A generative model for molecular distance geometry. arXiv preprint arXiv:1909.11459, 2019.   
172. Octavian Ganea, Lagnajit Pattanaik, Connor Coley, Regina Barzilay, Klavs Jensen, William Green, and Tommi Jaakkola. Geomol: Torsional geometric generation of molecular 3d conformer ensembles. Advances in Neural Information Processing Systems, 34:13757–13769, 2021.   
173. Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In International conference on machine learning, pages 9558–9568. PMLR, 2021.   
174. Weihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec, Devi Parikh, and C Lawrence Zitnick. Forcenet: A graph neural network for large-scale quantum calculations. arXiv preprint arXiv:2103.01436, 2021.   
175. Shitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic graph score matching. Advances in Neural Information Processing Systems, 34:19784–19795, 2021.   
176. Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.   
177. Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. arXiv preprint arXiv:2206.01729, 2022.   
178. Jaina Mistry, Sara Chuguransky, Lowri Williams, Matloob Qureshi, Gustavo A Salazar, Erik LL Sonnhammer, Silvio CE Tosatto, Lisanna Paladin, Shriya Raj, Lorna J Richardson, et al. Pfam: The protein families database in 2021. Nucleic acids research, 49(D1):D412–D419, 2021.   
179. Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019.   
180. Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng Liu, and Jian Tang. Peer: a comprehensive and multi-task benchmark for protein sequence understanding. Advances in Neural Information Processing Systems, 35:35156–35173, 2022.   
181. John Moult, Krzysztof Fidelis, Andriy Kryshtafovych, Torsten Schwede, and Anna Tramontano. Critical assessment of methods of protein structure prediction (casp)—round xii. Proteins: Structure, Function, and Bioinformatics, 86:7–15, 2018.   
182. R Dustin Schaeffer and Valerie Daggett. Protein folds and protein folding. Protein Engineering, Design & Selection, 24(1-2):11–19, 2011.   
183. Jie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping protein sequences to folds. Bioinformatics, 34(8):1295–1303, 2018.   
184. Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315– 1322, 2019.   
185. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.   
186. Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022.   
187. Vladimir Gligorijevic´, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-based protein function prediction using graph convolutional networks. Nature communications, 12(1):3168, 2021.   
188. Pedro Hermosilla, Marco Sch¨”afer, Mateˇj Lang, Gloria Fackelmann, Pere Pau Va´zquez, Barbora Kozl´ıkov´a, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. arXiv preprint arXiv:2007.06252, 2020.   
189. Xiangzhe Kong, Wenbing Huang, and Yang Liu. Conditional antibody design as 3d equivariant graph translation. arXiv preprint arXiv:2208.06073, 2022.   
190. Lin Li, Esther Gupta, John Spaeth, Leslie Shing, Tristan Bepler, and Rajmonda Sulo Caceres. Antibody representation learning for drug discovery. arXiv preprint arXiv:2210.02881, 2022.   
191. Xiangrui Gao, Lipeng Lai, and Changling Cao. Pretraining with a rational approach for antibody. bioRxiv, pages 2023–01, 2023.   
192. Tobias H Olsen, Iain H Moal, and Charlotte M Deane. Ablang: an antibody language model for completing antibody sequences. Bioinformatics Advances, 2(1):vbac046, 2022.   
193. Jinwoo Leem, Laura S Mitchell, James HR Farmery, Justin Barton, and Jacob D Galson. Deciphering the language of antibodies using self-supervised learning. Patterns, 3(7):100513, 2022.   
194. Tobias H Olsen, Fergus Boyles, and Charlotte M Deane. Observed antibody space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences. Protein Science, 31(1):141–146, 2022.   
195. Brennan Abanades, Guy Georges, Alexander Bujotzek, and Charlotte M Deane. Ablooper: fast accurate antibody cdr loop structure prediction with accuracy estimation. Bioinformatics, 38(7):1877–1880, 2022.   
196. Brennan Abanades, Wing Ki Wong, Fergus Boyles, Guy Georges, Alexander Bujotzek, and Charlotte M Deane. Immunebuilder: Deep-learning models for predicting the structures of immune proteins. Communications Biology, 6(1):575, 2023.   
197. James Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye Shi, and Charlotte M Deane. Sabdab: the structural antibody database. Nucleic acids research, 42(D1):D1140–D1146, 2014.   
198. Jared Adolf-Bryfogle, Oleks Kalyuzhniy, Michael Kubitz, Brian D Weitzner, Xiaozhen Hu, Yumiko Adachi, William R Schief, and Roland L Dunbrack Jr. Rosettaantibodydesign (rabd): A general framework for computational antibody design. PLoS computational biology, 14(4):e1006112, 2018.   
199. Nicholas A Marze, Sergey Lyskov, and Jeffrey J Gray. Improved prediction of antibody vl–vh orientation. Protein Engineering, Design and Selection, 29(10):409– 418, 2016.   
200. Jeffrey A Ruffolo, Jeremias Sulam, and Jeffrey J Gray. Antibody structure prediction using interpretable deep learning. Patterns, 3(2):100406, 2022.   
201. Yining Wang, Xumeng Gong, Shaochuan Li, Bing Yang, YiWu Sun, Chuan Shi, Hui Li, Yangang Wang, Cheng Yang, and Le Song. xtrimoabfold: Improving antibody structure prediction without multiple sequence alignments. arXiv preprint arXiv:2212.00735, 2022.   
202. Jeffrey A Ruffolo, Carlos Guerra, Sai Pooja Mahajan, Jeremias Sulam, and Jeffrey J Gray. Geometric potentials from deep learning improve prediction of cdr h3 loop structures. Bioinformatics, 36(Supplement 1):i268–i275, 2020.   
203. Natalia Zenkova, Ekaterina Sedykh, Tatiana Shugaeva, Vladislav Strashko, Timofei Ermak, and Aleksei Shpilman. Simple end-to-end deep learning model for cdr-h3 loop structure prediction. arXiv preprint arXiv:2111.10656, 2021.   
204. Gennady Bocharov, Vitaly Volpert, Burkhard Ludewig, Andreas Meyerhans, Gennady Bocharov, Vitaly Volpert, Burkhard Ludewig, and Andreas Meyerhans. Basic principles of building a mathematical model of immune response. Mathematical Immunology of Virus Infections, pages 15–34, 2018.   
205. F Stanzione, I Giangreco, and JC Cole. Chapter fouruse of molecular docking computational tools in drug discovery. Progress in Medicinal Chemistry, pages 273–   
343.   
206. Francesco Ambrosetti, Zuzana Jandova, and Alexandre MJJ Bonvin. A protocol for informationdriven antibody-antigen modelling with the haddock2. 4 webserver. arXiv preprint arXiv:2005.03283, 2020.   
207. Matthew McPartlon and Jinbo Xu. Deep learning for flexible and site-specific protein docking and design. bioRxiv, pages 2023–04, 2023.   
208. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Antibody-antigen docking and design via hierarchical equivariant refinement. arXiv preprint arXiv:2207.06616,   
2022.   
209. Justina Jankauskaite˙, Brian Jime´nez-Garcı´a, Justas Dapku¯nas, Juan Fern´andez-Recio, and Iain H Moal. Skempi 2.0: an updated benchmark of changes in protein– protein binding energy, kinetics and thermodynamics upon mutation. Bioinformatics, 35(3):462–469, 2019.   
210. Sisi Shan, Shitong Luo, Ziqing Yang, Junxian Hong, Yufeng Su, Fan Ding, Lili Fu, Chenyu Li, Peng Chen, Jianzhu Ma, et al. Deep learning guided optimization of human antibody against sars-cov-2 variants with broad neutralization. Proceedings of the National Academy of Sciences, 119(11):e2122954119, 2022.   
211. Cheng Tan, Zhangyang Gao, and Stan Z Li. Protein complex invariant embedding with cross-gate mlp is a oneshot antibody designer. arXiv preprint arXiv:2305.09480,   
2023.   
212. Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. Advances in Neural Information Processing Systems, 35:9754–9767, 2022.   
213. Shuai Zeng, Duolin Wang, and Dong Xu. Peft-sp: Parameter-efficient fine-tuning on large protein language models improves signal peptide prediction. bioRxiv, pages   
2023–11, 2023.   
214. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   
215. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   
216. Felix Teufel, Jose´ Juan Almagro Armenteros, Alexander Rosenberg Johansen, Magnu´s Halldo´r G´ıslason, Silas Irby Pihl, Konstantinos D Tsirigos, Ole Winther, Søren Brunak, Gunnar von Heijne, and Henrik Nielsen. Signalp 6.0 predicts all five types of signal peptides using protein language models. Nature biotechnology,   
40(7):1023–1025, 2022.  

# Appendix  

# Graph Neural Networks  

While graph neural networks (GNNs) [64] are not inherently generative models on their own, they represent important components of larger generative methods. GNNs are especially important for processing graph-structured data. In the context of proteins, nodes and edges represent amino acids and spatial or sequential proximity, respectively. Once data have been transformed into a graph $G = ( V , E )$ with nodes $V$ and edges $E$ , the GNN $\phi$ learns to map nodes to embeddings through message passing $\left( \phi _ { e } \right)$ and aggregation $\left( \phi _ { h } \right)$ . For each pair of nodes $v _ { i } , v _ { j }$ , the GNN outputs a “message” $\mathbf { m } _ { i j }$ based on existing features $\mathbf { h } _ { i } ^ { l } , \mathbf { h } _ { j } ^ { l }$ and coordinates $\mathbf { x } _ { i } ^ { l } , \mathbf { x } _ { j } ^ { l }$ at layer $l$ ( $\dot { \mathbf { \alpha } } _ { a _ { i j } }$ denotes the $( i , j )$ entry in adjacency matrix $A$ ):  

$$
{ \bf { m } } _ { i j } = \phi _ { e } ( { \bf { h } } _ { i } ^ { l } , { \bf { h } } _ { j } ^ { l } , { \bf { x } } _ { i } ^ { l } , { \bf { x } } _ { j } ^ { l } , a _ { i j } )
$$  

Then, the message received by any node $\boldsymbol { v } _ { i }$ is an aggregation of the messages from its neighbors:  

$$
\mathbf { m } _ { i } = \sum _ { j \in \mathcal { N } ( i ) } \mathbf { m } _ { i j }
$$  

Finally, the model combines old embeddings and positions with messages to create new embeddings and positions at layer $l + 1$ :  

$$
\mathbf { h } _ { i } ^ { l + 1 } , \mathbf { x } _ { i } ^ { l + 1 } = \phi _ { h } ( \mathbf { h } _ { i } ^ { l } , \mathbf { x } _ { i } ^ { l } , \mathbf { m } _ { i } )
$$  

Note that positions $\mathbf { x }$ and features $\mathbf { h }$ are often concatenated and denoted as a singular feature vector $\mathbf { h }$ , as they are treated equivalently in the general GNN case.  

# Equivariant Graph Neural Networks  

For the generation of 3D structures, equivariance is a useful inductive bias to incorporate into deep learning models. In general, two 3D structures should be treated equally if they only differ under a series of rotations, reflections, and translations.  

For some conditional distribution $p ( y | x )$ , it is equivariant to the action of rotations and reflections when $p ( R y ) = R p ( y )$ (alternatively, $p ( y | x ) \ = \ p ( R y | R x ) )$ for transformation $R$ . A distribution is invariant if $p ( R y ) = p ( y )$ . In most works, the transformations consist of the Euclidean $\operatorname { E } ( 3 )$ group generated by rotations, translations, and reflections.  

Satorras et al. [65] propose the EGNN, a simple adjustment to the traditional GNN framework to preserve equivariance. This is accomplished by defining separate point embeddings $\mathbf { x } _ { i }$ and feature embeddings $\mathbf { h } _ { i }$ for each node $\boldsymbol { v } _ { i }$ , and considering relative positions for message passing:  

$$
\begin{array} { c } { { \displaystyle { \bf m } _ { i j } = \phi _ { e } ( { \bf h } _ { i } ^ { l } , { \bf h } _ { j } ^ { l } , \| { \bf x } _ { i } ^ { l } - { \bf x } _ { j } ^ { l } \| , a _ { i j } ) } } \\ { { \displaystyle { \bf x } _ { i } ^ { l + 1 } = { \bf x } _ { i } ^ { l } + C \sum _ { j \ne i } ( { \bf x } _ { i } ^ { l } - { \bf x } _ { j } ^ { l } ) \phi _ { x } ( { \bf m } _ { i j } ) } } \end{array}
$$  

The final message aggregating and embedding update steps are equivalent to those of a GNN. Here, equivariance is preserved because we only consider relative positions/distances $\mathbf { x } _ { i } ^ { l } - \mathbf { x } _ { j } ^ { l }$ in message passing. Therefore, rotating, translating, or reflecting all atoms will result in equivalent functions.  

# Molecular Conformation Generation  

# Overview  

While learning on graph-structured data has found success in molecular generation, recent models have demonstrated that incorporating 3D information is crucial to understanding various chemical properties such as binding affinity. However, deriving a 3D structure from a 2D connectivity graph is nontrivial; given a 2D graph, there are numerous ways to construct a 3D arrangement of molecules that still adheres to the same connectivity structure. Two molecules with the same chemical formula but different 3D arrangements are called conformers (or conformational isomers).  

# Datasets  

The following datasets are used for conformation generation. Note that while GEOM-QM9 [74] is created with the same molecules as in the general QM9 [73] dataset, the GEOM-QM9 dataset contains a complete set of representative conformers for each molecule. QM9 only provides singular 3D structures and cannot be applied to conformation generation.  

GEOM-QM9 [74] - Set of all conformers for each molecule in QM9. Reference conformations are created through a standardized process using density functional theory. GEOM-Drugs [74] - Conformers for more complex druglike molecules, also used in general molecule generation (see page 5) ISO17 [61] - Set of molecular conformations for all molecules with the chemical formula $\mathrm { C _ { 7 } O _ { 2 } H _ { 1 0 } }$  

# Task  

Given a 2D molecule connectivity graph, this approach generates a set of stable 3D conformations that correspond to the given connectivity structure.  

# Metrics  

While other tasks like property prediction are often evaluated on models in this field, we focus on generative tasks like conformation generation. The following metrics are used to judge the quality of the conformation generation task. Intuitively, COV measures the diversity of the generated sample, while MAT measures quality/accuracy.  

COV [169] - Coverage, a percentage of how many groundtruth conformations are “covered” (below a threshold root-mean-square deviation (RMSD) with some generated conformation, typically $0 . 5 \mathrm { \AA }$ for QM9 and $1 . 2 5 \mathrm { \AA }$ for Drugs). MAT [169] - Matching, the average RMSD between each generated conformation and the closest ground-truth conformation.  

# Models  

Mansimov et al. propose a conditional variational graph autoencoder (CVGAE) [170] for molecule generation, directly applying a GNN to learn atom representations and generate 3D coordinates. GraphDG [171] improves upon this by modeling distance geometry between atoms, which incorporates rotational and translational invariance (i.e., two identical molecules are treated equally under transformation). Conditional graph continuous flow (CGCF) [169] applies a flow-based approach to model the distance geometry, combining this with an (EBM) as a “tilting” term for mutual enhancement. GeoMol [172] argues that methods solving for distance geometries are flawed, as creating distance matrices leads to overparameterization, and important geometric features like torsional angles are not directly modeled. To address this, GeoMol uses an MPNN to individually predict the local structure for each atom, allowing for a more direct representation of neighboring features. ConfGF [173] pulls ideas from force field methods in molecular dynamics, directly learning gradient fields of the log density for atom coordinates, using a GIN [174] to adjust for rotational and translational invariance. Dynamic graph score matching (DGSM) [175] extends the ideas of ConfGF by constructing dynamic graphs to incorporate long-range atomic interactions (instead of relying on static input graphs that only consider chemical bonds). GeoDiff [176] uses an Euclidean space diffusion process, treating each atom as a particle. GeoDiff incorporates Markov kernels that preserve equivariance, as well as roto-translational invariance through their proposed graph field network (GFN) layer, which draws inspiration from the eGNN architecture [65]. Torsional Diffusion [177] uses score-based diffusion in the space of torsion angles, which allows for improved representation and fewer denoising steps than GeoDiff.  

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Type of Model</td><td rowspan="2">Dataset</td><td colspan="2">GEOM-QM9</td><td colspan="3">GEOM-Drugs</td></tr><tr><td>COV (%, ↑)</td><td>MAT (A,↓)</td><td>COV (%, ↑)</td><td></td><td>MAT (A,↓)</td></tr><tr><td>CVGAE [170]</td><td>VAE,MPNN</td><td>QM9,COD,CSD</td><td>0.09 [175]</td><td>1.6713 [175]</td><td>0.00 [175]</td><td></td><td>3.0702 [175]</td></tr><tr><td>GraphDG [171]</td><td>VAE</td><td>ISO17</td><td>73.33 [175]</td><td>0.4245 [175]</td><td>8.27[175]</td><td></td><td>1.9722 [175]</td></tr><tr><td>CGCF[169]</td><td>Flow,EBM</td><td>G-QM9,G-Drugs,ISO17</td><td>78.05 [176]</td><td>0.4219 [176]</td><td>53.96 [176]</td><td></td><td>1.2487 [176]</td></tr><tr><td>ConfGF[173]</td><td>GIN,Diffusion</td><td>G-QM9, G-Drugs</td><td>88.49 [175]</td><td>0.2673 [175]</td><td>62.15 [175]</td><td></td><td>1.1629 [175]</td></tr><tr><td>GeoDiff** [176]</td><td>GFN,Diffusion</td><td>G-QM9, G-Drugs</td><td>90.07</td><td>0.209</td><td>89.13</td><td></td><td>0.8629</td></tr><tr><td>GeoMol [172]</td><td>MPNN</td><td>G-QM9, G-Drugs</td><td>71.26 [176]</td><td>0.3731 [176]</td><td>67.16 [176]</td><td></td><td>1.0875 [176]</td></tr><tr><td>DGSM** [175]</td><td>MPNN, Diffusion</td><td>G-QM9,G-Drugs</td><td>91.49</td><td>0.2139</td><td>78.73</td><td></td><td>1.0154</td></tr><tr><td>Torsional** [177]</td><td>Diffusion</td><td>G-QM9, G-Drugs,G-XL</td><td>92.8</td><td>0.178</td><td>72.7*</td><td></td><td>0.582</td></tr></table></body></html>

Table 8. An overview of relevant molecule conformation generation models. All benchmarking metrics are self-reported unless otherwise noted. GEOM-QM9 and GEOM-Drugs are denoted as G-QM9, and G-Drugs for brevity. [\*] Torsional Diffusion uses a threshold of 0.75 ˚A instead of $1 . 2 5 \mathrm { ~ \AA ~ }$ to calculate coverage on GEOM-Drugs, leading to a deflated score. Notably, Torsional Diffusion outperforms GeoDiff and GeoMol when tested on this threshold. [\*\*] represents the current SOTA.  

As shown in Table 8, Torsional Diffusion appears to outperform all other models, with DGSM and GeoDiff also exhibiting competitive performance. Notably, methods that focus on torsional angles seem to outperform other methods with similar architectures: GeoMol outperforms distance-graph methods like GraphDG and CGCF, while Torsional Diffusion outperforms GeoDiff.  

# Protein Representation Learning  

# Overview  

Protein representation learning involves learning embeddings to convert raw protein data into latent space representations, extracting meaningful features and chemical attributes. More specifically, given a protein $x = [ o _ { 1 } , o _ { 2 } , . . . o _ { L } ]$ , where each $\scriptstyle { o _ { 1 } }$ represents an amino acid (sequence-based) or atom coordinate (structure-based), learn an embedding $\mathit { z } ~ = ~ [ h _ { 1 } , h _ { 2 } , . . . h _ { L } ]$ , where each $\begin{array} { r l r } { h _ { i } } & { { } \in } & { \mathbb { R } ^ { d } } \end{array}$ represents a $d -$ dimensional token representation for amino acid $o _ { i }$ . This general framework can be extended to a multitude of tasks— for token property prediction tasks, additionally allocate a prediction $p ( y _ { i } | o _ { i } )$ in the embedding, and for pairwise prediction tasks, allocate a prediction $p ( y _ { i j } | o _ { i } , o _ { j } )$ . For overall protein property prediction, allocate predictions $p ( y | x )$ or $p ( y | x _ { 1 } , x _ { 2 } )$ for individual and protein-protein properties, respectively.  

These embeddings can be seen as creating “richer” data spaces for various models to work on; whereas raw atom graphs and amino acid sequences are entirely arbitrary, these embeddings are aimed to capture various chemical traits and properties (i.e. two proteins with similar properties should be represented with similar embeddings, even if their raw forms are very different). Thus, representation learning models do not directly perform any tasks on their own, but instead are incorporated in other generative models to improve their ability to model these important chemical features.  

# Datasets  

Note that a plethora of datasets exist for all various downstream tasks; we do not describe all such datasets. We focus on datasets used for pre-training in protein representation learning, described below. Note that UniRef, UniParc, and ProteinKG are typically used for sequence-based learning, and PDB and AlphafoldDB are primarily used for structure-based learing. Pfam has been used in both types of models.  

UniRef [120] - A clustered version of the Unified Protein KnowledgeBase (UniProtKB), part of the central resource UniProt, which is a curated and labeled set of protein sequences and their functions   
UniParc [120] - A larger dataset of protein sequences, part of the central resource Uniprot, which includes UniProtKB and adds proteins from a variety of other sources   
ProteinKG [38] - A dataset created by Zhang et al. (OntoProtein), aligning biological knowledge with protein sequences with knowledge graphs (KG), used for directly injecting biological knowledge in the representation learning process   
PDB [91] - Protein Data Bank, a central archive for all experimentally determined protein structures, widely used in almost all protein structure-related tasks.   
AlphaFoldDB [141] - AlphaFold Protein Structure Database, a large dataset created by using AlphaFold2 to predict structures from sequence datasets, such as UniProt and Swiss-Prot.   
Pfam [178] - A collection of protein families, used in multiple sequence alignments (MSA)  

# Tasks  

Representation learning models do not directly address tasks on their own but rather supplement a wide range of tasks by refining a latent space. Thus, the protein representation learning field in particular lacks uniformity in testing and training methods, making it hard to make generalized evaluations. Some methods like Tasks Assessing Protein Embeddings (TAPE) [179] and, more recently, Protein sEquence undERstanding (PEER) [180] seek to address this by compiling various testing metrics into a standardized benchmark for evaluating these language models. A few of the most prominent tasks include:  

Contact Prediction - Given two amino acid residues, predict the probability that they “contact”, or are within some 3D distance threshold. Measured using precision of top $L / 5$ medium-range and long-range contacts (contact between sequentially distant amino acids) according to CASP standards [181].  

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Type of Model</td><td rowspan="2">Dataset</td><td rowspan="2">Contact (↑)</td><td colspan="3">Famild Supefeatin ( Fold</td><td rowspan="2">Stability (↑)</td></tr><tr><td></td><td></td><td></td></tr><tr><td>UniRep [184]</td><td>LSTM RNN</td><td>UniRef50</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ProtBERT[126]</td><td>BERT</td><td>BFD100,UniRef100</td><td>0.556[40]</td><td>0.528 [40]</td><td>0.192 [40]</td><td>0.170 [40]</td><td>0.651 [40]</td></tr><tr><td>ESM-1B[185]</td><td>Transformer</td><td>UniParc</td><td>0.458</td><td>0.978</td><td>0.601</td><td>0.268 [186]</td><td>0.695 [186]</td></tr><tr><td>MSA Trans. [140]</td><td>Transformer</td><td>26M MSAs</td><td>0.618 [40]</td><td>0.958 [40]</td><td>0.503 [40]</td><td>0.235 [40]</td><td>0.796 [40]</td></tr><tr><td>RSA** [40]</td><td>Transformer</td><td>Pfam</td><td>0.717</td><td>0.987</td><td>0.677</td><td>0.267</td><td>0.987</td></tr><tr><td>OntoProtein** [38]</td><td>ProtBERT,BERT/Trans.</td><td>ProteinKG*</td><td>0.40</td><td>0.96 [40]</td><td></td><td>0.24</td><td>0.75 [40]</td></tr><tr><td>KeAP** [39]</td><td>BERT,Transformer</td><td>ProteinKG</td><td>0.62</td><td></td><td>1</td><td>0.29</td><td>0.82</td></tr><tr><td>GearNET** [186]</td><td>Geo-EGNN</td><td>AlphaFoldDB</td><td></td><td>0.995</td><td>0.703</td><td>0.483</td><td></td></tr><tr><td>DeepFRI [187]</td><td>LSTM,GCNN</td><td>Pfam</td><td></td><td>0.732[188]</td><td>0.206 [188]</td><td>0.153[188]</td><td></td></tr><tr><td>IEConv [188]</td><td>CNN</td><td>PDB</td><td></td><td>0.997</td><td>0.806</td><td>0.503</td><td></td></tr></table></body></html>

Table 9. An overview of the most relevant sequence-based protein representation learning models. All metrics are self-reported unless otherwise noted; metrics reported by different papers may be incomparable due to different test settings. Note that OntoProtein and KeAP have two separate architectures for protein, and knowledge encoding respectively. $[ { ^ { * * } } ]$ denotes the current SOTA.  

Fold Classification - Also known as remote homology prediction. Given an amino acid sequence, predict the corresponding fold class [182]. Used for finding structural similarities for distantly related inputs. Tested using the dataset proposed by Hou et al. [183], which comprises three versions of increasing difficulty: Family (test set in the same family as the training set), Superfamily (same superfamily as training) and Fold (same fold as training). Measured using accuracy.   
Stability Prediction - Given a protein, output a label $y \in \mathbb { R }$ representing the most extreme circumstances under which a protein maintains its fold above a concentration threshold. Measured using Spearman’s $\rho$ .   
PPI - Protein-Protein Interaction, given two proteins, predict whether or not they interact (binary classification). Measured using accuracy.  

# Metrics  

The following metrics are used to evaluate the above tasks:  

Accuracy - Ratio between correct classifications to total classifications. Precision - Accuracy within the set of selected elements; in contact prediction, this refers to the following: of the top $L / 5$ contacts with the highest predicted probability, find the percentage of these which are ground-truth contacts. $L$ refers to the length of a protein sequence. Spearman’s $\rho \cdot \mathbf { \partial }$ Spearman’s rank correlation coefficient, found by sorting the ground-truth and predicted output values into ranked order (so each protein has a true rank $y _ { i } \in \mathbb { N }$ and predicted rank $x _ { i } \in \mathbb { N }$ ) and computing:  

$$
\rho = 1 - \frac { 6 \sum d _ { i } ^ { 2 } } { n ( n ^ { 2 } - 1 ) } ,
$$  

where $d _ { i }$ refers to the numerical difference in ranks for each $y _ { i } , x _ { i }$ , and $\scriptstyle { n }$ refers to the total number of elements in each set.  

# Models  

As mentioned above, protein representation learning models can be divided into two broad categories: sequence-based and structure-based. This distinction simply refers to the type of input these models learn representations for; we begin by discussing sequence-based learning models. Both amino acid sequences and protein structures carry valuable information about a protein’s function and binding properties, and thus these two types of learning can be seen as complements to each other.  

Unified Representation (UniRep) [184] used a multiplicative long-short-term memory recurrent neural network (mLSTM RNN) to approach the task of representation learning. By iterating through each amino acid and comparing its prediction with the true amino acid type, adjusting its parameters at each step, UniRep gradually learns a better representation of the sequence. While the LSTM model has been proven inferior to newer transformer and natural language models, the basic concepts laid an important groundwork for future representation learning models.  

ProtBERT [126] improved upon performance by applying a BERT model to amino acid sequences, treating each residue as a word. BERT stands for Bidirectional Encoder Representations from Transformers, which is a pre-trained language model for neural language processing. Several variations, such as RoBERTa and DeBERTa, improve upon BERT by enhancing parameters and attention mechanisms and are used in models discussed later. ESM-1B [185] trained a deep transformer with 33 layers and 650 million parameters, using the masked language modeling (MLM) objective for training. ESM-1b considered residue contexts across the entire sequence through its stacked attention layers, and its complexity and size allowed it to outperform other SOTA models on a wide range of tasks, as shown below. ESM-1b’s massive size also resulted in expensive training costs, however, and smaller research groups would had to find more unique approaches to compete with ESM-1b on representation learning.  

Following ESM-1b’s breakthrough, a series of models applied the concept of direct knowledge injection. Previous models had simply treated these sequences as generic sequences of tokens, failing to take advantage of the vast biological knowledge in existence today. One avenue for knowledge injection comes from the analysis of Multiple Sequence Alignments (MSA), which groups proteins concerning their evolutionary families. Because inherent patterns appear as a result of the evolutionary process, analyzing MSAs can result in a richer understanding of protein sequences. MSA Transformer [140] was the first model to intersect a protein language model with MSA analysis, pretraining a transformer model on MSA input. While MSAs can provide large benefits, the alignment process is computationally costly. Retrieved Sequence Augmentation (RSA) [40] provides a solution to this, using a dense sequence retriever to directly augment input proteins with corresponding evolutionary information from the Pfam database. This allows RSA to incorporate similar information as traditional MSA methods, without the computationally costly MSA alignment process. OntoProtein [38] incorporates direct knowledge injection by pretraining on gene ontology (GO) knowledge graphs (KG). OntoProtein uses the ProtBERT architecture for encoding protein sequences but introduces a second encoder for training on the GO input. To perform this hybrid encoding, Zhang et al. [38] create a new dataset, ProteinKG25, which aligns protein sequences with their respective GO annotations in text format. Knowledgeexploited Auto-encoder for Protein (KeAP) [39] extends upon this by exploring protein and annotated text on a more granular level; while OntoProtein models relationships over proteins and texts as a whole, KeAP uses a cross-attention mechanism to perform token-level exploration on individual amino acids and words.  

Now, we discuss a few structure-based learning models. IEConv [188] incorporates both intrinsic distance (based on connectivity in a node-graph representation) and extrinsic distance (based on physical 3D distance) information in a convolutional neural network (CNN), using contrastive learning to pre-train. Additionally, IEConv uses protein-specific pooling methods, like reducing each amino acid to its alpha carbon, to reduce computational complexity and allow for more learned features per atom under memory constraints. DeepFRI [187] combines the idea of sequence representation learning with structural representation learning, incorporating both a pretrained LSTM to learn sequence data and a GCN to learn from contact map input (matrix representing 3D distances between residues). GearNET [186] applies a GNN to a graph of residues, with three types of directed edges defined for sequential/physical distance and K-nearest neighbors. While previous models had only considered message-passing between residues, GearNET applied direct message-passing between edges in its variant model GearNET-Edge, which led to even further improved performance on various metrics. In addition, GearNET-IEConv added an intrinsic-extrinsic convolutional layer inspired by Hermosilla et al. [188] to improve performance, with GearNET-Edge-IEConv combining both of these features.  

# Antibody  

# Antibody Task Background  

Antibodies are Y-shaped proteins used by the immune system to identify and neutralize foreign objects like bacteria and viruses. Each antibody contains two identical sets of chains, with each set containing one heavy chain and one light chain. The variable domains of these chains control binding specificity for various antigens, containing the binding surface, known as the paratope. The paratope binds with specificity to a target antigen at its respective binding region, known as the epitope. The paratope is typically located within six ComplementarityDetermining Regions (CDRs) on the antibody: three from the light chain (CDR-L1, CDR-L2, CDR-L3) and three from the heavy chain (CDR-H1, CDR-H2, CDR-H3). The Complementary-Determining Region 3 (CDR-H3) is the most diverse in terms of amino acid sequence and length— due to its lack of constraints, it is often the most complex to produce, making its generation the primary focus for modern generative models.  

The end goal of machine learning is to generate antibodies in silico. In approaching this goal, the typical pipeline often includes an amino acid sequence input, sequence representation learning, structure prediction, paratope-epitope prediction, antibody-antigen docking, CDR generation, and evaluation, typically in the form of affinity prediction. The final output is a generated antibody, either in the form of a 1D sequence, 3D structure, or both. Sometimes, a 3D structure is input directly, with the sequence being predicted from the structure. Machine learning methods have been applied to various fragments of the process, with some more recent models [189], [158], [157] handling both sequence and structure generation simultaneously, and others [158] even addressing the entire endto-end process. Other subtasks like drug-applicable attribute prediction and affinity prediction have been explored using predictive AI models, but these tasks are not generative by nature and not be discussed in detail.  

# Antibody Representation Learning  

# Overview  

This task is identical to the protein sequence representation learning, but language models in this field pretrain on antibodyspecific data, in particular, outperforming general protein representation models on antibody-based tasks.  

# Datasets  

All reported methods pre-train on the OAS dataset. In all methods, OAS was divided between heavy and light chains, with some [191, 192] also using Linclust to cluster the data. Leem et. al also includes pre-training on the general Pfam database for protein sequences, as a comparison to pre-training on the antibody-specific OAS dataset.  

OAS [194] - Observed Antibody Space, a compilation of over one billion raw antibody sequences.  

# Tasks  

Given the variety of applications for representation learning, the benchmarks used for this task vary widely; some use Pearson correlation to measure affinity binding prediction [193], while others [191] use amino acid recovery (AAR) to measure accuracy in a CDR-H3 sequence prediction task. Some use a masked language modeling task (MLM) to measure the quality of BCR sequence representation [193], while others measure accuracy in paratope prediction [193, 116]. However, we do not have a standardized benchmarking system as we did with protein representation learning, so we only report CDR-H3 sequence prediction in Table 10, which will be elaborated on more in the Antibody CDR Generation section (see page 28). Other evaluations are left as individual notes for each model.  

# Metrics  

As noted above, we only report results on the CDRH3 prediction task, which is measured with amino acid recovery (AAR), which represents the percentage of matching amino acids between ground truth and generated amino acid sequences.  

# Models  

Li et al. [190] created the BERTTransformer model, serving as the first representation learning model aimed specifically at antibody learning. Li et al. find that pre-training on Pfam results in improved performance on affinity binding prediction when compared to pre-training on OAS, suggesting some merit to general protein datasets in pre-training for antibody representation learning. AntiBERTy [116] is a BERT-based model, and produces valuable embeddings used in structure prediction by IgFold [115]. AntiBERTa [193] and AbLang [192] both use the more optimized RoBERTa architecture, while the most recent model PARA [191] uses the current state-of-the-art DeBERTa architecture, and outperforms previous pre-trained models in AAR.  

<html><body><table><tr><td>Model</td><td>Type of Model</td><td>Dataset</td><td>AAR (%,↑)</td><td>Additional Self-Benchmarking Notes</td></tr><tr><td>BERTTransformer [190]</td><td>BERT</td><td>OAS</td><td></td><td>Improved binding affinity prediction over generic CNN</td></tr><tr><td>AntiBERTy [116]</td><td>BERT</td><td>OAS</td><td>26.00 [191]</td><td>Reveals evolutionary trajectories for affinity maturation</td></tr><tr><td>AbLang [192]</td><td>RoBERTa</td><td>OAS</td><td>33.60 [191]</td><td>Clustering better represents distinction between B-cell type and other metrics (compared to ESM-1b)</td></tr><tr><td>PARA [191]</td><td>DeBERTa</td><td>OAS</td><td>34.20 [191]</td><td>Improved heavy/light chain matching (compared to AntiBERTy)</td></tr><tr><td>AntiBERTa [193]</td><td>RoBERTa</td><td>OAS</td><td></td><td>Clustering better represents distinction between B-cell type (compared to ProtBERT),improves paratope prediction</td></tr></table></body></html>

Table 10. An overview of relevant antibody representation learning models. Due to the lack of standardization in benchmarking metrics, we note each model’s benchmarking findings.  

A comprehensive overview of representation learning models can be seen in Table 10. As representation learning serves as a general tool, rather than the optimization for a specific task, these methods all use a variety of selfchosen applications to measure performance. PARA tests amino acid recovery against AbLang and ‘ on the CDRH3 prediction task, reporting improved accuracy. AntiBERTy and AntiBERTa report improved paratope prediction, while AbLang and AntiBERTa focus on improved clustering to distinguish between naive and memory B-Cells. AntiBERTa also explores clustering quality in terms of drug-applicable traits, such as origin, closest human V gene identity, and anti-drug antibody (ADA) response scores.  

# Antibody Structure Prediction  

# Overview  

Antibody structure prediction extends general protein structure prediction methods, with the key difference lying in the reliance on multiple sequence alignment (MSA) of homologous proteins mapping evolutionary relationships between genetically related sequences. Because the relevant evolutionary histories for CDR-H3 loop sequences are lacking, MSAs in antibodies are not always available. This makes models like AlphaFold [110], designed for general protein structure prediction, highly inefficient and slow. Thus, antibody-specific structural prediction methods seek to predict antibody structure without the need for an input MSA [115].  

# Datasets  

Models in this field all train on the Structural Antibody Database (SAbDab), with the Rosetta Antibody Benchmark (RAB), used as a validation set in some cases [195, 196]. Note that “canonical conformations” do not exist for the highly variable H3 region due to high variability, which remains a challenge to structurally model.  

SAbDab [197] - Structural Antibody Database, a collection of all antibody structures in PDB [91]. Structures are annotated with antibody-specific structural data like canonical conformations for complementarity determining regions (CDR), orientation between the variable domains on the light and heavy chains, and the presence of constant domains in the structure. RAB [198] - Rosetta Antibody Benchmark, a handselected set of 60 antibody-antigen complexes, chosen to be as diverse in CDR lengths and clusters as possible.  

# Task  

Given an antibody amino acid sequence (organized into heavy chain, light chain, and linker sequences), generate a set of 3D point coordinates for each amino acid residue.  

# Metrics  

For benchmarking, models compare similarities between the predicted and ground truth structures through the following metrics:  

RMSD - Root-mean-square deviation, measures distances between ground truth and generated residue coordinates. Also used in protein structure prediction (see page 9) OCD [199] - Orientational coordinate distance, measures similarity between light-heavy orientational coordinates (LHOC) of ground truth and generated residues. LHOC is defined by Marze et al. [199], consisting of four metrics to measure orientation between light and heavy chains.  

used root-mean-square deviation (RMSD) to measure the discrepancy between predicted and ground truth 3D structures, while some models [115], [200], [117] also used orientational coordinate distance (OCD) to measure the accuracy of predicted relative position between heavy and light chains in some cases.  

# Models  

tFold-Ab [117], xTrimoABFold [201], and ABodyBuilder2 [196] all apply similar methods to AlphaFold, replacing the unnecessary MSA searching component. AbLooper [195] uses five EGNNS to each individually predict CDR structures, outputting the positional average of the five structures— this allows for the evaluation of confidence as the deviation between these predicted structures. Some models do not predict structures directly: DeepH3 [202] uses a deep residual neural network to first predict structural restraints, using Rosetta to then output atomic structures. SimpleDH3 [203] adds ELMo embeddings to DeepH3, and DeepAb [200], the most recent version, adds interpretable attention layers to enhance output from the neural network.  

Igfold [115] takes advantage of AntiBERTy’s [116] sequence embeddings to predict antibody structures using invariant point attention. While some other models, such as tFold-Ab [117], xTrimoABFold [201], and ABodyBuilder2 [196], demonstrate more accurate structures, IgFold produces structures at a superior speed, making it the established state-of-the-art model. Igfold’s downside is its reliance on PyRosetta to produce side-chain conformations, as the model Igfold itself only outputs backbone atoms. This adds significant time to the full process, as it takes around 0.46 seconds for Igfold to produce backbone atoms, but an additional 21.86 seconds, on average, for PyRosetta to convert these into full-atom structures [117].  

<html><body><table><tr><td>Model</td><td>Type of Model</td><td>Dataset</td><td>H3 RMSD (A,↓)</td><td>OCD (↓)</td><td>Generation Time (s,↓))</td></tr><tr><td>IgFold** [115]</td><td>BERT,Graph</td><td>OAS,SAbDab</td><td>2.99</td><td>3.77 [115]</td><td>0.46[115]</td></tr><tr><td>DeepAB [200]</td><td>LSTM,NN</td><td>OAS,SAbDab</td><td>3.28</td><td>3.6[115]</td><td>1620.22 [117]，600 [115]</td></tr><tr><td>ABLooper [195]</td><td>EGNN</td><td>SAbDab,RAB</td><td>3.2</td><td>4.53 [115]</td><td>30-60 [115]</td></tr><tr><td>ImmuneBuilder [196]</td><td>AlphaFold -Multimer</td><td>SAbDab,RAB</td><td>2.81</td><td>4.9 [115]</td><td>5*</td></tr><tr><td>tFold-Ab** [117]</td><td>AlphaFold -Multimer</td><td>SAbDab</td><td>2.74</td><td>3.21 [115]</td><td>2.23*[117]</td></tr><tr><td>xTrimoABFold [201]</td><td>AlphaFold -Multimer</td><td>PDB</td><td>1.25</td><td></td><td>/</td></tr></table></body></html>  

Table 11. An overview of the most relevant antibody structure prediction models. All benchmarking metrics are self-reported unles otherwise noted. Igfold and tFold-Ab use slightly different benchmarking methods, so their results may not be comparable. [\*] denotes significant discrepancies which result from varying benchmarking methods. $[ { ^ { * * } } ]$ denotes the current SOTA.  

![](images/669ba1e939f1e1f56ae9ebacd9a0e17012908e987b7bb20fde678be0f44ab3cf.jpg)  
Fig. 5. A comprehensive overview of the antibody generation pipeline for CDR-H3 design [20, 23, 204, 205, 206]. The inputs are a target antigen and antibody information (without CDR-H3), and the output is an antibody-antigen complex with a designed CDR-H3 sequence. Note that while most antibody CDR-H3 generation methods only generate the CDR-H3 region, needing a docked structure as input, some methods like DockGPT [207], HERN [208], and dyMEAN [158] perform multiple steps of the pipeline on their own.  

By contrast, tFold-Ab [117] generates full-atom structures in less time than the Igfold $^ +$ PyRosetta pipeline (2.23 seconds compared to 22.32 seconds).  

Notably, in the training process, ImmuneBuilder and xTrimoABFold choose to include antibodies with identical sequences to expose models to antibodies with multiple conformations (identical sequences with different structures). On the other hand, DeepAB includes an entirely non-redundant set of sequences, seeking to expose its model only to unique items.  

Table 11 presents detailed results for the performance of the discussed models. While IgFold does not have the absolute highest performance in terms of RMSD and OCD when compared to other methods, its speed makes it the stateof-the-art model. This speed advantage is contested only by tFold-Ab, which generates full-atom structures in less time than IgFold, as it circumvents IgFold’s reliance on computationally expensive Rosetta energy functions for side-chain prediction. Note that discrepancies in time results may result from varying benchmarking methods— time results from tFold-ab were reported on their own SAbDab-22H1-Ab/Nano dataset, while time results from IgFold were reported using their IgFold-Ab benchmark.  

# Antibody CDR Generation  

# Overview  

CDR generation involves the generation of the variable regions of the antibody, directly determining binding affinity and making it the core of antibody generation. As mentioned above, the CDR-H3 region is the most variable, making it the most difficult region for models to generate and thus the primary region of focus.  

# Datasets  

The primary datasets used in recent models are listed below. Note that SAbDab is the main dataset typically used for training, whereas RAbD and SKEMPI are specialized datasets used for testing specific downstream tasks.  

SAbDab [197] - The Structural Antibody Database, an annotated dataset of all antibody structures contained in PDB [91], used in structure prediction (see page 27) RAB [198] - Rosetta Antibody Benchmark, a handselected group of 60 diverse antibody-antigen complexes, also used in antibody structure prediction (see page 27). SKEMPI [209] - Structural Database of Kinetics and Energetics of Mutant Protein Interactions, a dataset of energy changes for mutations of protein-protein interactions.  

# Tasks  

Three general tasks are used to benchmark models in this field:  

Sequence and Structure Modeling - Given an antibody structure, predict its corresponding sequence (or viceversa). AAR is used to evaluate generated sequences, while RMSD is used to evaluate generated structures. Testing for this task is done on the SAbDab dataset. CDR-H3 Generation - Given a target antigen, generate the binding antibody CDR-H3 region. AAR is used to evaluate sequences, while RMSD and TM-Score are used to evaluate structures. Testing is done on the RAbD dataset. Affinity Optimization - Given an antibody-antigen pair, make changes to the CDR region to optimize its binding affinity with the target antigen. Performance is measured by $\Delta \Delta G$ , and testing is done on SKEMPI v2.0.  

# Metrics  

The following metrics are used to evaluate CDR generation models:  

AAR - Amino acid recovery, a comparison between ground truth and generated amino acid sequences, also measured in antibody representation learning (see 26). RMSD - Root-mean-square deviation, measures distances between ground truth and generated residue coordinates. Also used in protein structure prediction (see page 9)  

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Type of Model</td><td rowspan="2">Dataset</td><td colspan="2">AAR (%RMSD</td><td colspan="3">AAR (%, ↑)CDM-H3 De(</td><td rowspan="2">Afn(4)</td></tr><tr><td></td><td>(↓)</td><td></td><td></td><td>RMSD (↓)</td></tr><tr><td>LSTM [156]</td><td>LSTM</td><td>Self-made</td><td>15.69 [189]</td><td></td><td>22.36 [189]</td><td></td><td></td><td>-1.48 [189]</td></tr><tr><td>RefineGNN [157]</td><td>GNN</td><td>SAbDab</td><td>21.13 [189]</td><td>6[189]</td><td>29.79 [189]</td><td>0.8308[189]</td><td>7.55 [189]</td><td>-3.98 [189]</td></tr><tr><td>MEAN[189]</td><td>EGNN</td><td>SAbDab</td><td>36.38</td><td>2.21</td><td>36.77</td><td>0.9812</td><td>1.81</td><td>-5.33</td></tr><tr><td>AntiDesigner** [211]</td><td>MLP</td><td>SAbDab</td><td>37.37</td><td>1.97</td><td>40.94</td><td>0.985</td><td>1.55</td><td>-10.78</td></tr><tr><td>DiffAB [212]</td><td>Diffusion</td><td>SAbDab</td><td>26.78</td><td>3.597</td><td>35.31 [158]</td><td>0.9695[158]</td><td>一</td><td>-2.17 [158]</td></tr><tr><td>DockGPT[207]</td><td>Transformer</td><td>SAbDab</td><td></td><td></td><td></td><td></td><td>1.88</td><td></td></tr><tr><td>HERN [208]</td><td>EGNN</td><td>SAbDab</td><td></td><td></td><td>34.1</td><td></td><td></td><td>！</td></tr><tr><td>dyMEAN** [158]</td><td>EGNN</td><td>SAbDab</td><td></td><td></td><td>43.65</td><td>0.9726</td><td></td><td>-7.31</td></tr></table></body></html>

Table 12. A summary of relevant CDR generation models. All benchmarking metrics are self-reported unless otherwise noted. dyMEAN and DiffAB use slightly different evaluation conditions, so their results may not be fully comparable. $[ { ^ { * * } } ]$ denotes the current SOTA.  

TM-score [108] - Template Modeling Score, a distance measurement capturing both local and global similarities, also used in protein structure prediction (see page 9). ∆∆G - Change in binding energy after affinity optimization. Binding energy is predicted by the geometric network proposed by Shan et al. [210].  

# Models  

While some approaches to the CDR generation problem have been sequence-based, such as the LSTM used by Akbar et al. [156], the field has seen movement towards structure-based design and even more recently, sequence-structure co-design, as this allows models to incorporate both 1D amino acid sequence information and 3D structure information in the generation process. RefineGNN [157] was the first to introduce structure/sequence co-design, representing both structural and sequential information in a graph and performing iterative refinement. A Multi-channel Equivariant Attention Network (MEAN) [189] improves efficiency by predicting all amino acids within a CDR region at once, allowing for fewer iterations than RefineGNN’s prediction for each amino acid residue. AntiDesigner [211] implements a Protein Complex Invariant Embedding (PCIE) and dual Multi-Layer Perceptrons (MLPs) to design structures and sequences in a one-shot manner, removing the need for iterative refinement entirely. DiffAb [212] defines diffusion processes for both amino acid sequences and 3D backbone coordinates, serving as one of the first DDPMs in the field of antibody generation.  

While the above models all show significant progress in the specific CDR generation task, these models still require the input of fully docked antibody-antigen structures; thus, in real-world applications where only sequence and/or structure data is available, these models must be paired with structure prediction and docking methods to operate. As a result, some methods simultaneously implement other parts of the antibody generation process in their model. The deep learning method  

DockGPT [207] includes an adaptation of their protein-protein docking method for antibody CDR-H3 docking and design. By adding an encoding feature to distinguish CDR residues during training and providing structures with missing CDR regions as input, DockGPT simultaneously docks and designs all CDR regions. HERN [208] uses hierarchical equivariant refinement to both generate and dock paratopes for target antigens. However, HERN requires a defined epitope region as input and needs to be combined with epitope prediction models to dock with antigens whose epitopes are not identified. Dynamic Multichannel Equivariant Graph Network (dyMEAN) [158] extends these ideas even further by creating the first endto-end method, incorporating structure prediction, docking, and CDR generation in a singular model. To demonstrate its improved performance, dyMEAN benchmarks against pipelines combining models for each subtask $( \mathrm { I g f o l d } \Rightarrow \mathrm { H D o c k } \Rightarrow \mathrm { M E A N }$ to perform structure prediction, docking, and CDR generation, for example).  

# Peptide  

# Signal Peptide Prediction  

Signal peptides play a crucial role in protein translocation, but predicting these regions remains difficult due to the lack of clearly defined motifs. To address this issue, PEFT-SP [213] takes advantage of the abilities of existing large protein language models like ESM2. PEFT-SP uses efficient fine-tuning methods like Prompt Tuning [214] and Low-Rank Adaptation (LoRA) [215] to fine-tune ESM2 on an annotated signal peptide dataset created for SignalP 6.0 [216], the previous SOTA model. For benchmarking, performance was measured separately for each of the four organism groups in the dataset: Archaea, Eukarya, Gram-positive, and Gram-negative bacteria. PEFTSP outperforms SignalP 6.0 and other models over the majority of signal peptide prediction categories.  