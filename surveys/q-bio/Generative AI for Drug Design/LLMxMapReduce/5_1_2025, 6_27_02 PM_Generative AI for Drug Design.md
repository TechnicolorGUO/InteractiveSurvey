# 5/1/2025, 6:27:02 PM_Generative AI for Drug Design  

# 0. Generative AI for Drug Design  

# 1. Introduction  

<html><body><table><tr><td>Aspect</td><td>Traditional Drug Discovery</td><td>Al-Driven Drug Discovery</td></tr><tr><td>Time to Market</td><td>Typically 10-15 years [4,5,9,11,15,17,22,23,24,26,2 7,35]</td><td>Accelerated,potential for significant reduction (e.g., 18 months to Phase I[11])</td></tr><tr><td>Cost per Approved Drug</td><td>Frequently> 1billion,often 2.6 - $2.8 billion [4,5,9,11,12,13,17,23,26,27]</td><td>Potential for significant reduction (e.g., $2.7 million to Phase I[11])</td></tr><tr><td>Success Rate</td><td>Low,<10% of candidates reach market from clinical trials [5,12,14,17,35]</td><td>Potentialforimprovement by increasing quality of candidates [1,2,9,10,22,23,39]</td></tr><tr><td>Chemical Space Exploration</td><td>Labor-intensive screening, limited exploration [28]</td><td>Efficient analysis of vast data,explores larger space [4,9,15,22,34]</td></tr><tr><td>Process</td><td>Relies on researcher experience, repeated experiments, high failure rates[17,27,28,35]</td><td>Data-driven,pattern analysis, prediction, generation of novel structures [4,9,15,22,34]</td></tr></table></body></html>  

Traditional drug discovery and development is widely recognized as a lengthy, costly, and complex process [6,14,18,24,27]. Bringing a new drug to market typically takes over a decade, often averaging 10 to 15 years   
[4,5,9,11,15,17,22,23,24,26,27,35], and incurs substantial costs, frequently exceeding ​1billionandof tencitedas 2.6 billion to ​2.8billionperapproveddrug[4, 5, 9, 11, 12, 13, 17, 23, 26, 27].F urthermore, thesuccessrateisremarkablylow, withlessthan10 10^{20}​and $1 0 \cdot \{ 2 4 \} \$ 5$ [4,35], requiring extensive and often labor-intensive screening [28]. Traditional methods heavily rely on researchers' experience and repeated experiments, contributing to high screening failure rates and the phenomenon known as "Eroom's Law," the reverse of Moore's Law in the pharmaceutical sector, indicating exponentially decreasing returns on investment over time [17,27,35].​  

In response to these formidable challenges, Artificial Intelligence (AI) has emerged as a disruptive technology poised to transform the pharmaceutical landscape [2,10,22,23,27,39]. Generative AI (GenAI), in particular, is rapidly changing the field, offering unprecedented opportunities to accelerate drug discovery and development [1,9,15,22]. GenAI is capable of accelerating processes, significantly reducing costs, and improving the quality of potential drug candidates [1,2,9,10,22,23,39]. It achieves this by efficiently analyzing vast biological and molecular data, exploring a much larger chemical space than traditional methods, and crucially, creating novel molecular structures with desired properties [4,9,15,22,34]. Specific examples highlight this potential, such as the AI-driven identification of inhibitors for fibrosis-related kinases in just 21 days [2,9] or the rapid advancement of a novel small molecule to Phase I clinical trials within 18 months and at a cost of $\$ 2.7$ million using an AI platform [11]. Beyond discovery, Generative AI also shows promise in areas like pharmaceutical formulation development by creating digital product variants for analysis and optimization [25].  

The application of AI in drug discovery has evolved over time. Early computational methods like high-throughput screening (HTS) and virtual screening (VS) aimed to improve efficiency [9,16,41]. The rapid advancements in AI, particularly Machine Learning (ML) and Deep Learning (DL), fueled by increasing computational power and the availability of large-scale data  

(including real-world data) [33,42], have revolutionized these computational techniques [6,9,24,27,30,41]. This progression has culminated in the emergence of deep generative models (DGMs) and Generative AI, capable of de novo molecular design and complex data synthesis, marking a paradigm shift in how potential drug candidates are conceived and evaluated [4,9,24,42]. This evolution is reflected in the increasing investment and emergence of AI-driven pharmaceutical companies [18,27,39].​  

This survey aims to provide a comprehensive overview of the application of Generative AI in drug design. We explore the transformative potential of AI, particularly generative models, in revolutionizing the drug discovery paradigm [22,24]. The survey is structured to cover the key areas where Generative AI is making a significant impact, analyzing its capabilities, challenges, and future prospects in accelerating research, improving productivity and quality, and ultimately lowering the costs associated with bringing life-saving medicines to patients [6,14,22,27].  

# 2. Fundamentals and Generative AI Techniques for Drug Design  

The application of artificial intelligence (AI), machine learning (ML), and deep learning (DL) has become increasingly central to modern drug discovery, fundamentally transforming traditional paradigms and accelerating various stages from target identification to molecule design and optimization [6,16,24,26]. AI, encompassing ML and DL, facilitates the automatic identification of patterns and predictions from vast datasets [6], moving beyond simple classification and prediction towards tasks requiring creativity and innovation [1,3,22]. DL, a subfield of ML, leverages deep artificial neural networks with multiple hidden layers to learn complex, hierarchical data representations, enabling systematic analysis and prediction in areas like molecular property prediction and novel structure generation [6,37,39].  

Effective application of these AI techniques, particularly DL, hinges on the appropriate representation of molecular structures [9,11,21]. Common molecular representations include 1D strings like SMILES (Simplified Molecular Input Line Entry System), 2D molecular graphs (where atoms are nodes and bonds are edges), and 3D coordinates capturing spatial conformation [11,21]. SMILES notation is suitable for sequential processing models but can lead to the generation of chemically invalid strings [9,21]. Molecular graphs inherently encode atomic connectivity, aligning well with graph-based models, although generating valid graphs can be challenging [21]. 3D representations capture rich conformational information but introduce complexity in handling multiple possible conformations [21]. The choice of representation significantly impacts model architecture design and performance [21].  

Generative AI models represent a major advancement, focusing on the autonomous creation of new data, including novel molecular structures [1,3,22]. These models, often built upon deep learning algorithms, learn the underlying distributions of chemical space and generate novel molecules with desired properties [2,21].  

<html><body><table><tr><td>Technique</td><td>Molecular Representation Used</td><td>Key Strengths</td><td>Key Challenges</td><td>Primary Drug Design Use Case</td></tr><tr><td>Variational Autoencoders (VAEs)</td><td>SMILES, Graphs [9,11,13,21,37]</td><td>Continuous latent space for property optimization; Conditional generation variants [2,9,11]</td><td>Generating invalid structures; Requires large datasets [3,8,21]</td><td>De Novo Molecular Design</td></tr><tr><td>Generative Adversarial Networks (GANs)</td><td>Various (often implicit) [9,11,13]</td><td>Generates novel structures, potentially new scaffolds; Can be conditioned [9,34]</td><td>Training stability issues; Mode collapse; Difficult convergence [8,9]</td><td>De Novo Molecular Design</td></tr><tr><td>Recurrent Neural</td><td>SMILES (Sequential) [6,9,11,16,29]</td><td>Effective for sequential data; Learns</td><td>Large data requirement; Challenges with</td><td>De Novo Molecular Generation</td></tr></table></body></html>  

<html><body><table><tr><td>Networks (RNNs)</td><td></td><td>chemical Variants handle long dependencies [9,29] language;</td><td>graph complexity; Generates invalid strings [2,8,9] Generating</td><td>Property</td></tr><tr><td>Graph Neural Networks (GNNs)</td><td>Molecular Graphs [8,9,29]</td><td>Naturally handles molecular structure topology; Learns features automatically; Integrates geometric info [14,24,36]</td><td>valid graphs can be challenging; Data requirements [8,21]</td><td>Prediction, De Novo Generation</td></tr><tr><td>Transformers & LLMs</td><td>SMILES (Sequential), Text corpora [4,9,11,19,29]</td><td>Captures long- range dependencies; Learns chemical/biolog ical language; Interactive workflows [4,9,19,22,29]</td><td>Potential for generating inaccurate/hallu cinated results [3,22]</td><td>De Novo Molecular/Macr omolecule Design</td></tr><tr><td>Diffusion Models</td><td>3D Coordinates, Graphs [2,9,11]</td><td>Generates high- quality data samples; Shows promise in 3D molecule/protei n design [9,11,19,22]</td><td>Computationall y intensive; Can be complex to implement [2,9]</td><td>De Novo Molecular/Prote in Design</td></tr></table></body></html>  

Key generative AI techniques applied in drug design include Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs), Graph Neural Networks (GNNs), Transformers, Large Language Models (LLMs), and Diffusion Models [2,8,9,11,15,16,22,24,29,39].  

Variational Autoencoders (VAEs) are generative models comprising an encoder-decoder structure that maps molecular representations into a continuous latent space and decodes vectors back into molecules [9,11,13]. Regularization of the latent space enables sampling for novel molecule generation [9,11], and optimization within this space allows for generating molecules with targeted properties [2,11]. Variants like CVAEs and SSVAEs integrate property information for conditional generation [9]. Challenges include generating chemically invalid structures and requiring large datasets [3,8,21].  

Generative Adversarial Networks (GANs) utilize an adversarial game between a generator and a discriminator to learn to produce molecules indistinguishable from real data [9,11,13,18,25,29,39]. This framework facilitates the generation of novel structures, potentially with entirely new scaffolds [9,34]. GANs can be conditioned to generate molecules with desired properties [9,29]. Key challenges include training stability, mode collapse, and difficulty in convergence [8,9].  

Recurrent Neural Networks (RNNs), particularly LSTMs and GRUs, are effective for processing sequential molecular representations like SMILES strings [6,9,11,16,29]. They learn the chemical language to sequentially generate novel molecular structures [2,9]. LSTM/GRU mitigate the vanishing gradient problem, enabling capture of long-range dependencies crucial for complex structures [9,29]. Limitations include the need for large data, challenges with graph complexity, and the potential for generating invalid structures from SMILES [2,8,9]. Alternative representations like DeepSMILES and SELFIES aim to address the invalidity issue [8].  

Graph Neural Networks (GNNs) are inherently suited for molecular representation as graphs, directly processing atomic and bond information [8,9,29]. GNNs capture molecular structure topology and can integrate geometric information [14,24], learning relevant features without predefined descriptors [36]. They are widely used for property prediction (e.g., ADMET), drug-target interaction prediction, and generative tasks, often employing conditional generation to include desired properties [2,9,14,20,24,36,40].​  

Transformers, utilizing self-attention, efficiently handle long sequential data like SMILES, enabling accurate property prediction and modeling of complex chemical reactions [4,9,11,29]. Large Language Models (LLMs), built on Transformers, learn the language of chemistry and biology from vast scientific corpora [4,19,22], enabling de novo generation of molecules and macromolecules with desired properties [4,9,22]. LLMs also facilitate novel human-computer interaction paradigms and agent workflows in drug design [1,3,22]. A significant challenge is the potential for generating inaccurate or "hallucinated" results [3,22].​  

Diffusion models operate by progressively adding and then reversing noise to generate data samples, including molecular structures [2,9,11]. Equivariant Diffusion Models (EDMs) specifically generate 3D molecules from atom types and coordinates [9]. These models show promise in novel protein design, generating diverse and functional structures with wet lab validation [11,19,22].​  

Each of these generative AI techniques offers distinct advantages and faces specific limitations in drug design. VAEs provide a structured latent space for property optimization, GANs excel at generating novel scaffolds through adversarial training, RNNs are adept at sequential generation from strings, GNNs naturally handle graph structures for prediction and generation, Transformers and LLMs capture long-range dependencies and enable interactive workflows, and Diffusion Models show strength in 3D structure and protein generation. The combination of various techniques can yield synergistic effects, for example, integrating generative models with reinforcement learning for property optimization or combining different neural network architectures for enhanced performance in specific tasks [15]. Addressing common challenges, such as data requirements, ensuring chemical validity and synthesizability of generated molecules, improving model controllability over desired properties, and enhancing training stability and interpretability, remains critical for the continued advancement and widespread adoption of generative AI in drug discovery.​  

# 2.1 Deep Learning Models Overview  

Deep learning (DL) models, inspired by biological neural networks, are computational frameworks adept at learning complex, hierarchical patterns from large datasets [6,39]. Their ability to systematically analyze vast amounts of data using complex rules makes them invaluable in the drug discovery process, speeding up tasks such as predicting molecular properties, designing novel structures, and identifying potential targets [37,39]. A variety of deep learning architectures have been widely applied in drug design, including Deep Neural Networks (DNNs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Graph Neural Networks (GNNs), Generative Adversarial Networks (GANs), Autoencoders (AEs), and Deep Belief Networks (DBNs) [6,11,16,29,37].  

Deep Neural Networks (DNNs) are foundational feed-forward architectures utilizing multiple hidden layers to extract complex features from input data [16]. They are broadly used for predicting small molecule activity, often trained on molecular fingerprint data like Morgan Fingerprint [24]. DNNs have demonstrated utility in Quantitative Structure-Activity Relationship (QSAR) modeling, benefiting from increased computational power to address challenges like overfitting [8,13]. While DNN predictions in QSAR can be comparable to other methods when trained on similar data, their ability to handle multi-task learning contributes to their increasing popularity in drug design [8,10]. Specific DNN variants, such as Deep Residual Networks (DRNs) composed of preactivation residual blocks, have been employed for tasks like predicting bloodbrain barrier penetration [43].​  

Convolutional Neural Networks (CNNs) are particularly effective for processing grid-like data such as images, utilizing convolutional layers to learn a multitude of convolutional kernels from the training data [6,16]. In drug design, CNNs are applied to process molecular structure images and model specific prediction problems [6,29]. Their architecture, comprising convolutional, pooling, and fully connected layers, enables feature extraction suitable for molecular representation [16].  

Recurrent Neural Networks (RNNs), including variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), are designed to handle sequential data by incorporating relationships between consecutive steps [6,11,16]. This makes them well-suited for processing chemical molecule sequences, such as SMILES strings, which describe molecular structures linearly [9,29]. LSTM and GRU units specifically address the vanishing gradient problem, enabling the retention of key information over longer sequence steps necessary for generating complex molecular structures [9].  

Graph Neural Networks (GNNs), including Graph Convolutional Networks (GCNs), are a logical choice in chemistry due to the inherent graph structure of molecules, where atoms are nodes and bonds are edges [8,11]. GNNs are designed to process molecule graphs and effectively capture protein-ligand binding interactions [29,36]. Studies have demonstrated that GNN models like GCN, Message Passing Neural Network (MPNN), and AttentiveFP can outperform traditional machine learning models like Random Forest (RF) in specific tasks [20]. Graph-CNNs, a type of GNN, have shown effectiveness in learning fixedsize representations for protein pockets and modeling binding interactions [36].​  

Generative models, such as Generative Adversarial Networks (GANs) and Autoencoders (AEs), are employed for tasks like chemical structure design [29,37]. GANs operate by training a generative model to produce data from a defined target distribution, typically through an adversarial process involving a discriminator network [6]. AEs, including denoising autoencoders (DAEN) and variational autoencoders (VAEs), are primarily used for data dimensionality reduction, compression, and obtaining low-dimensional representations, which can also be leveraged for generating new data points within the learned latent space [6,37]. While both GANs and VAEs are generative models used for molecule design, RNNs approach generation from a sequential perspective (e.g., building molecules from SMILES strings), offering distinct strategies for exploring chemical space [6,9,15].  

Beyond these common architectures, specialized deep learning models like Deep Belief Networks (DBNs) composed of nonlinear hidden layers are utilized to mitigate issues like redundancy and overfitting [16]. Transformers are also increasingly applied for long sequence data, relevant for modeling multi-step chemical reactions in virtual screening [29]. Furthermore, specific models are developed for niche applications, such as MitoReID, a deep learning model utilizing a re-identification framework pre-trained with time-dimension enhancement, applied for analyzing mitochondrial dynamics and predicting drug mechanisms of action [31].​  

The application of these diverse DL models has led to tangible successes in drug discovery. Training on large datasets of known active compounds has demonstrated their ability to discover novel drug candidates [15]. For instance, ML algorithms (including DL variants) have been used to identify novel inhibitors for key therapeutic targets such as MEK and BACE1, relevant for cancer and Alzheimer's disease, respectively [15]. Deep learning has also improved prediction accuracy in various areas, from predicting small molecule activity based on fingerprints [24] to analyzing patient medical information for cancer treatment strategies and enabling rapid disease detection [26]. Specific examples like the performance improvement of GNNs over traditional RF models underscore the enhanced capabilities DL brings to tasks like predicting molecular properties or interactions [20].  

# 2.2 Variational Autoencoders (VAEs)  

Variational Autoencoders (VAEs) constitute a class of generative models extensively employed in de novo molecular design. The fundamental architecture of a VAE comprises an encoder network and a decoder network [9,11,13]. The encoder maps high-dimensional molecular representations, such as Simplified Molecular Input Line Entry System (SMILES) strings or graph structures, into a lower-dimensional continuous latent space [9,11,13,21,37]. The decoder subsequently translates vectors from this latent space back into valid molecular structures [2,9,13,21,37]. VAEs utilize regularization techniques, such as Kullback-Leibler (KL) divergence, within the latent space to ensure continuity and mitigate overfitting and discontinuity issues [9,11]. This regularization allows for the generation of novel molecules by sampling from the latent distribution [11].​  

A key aspect of leveraging VAEs for drug design involves optimizing or manipulating the latent space to facilitate the generation of molecules possessing specific, desired properties [11]. By navigating or optimizing within this learned continuous space, researchers can search for latent vectors that, upon decoding, yield molecules predicted to exhibit enhanced characteristics like solubility or activity [2,37]. A seminal demonstration of this approach is the work by GómezBombarelli et al., which utilized a VAE framework where the latent space was optimized, potentially through an additional network or methods like Bayesian optimization, to reflect a particular target property [8,37]. More recent efforts, such as CogMol proposed by Chenthamarakshan et al. in 2020, have incorporated multi-attribute controlled sampling schemes within the VAE framework to design molecules with predefined sets of attributes [6]. Examples like ChemVAE further illustrate the capacity of VAEs to generate compounds with targeted profiles [2]. Furthermore, variations like Variational  

Graph Autoencoders (VGAEs) adapt the VAE principles for graph-based molecular representations, deriving low-dimensional latent variables as universal molecular descriptors, as seen in frameworks like NYAN [40].  

While standard VAEs learn a latent representation of molecular structures, their ability to directly control generated properties can be limited. To address this, variants such as Conditional VAEs (CVAEs) and Semi-supervised VAEs (SSVAEs) have been developed [9]. CVAEs integrate molecular property information directly during the encoding process, thereby allowing for the generation of molecules conditioned on desired properties [9]. This makes CVAEs particularly suitable for scenarios where specific property targets are well-defined and sufficient labeled data is available. SSVAEs, on the other hand, are designed to combine molecule generation and property prediction tasks within a single framework [9]. This hybrid approach is advantageous in applications where attribute data is limited, as it can leverage both labeled and unlabeled data for training. Other related models like Adversarial Autoencoders (AAEs) employ adversarial training to shape the latent space [9], while variations like GrammarVAE and Syntax-directed VAE focus on generating valid molecular strings [11].​  

Despite the promise demonstrated by VAEs and their variants in de novo molecular design, significant challenges remain. A primary challenge identified early in the application of VAEs to molecular generation is the potential for generating chemically invalid structures [21]. Ensuring that the decoded outputs represent chemically plausible and synthesizable molecules is crucial for practical drug discovery applications [3]. Furthermore, VAEs, like many deep learning models, typically require large volumes of data for effective training to accurately model chemical space distributions [8]. Addressing these challenges necessitates improvements in decoder design, regularization techniques, and potentially integrating chemical domain knowledge or validity checks into the generative process.  

# 2.3 Generative Adversarial Networks (GANs)  

Generative Adversarial Networks (GANs) constitute a class of deep learning models employed in the de novo generation of molecules within the domain of drug design [16,18]. A GAN architecture fundamentally comprises two neural networks: a generator (G) and a discriminator (D) [9,11,13,18,25,29,39]. These networks are pitted against each other in an adversarial, zero-sum game framework [11,13,18].​  

The process involves the generator network producing candidate molecular structures, often initialized from random noise [11]. Concurrently, the discriminator network is trained to distinguish between these generated molecules and authentic molecular structures from a training dataset [9,11,13,25,29,39]. During training, the generator aims to produce molecules that are indistinguishable from real ones, effectively trying to "fool" the discriminator, while the discriminator strives to correctly identify synthetic molecules and maximize their error rate [9,11]. This iterative competition drives the generator to produce increasingly realistic and diverse molecular structures [2,13], enabling the learning and generation of novel structures, sometimes with entirely new molecular skeletons [9,34,39]. The training objective for GANs is typically formulated as a minimax game:​  

# 无效公式  

where G is the generator, D is the discriminator, x represents real data samples from the data distribution p₍data₎, and z represents noise samples from the prior distribution $\mathsf { p } _ { ( } Z _ { \mathsf { 1 } }$ [11].  

GANs have been successfully applied in various drug design tasks. They are useful for virtual screening by generating new molecule structures [29]. Training can be directed towards generating molecules with desired properties, potentially reducing experimental costs [6,9,29,39]. Specific examples include the ORGAN model used for generating inhibitors against the COVID-19 protease [2] and the ORGANIC model, both applied to molecule generation [11]. Conditional GAN variants, such as the entangled conditional adversarial autoencoder (ECAAE) proposed in 2018, enable generation conditioned on specific attributes like activity, solubility, or synthetic performance against target proteins [6]. Beyond molecular structure, GANs have also been employed to generate 3D images of drug formulations, conditioned on key quality attributes, for in silico analysis and optimization [25]. Companies like Insilico Medicine leverage GANs for generating novel molecular structures [34], aiming to develop candidates for various diseases, including cancer, metabolic, and neurodegenerative conditions, as well as molecules with potential anti-fibrotic activity [28,39]. This approach has demonstrated the potential for accelerated discovery, evidenced by a project completing target discovery to drug design in just 46 days [28].  

Despite their capabilities, training GANs presents significant challenges. Convergence is not always straightforward [8], and issues such as mode collapse and instability can occur [8,9]. Mode collapse refers to the generator producing only a limited variety of samples, failing to capture the full diversity of the real data distribution [8,9]. Training can also be unstable,  

sometimes resulting in the discriminator overwhelming the generator [8]. While these challenges are recognized in the application of deep learning to computational chemistry and drug design, specific methods for addressing them within the context of molecular generation applications are areas of ongoing research.  

Generative models like GANs have been compared with other generative architectures, such as Variational Autoencoders (VAEs), in the context of extracting molecular features and generating candidates [13]. However, a detailed comparative analysis highlighting the specific strengths and weaknesses of GANs relative to VAEs based on their performance or characteristics in drug design applications is necessary for a comprehensive understanding of their respective utility.  

# 2.4 Recurrent Neural Networks (RNNs)  

Recurrent Neural Networks (RNNs) constitute a class of neural networks particularly well-suited for processing sequential data, a characteristic that aligns with the representation of molecules as strings. RNNs have been extensively applied in the field of drug discovery, enabling tasks such as molecule generation and virtual screening [16,29]. A common approach involves utilizing Simplified Molecular Input Line Entry System (SMILES) strings as the sequential input and output [9,11]. By processing SMILES strings, RNNs can learn the underlying chemical language and generate novel molecular structures sequentially, effectively modeling molecular distributions [2,9].​  

A key challenge in training traditional RNNs is the vanishing gradient problem, which limits their ability to capture longrange dependencies in sequential data. To mitigate this, variants such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) have been developed [11]. These architectures incorporate gating mechanisms that allow them to selectively retain relevant information over extended sequence steps, thereby improving their capacity to model complex sequential patterns inherent in molecular structures represented as SMILES strings [9,29]. LSTM and GRU units enable RNNs to retain key input information across longer sequences, which is crucial for accurately representing chemical connectivity and structural motifs [9]. Specific applications leveraging LSTM architectures include the optimization of molecular properties like quantitative estimate of drug-likeness (QED) and synthetic feasibility in drug design workflows [2]. Furthermore, LSTM-based models have been successfully applied to generate targeted chemical libraries for specific biological targets, demonstrating their utility in generating novel lead compounds [6]. For instance, studies have utilized LSTM models trained on databases like ChEMBL and fine-tuned on specific datasets to generate potential inhibitors for targets such as p300/CBP [6]. Another generative deep learning (GDL) model based on the LSTM algorithm has been proposed to generate molecules following the distribution of a training set, enabling the creation of customized virtual compound libraries [6].​  

Despite their successes, RNNs, particularly when relying on SMILES, face certain limitations. One significant challenge is the substantial volume of data required for training, as these models learn by modeling molecular distributions [8]. Furthermore, RNNs can struggle with the inherent complexity of molecular graphs, which are not always intuitively represented by linear SMILES strings. A notable practical issue is the potential for RNNs trained on SMILES to generate syntactically or chemically invalid molecular strings [2,9]. This necessitates post-processing validation steps. To address some of the limitations associated with the SMILES syntax in the context of deep learning, alternative molecular representations such as DeepSMILES and SELFIES have been developed and explored [8]. These representations aim to minimize or eliminate the generation of invalid structures, offering potentially more robust inputs for sequential generative models.​  

# 2.5 Graph Neural Networks (GNNs)  

Graph Neural Networks (GNNs) are a class of deep learning models particularly well-suited for processing data represented as graphs, which naturally aligns with the structural representation of molecules [8,9,29]. In molecular graphs, nodes typically represent atoms and edges represent the bonds connecting them [11]. This structure allows GNNs to effectively capture the intricate relationships between atoms and bonds within a molecule [29].  

A significant advantage of GNNs lies in their ability to learn complex molecular features and interactions directly from the graph structure. Unlike traditional methods that rely on predefined molecular descriptors or fingerprints, GNNs can learn molecular structure topology information [24] and integrate geometric information [14], automatically extracting features that are relevant to the prediction task [36]. This contrasts favorably with traditional molecular fingerprints, such as Morgan's circular fingerprint, which are fixed-length vectors derived through predetermined rules. Methods like the neural fingerprint proposed by Duvenaud et al. utilize graph convolution models to learn a fixed-length vector representation from a molecular state matrix [37]. The bit values in this neural fingerprint are learned through training and are differentiable, offering an advantage over the non-differentiable nature of traditional fingerprints [37].  

GNNs are widely applied in predicting various molecular properties and biological interactions crucial for drug discovery. They can be used to predict ligand-protein binding energy [24] and drug-target interactions (DTI) by processing both molecules and proteins (or protein pockets) as graphs [36]. For instance, the Graph-CNN framework extracts features from protein pocket graphs and ligand graphs to predict DTIs [36]. GNNs are also suitable for predicting protein-protein interactions [2], ADMET properties with superior performance due to their ability to integrate geometric information [14], and for training data from DEL screening using models like graph convolutional networks (GCN) and message passing neural networks (MPNN) [20]. Specific GNN-based architectures like SchNet, PotentialNet, and DimeNet have been developed and successfully applied for molecular property prediction tasks [11].  

Beyond property prediction, GNNs are powerful tools for generating novel molecular structures [2,9]. They can generate new molecular graphs, often employing conditional generation techniques to incorporate desired chemical properties into the generated molecules [9]. Examples include using models like GraphGMVAE with a dual message-passing network (DualMPNN) for scaffold hopping in molecular design [2]. Novel architectures such as the hierarchical graph encoder-decoder proposed by Jin et al. have shown improved performance in various molecular generation tasks [21]. Graph-based conditional generative models have also been applied to design synthesizable drug-like molecules with specific dual inhibitory activities, demonstrating the potential for designing molecules with targeted polypharmacology [21]. Variational graph encoders, which combine GCNs with variational autoencoders, are another type of GNN-based generative model widely employed for biomedical graph-structured data, including molecular generation [40]. GNNs are capable of handling tasks at different granularity levels, including node-level, edge-level, and graph-level operations [11], making them versatile for diverse molecular design and analysis tasks.​  

# 2.6 Transformers and Large Language Models (LLMs)  

Transformer models have emerged as powerful architectures for processing sequential data, a capability highly relevant to drug design where molecules can be represented as sequences (e.g., SMILES) or graphs. A key innovation of Transformers is the self-attention mechanism, which allows the model to capture long-range dependencies and contextual information within sequences [4,9,11]. This ability to weigh the importance of different parts of a sequence simultaneously, rather than processing sequentially as in recurrent models, makes Transformers particularly well-suited for tasks involving long molecular data [29]. Consequently, Transformers facilitate more accurate predictions of molecular properties by effectively modeling complex relationships within the molecular structure [29]. Representative models like SMILES-BERT and SMILESTransformer leverage this mechanism for molecular property prediction and representation learning [11]. The core principle often involves pre-training these models on massive unlabeled molecular data to learn robust representations, which are then fine-tuned on smaller, high-precision datasets for specific downstream tasks like property prediction and virtual screening, thereby improving generalization and potentially reducing wet lab costs [19].  

Beyond property prediction, Transformers show significant potential in predicting chemical reactions, including complex multi-step processes [29]. They are applied to retrosynthetic analysis, predicting regioselectivity and stereoselectivity, and extracting reaction fingerprints [14]. Models such as RetroExplainer reframe retrosynthesis as a molecular assembly problem within an interpretable deep learning framework, demonstrating improved interpretability and performance [14].  

Large Language Models (LLMs), built upon the Transformer architecture, are a category of generative AI models distinguished by their capacity to generate coherent text [1,3,4,22]. These models learn the "language" of chemistry and biology by being trained on vast corpora of chemical structures, biological sequences, and scientific text [4,19,22]. This training allows them to perform representation learning on molecules and macromolecules like nucleic acids and amino acids [19,32]. Through this acquired understanding, LLMs can be employed for denovomolecular design, generating novel molecules or macromolecular components with desired properties [4,9,22]. Examples include the generation of novel antimicrobial peptides with diversity surpassing training data [2] and the design of new drug carriers [32]. LLMs facilitate a new paradigm of human-computer interaction, enabling agent workflows where conversational agents, supported by LLMs, can generate responses and perform actions to achieve drug design objectives throughout the development process, from generation to clinical trials [1,3,15,22]. The emergence of models like ChatGPT and Gemini underscores the growing influence of LLMs in drug research and development [12]. They can also function as virtual experts, aiding decision-making in drug development, particularly for early-stage biotechnology companies [17].  

Despite their capabilities, a significant challenge associated with using LLMs in drug design is their potential to generate inaccurate or "hallucinated" results, which may manifest as chemically invalid structures or incorrect property predictions [3,22]. Addressing these challenges requires careful model design and training strategies. One approach to improve the reliability of LLM outputs involves developing agent systems that break down complex requests into smaller, more manageable components [3,22]. This decomposition allows for stepwise processing and verification, thereby enhancing the accuracy of the generated responses and actions [3,22].​  

The potential of these models can be further enhanced through the integration of reasoning skills and the ability to utilize external tools, leading to what are sometimes referred to as Augmented Language Models (ALMs) [4]. While direct quantitative comparisons against all traditional drug design methods across diverse tasks are complex and not uniformly detailed in the provided literature, the capabilities offered by Transformers and LLMs, such as effective handling of long sequences, learning complex molecular representations, and enabling interactive agent-based workflows, represent a significant advancement and offer transformative potential for enhancing the efficiency and creativity of the drug discovery process compared to many traditional, often more rigid, computational approaches [3].​  

# 2.7 Diffusion Models  

Diffusion models represent a significant class of generative models applied in drug design, particularly noted for their capability in generating molecular structures. The fundamental principle involves a two-stage process: initially, random noise is progressively introduced to data samples—often following a Markov chain—effectively transforming the data into a noisy distribution, such as a Gaussian distribution [2,9,11]. Subsequently, a deep network is trained to learn and reverse this noisy process, allowing the model to reconstruct data-like samples or generate entirely new molecules from noise [2,9,11].  

Regarding architectures and approaches within this framework, Equivariant Diffusion Models (EDM) are specifically designed to handle the generation of 3D molecules by operating directly on atom types and coordinates [9]. Another notable approach is Chroma, which integrates diffusion models with graph neural networks to generate high-quality protein structures [19].  

Diffusion models have shown rapid progress, particularly in the domain of novel protein design [22]. Examples like RFdiffusion [11,19] and Family-wide Hallucination [11] are diffusion-based methods successfully applied to protein design tasks. RFdiffusion has demonstrated the feasibility of diffusion models for large molecule drug design, supported by wet lab validation on specific targets [19]. Furthermore, studies have shown that proteins designed computationally using RFdiffusion can achieve high affinity without the need for experimental optimization [19]. Chroma has also demonstrated the generation of diverse and innovative protein structures from scratch, including over 300 novel proteins exhibiting programmable properties not observed in natural proteins, with successful validation in laboratory settings [19]. These results underscore the potential of diffusion models in generating diverse and novel molecular structures relevant to drug discovery.​  

# 3. Optimization Techniques in Generative AI for Drug Design  

Optimization techniques are fundamental to enhancing the performance and utility of generative AI models in drug design, enabling the efficient discovery and optimization of molecules with desired properties [2,9]. These methods guide the AI models to navigate vast chemical spaces more effectively, refine generated structures, and accelerate the overall drug discovery pipeline. This section provides an overview of key optimization strategies employed, focusing on Reinforcement Learning, Transfer Learning, and Multi-objective Optimization [9,21].​  

Reinforcement Learning (RL) plays a significant role by treating the generative model as an agent that learns through iterative interactions with an environment representing the chemical space [8]. The agent receives feedback in the form of rewards based on the properties of generated molecules, allowing it to refine its generation policy to maximize desired outcomes, such as target binding affinity or drug-likeness [2,3,8,13]. RL techniques, including policy-based and value-based methods [9], are applied in tasks like de novo molecule generation and virtual screening to produce compounds optimized for specific targets [24,27]. Successful examples include the application of models like DrugEx for designing effective antagonists [2] and RL-biased generative models for optimizing multi-property profiles [16,21]. Despite these successes, challenges persist, notably in designing accurate and effective reward functions that capture complex biological activities and in balancing the exploration of novel chemical space with the exploitation of promising regions [8,34].​  

Transfer Learning accelerates the drug design process by leveraging knowledge encoded in models pre-trained on large, general molecular datasets [2]. This approach allows models to capture general chemical principles and representations, providing a strong starting point for more specific tasks. By transferring these learned features and fine-tuning the models on smaller, task-specific datasets—often proprietary—researchers can adapt them to predict or generate molecules tailored for specific biological targets or therapeutic areas [2,9,17]. This enables efficient adaptation and high predictive accuracy even with limited target-specific data, as demonstrated by the use of transfer learning phases in generative models [21].  

Furthermore, Multi-objective Optimization is critical in drug design because the process typically involves balancing multiple, often conflicting, molecular properties simultaneously, such as potency, selectivity, solubility, metabolic stability, and synthetic accessibility [21,34]. Techniques are employed to address these complex trade-offs and identify molecules that represent optimal compromises across multiple desirable characteristics [21].  

The following sub-sections will delve deeper into the principles, applications, specific examples, and ongoing challenges associated with each of these optimization techniques in the context of generative AI for drug design.  

# 3.1 Reinforcement Learning (RL) for Optimization  

Reinforcement learning (RL) is increasingly utilized in generative drug design to guide the search for novel molecules with specific desirable properties [2,3,8,16,24,34]. In this paradigm, the generative model is treated as an agent that learns to perform actions, typically represented as modifications or additions of chemical characters (such as in SMILES strings), within an environment corresponding to the chemical space or molecule generation process [8]. The agent's objective is to maximize a cumulative reward signal, which is based on the predicted properties of the generated molecules [8,13]. This feedback mechanism allows RL algorithms, including policy-based and value-based methods [9], to iteratively refine the generation process towards optimizing target properties like binding affinity or drug-likeness [2,3].​  

RL plays a crucial role in de novo molecule generation and virtual screening [27]. Models like REINVENT, when combined with RL, can generate small molecule compounds optimized for specific targets [24]. The DrugEx model, another example, has been successfully applied to design highly effective antagonists for targets such as the adenosine A2A receptor by optimizing molecular properties through feedback [2]. Real-world applications demonstrate the efficacy of this approach; for instance, Niclas Stahl et al. utilized RL to generate over two million chemical structures, identifying 387 compounds possessing desired ideal properties and molecular features, showcasing improved efficiency in generating hit compounds [16]. Furthermore, the integration of RL with other techniques has accelerated discovery cycles. Zhavoronkov et al. combined RL with variational inference and tensor factorization, employing a reward function to prioritize compounds for DDR1 kinase inhibitors, leading to the identification of six candidates within two months, with two showing strong in vitro activity and one demonstrating good in vivo efficacy and pharmacokinetics in mice [21]. Similarly, Bung et al. combined generative and predictive models biased by RL to design central nervous system-penetrant molecules modulating the 5- HT1B receptor, optimizing multiple properties simultaneously including docking score, blood-brain barrier permeability, hydrophobicity, and molecular weight [21]. Leading companies like Insilico Medicine leverage RL for generating novel molecular structures [34].​  

Despite its promise, applying RL to de novo drug design presents significant challenges, particularly concerning the design of effective reward functions and balancing exploration and exploitation [8,34]. A critical challenge lies in designing appropriate reward functions that accurately reflect complex biological activities and desirable pharmacological profiles [34]. Often, the optimization requires balancing multiple, potentially conflicting, objectives, such as efficacy, safety, solubility, and metabolic stability [34]. Creating a composite reward function that effectively weighs these diverse properties is non-trivial. Moreover, ensuring the synthetic feasibility of the generated molecules remains a significant hurdle, requiring reward mechanisms or post-processing steps to filter or prioritize synthesizable structures [34]. The inherent trade-off between exploring novel chemical space to discover truly innovative structures and exploiting known scaffolds to generate high-probability candidates with desired properties is also a critical challenge that RL algorithms must address to avoid converging prematurely to suboptimal solutions or generating invalid structures [8,34].​  

# 3.2 Transfer Learning  

Transfer learning represents a significant advancement in accelerating the drug design process by leveraging knowledge encoded in pre-trained models developed on extensive molecular datasets [2]. This approach allows models to quickly adapt to specific tasks within drug discovery, circumventing the need to train models from scratch on limited proprietary data. The fundamental principle involves transferring learned features and representations from a source domain (e.g., a large general molecular library) to a target domain (e.g., the design of anti-cancer drugs or prediction of activity against a specific target) [2].​  

Pre-trained models, often developed on vast collections of publicly available molecular structures and properties, capture general chemical principles and patterns. These models serve as a strong initialization point for more specialized tasks. For instance, a generative model pre-trained on a large molecular database can be adapted for generating molecules with desired properties or activities. Bung et al., as reported, employed a generative model obtained through a transfer learning phase, illustrating the application of this methodology in practice [21].​  

Fine-tuning is the subsequent crucial step where these pre-trained models are adjusted using smaller, task-specific datasets. In biological and pharmaceutical research, accumulating proprietary data is often essential for achieving high predictive accuracy. Fine-tuning the pre-trained models with such proprietary data allows them to specialize and attain performance levels superior to those achievable with general models alone [17]. This process tailors the model to specific biological targets or drug design objectives, enabling accurate predictions for tasks like assessing target binding affinity or predicting ADMET properties, ultimately accelerating the identification and optimization of potential drug candidates [2].  

# 4. Applications of Generative AI in Drug Discovery  

Generative Artificial Intelligence (AI) is rapidly transforming the landscape of drug research and development, offering unprecedented capabilities to accelerate and enhance various stages of the discovery pipeline [1,2,6,9,10,16,17,18,23,24,27,28,29,30,39]. By leveraging complex algorithms and vast datasets, AI facilitates significant improvements over traditional methods, leading to increased efficiency, enhanced accuracy, and potentially reduced costs throughout the drug discovery process [16,17,27].  

![](images/53b967da8c2bb159d91e00c739bc83da9c880910d9859f913fcc2e31ea586dc8.jpg)  

The application of generative AI spans critical phases, beginning with Target Identification and Validation, where AI analyzes multi-omics and literature data to pinpoint disease-relevant targets and pathways [1,9,15,23]. In Hit Discovery and Virtual Screening, AI models rapidly sift through enormous chemical spaces, predicting binding affinities and properties to identify promising lead compounds more effectively than conventional high-throughput screening or simpler computational methods [6,9,10,18,24,27,29].​  

De Novo Molecular Design represents a paradigm shift, enabling the creation of entirely novel molecular structures optimized for desired pharmacological profiles from scratch [6,18,39]. Subsequently, Lead Optimization benefits from AI's ability to predict and refine key drug properties, including efficacy, selectivity, and pharmacokinetics [6]. ADMET Prediction and Optimization are significantly enhanced by AI's capacity to model complex absorption, distribution, metabolism, excretion, and toxicity profiles early in the process, mitigating late-stage failures [7,9,29].  

Furthermore, AI assists in Formulation Optimization by guiding the design of drug delivery systems with improved stability and bioavailability. Drug Repurposing is accelerated through AI's capacity to identify new therapeutic uses for existing drugs by analyzing diverse biological and clinical data sources [6,9]. Synthesis Planning and Automated Synthesis leverage AI to predict viable reaction routes and automate chemical synthesis, streamlining the process from design to physical molecule [10,12,14]. Finally, AI-Driven Phenotypic Drug Discovery utilizes image-based profiling and deep learning to analyze complex cellular responses, inferring drug mechanisms of action and facilitating target identification or repurposing [31].  

Across these stages, AI's strength lies in its ability to integrate and analyze vast, heterogeneous datasets, identify subtle patterns, and generate testable hypotheses or optimized molecular designs, significantly accelerating the pace and increasing the success rate of bringing novel therapies to patients compared to traditional, often labor-intensive and empirical approaches.  

# 4.1 Target Identification and Validation  

The identification and validation of suitable drug targets represent a critical initial phase in the drug discovery pipeline. Artificial intelligence (AI) and machine learning (ML) algorithms are increasingly pivotal in this process by enabling the integration and analysis of vast and disparate biological and biomedical data sources [1,7,9,14,15,22,27,28,35,38]. These methods facilitate the identification of novel drug targets and the elucidation of complex disease mechanisms [23].  

AI/ML approaches leverage diverse data modalities, including large multi-omics datasets (genomic, transcriptomic, metabolomic, etc.), biological networks (protein–protein interaction, metabolic, transcriptional), and extensive scientific literature [5,7,12,13,14,35]. By mining and analyzing these datasets, AI can provide insights into the potential structure and function of biological targets and predict their roles in disease pathways [7]. Natural Language Processing (NLP) techniques are employed to extract information from scientific publications, identifying associations between drugs, diseases, genes, and targets from sources like Medline abstracts [12,14,35,37]. NLP can also map gene functions to enhance target identification sensitivity [12,14,35]. Generative AI, in particular, excels at knowledge extraction from scientific literature and real-world data, aiding in the exploration of novel drug targets and potential new indications for existing molecules [3,22,32]. It can identify genes or proteins linked to new diseases by processing vast amounts of text [3,22].  

Machine learning methods analyze large databases of target function information to predict potential causal relationships [37]. Examples include decision tree–based meta-classifiers built using network topology of protein–protein, metabolic, and transcriptional interactions, along with tissue expression and subcellular localization, to predict morbidity-associated genes [37]. Support Vector Machine (SVM) classifiers have been established using genomic datasets to distinguish drug from nondrug targets in specific cancers [37]. Graph deep learning methods, combined with multi-omics networks, are used to predict cancer genes [12,14]. Computational methods identify disease-related genes and proteins within large biological networks (e.g., PPI) using techniques like correlation or Bayesian networks and network propagation algorithms [13]. Furthermore, methods like the Graph-CNN framework contribute by predicting protein–ligand interactions without requiring pre-existing target–ligand complex structures [36]. Specific platforms, such as PandaOmics, integrate multi-omics data for target identification, successfully identifying targets like TRAF2- and NCK-interacting kinases for antifibrotic therapies, which has driven the development of inhibitors like INS018_055 [12,14]. Similarly, Insilico Medicine employed generative AI to analyze omics data, identifying TNIK as a potential target for idiopathic pulmonary fibrosis (IPF) by comparing transcriptomic, genomic, and metabolomic differences between patients and healthy individuals [5]. The MitoReID model demonstrates target prediction by analyzing mitochondrial feature similarity between new drugs and known targets, accurately classifying FDA-approved drugs by their mechanism of action (MOA) [31].  

Compared to traditional, often hypothesis-driven or single-dataset approaches, AI-based target identification offers significant advantages [27]. AI's ability to integrate and analyze massive, heterogeneous datasets at scale allows for the identification of non-obvious targets and complex biological patterns that are difficult to discern manually or with simpler methods [7,12,14,35]. While explicit, quantitative comparisons of speed and accuracy advantages over traditional methods are often context-dependent, the demonstrated capability of AI systems to rapidly process vast amounts of information and generate hypotheses strongly suggests improvements in efficiency and the potential for enhanced accuracy in identifying relevant therapeutic targets.​  

Despite the advancements, challenges remain in the validation of AI-identified targets and their translation into effective therapies [1,7,9]. The identification process itself, even when accelerated by AI, requires thorough validation, which can still be time-consuming (taking weeks or months) and is crucial for ensuring the quality and reliability of results [3,22]. Translating a validated target into a successful drug involves navigating the complexities of preclinical and clinical development. Data-related challenges are particularly pertinent, including the intricate task of integrating diverse multiomics datasets and mitigating biases, such as publication bias, inherent in the data sources used to train AI models [12,14,35]. Furthermore, ensuring the interpretability of complex AI models used for target identification poses a challenge for biological validation and regulatory assessment [12,14,35].​  

To enhance the reliability of target selection, AI predictions must be complemented by robust experimental validation techniques [35]. High-throughput single-cell sequencing, such as Singleron's GEXSCOPE® technology, enables precise characterization of cancer cell transcriptomics, aiding in the identification and initial validation of potential therapeutic targets [38]. Integrating single-cell datasets with clinical records, as done by platforms like SynEcoSysTM, provides critical support for validating targets by associating molecular findings with real-world patient conditions [38]. Dedicated pipelines, such as SingleTronTM, are being developed to provide reliable evaluation frameworks for predicted drug targets [38]. The synergistic integration of AI-driven computational predictions with advanced experimental validation techniques is essential for improving the efficiency and success rate of drug target identification and ultimately translating these findings into viable therapeutic options.​  

# 4.2 Hit Discovery and Virtual Screening  

Hit discovery and virtual screening (VS) represent crucial early stages in the drug development pipeline, aiming to identify molecules with potential activity against a biological target. Traditionally, this involved high-throughput screening (HTS) of large physical compound libraries or computationally intensive docking simulations. Artificial intelligence (AI), particularly machine learning (ML) and deep learning (DL), has revolutionized these processes by enabling the generation and rapid screening of vast virtual compound libraries to identify potential hits with desired binding affinity and selectivity [2,6,9,10,18,23,24,27,28,29]. AI accelerates VS by predicting chemical properties, biological activity, potential efficacy, and adverse events based on compound specificity and affinity for targets, and can clarify drug-target interactions [7].  

Compared to traditional methods, AI-driven virtual screening offers significant advantages, particularly in speed, accuracy, and the capacity to explore vast chemical spaces and identify novel molecular structures [6,8,9,24,29]. ML provides a more efficient screening strategy compared to traditional HTS [24,28]. AI enables rapid analysis of extensive data to predict compound bioactivity, thereby significantly reducing screening time [28]. For instance, Atomwise utilized deep learning to screen for potential Ebola virus treatments in days, a process that would traditionally take months or years [28]. Generative AI (Gen AI) can further accelerate screening through advanced chemical models, predicting subsequent atoms or amino acids in molecular structures, potentially improving compound activity model performance by 2.5 times and speeding up the identification of new compound series by over 4 times, reducing the timeline from months to weeks [32]. AI facilitates ultra-large-scale virtual screening; Sanofi's collaboration with Atomwise, leveraging the AtomNet platform (based on convolutional neural networks), allowed screening of over 3 trillion synthesizable compounds [18]. Methods like V-SYNTHES utilize hierarchical enumeration screening to screen virtual libraries of over 11 billion compounds efficiently [6]. AI expands the number and diversity of molecules in virtual compound libraries beyond readily available physical compounds [6].  

A variety of AI algorithms and models are employed in virtual screening. ML models such as nearest neighbor classifiers, Random Forests (RF), Support Vector Machines (SVM), Extreme Learning Machines, and Deep Neural Networks (DNNs) are commonly used to predict molecular activity and toxicity [26]. Deep learning models can learn intricate relationships between compounds and target proteins from large datasets, extracting valuable features and interaction patterns [29]. Convolutional Neural Networks (CNNs), as implemented in platforms like AtomNet, are adept at structure-based drug design and large-scale screening [18]. Graph-CNNs have shown performance comparable to or better than traditional methods like AutoDock Vina and other ML methods (RF-Score, NNScore) on benchmark datasets [36]. Specific tools like PyRMD, which implements the Random Matrix Discriminant (RMD) framework, enable efficient screening of millions of compounds in hours and can be trained automatically using bioactivity data from repositories like ChEMBL [41]. Iterative ML methods, such as HASTEN, accelerate structure-based VS by predicting molecular docking scores [6]. AI can also be used to generate candidate molecules for screening; for example, Insilico Medicine used Generative Adversarial Networks (GANs) to generate and screen compounds targeting the SARS-CoV-2 main protease [2]. Furthermore, models like GeminiMol utilize molecular conformation space similarity contrast learning to identify potential active molecules with similar 3D conformations despite potentially dissimilar 2D structures, performing well in ligand-based VS, target identification, and property prediction [15]. AI models trained on DNA-Encoded Library (DEL) screening data have been successfully used to predict and screen external compound libraries [20].  

AI enhances virtual screening by improving the accuracy of quantitative structure-activity relationship (QSAR) models and optimizing docking score functions [24]. QSAR models combined with deep learning can predict the activity and selectivity of small molecules [24]. AI-based receptor-ligand docking models aim to predict ligand spatial transformations and generate complex atomic coordinates for VS [12]. Models based on AlphaFold2 and RosettaFold predict complex structures from sequence information [12]. AI models are trained on diverse datasets, including target bioactivity data from ChEMBL [41], DEL screening data [20], datasets for specific properties like blood-brain barrier penetration (e.g., BBBP dataset) [43], and toxicity data (e.g., Tox21 project data) [39]. Validation methods typically involve benchmark metrics [41], identification of novel active compounds subsequently validated experimentally [6,29,38], and performance comparisons on standard datasets [36].  

Despite the significant advancements, challenges remain in AI-driven virtual screening. Accurate prediction of binding affinities remains a key hurdle [6], alongside the handling of false positives. Accurately capturing 3D protein-ligand interactions is challenging, particularly when target structures are unavailable, limiting the effectiveness of sequence-based predictions [12,14]. While AI-based docking models predict spatial transformations, they may sometimes generate unrealistic ligand conformations requiring subsequent optimization or filtering [12,14]. In pocket-directed docking, deep learning models are not consistently superior to physics-based methods [12,14]. Phenotype-based VS models, although promising for identifying compounds based on cellular responses (e.g., inducing cancer cell aging), often lack generalizability and face issues like data sparsity [12,14]. Developing universal virtual screening models that perform robustly across diverse targets and chemical spaces remains a critical goal [12,14]. Potential solutions and strategies to address these challenges include developing more sophisticated AI techniques, leveraging active learning and Bayesian optimization to iteratively refine models and focus screening efforts on the most promising candidates [8,12,14], and improving the quality and diversity of training data. Active learning, specifically, trains a cheaper surrogate model iteratively on a subset of docking results to reduce the need for expensive calculations on the entire library [8]. Providing reliable estimates of prediction uncertainty is crucial for effective exploration in these active learning approaches [8].  

# 4.3 De Novo Molecular Design  

De novo molecular design, often considered a "holy grail" in drug research and development, involves creating entirely new molecular structures from scratch using computational methods, primarily deep learning [18,39]. This approach aims to generate novel drug candidates with optimized properties tailored to specific therapeutic needs, offering advantages over traditional methods by significantly shortening the design cycle [14,26,35]. AI enables the design of millions of chemical entities and the prediction of potential synthesis routes, thereby reducing the need for extensive experimental validation in early stages [26,35].​  

The core of AI-driven de novo design lies in generative models trained to model the distribution of known molecules and sample those with desired characteristics from the vast chemical space [10,11,13]. A variety of deep learning architectures have been employed, including Recurrent Neural Networks (RNNs), Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and more recently, Large Language Models (LLMs) [1,3,9,16,21,22,37].​  

Molecular generation methods can broadly be categorized based on how molecules are represented. Chemical language models often treat molecules as sequences, such as SMILES strings, and generative tasks become sequence generation problems [12,14]. RNNs trained on large SMILES datasets, for example, can generate new sequences corresponding to potentially active substances for targets like Staphylococ usaureusand Plasmodiumfalciparum[9]. However, sequencebased methods may require extensive pre-training and can sometimes produce invalid strings [12]. Graph-based models represent molecules as graphs, which naturally captures their structure, but these also present their own challenges [12,14]. LLMs have demonstrated significant progress by extending their capabilities from natural language processing to handling complex chemical and biological "languages," offering new tools for drug molecule design and protein redesign, including antibody design [1,3,17,22].​  

These generative models are guided to produce molecules with desired properties, such as improved binding affinity, favorable ADMET profiles (absorption, distribution, metabolism, excretion, toxicity), and synthetic accessibility [4,7,9,21,34,44]. Guidance is often achieved through scoring functions, which evaluate generated molecules based on similarity to known actives or predicted biological activity [12,14]. Reinforcement learning can further optimize the design process by rewarding the generation of molecules with desirable properties [16,21]. Integrating constraints, such as diseaserelated gene expression or protein pocket conditions, enhances the precision of the generated designs [12,14].​  

A significant challenge in de novo design is balancing the quality of generated molecules (e.g., potency, drug-likeness) with chemical diversity to explore a broad space of possibilities [1,3,12,14,22]. The ability to generate a large number of molecules is less impactful if they lack optimal characteristics or novelty [1,3,22].  

AI-driven de novo design can be broadly approached through structure-based design (utilizing target protein structure), ligand-based design (learning from known ligands), and property-based design (optimizing for specific physiochemical or biological properties) [9]. While specific advantages and disadvantages of each approach vary depending on the target and data availability, AI facilitates these by enabling the prediction of 3D protein structures and the assessment of candidate impact on targets [7].​  

Several case studies highlight the potential of AI in this domain. Insilico Medicine, utilizing generative chemistry platforms like Chemistry42, has demonstrated rapid discovery timelines. For instance, in January 2023, they reported the discovery of INS018_055, a novel anti-fibrotic small molecule inhibitor targeting TNIK for Idiopathic Pulmonary Fibrosis (IPF), which was designed from scratch [5,33]. Their approach involves algorithms learning from effective molecules to prioritize novel, logical, and synthesizable structures, selecting a refined set of candidates from a large pool of designs [34]. Another example is the DeepLigBuilder developed by Lai Luhua's team in 2021, a deep generative model specifically designed to construct and optimize ligands directly within the target binding pocket [6]. The GraphGMVAE model has been used to generate molecules with novel scaffolds, with one compound showing promising high activity $( 5 . 0 \mathsf { n M } )$ ) in in vitro experiments, validating the feasibility of AI-driven design [2]. The DeepBlock method focuses on generating molecules from building blocks, emphasizing synthetic feasibility and toxicity control while optimizing for target affinity and drug-likeness [44]. Companies like Exscientia employ end-to-end machine learning platforms capable of global optimization across various properties including efficacy, toxicity, pharmacokinetics, and manufacturability [4]. Beyond small molecules, AI is also applied to design small proteins and modify industrial enzymes, demonstrating significant functional improvements like increased catalytic efficiency [17]. AI plays a key role throughout the de novo design pipeline, from data collection and feature extraction to model training, molecule generation, evaluation, screening, optimization, and synthesis planning [18]. Other notable platforms and companies involved in generative AI for de novo design include Iktos's Makya, Ro5's De Novo Platform, Recursion Pharmaceuticals, Deep Cure, and Standigm [18].​  

# 4.4 Lead Optimization  

Lead optimization is a crucial stage in drug discovery focused on improving the pharmacological and pharmacokinetic properties of initial hit compounds to transform them into suitable drug candidates. Artificial intelligence (AI) and machine learning (ML) algorithms play a pivotal role in this process by enabling the prediction of lead compound properties based on their chemical structures and guiding their optimization towards desired characteristics [6,7,29]. This involves predicting a wide range of attributes, including biological activity, physicochemical properties, and absorption, distribution, metabolism, excretion, and toxicity (ADMET) profiles [6].  

A variety of AI models are employed for predicting drug properties. Deep Neural Networks (DNNs), for instance, have demonstrated significant capability in predicting the properties and activities of small molecules [37]. Deep learning can also be utilized to develop multi-task prediction models capable of predicting several activities concurrently, such as biological activity and ADME properties, by sharing input and hidden layers while having distinct output nodes for each predicted property [13]. Specific property predictions are critical; for example, ML algorithms can assess a candidate molecule's ability to absorb or lose water, directly impacting its solubility and subsequent absorption in the body, as demonstrated by analysis of 89 small molecule drug candidates [39]. Identifying substructures that facilitate specific properties, such as blood-brain barrier penetration, is also a target for ML-based prediction to aid rational drug design [43]. Accurate prediction of properties like solubility, permeability, and binding affinity is fundamental in lead optimization, as it substantially improves the selection of promising drug candidates for further development [6].​  

Optimizing ADMET properties is paramount for a drug's success and safety. AI is extensively used to predict and mitigate potential toxicity and metabolic liabilities, thereby refining ADMET profiles [6]. Early prediction of toxicity by analyzing chemical properties and biological responses allows for the screening out of high-risk compounds, potentially reducing clinical trial failure rates and enhancing safety [35]. Toxicity-induced safety issues are a leading cause of drug withdrawal, and studies suggest that mitigating toxicity could increase clinical trial success rates by over $4 0 \%$ [40]. AI-driven optimization enhances lead compounds by improving their binding affinity to the target while predicting and improving their ADMET properties and reducing off-target effects [6,21]. The DeepBlock method, for instance, optimizes lead compounds specifically by controlling ADMET properties and improving binding affinity while aiming to reduce toxicity, designing molecules for high target affinity, drug-like characteristics, and low toxicity [44]. AI is also applied to optimize general physicochemical and pharmacokinetic properties of drugs [23].​  

Generative AI algorithms excel at designing molecules de novo or modifying existing leads to bind to specific targets and optimizing these molecules for desired characteristics such as potency, selectivity, and ADMET profiles [15]. Strategies for optimizing lead compounds using AI-driven modifications aim to enhance their therapeutic potential [26,35]. Generative models can iteratively refine lead compounds by predicting how modifications will impact their properties and suggesting structural changes to improve the overall profile [13]. Examples include the use of Recurrent Neural Network (RNN) models to optimize properties like quantitative estimate of drug-likeness (QED) and synthetic feasibility, leading to the identification of candidates with high binding affinity [2]. AI can analyze known drug structures and activity data to propose new compound designs or suggest modifications for existing leads during the optimization process [28]. This includes optimizing complex molecules like interleukin-2 by identifying effective distal mutations to quantitatively modulate its immune activation and suppressive functions [17]. Furthermore, machine learning systems, including Support Vector Machine (SVM) algorithms, have been explored for improving drug efficacy and safety, including predicting patient response to drugs [34]. Co-modeling approaches offer strategies for optimizing initial hits, particularly when conventional methods face challenges, such as the optimization of active compounds identified via DNA-encoded libraries (DEL) [20].  

Despite the significant progress, lead optimization using AI/ML presents considerable challenges. A primary difficulty lies in balancing multiple, often conflicting objectives, such as maximizing potency while minimizing toxicity and ensuring favorable pharmacokinetic properties [6,7,13,40]. Achieving an ideal balance between efficacy and toxicity is central to successful drug design [40]. Another key challenge is avoiding unintended consequences, including off-target effects, which can lead to adverse events [6,7,13]. AI models must be robust enough to navigate this complex multi-objective landscape and predict potential liabilities accurately before synthesis and testing.​  

# 4.5 ADMET Prediction and Optimization  

Predicting the absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties of potential drug candidates is a critical step in pharmaceutical research and development. Poor ADMET profiles are a major cause of high attrition rates in preclinical and clinical trials, leading to significant financial and time investments in failed compounds [7,9,29]. Therefore, early and accurate prediction of these properties is paramount to identifying and filtering out unsuitable drug candidates upfront, thus reducing late-stage failures in drug development [9,14]. The increasing recognition of the importance of early ADMET prediction dates back to principles like Lipinski's rule of five [29].​  

Artificial intelligence (AI) and machine learning (ML) models have emerged as powerful tools for predicting ADMET properties based on molecular structure and other relevant data, including biological characteristics [6,7,9,10,14,17,23,24,27,29,30,35]. These models utilize molecular fingerprints, descriptors, or graph representations as key features to establish relationships between compound structure/characteristics and their properties [10,12,14,23]. AI is employed to predict potential toxicity by analyzing molecular structures and biological characteristics and to establish models for drug absorption, distribution, metabolism, and excretion in vivo [23]. The federal government's Tox21 project has created a database suitable for training AI to identify relationships between compound structures, characteristics, functions, and potential toxic side effects [39]. This capability allows for the early screening of compounds for potential toxicity, aiding in reducing the risk of clinical trial failures due to poor ADMET profiles [26,35]. Computational ADMET modeling has been pioneered by companies like GlaxoSmithKline [16].​  

A variety of AI/ML models are employed for ADMET prediction. Traditional machine learning algorithms such as random forests (RF) and support vector machines (SVM) have been widely used, often leveraging molecular fingerprints like circular extended connectivity fingerprints (ECFP) [10,12,14]. For instance, Bayer's platform reportedly uses RF and SVM with ECFP descriptors for accurate ADMET predictions [12,14], and SVM has demonstrated high accuracy in predicting the ADME properties of marketed drugs [26]. Deep learning (DL) models, including deep neural networks (DNNs), Transformers, convolutional neural networks (CNNs), and graph neural networks (GNNs), offer advanced capabilities for modeling molecular properties from SMILES strings or molecular graphs [14,29]. A Kaggle competition organized by Merck Sharp demonstrated that DNNs could achieve slightly better performance than standard RF in predicting ADME parameters and biochemical indicators in many assay systems [37]. GNNs, in particular, show superior performance by effectively incorporating geometric information inherent in molecular graphs [12,14,29]. Multi-task learning is considered advantageous for predicting multiple ADMET properties simultaneously [16]. For example, a multi-task learning strategy with feature enhancement and consensus inference leveraging the latent space of a Variational Graph Encoder (NYAN) was devised for multi-endpoint acute toxicity estimation, demonstrating competitive or better performance compared to other state-of-the-art molecular property prediction methods [40]. Specific DL models like directed message passing neural networks (D-MPNN, Chemprop) have been successfully applied to predict toxicity endpoints, leading to the identification and validation of safe compounds [29]. AI tools such as XenoSite, FAME, SMARTCyp, cypules, MetaSite, MetaPred, and WhichCyp are used for specific tasks like determining drug metabolism sites and identifying CYP450 subtypes [26]. AI can even be integrated into de novo design processes, as seen with the DeepBlock method, which optimizes molecule generation for controlled toxicity by predicting and optimizing ADMET properties concurrently [44].  

Despite the advancements, significant challenges persist in developing more accurate and reliable ADMET prediction models [9]. Key challenges include data scarcity for specific ADMET endpoints, which can lead to model overfitting, and insufficient model interpretability [9,12,14]. Current limitations necessitate further research into techniques such as unsupervised and self-supervised learning, the development of effective attention mechanisms, and the incorporation of domain-specific chemical knowledge to improve prediction accuracy and reliability [12,14].  

A critical ADMET property, particularly for central nervous system (CNS) drugs, is the ability to cross the blood-brain barrier (BBB) [43]. ML models are being developed to predict BBB penetration, and these models can identify specific molecular features and substructures that influence this property [43]. To address the interpretability challenge, explainable AI (XAI) methods, such as Local Interpretable Model-agnostic Explanations (LIME), are employed to understand the basis of these models' predictions. By applying XAI techniques, researchers can interpret the ML models' output and identify the specific substructures that are most influential in determining BBB penetration, providing valuable insights for medicinal chemists [43].​  

# 4.6 Formulation Optimization  

Generative AI offers a promising approach to optimizing drug formulations, aiming to enhance drug delivery and efficacy [25]. This is achieved by employing AI models to predict the properties of various potential formulations and subsequently identify those that are most likely to yield improved outcomes [25].  

The optimization process necessitates careful consideration of multiple critical factors that govern formulation performance. These factors include, but are not limited to, solubility, stability, and bioavailability [25], alongside more specific characteristics such as drug load, particle distribution, and release kinetics [25]. By analyzing and predicting these properties across a vast design space, generative AI can accelerate the identification of optimal formulation compositions and manufacturing parameters.​  

Examples of how AI facilitates formulation optimization include its application in the realm of nucleic acid-based therapies. For instance, AI can be synergistically utilized to optimize both the non-coding and coding regions of messenger RNA (mRNA) constructs. This optimization is crucial for improving the stability and intracellular expression levels of mRNA vaccines and other therapeutic mRNA products [17]. Similarly, AI algorithms are capable of enhancing the targeting specificity and precise silencing efficiency of small interfering RNA (siRNA), thereby optimizing the formulation and delivery effectiveness of these gene-silencing therapeutics [17]. These cases highlight the potential of AI in addressing complex formulation challenges across different drug modalities. While the provided materials outline the general principles and some specific examples like mRNA and siRNA optimization, further research detailing successful comprehensive formulation optimization projects using generative AI would provide more specific instances of its impact in diverse drug development scenarios.  

# 4.7 Drug Repurposing  

Drug repurposing, also known as drug repositioning, involves identifying new therapeutic uses for existing drugs that are already approved or are in preclinical or clinical development for other indications [6]. This strategy offers significant advantages over traditional de novo drug discovery, primarily by reducing the time and cost required to bring a drug to market [6,9]. Repurposed drugs have already undergone extensive safety testing, potentially allowing them to enter clinical trials at later stages, such as Phase II, which can substantially lower research and development expenditure, with one estimate suggesting a reduction from \​ $\textcircled{8}$ 无效公式 8.4 million for drug-target interaction prediction approaches [26].  

Generative artificial intelligence (AI) and related machine learning (ML) techniques are increasingly utilized to identify potential drug repurposing candidates [2,4,9,10,17,21,22,27,28,35]. These methods analyze vast quantities of large-scale biological and clinical data to uncover previously unknown therapeutic properties of existing drugs and identify new treatment options for diseases [7,12,14,23,28,35,42]. Relevant data sources include real-world data (RWD) such as electronic health records (EHRs), clinical trial registries, and insurance claims data [7,12,14]. AI can also analyze unstructured text data from research articles, patents, and integrate and mine diverse biomedical datasets, including omics data [12,14,18].​  

Various AI methodologies are applied for drug repurposing. Network analysis techniques and network-based proximity analysis are used to identify potential opportunities and predict drug-target interactions [13,18]. Specific tools like  

ChemMapper and Similarity Ensemble Approach (SEA) are employed for predicting these interactions [26]. Deep learning models, such as recurrent neural networks (RNNs), have been utilized to analyze medical claims databases to identify potential candidates for conditions like coronary artery disease [12,14]. Graph Convolutional Networks (GCN) combined with deep learning-based network biology analysis have been applied to identify potential treatments, notably for COVID-19 [24]. Furthermore, applying deep learning to omics data facilitates drug classification based on drug-induced transcriptional disturbances, offering new avenues for repurposing [12,14].​  

Several initiatives and companies have demonstrated success in AI-driven drug repurposing. IBM Watson®, an early cognitive computing technology integrating vast amounts of medical and biological data, was applied in pilot studies to accelerate the identification of new drug candidates and targets, particularly for rare diseases [42]. Similarly, the mediKanren platform, developed by the Hugh Kaul Institute for Precision Medicine, leverages knowledge graphs to link diverse data for identifying therapeutic options for rare diseases [42]. Companies like Healx, BenevolentAI, and BioXcel Therapeutics actively employ AI strategies for drug repurposing [18]. The utility of AI in accelerating drug repositioning was particularly evident during the COVID-19 pandemic [14], where AI-driven analyses helped identify potential treatment drugs [24].​  

Beyond computational analysis of existing data, AI can also contribute to drug repurposing through experimental approaches like high-content screening. AI-driven high-content screening can verify the mechanism of action of various drugs and classify them into different treatment categories, thus opening new directions for drug reuse [35]. This connection highlights the role of AI in phenotypic drug discovery approaches for repurposing, where the effect of drugs on cellular phenotypes is analyzed to infer potential new uses [35].  

Despite the significant progress, challenges remain in the widespread application of AI for drug repurposing. These include issues related to the quality of input data, the interpretability and generalizability of AI models, validation costs for computationally predicted candidates, and regulatory hurdles [12,14]. Addressing these challenges is crucial for fully leveraging the potential of AI in accelerating drug repurposing.  

# 4.8 Synthesis Planning and Automated Synthesis  

Artificial intelligence (AI) is poised to revolutionize chemical synthesis, significantly assisting chemists in identifying reaction routes and automating synthesis processes [10,12,14]. Computer-Assisted Synthesis Planning (CASP) is a key area where AI provides substantial support [10,12,14]. AI facilitates CASP by predicting plausible synthesis routes and suggesting alternative reactions, effectively alleviating the burden on chemists [10,14]. The core idea behind CASP often involves retrosynthetic analysis, where a target molecule is computationally broken down into simpler, readily available precursors through simulated reverse reactions [12,13].​  

Machine learning, particularly deep learning models, has become instrumental in planning synthesis routes for artificial small molecules and natural products [12,14]. These models are typically trained on vast databases of known chemical reactions extracted from scientific literature or patents [10,13,37]. For instance, neural sequence-to-sequence models trained on databases containing tens of thousands of reactions from US patents have demonstrated accuracy comparable to rule-based methods for retrosynthetic prediction [13,37]. Other approaches include combining a strategy network with Monte Carlo tree search (MCTS), trained on extensive libraries of 12 million scientific literature reactions, to navigate the retrosynthetic search space [37]. Deep learning models can also predict reaction success and plan synthesis paths from starting materials to target molecules by learning from chemical synthesis data [29]. Schwaller et al., for example, combined deep learning and symbolic AI within a "MoleculeNet" framework for chemical synthesis planning [29]. Transformer models have also been adopted for retrosynthetic analysis, as well as for predicting region and stereoselectivity and extracting reaction fingerprints [12,14]. To enhance interpretability and performance in retrosynthesis, models like RetroExplainer have been developed, reframing the task as a molecular assembly process using explainable deep learning [12,14]. Beyond predicting primary products, neural networks have been used to rank candidate products for reactions, achieving high correct classification rates (e.g., over $9 0 \%$ for the top 5 candidates) when trained on reaction libraries [37]. Template-free methods, such as those employing Weisfeiler-Lehman difference networks, have been proposed to improve coverage and efficiency compared to traditional template-based approaches [37]. Ensuring the synthetic accessibility of AI-designed molecules from the outset is also crucial; methods like DeepBlock achieve this by using reactive molecular building blocks in the generation process [44].​  

The application of AI in synthesis planning offers significant benefits, including optimizing reaction conditions, predicting reaction outcomes, and ultimately reducing synthesis costs and timelines [10]. The speed at which AI can assist in synthesis was highlighted during the COVID-19 pandemic when AI prediction algorithms, coupled with experimental verification, helped accelerate the process of confirming the optimal crystal form of Paxlovid within weeks [17].​  

AI-driven design-make-test-analyze (DMTA) platforms represent a powerful approach to accelerate the entire drug discovery cycle [12,14]. These platforms integrate deep learning for molecular design with automated synthesis capabilities, such as those using microfluidic chips. This integrated workflow has successfully been demonstrated, for example, in the generation of liver X receptor agonists [12,14].  

A critical aspect of integrating AI into drug design workflows is assessing the synthesizability of newly designed molecules [2]. Incorporating synthesis route prediction tools, such as RetroSyn, early in the design process allows researchers to evaluate the feasibility of synthesizing potential candidates and prioritize those that are more readily accessible [2]. This proactive assessment helps guide the design process towards synthetically viable structures [43].  

Despite the significant progress, challenges remain in fully automating chemical synthesis. Issues such as optimizing multistep reactions, predicting solubility accurately, and reducing solid formation during reactions continue to pose difficulties for automation [14]. Furthermore, the application of AI is still limited by the need for suitable chemical reaction representations and the scarcity of high-quality, comprehensive reaction data for training robust models [29].  

# 4.9 AI-Driven Phenotypic Drug Discovery  

AI significantly advances phenotypic drug discovery, particularly through the application of image‐based profiling technologies [31]. This approach leverages the power of computational analysis to decipher complex cellular responses induced by drug treatments, moving beyond target‐centric methods [31]. The process typically involves high‐content imaging, which captures detailed, often dynamic, images of cells after treatment with candidate compounds [31]. These images provide rich phenotypic information about cellular state and morphology [31]. Deep learning algorithms are then applied to analyze this large volume of imaging data [31]. These models are capable of extracting subtle, complex features from the images that represent specific drug‐induced phenotypes [31]. For example, deep learning models like MitoReID have been utilized to analyze dynamic images of rat cardiomyocytes, specifically focusing on extracting features related to mitochondrial phenotypes [31]. By comparing the extracted phenotypic features of new or candidate compounds to a reference database of phenotypes induced by drugs with known mechanisms of action (MOA), the AI model can predict the likely MOA of the new compounds [31]. This capability is crucial for both target identification and drug repurposing [31]. By predicting the MOA, researchers can infer potential biological targets perturbed by the compound [31]. Furthermore, if a compound induces a phenotype similar to that of a known drug used for a different indication, it suggests potential for repurposing that compound [31]. Experimental validation is an essential step following the AI‐driven prediction to confirm the predicted mechanisms of action and therapeutic potential [31].​  

# 5. AI for Complex Biological Systems  

Navigating the inherent complexity of biological systems represents a fundamental challenge in drug design and therapeutic development. This section explores how Artificial Intelligence (AI), particularly Generative AI (GenAI), is being leveraged to address this complexity across multiple facets, from designing novel biological entities to optimizing therapeutic strategies for individual patients and transforming clinical workflows [14,21].  

A key contribution of GenAI lies in the ability to understand and modulate complex biological components and interactions. This includes the de novo design of proteins with tailored functions, extending beyond natural templates and opening new avenues for therapeutic protein development [3,11,19]. Furthermore, AI aids in deciphering complex biological networks, such as protein–protein interactions and signaling pathways, by accurately predicting protein structures and identifying potential therapeutic targets within these intricate systems [1,3,4,21,26]. The ability to predict targets for specific compounds further enhances the understanding of molecular interactions within these systems [31].​  

Building on the understanding of complex systems, AI is uniquely positioned to address the challenges of multi-target drug design [21]. Complex diseases often involve multiple pathological nodes, necessitating therapies that modulate several targets simultaneously. GenAI models facilitate the simultaneous optimization of drug candidates for activity against multiple targets, enabling the design of molecules tailored to achieve desired multi-parameter profiles [21]. This includes using techniques like multi-task learning for de novo design and applying generative models to create molecules with dual or multiple specific inhibitory activities [16,21].  

Beyond the discovery phase, AI is revolutionizing the application of therapeutics through personalized medicine and the transformation of clinical research and healthcare delivery [14,21]. By analyzing diverse patient data, including genomic profiles, clinical history, and real‐world data, AI models can predict individual patient responses to drugs, identify relevant biomarkers, and forecast pharmacokinetic and pharmacodynamic properties [4,13,14,21,28,35,42]. This predictive capability supports the design of individualized treatment plans and personalized dosage regimens, optimizing therapeutic outcomes and minimizing adverse effects [12,14,28,42]. Concurrently, AI is streamlining clinical research by optimizing trial design, improving patient selection and recruitment, and enhancing data analysis from traditional and real‐world sources [1,3,7,14,26,35]. These advancements collectively accelerate drug development and pave the way for more efficient and patient‐centric healthcare delivery, including the development and utilization of digital therapeutics [14,42]. While significant challenges related to data quality, model interpretability, and regulatory frameworks persist, the integration of AI across these domains underscores its transformative potential in addressing the complexities of biological systems for therapeutic benefit [12,14].  

# 5.1 Generative AI for Protein Design  

Generative AI (GenAI) is significantly transforming the field of protein design, offering the potential to create novel proteins with specific functions that may not exist in nature [3,19]. This capability extends beyond merely modifying existing proteins, enabling the de novo generation of protein structures [19]. Models such as RFDiffusion and Chroma are being employed to achieve this, generating protein structures from scratch without relying on pre-existing natural templates [19]. Diffusion-based methods like RFdiffusion and Family-wide Hallucination represent key approaches in this nascent area of AIdriven protein design [11]. Large Language Models (LLMs) are also seeing rapid advancements in protein design [3].  

AI-designed proteins are beginning to demonstrate therapeutic potential. Examples include the design of novel antibodies [22] and small proteins, typically around 60 to 100 amino acids in length [17]. These small proteins exhibit favorable characteristics such as good stability in the body and even potential membrane permeability [17]. Compared to traditional protein engineering methods, GenAI platforms offer advantages like significantly shortened engineering cycles, potentially reducing the time from months to weeks [4]. AI technology also shows promise in designing dynamic protein drugs capable of altering their conformation in response to different environmental conditions [27]. These advancements in creating new therapies are crucial for impacting the quality and sustainability of research and development [3].  

Furthermore, AI-based tools have made a significant impact on protein structure prediction, particularly in the realm of protein folding. Tools like AlphaFold and RoseTTAFold [4,17] have demonstrated the power of AI and computational techniques in the biopharmaceutical field [17]. The rapid evolution of AI-driven protein folding technology [3] improves protein structure modeling performance [27]. While the current emphasis of this technology is often on analyzing natural proteins to identify biological targets [3], the ability to accurately predict protein structures has profound implications for drug development, informing both the design of novel proteins and the understanding of target interactions.  

# 5.2 Generative AI for Multi-Target Drug Design  

Designing drugs that simultaneously modulate multiple biological targets presents a significant challenge in addressing complex diseases characterized by intricate pathological networks [21]. Generative Artificial intelligence (AI) offers a promising approach to navigate this complexity by enabling the simultaneous optimization of drug candidates for activity against multiple therapeutic targets [21]. This capability is crucial for developing multi-target therapies, which may offer enhanced efficacy and reduced resistance compared to single-target drugs.  

Various AI models have been explored for predicting and optimizing the interactions of small molecules with multiple proteins. Generative models, in particular, are utilized for multi-parameter or multi-objective optimization problems inherent in multi-target drug design [21]. These models can generate molecular structures tailored to desired property profiles, including activity against a predefined set of targets. For instance, graph-based conditional generative models have been employed to design molecules exhibiting dual inhibitory activity against two distinct targets [21].  

Beyond generative approaches focused on molecular structure, other deep learning paradigms contribute to multi-target considerations. Multi-task learning (MTL), for example, allows for the simultaneous screening of multiple chemical properties or biological activities of synthetic compounds, a methodology directly applicable to the de novo design of multitarget drugs [16]. Furthermore, AI models designed for target identification can inform multi-target strategies. The MitoReID model, initially developed for mitochondrial phenotyping, has demonstrated utility in predicting potential targets for compounds, such as identifying cyclooxygenase (COX) as a potential target for epicatechin, an active ingredient from traditional Chinese medicine [31]. In vitro experiments subsequently confirmed epicatechin's direct binding to and significant inhibition of COX-2 protein activity, validating the AI model's predictive power in identifying relevant biological interactions [31].​  

Such predictive capabilities, when extended to identifying multiple relevant targets for a single molecule or designing molecules hitting multiple predicted targets, underscore the potential of AI in constructing multi-target drug candidates. While the application of these AI-designed multi-target drugs in complex diseases like cancer or neurodegenerative disorders is a key area of research, the methodologies described provide the foundational AI capabilities necessary for developing such advanced therapeutics.​  

# 5.3 AI in Clinical Trials and Real-World Healthcare Applications  

<html><body><table><tr><td>Application Area</td><td>AI/ML Role & Impact</td><td>Key Data/Methods</td><td>Challenges</td></tr><tr><td>Biomarker Discovery</td><td>Identify diagnostic, prognostic, predictive markers; Enhance accuracy through multi-</td><td>Multi-omics data, Imaging data, Deep Learning, GNNs, NLP, Platforms (PandaOmics)</td><td>Data heterogeneity; Interpretability; Validation costs [12,14,35]</td></tr><tr><td>PK/PD Prediction</td><td>[12,14,35,37,42] Faster, more accurate prediction of drug kinetics/dynamics; Optimize dose- response; Enhance</td><td>High-dimensional data; Multi-omics; Computational models; Al methods; NLP on labels [12,23,35]</td><td>Data scarcity/quality; Complexity of biological systems; Interpretation [12,23]</td></tr><tr><td>Personalized Therapy</td><td>safety [12,23,35] Predict individual patient response, adverse events, interactions; Support individualized dose adjustments [4,12,13,35,42]</td><td>Patient data (genomic, history, lifestyle, physiological); Al models [4,12,13,35,42]</td><td>Data access/privacy; Model interpretability; Bias [12,14,42]</td></tr><tr><td>Clinical Trial Optimization</td><td>Optimize trial design,patient selection/recruitmen t; Improve data analysis; Support decentralized trials [1,3,7,12,13,14,22,26, 27,32,35]</td><td>Patient data (EHR, RWD); Trial simulation (Trial Pathfinder); Conversational agents [1,3,12,13,14,22,26, 2,35,42]</td><td>Data heterogeneity/qualit y; Interpretability; Regulatory hurdles; Validation of synthetic cohorts [3,12,14,42]</td></tr><tr><td>RWD Analysis/Post- Market Surveillance</td><td>Assess effectiveness/safety in diverse populations; Identify ADEs,drug interactions; Monitor long-term</td><td>Real-World Data (EHR,claims, wearables); Al algorithms [7,13,23,35,37,42]</td><td>Data quality/privacy; Regulatory considerations; Fragmented data [13,23,42]</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>outcomes [7,13,23,35,37,42]</td><td></td><td></td></tr></table></body></html>  

Artificial intelligence (AI) and machine learning (ML) are increasingly leveraged to advance clinical research and streamline processes within the pharmaceutical and healthcare sectors [7,35]. These technologies facilitate the analysis of data from both interventional and observational studies to evaluate drug safety and efficacy and can inform the design of traditional and non-traditional trials, such as decentralized clinical trials [7]. AI's applications in this domain are multifaceted, encompassing areas such as biomarker discovery, prediction of pharmacokinetic (PK) and pharmacodynamic (PD) properties, optimization of clinical trial design and execution, personalized therapy approaches, and post-market surveillance [12,14,23,35,42].​  

AI significantly enhances the efficiency and accuracy of biomarker discovery [12,14,35]. Through advanced techniques like deep learning models and graph neural networks, AI can identify diagnostic, prognostic, and predictive markers [12,35]. Integrating multi-modal data, such as protein interaction data, imaging data, and multi-omics datasets, further improves the accuracy of marker identification [35,37,42]. For instance, AI technologies examining omics data are particularly useful in oncology for identifying diagnostic and prognostic biomarkers and predicting therapeutic outcomes [42]. The "nuclei.io" digital pathology framework exemplifies this by combining active learning with real-time human-machine interaction for improved diagnostic accuracy based on nuclear statistics [12,14]. Deep learning models analyzing blood sample characteristics, such as ${ \mathsf { C D } } 8 +$ T cell morphology and proteomics biomarkers, can predict disease progression and survival, supporting personalized treatment strategies [12,14]. The development of digital biomarkers through AI's ability to learn complex data representations is also emerging as a key element for precision medicine [3].  

Traditional clinical trials face limitations in accurately predicting drug PK/PD in humans. AI addresses this by utilizing computational models and AI methods for faster, more economic, and accurate prediction of drug PK/PD [23]. AI excels at processing high-dimensional data and complex nonlinear relationships, which is crucial for optimizing dose-response relationships, enhancing drug safety, and expanding the therapeutic window through multi-omics data integration and analysis [12,35]. AI can extract pharmacokinetic information from various sources, including prescription labels using natural language processing (NLP) models like PharmBERT, aiding in the identification of adverse reactions and drug interactions [12,14].​  

A significant application of AI lies in personalizing drug therapies and optimizing dosage regimens [14,35]. By analyzing vast amounts of patient data, including genetic information, medical history, lifestyle factors, and physiological characteristics, AI can predict individual responses to medications [4,13,35]. This allows for the prediction of adverse events and drug interactions, providing strong support for individualized dose adjustments and precision medicine [12,35,42]. AI can offer personalized dosage recommendations based on a patient's unique profile to improve treatment effectiveness and minimize adverse effects [12].​  

AI offers substantial improvements in the efficiency of clinical trials across multiple stages [12,13,14,27,35]. It optimizes trial design by analyzing patient data and identifying characteristics affecting drug response [35]. Patient selection is enhanced by using AI to analyze patient-specific data, including genome exposure profiles or real-world data from health records, to identify eligible individuals who precisely meet study inclusion criteria [4,13,26]. Tools like Trial Pathfinder can simulate clinical trials using real-world data like electronic health records (EHRs) to assess the impact of relaxing trial standards on eligibility and outcomes, demonstrating the potential to double eligible patient populations and improve survival in specific contexts [12,14]. AI also contributes to monitoring patient compliance, as shown by mobile software developed by AiCure, which improved medication intake compliance in schizophrenia patients by $2 5 \%$ during a Phase II trial [26]. Generative AI (GenAI) specifically can support the drafting of new trial protocols based on constraints, facilitate communication among stakeholders, enhance patient experience, and improve clinical trial data quality [1,3,22,32]. Advanced analytics platforms, sometimes referred to as "clinical control towers," leverage AI to provide insights for operational decision-making, potentially increasing patient recruitment speed by $1 0 \mathrm { - } 2 0 \%$ [32]. By optimizing patient selection and outcome measurement, AI contributes to improved trial success rates and accelerates drug development transformation [35].  

AI algorithms are also critical for analyzing real-world data (RWD) derived from sources such as EHRs, insurance claims, and wearable devices [7,13,35,37]. This analysis enables the assessment of drug effectiveness and safety in diverse populations [7,35]. Furthermore, AI algorithms can analyze RWD to identify adverse drug events (ADEs), predict drug-drug interactions, and assess the long-term safety and efficacy of drugs in post-market surveillance [35,42]. AI-based conversational agents (chatbots) integrated with remote monitoring can facilitate reporting of adverse events and improve treatment compliance, potentially leading to cost reduction and improved patient outcomes for various chronic diseases [42].  

Despite the significant advancements, the application of AI in clinical trials and healthcare faces several challenges. These include issues related to data heterogeneity, the interpretability of complex AI models, and potential biases inherent in the training data [14]. While GenAI shows promise in generating high-quality documents and enhancing productivity, its actual translation into consistent speed and productivity gains in clinical settings requires further validation [3,22]. Digital twin technology, used to create virtual replicas or synthetic cohorts serving as control groups to potentially increase experimental group sizes and improve trial efficiency, also faces hurdles [12,14]. These challenges include high computational costs, ethical considerations regarding the use of synthetic data derived from real patients ("digital patients"), and the fundamental need for further validation of their reliability and applicability in regulatory contexts [3,14,42]. Challenges in using AI for post-market surveillance and RWD analysis also encompass ensuring data quality, addressing patient privacy concerns, and navigating regulatory considerations [23]. Although challenges related to big data and reliable datasets exist in areas like PK/PD research, AI holds substantial potential to transform these fields [23].​  

# 6. Datasets, Resources, and Data Challenges  

<html><body><table><tr><td>Area</td><td>Key Types & Examples</td><td>Significant Challenges</td><td>Mitigation Strategies</td></tr><tr><td>Datasets</td><td>Chemical Databases (PubChem,ChEMBL, ZINC); Property/Bioactivity (ChEMBL,Tox21, BBBP); Reaction Data (Reaxys); Patient/Clinical (TCGA, UK Biobank, EHR); Virtual Libraries (MCE MegaUni) [8,9,10,11,13,15,16,2 0,37,40,41,43]</td><td>Scarcity for specific targets/diseases; Data acquisition costs; Privacy regulations; Limited sharing; Insufficient negative data [9,10,12,14,39,42]</td><td>Data Augmentation; Synthetic Data Generation; Federated Learning; Data Sharing Platforms; Standardized Formats [2,3,13,16,22,35,42]</td></tr><tr><td>Data Quality</td><td>Experimental data; Annotated data</td><td>Incompleteness; Errors; Biases (e.g., publication bias, consent bias); Inconsistencies; active/inactive, in vitro/in vivo) [9,10,12,14,21,35,39, 42] Imbalance (e.g.,</td><td>Data Cleaning & Curation; Bias Detection/Mitigation Techniques; Data Harmonization; High-quality experimental methods [9,12,14,17,42,43]</td></tr><tr><td>Data Integration</td><td>Multi-modal data (omics,images, text, clinical)</td><td>Heterogeneity across sources/formats; Fragmentation across systems</td><td>Sophisticated Data Harmonization Tools; Integrated Platforms; Knowledge Graphs [12,13,42]</td></tr><tr><td>Computational Resources</td><td>High-performance computing; Cloud resources; Libraries (RDKit, Open Babel,</td><td>High cost for training large models; consumption; Need for standardized Scalability; Energy</td><td>Distributed Computing; Cloud Utilization; Model Compression; Optimized</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>DeepChem,Pandas) [11,39,41,43]</td><td>tools/platforms [3,8,11,24,25,35,41]</td><td>Algorithms; Open- source tools [8,41]</td></tr></table></body></html>  

The efficacy of generative AI models in drug design is fundamentally contingent upon the availability and characteristics of the training data. A diverse array of datasets and computational resources are utilized, each possessing distinct strengths and limitations. Commonly employed public databases include PubChem, ChEMBL, and ZINC, which provide extensive collections of molecular structures, properties, and target information [11,16]. For instance, ChEMBL contains approximately 50 million compounds, while PubChem lists 247 million and ZINC 230 million compounds [16]. Specialized databases focus on particular aspects, such as the TOXRIC database for toxicity data and the BBBP dataset from MoleculeNet for blood-brain barrier permeability information [40,43]. Beyond molecular compound libraries, resources include protein pocket representations derived from binding sites [36], image data capturing cellular phenotypes or formulation characteristics [25,31], and proprietary collections like the 18 million compounds provided by MCE China for AI competitions [15]. Tools such as RDKit facilitate molecular manipulation and feature generation [11]. While public resources are valuable, proprietary datasets are also generated by companies like insitro and Recursion to address specific research needs [39]. Comparing chemical spaces of different libraries, such as DELs with DrugBank and Natural Products Atlas, provides insights into their structural diversity and relevance [20].​  

The performance of AI models, particularly deep learning models like Variational Autoencoders (VAEs) and Recurrent Neural Networks (RNNs), is highly dependent on the quantity and quality of training data [8,12,26,37,39,42]. Accurate, organized, and complete datasets are essential for training effective models [37]. Data quality and meticulous curation are critical for ensuring the accuracy and reliability of AI models [13,39]. The quality of experimental data, including dataset size, chemical and property space coverage, diversity, and the presence of errors, directly impacts the performance of deep generative models [9].​  

Despite the availability of various data sources, the field faces significant data challenges. Datasets are frequently limited in size, especially concerning specific targets, with many targets in resources like ChEMBL having fewer than 100 related compounds [9,10]. Obtaining sufficient high-quality data is particularly challenging for rare diseases or novel drug targets due to the scarcity of experimental information [12,39]. Furthermore, data acquisition is often costly, and restrictive privacy regulations and insufficient data sharing among institutions limit the availability of comprehensive datasets [12,14,42].  

Existing data commonly suffers from issues such as incompleteness, errors, biases, and inconsistencies [12,14,42]. There is often a significant imbalance between different data types, such as the disproportionate amount of in vitro data compared to in vivo effects [10], or imbalanced distributions between active and inactive compounds [21]. The underrepresentation of "negative" data (compounds that do not interact with a target or are inactive) hinders a comprehensive understanding of drug-target interactions [14]. Public datasets, compared to proprietary ones, are often smaller and more prone to bias [9]. Biological and medical data present unique challenges due to their high dimensionality, complexity, and inherent noise, making traditional analysis methods inefficient [28]. Moreover, representing drug discovery information in a standardized, computer-readable format remains a challenge, and data appropriateness is crucial for reliable predictions [13,21]. Healthcare data, frequently limited and inconsistent, is often heterogeneous (multimodal and multisite) and fragmented across different systems and formats, posing substantial difficulties for AI model development [13,42].​  

These data issues significantly impact the performance and generalization capabilities of AI models. Low-quality data can lead algorithms to identify spurious correlations, resulting in magnified inaccuracies [39]. Data inconsistencies reduce the reliability of models trained on such inputs [12]. Biases, errors, and missing data directly undermine the performance and generalization of AI models across diverse applications [2,14,15,22,26,35]. Combining data sources without careful curation can introduce further biases and errors [9]. The inherent heterogeneity and fragmentation of data, especially in clinical contexts, mean that models developed and validated on data from a single center may not perform equally well when applied to different healthcare settings or patient populations [42].​  

To mitigate these challenges, various strategies are being explored and implemented. Addressing data scarcity can involve data augmentation techniques, such as customized data enhancement used for peptide sequences [2,16,17,22,35]. Synthetic data generation, which creates data reflecting the statistical properties of real datasets, offers a promising approach to enrich training data and overcome availability issues, although rigorous validation is necessary to prevent extrapolation risks [3,22]. Synthetic data also holds potential for improving the inclusion of underrepresented groups in clinical trial datasets [3]. Data cleaning and curation steps are crucial for improving quality, which includes rectifying errors like inconsistent labels or SMILES representations [9,43]. Data integration is necessary to combine information from diverse sources [2,16,22,35]. For sparse data, imputation techniques can be used as a pre-processing step [8]. To overcome challenges related to data sharing and privacy while leveraging distributed data, federated learning approaches allow algorithms to be trained locally across different sites without sharing the raw data, as demonstrated by the MELLODDY project involving pharmaceutical companies and distributed networks like the European Health Data and Evidence Network (EHDEN) or the Observational Health Data Sciences and Informatics (OHDSI) Collaborative [2,42]. Data harmonization and the development of models on large, representative, multicenter datasets are essential for improving generalizability and reliability [42]. The need for standardized data formats and sharing platforms is highlighted to facilitate collaboration and accelerate research across the field [13]. Furthermore, explicitly addressing data biases and inconsistencies is vital to enhance the reliability of AI models [14].​  

# 7. Challenges and Limitations of Generative AI in Drug Design  

<html><body><table><tr><td>Challenge</td><td>Description / Impact</td><td>Potential Mitigation /Future Direction</td></tr><tr><td>Interpretability/Explainabil ity</td><td>"Black box" nature of deep learning; Difficulty understanding model reasoning or mechanism behind predictions [2,7,8,10,11,14,15,16,22,24,3 7,42]</td><td>Develop/apply Explainable Al (XAI) methods (e.g., LIME, feature attribution, visualization); Quantify uncertainty [2,8,16,21,22,36,43]</td></tr><tr><td>Validation</td><td>Gap between in silico predictions and experimental outcomes; Need for extensive in vitro/in vivo testing; Translating to clinical efficacy [7,16,20,22,25,35,42]</td><td>Integrate Al with high- throughput experiments; Prospective validation studies; Develop predictive preclinical models; Innovative trial designs [16,21,26,35,42]</td></tr><tr><td>Computational Cost/Scalability</td><td>High expense for training/deploying large models; Limits hyperparameter tuning; Energy costs [3,8,11,24,25,35]</td><td>Distributed computing; Cloud resources; Model compression techniques; Optimize algorithms [8,41]</td></tr><tr><td>Generalizability/Robustnes S</td><td>Poor performance on out-of- distribution data, novel targets,or scaffolds ("super- memorizers"); Amplifies data biases [39,41]</td><td>Careful model selection; Strategies for OOD performance; Address data biases; Develop models capable of "scaffold hopping" [39,41]</td></tr><tr><td>Synthetic Feasibility</td><td>Generated molecules may be difficult or impossible to synthesize in the lab; Insufficient assessment methods [2,9,12,13,44]</td><td>Incorporate synthetic accessibility scores; Multi- objective optimization; Generate from building blocks; Integrate retrosynthesis tools [2,12,13,44]</td></tr></table></body></html>  

The application of generative artificial intelligence (AI) in drug design holds immense promise, yet its widespread adoption and full realization are confronted by a spectrum of significant challenges and inherent limitations  

[2,7,8,9,10,14,15,16,22,24,27,30,39]. These hurdles span technical, operational, and translational domains, necessitating concerted efforts for effective mitigation and further advancement [22].  

A primary technical challenge lies in the interpretability and explainability of generative AI and deep learning models [2,7,8,14,15,16,22,24]. The complex, often opaque nature of these "black box" models [2,8,11,16,37] makes it difficult to discern the underlying reasoning for their predictions or understand the mechanisms driving desired molecular properties [8,10,11,24,37,42]. This lack of transparency impedes trust in AI-generated candidates, hinders the rational optimization of molecular structures based on model insights, and complicates regulatory acceptance [2,8,10,37,42]. Addressing this requires the development and application of Explainable AI (XAI) methods, including visualization, feature attribution techniques like atom coloring, and model-agnostic approaches such as LIME, to provide greater transparency and mechanistic insight [2,8,16,22,36,43].​  

Beyond theoretical predictions, a critical practical challenge is the rigorous validation of AI-generated candidates [7,16,22,35]. A significant gap often exists between in silico predictions and experimental outcomes [7,16,25], necessitating extensive in vitro and in vivo validation to confirm biological activity, selectivity, safety, and synthetic feasibility [7,16,20,22,35,44]. Translating AI predictions to real-world clinical efficacy is complex due to biological variability and the intricate nature of disease [42]. Robust validation requires the integration of AI predictions with high-throughput experimental data [26,35], prospective validation studies [21], and potentially novel clinical trial designs [16,42]. While some AI-designed candidates have reached early clinical stages, definitive conclusions regarding their success require longer observation periods and larger studies [5].  

The deployment and training of complex generative models also incur substantial computational costs and scalability challenges [8,11,24,25,35]. Training large, state-of-the-art models demands significant computational resources, presenting a financial barrier and limiting extensive hyperparameter optimization crucial for model performance [3,8]. Even inference can have notable energy costs [3]. While some specific applications may be made computationally efficient [41], the overall trend towards larger models exacerbates this issue. Mitigating these costs involves leveraging distributed computing, cloud resources, and model compression techniques [8].  

Ensuring the generalizability and robustness of AI models is critical for their utility in discovering truly novel drugs [39]. Models can sometimes behave as "super-memorizers," performing poorly on out-of-distribution data, novel targets, or previously unseen chemical scaffolds [39,41]. This limits their ability to achieve "scaffold hopping" and explore genuinely new areas of chemical space not represented in the training data [41]. Furthermore, biases present in the training data can be amplified by algorithms, raising concerns about the generalizability and ethical implications of predictions [7]. Challenges like mode collapse in generative models also reduce the diversity of generated compounds [9]. Addressing generalizability requires careful model selection and developing strategies to improve performance on novel data [39,41].  

Finally, a significant practical bottleneck is ensuring the synthetic feasibility of AI-generated molecules [2,9,12,13,44]. While AI can propose novel structures, these may be complex or impossible to synthesize in a laboratory setting [2,9]. Current methods for assessing synthetic accessibility may be insufficient [12]. Effective drug design necessitates balancing multiple objectives, including biological activity, ADMET properties, and synthetic tractability [12,13]. Solutions involve incorporating synthetic accessibility scores, using multi-objective optimization, generating molecules from readily available building blocks, and integrating computational tools for retrosynthesis and synthesis route prediction into the generative process [2,13,44].​  

In summary, despite the transformative potential, generative AI in drug design faces substantial challenges related to model interpretability, experimental and clinical validation, computational resource requirements, generalization to novel chemical space, and the synthetic feasibility of generated compounds [2,7,8,9,10,14,15,16,22,24,27,30,39]. Addressing these limitations through continued research into explainable AI, robust validation methodologies, computational efficiency, improved generalizability techniques, and the integration of synthesis considerations is paramount for AI to reach its full potential in accelerating drug discovery and development [22]. Furthermore, challenges related to data quality and availability remain fundamental, requiring high-quality, large-sample datasets and methods for effective learning from small samples [6,11,14,24,39]. It is also recognized that human expertise remains indispensable throughout the drug discovery process [13,21,39].​  

# 7.1 Model Interpretability and Explainability  

A significant challenge in the application of deep learning and generative AI models in drug design stems from their inherent "black box" nature [2,8,14,15,16,22]. This characteristic makes it difficult to interpret the complex internal workings of these models and understand the reasoning behind their predictions [8,11,24,37]. Unlike human experts who can often articulate the rationale behind their classifications or predictions [13], deep learning models frequently lack the ability to communicate effective prediction confidence and mechanistic reasoning to users [10]. This opaqueness can hinder the broader adoption of these AI technologies in drug discovery workflows, even when their predictive performance surpasses human capabilities [37].​  

The interpretability of AI models is critically important in drug design for several reasons. Firstly, it is essential for building trust in AI-generated molecules and the overall AI-driven design process [8]. Without understanding whya model predicts a certain molecule is promising or predicts a specific property, researchers may be reluctant to commit significant resources to experimental validation. Secondly, interpretability is crucial for guiding further optimization efforts [2,8]. Insights into the model's decision-making process can highlight key molecular features or structural motifs responsible for desired (or undesired) properties, enabling chemists to rationally design improved candidates. There is a specific need for interpretable AI models that can provide explicit insights into the complex relationships between chemical structures and their biological activities [16]. Understanding this structure–activity relationship is fundamental to medicinal chemistry, and interpretable AI can help reveal key molecular features, such as identifying potentially toxic groups, thereby enhancing the understanding of the logic underpinning molecular generation or property prediction [2]. Furthermore, interpretable predictions offer valuable insights for prospective drug design campaigns [21].  

The challenge of interpreting predictions is particularly pronounced with increasingly complex deep learning architectures [21,37]. Developing methods to quantify model uncertainty and make informed decisions under this uncertainty is also gaining importance [21].​  

To address the "black box" issue and improve model interpretability, various methods have been proposed and explored [2,16,22,35,36]. These include techniques from the field of Explainable AI (XAI) [2,8]. XAI methods aim to provide transparency, justification, informativeness, and uncertainty estimation [8]. Specific approaches include visualization techniques, such as visualizing key pocket residues and ligand atoms contributing to classification decisions in drug–target interaction models [36]. Feature attribution methods, like atom coloring, are commonly used local explanation approaches in computational chemistry applications of deep learning [8]. Other methods, such as Local Interpretable Model-agnostic Explanations (LIME), have been applied to build local linear models around test instances to explain predictions, for example, in the context of BBB penetration prediction [43]. Attention mechanisms, commonly used in sequence-based models, are also suggested as means to highlight the most relevant parts of the input data influencing a prediction [22]. By implementing and developing such interpretability techniques, researchers aim to make AI models more transparent, trustworthy, and ultimately more effective tools for drug discovery.​  

# 7.2 Validation of AI-Generated Candidates  

The successful translation of AI-driven drug discovery efforts into tangible therapeutic candidates hinges critically on the experimental validation of AI-generated molecules [7,16,22,35]. This validation, primarily through in vitro and in vivo studies, is essential to confirm their predicted biological activity, selectivity for the intended target, safety profile, synthetic feasibility, and non-toxicity [7,16,22,35,44].​  

A significant challenge lies in bridging the inherent gap between in silico predictions and real-world experimental results [7,16]. While in silico methods can provide initial estimations, such as particle size distribution, porosity, and permeability for formulations or release predictions for implants [25], these simulations alone are often insufficient and necessitate further in vitro or in vivo verification [25]. Predictive models, particularly those based on data types like DEL screens, typically require extensive wet lab validation to identify highly active compounds [20]. Moreover, the translation of AI predictions into real-world clinical outcomes requires careful consideration of biological complexity, including genetic variability within patient populations [42]. Human expertise remains indispensable for designing and executing validation experiments in wet labs and navigating the complexities of real-world clinical studies [13].​  

Consequently, there is a pronounced need for robust validation methods to ensure the reliability and generalizability of AIdriven drug candidates [21,26,35]. This encompasses not only experimental validation of AI predictions [21,26,35] but also the implementation of prospective validation studies [21]. Validation should extend beyond statistical performance metrics to encompass a clinical perspective [42]. This often requires external validation processes utilizing sufficiently large datasets from diverse sources, and the establishment of clear standards and protocols for clinical validation to ensure patient safety [42]. Furthermore, validating the interpretability or explainability of AI models, such as LIME results, requires comparison with domain knowledge, conducting experiments with existing data, statistical testing, sensitivity analysis, and evaluating the impact of identified features on model predictions [43].​  

Strategies for improving the validation process include developing more predictive preclinical models that better recapitulate human biology and designing innovative clinical trial methodologies tailored for AI-assisted discoveries [16,42]. Crucially, robust validation necessitates the integration of AI predictions with experimental data obtained from highthroughput screening and in vivo studies [26,35]. While relatively few studies have explored the synthesis and subsequent in vitro evaluation of de novo generated molecules, initial results concerning effectiveness evaluations are generally positive [9]. Specific examples of experimental validation include testing AI models on common virtual screening benchmark datasets [36] and in vitro validation of AI model predictions for novel drug targets, as demonstrated by the MitoReID model achieving a rank-1 accuracy of $7 6 . 3 \%$ and mAP of $6 5 . 9 \%$ in classifying FDA-approved drugs by mechanism of action [31]. Progress is also evidenced by AI-designed candidates reaching clinical stages, such as ISM001-055 completing Phase IIa clinical trials and demonstrating proof-of-concept [5]. However, it is important to acknowledge that current clinical trial data for AI-designed drugs are based on short observation times and relatively small subject numbers, precluding definitive conclusions at this stage [5].​  

# 7.3 Computational Cost and Scalability  

The application of generative AI and deep learning models in drug design is significantly impacted by substantial computational costs and inherent complexities . Training and deploying these sophisticated models demand considerable computational resources, which can restrict their widespread applicability and accessibility . A primary challenge lies in the high computational expense associated with training generative AI models, particularly as the drive towards larger and more complex model architectures persists . For instance, the compute costs for training a single large language model like GPT-3 were estimated to be around ​4.6 million . This escalation in training costs not only poses a financial barrier but also limits the feasibility of comprehensive hyperparameter optimization, which is crucial for model performance. Furthermore, these high costs can have serious implications for rigorous scientific review and debate regarding the development and validation of new AI techniques and their results . Beyond training, the complexity extends to the difficulty of effectively optimizing models for complex molecular properties . Even the inference phase, such as querying large language models, incurs notable energy costs, necessitating cautious and efficient usage .​  

While training large, general-purpose generative models remains computationally intensive, it is important to note that the computational requirements can vary depending on the specific task and model architecture. For example, tools designed for specific tasks like high-throughput virtual screening can be optimized for efficiency. PyRMD, a tool developed for automated AI-driven ligand virtual screening, demonstrated the capability to screen millions of compounds within a few hours even on computers with modest configurations . This suggests that while training frontier models is costly, certain AIdriven workflows in drug design can be made computationally tractable on less resource-intensive hardware .​  

Addressing the significant computational costs associated with training and deploying generative AI models is crucial for advancing their utility in drug discovery . Strategies aimed at reducing these costs include leveraging distributed computing frameworks, utilizing cloud-based platforms for scalable resources, and applying model compression techniques to optimize models for deployment on less powerful hardware . Implementing such strategies can help make powerful AI tools more accessible and sustainable for researchers in computational chemistry and drug design.​  

# 7.4 Generalizability and Robustness  

A critical challenge in applying generative AI to drug design is ensuring the generalizability and robustness of models across diverse datasets, novel targets, and evolving chemical spaces [39]. Deep learning models, while powerful pattern recognizers, can sometimes behave like “super-memorizers,” potentially lacking the necessary flexibility when encountering situations significantly different from their training data [39]. This can manifest as poor performance on unseen targets or chemical scaffolds that were not well-represented in the training distribution.  

For instance, evaluations of the PyRMD ligand virtual screening tool demonstrated satisfactory overall performance but showed lower efficacy in temporal split benchmarks compared to other evaluation settings [41]. Specifically, the model underperformed significantly for targets such as AKT2, PTN1, HIVRT, and to a lesser extent, CP3A4 [41]. Analysis of the  

chemical space related to AKT2 revealed that false positives were often structurally distant from the training data points, belonging to patented scaffolds reported afterthe training set’s cutoff date [41]. This observation highlights a limitation in “scaffold hopping,” or the ability of the model to generalize predictions to entirely novel chemical structures, particularly those representing emerging chemical novelty not present during training [41]. Such results underscore the difficulty in developing models robust to temporal shifts in chemical space and capable of handling genuinely novel molecular designs.  

Conversely, some models have demonstrated successful generalization to unseen data. The MitoReID model, designed for mitochondrial phenotype analysis, was able to successfully identify the targets of six “new drugs” that were not included in its training set [31]. This indicates that, depending on the specific task and dataset characteristics, AI models can achieve generalization to novel compounds not previously encountered [31].​  

Addressing the generalizability and robustness requires careful consideration of model architecture selection, as not all problems within the pharmaceutical domain are optimally suited for a single type of AI approach, such as deep learning alone [39]. Furthermore, strategies are needed to improve model performance on out-of-distribution data, including temporal splits and novel chemical scaffolds, to ensure their practical applicability in discovering truly innovative drug candidates [41]. The ability to perform reliably on diverse biological targets and chemical spaces is paramount for the widespread adoption and success of AI in drug design [39].​  

# 7.5 Synthetic Feasibility  

A significant challenge in generative AI for drug design is ensuring that the generated molecules are not only potent and selective but also synthetically feasible and possess favorable ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) properties [13]. While AI excels at exploring vast chemical spaces and identifying novel structures, the potential difficulty in synthesizing these AI-generated molecules presents a practical bottleneck in the drug discovery pipeline [2]. Current methods for assessing synthetic feasibility have been noted as potentially insufficient, which can lead to the generation of molecules that are challenging or impossible to synthesize in practice [12].​  

To mitigate these issues, various computational strategies are employed. One approach involves utilizing synthetic accessibility scores, which provide a numerical estimate of how easy a molecule is to synthesize. Furthermore, multiobjective optimization techniques are integrated into the generative process to balance the conflicting objectives of desirable biological activity, favorable ADMET profiles, and synthetic tractability [13]. Some methods directly incorporate synthetic considerations into the molecule generation process itself. For instance, the DeepBlock method addresses synthetic feasibility by constructing molecules directly from readily available reactive molecular building blocks, thereby inherently increasing the likelihood that the generated structures are easier to synthesize [44].  

Another crucial aspect is the integration of computational methods supporting retrosynthesis and synthesis route prediction tools into AI-driven drug design workflows [13]. These tools assess the feasibility of synthesizing AI-designed molecules by proposing potential synthetic routes or decomposing the target molecule into simpler precursors using reverse chemical reactions [2,13]. Integrating such predictive capabilities allows for the early identification of molecules with high synthesis difficulty, enabling researchers to prioritize the synthesis of more accessible candidates or refine the generative process to favor synthetically viable structures. Despite these advances, improving the accuracy and reliability of synthetic feasibility assessment and route prediction remains an active area of research.  

# 8. Datasets and Resources for Generative AI in Drug Design  

The effectiveness and reliability of generative artificial intelligence (AI) models in drug design are fundamentally dependent on the quality, quantity, and diversity of the datasets used for training and evaluation. These datasets span various domains, reflecting the multidisciplinary nature of drug discovery.  

Datasets utilized in generative AI for drug design can be broadly categorized based on the type of information they contain. Key types include large chemical databases containing millions of small molecules, such as PubChem (hosting 247 million compounds) [11,16] and ZINC (with 230 million compounds, frequently used for training models like VAEs and RNNs) [8,16,37]. Other significant chemical structure repositories include TargetMol, holding approximately 5000 natural molecules [16], DrugBank [20], and Natural Products Atlas 2.0 [20]. Bioactivity and property data are crucial for training models to generate molecules with desired characteristics. ChEMBL is a prominent database in this category, containing 16 million bioactivity measurements, although specific data types, such as liver toxicity (DILIRank), may be limited with only 1036 qualitative data points [10,11,20,41]. Other bioactivity/property datasets mentioned include those for molecular properties like BBB Martins, Pgp Broccatelli, and CYP2D6 Veith, sourced from packages like TDCommons [40], the Tox21 dataset [40], and the BBBP dataset from MoleculeNet [43]. The DUD-E website is used for generating decoys in screening studies [20]. Reaction data, such as that within the Reaxys database (approximately 11 million reactions and 300,000 rules), is valuable for applications like retrosynthesis [37]. Information regarding target-disease associations can be sourced from databases like Medline [37]. Furthermore, efforts exist to integrate large-scale patient and clinical data, exemplified by initiatives like The Cancer Genome Atlas (TCGA), Alzheimer’s Disease Neuroimaging Initiative (ADNI), Osteoarthritis Initiative (OAI), and UK Biobank, which compile information from large patient populations to support drug development [13]. Beyond curated databases, AI algorithms are also employed to generate extensive virtual compound libraries, such as MCE's MegaUni library of 10 million virtual drug-like compounds [15]. When evaluating datasets, particularly benchmark datasets, it is noted that a distinction should be made between those suitable for generative models (DGMs) and those for quantitative structureactivity relationship (QSAR) models [9].​  

The quality and meticulous curation of these datasets are paramount for ensuring the accuracy, reliability, and generalizability of trained AI models [13,39]. High-quality data reduces noise and biases, leading to more robust model performance. Strategies to enhance data value include improving quality through automated or standardized high-quality experimental methods, reducing acquisition costs, developing new methods to increase data dimensions, and applying appropriate data analysis techniques [17]. The scarcity of comprehensive data for certain properties, such as liver toxicity in ChEMBL, underscores the ongoing challenges in data availability and quality for specific tasks [10].​  

Accessing and integrating diverse datasets from disparate sources present significant challenges in the field [13]. Datasets often reside in isolated silos, maintained by different institutions or companies, with varying levels of accessibility and data formats. Integrating information from chemical structures, biological assays, clinical trials, and patient genomics requires sophisticated data harmonization and infrastructure.  

Addressing these challenges necessitates the development and adoption of standardized data formats and sharing platforms [13]. Standardized formats would enable seamless data exchange and integration across different research groups and databases. Establishing shared platforms could facilitate collaboration, accelerate research cycles, and provide larger, more comprehensive datasets for training AI models. The proposal for a standardized molecular database, analogous to ImageNet in computer vision, highlights the recognized need for such resources to promote standardized evaluation and progress in the field [2]. Exponential data and knowledge are increasingly compiled in structured biomedical databases managed by entities like the European Bioinformatics Institute (EBI) or the National Center for Biotechnology Information (NCBI), representing significant efforts towards centralizing biological and chemical information [13].​  

Researchers leverage various open-source tools and libraries to process, analyze, and model these datasets. Cheminformatics libraries like RDKit and Open Babel are widely used for handling molecular structures [41,43]. General data science libraries such as Pandas, Numpy, Matplotlib, and scikit-learn provide essential functionalities for data manipulation, analysis, and model development [41]. Specific libraries like DeepChem [43] and packages like TDCommons [40] are designed for chemistry and toxicology data, supporting specific tasks within AI-driven drug discovery. Explanatory tools like LIME are also employed for model interpretation [43]. Public platforms like TOXRIC also provide access to relevant toxicology datasets [40]. These resources form the foundational data infrastructure supporting advancements in generative AI for drug design.​  

# 9. Ethical Considerations and Regulatory Landscape  

<html><body><table><tr><td>Area</td><td>Key Aspects /Concerns</td><td>Regulatory Status/Guidance</td></tr><tr><td>Regulatory Approval</td><td>Ensuring safety and efficacy of Al-driven drugs; Clear standards/regulations needed for Al use; Assessing Al tool reliability [4,13,15,19,25]</td><td>Evolving guidelines (e.g., Us FDA guidance on Al use, EU Al Act classifying healthcare Al as high-risk); Q3 consistencyassessment [4,7,15,25,42]</td></tr><tr><td>Data Privacy & Security</td><td>Handling sensitive patient data; Consent mechanisms; Security of health</td><td>Strict adherence to regulations (GDPR, HIPAA, PIPL); Challenges in</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>information [3,13,14,19,22,28,42]</td><td>obtaining informed consent [42]</td></tr><tr><td>Algorithmic Bias</td><td>Opaque models can conceal biases (especially from clinical data); Risk of developing biased/discriminatory drugs [3,8,14,15,19,22,28]</td><td>Emphasis on trustworthy Al; Standards development (NIST,ISO,IEEE,IEC); Need for transparency in decision- making[7,42]</td></tr><tr><td>Accountability</td><td>Lack of consensus on responsibility for Al system errors; Need for redressal mechanisms [42]</td><td>Legal frameworks required to define responsibility and accountability [42]</td></tr><tr><td>Intellectual Property</td><td>Challenges in protecting novel molecules generated by Al algorithms [13]</td><td>Ongoing discussion and development needed for IP frameworks [13]</td></tr><tr><td>Human Role</td><td>Al augmenting, not replacing, human expertise; Human judgment needed for ethical implications and interpretation [13,21,39,42]</td><td>Emphasis on training healthcare professionals; Ultimate responsibility for decisions rests with professionals [42]</td></tr><tr><td>Equitable Access</td><td>Ensuring Al-driven advances benefit all populations; Avoiding exacerbating healthcare disparities [33,42]</td><td>Requires careful consideration in data use, model development,and policy [33,42]</td></tr></table></body></html>  

The advent of generative artificial intelligence (AI) in drug discovery necessitates careful consideration of both the evolving regulatory landscape and critical ethical implications. Regulatory bodies worldwide are beginning to establish guidelines for the use of AI in pharmaceutical research and development, highlighting the need for clear standards and regulations to ensure the safety and efficacy of AI-driven drugs [4,13,19]. The U.S. Food and Drug Administration (FDA) has released guidelines outlining potential key application areas for AI in drug discovery, emphasizing risk reduction and the development of clearer standards [15]. Generative AI tools can even offer new solutions for specific regulatory assessments, such as the FDA Q3 consistency assessment concerning drug product structural aspects [25]. Furthermore, large language models (LLMs) can support regulatory compliance by analyzing extensive documentation and tracking the latest requirements, thereby enhancing efficiency and reducing compliance risks, potentially avoiding delays in drug approval processes [12,14]. Beyond the U.S. context, regulatory frameworks are also developing internationally; for instance, the European Union is progressing towards enacting the AI Act, which classifies many healthcare-related AI applications as "high-risk" due to their profound implications for individual well-being and fundamental rights [42]. Concurrently, there is increasing emphasis on promoting trustworthy and ethical AI, with organizations like NIST, ISO, IEEE, and IEC actively developing standards to address crucial issues such as data quality, interpretability, and model performance in AI/ML applications within pharmaceutical development [7]. Macro-level policies in countries like China also favor AI-driven drug discovery, recognizing it as a key industry for national development and encouraging its application, including in areas like Traditional Chinese Medicine research [19,23].​  

Alongside regulatory evolution, the use of AI in drug development raises substantial ethical considerations. These include concerns surrounding data privacy and security [3,14,19,22,28,42], algorithmic bias [3,14,19,22], the potential for job displacement [19], and the risk of developing biased or discriminatory drugs [19].  

A critical ethical implication is the use of patient data to train generative AI models, necessitating robust data privacy protections [13]. Data privacy, consent, and security are paramount, especially given the sensitive nature of health information [28,42]. Handling sensitive data requires strict adherence to established data privacy regulations globally, such as the General Data Protection Regulation (GDPR) in Europe, the Health Insurance Portability and Accountability Act (HIPAA) in the United States, and the Personal Information Protection Law (PIPL) in China [42]. Obtaining informed consent for AI utilization in healthcare or research contexts is challenging yet essential [42]. However, the consent mechanism itself can inadvertently introduce biases by creating selection effects among patients who provide data, potentially leading to biased algorithms [42]. Ethical aspects must also be carefully considered in AI-based clinical trials, particularly concerning equitable participant recruitment, data integrity, participant autonomy, and confidentiality [42].​  

Algorithmic bias is another significant concern. Opaque models, often employed in deep learning, can conceal certain biases [8,15], which become particularly critical when clinical data is used for training [8,15]. The interpretability and reliability of AI models are crucial, especially when decisions may impact patient health [28]. Transparency in the decisionmaking processes of AI models, particularly in clinical settings, is vital for ensuring ethical data use and building trust among healthcare professionals and patients [42].  

Addressing these challenges requires concerted efforts from regulatory agencies and pharmaceutical companies to promote the responsible use of AI in drug discovery [19]. This includes developing clearer guidelines and standards for the regulatory frameworks governing the approval of drugs developed using AI [13]. It is important to note that regardless of whether GenAI is used in the design and development, new drugs must still undergo rigorous testing and evaluation to gain FDA approval [4]. Furthermore, legal frameworks are needed to contemplate responsibility and accountability in the event of incorrect functioning of AI systems to protect interests and secure redressal mechanisms [42]. Human wisdom and judgment remain necessary when implementing AI to consider ethical implications [13], and the ultimate responsibility for diagnostic or treatment decisions informed by algorithms rests with healthcare professionals [13]. There is ongoing discussion and a lack of consensus regarding the optimal governance mechanism—whether driven by private actors or public institutions—to supervise the ownership, sharing, and use of patient data in the context of AI implementation [42]. Finally, the use of AI algorithms to generate novel drug candidates introduces challenges regarding the protection of intellectual property [13]. These complex ethical and regulatory challenges must be proactively addressed to ensure the safe, equitable, and effective integration of AI into the future of drug discovery and development.  

# 10. Case Studies and Success Stories  

<html><body><table><tr><td>Company / Study</td><td>Al Focus /Platform</td><td>Notable Achievement / Candidate</td><td>Impact (Time/Cost Savings)</td></tr><tr><td>Insilico Medicine</td><td>GANs, Chemistry42 (GENTRL, AAE), PandaOmics, InSilicoFlow [5,6,9,11,12,14,15,18, 33,34]</td><td>INS018_055(TNIK inhibitor for IPF), Phase Il clinical trial, Orphan Drug Designation [4,5,6,11,15]</td><td>Discovery-to- preclinical in 18 months ($2.6-2.7M); Target ID to drug design in 46 days [5,11,28]</td></tr><tr><td>Exscientia</td><td>End-to-end ML platform</td><td>DSP-1181 (for OCD), entered Phase I clinical trials [2,4,6,11,15]</td><td>Reached Phase lin 12 months (immuno- oncology candidate) [13]</td></tr><tr><td>Atomwise</td><td>AtomNet (CNNs)</td><td>Screened for Ebola virus treatments; Collaboration with Sanofi [18,28]</td><td>Screening potential treatments in days vs.months/years [28]; Screened 3 trillion compounds [18]</td></tr><tr><td>Variational Al</td><td>Generative Platform</td><td>Al-designed COVID- 19 antiviral,breast cancer treatment</td><td>(lmpact not specified in text)</td></tr><tr><td>Recursion Pharmaceuticals</td><td>Image-based phenotypic analysis, Al platform</td><td>candidates [4] Accelerating rare disease drug development [28,31]</td><td>(Impact not specified in text)</td></tr></table></body></html>  

<html><body><table><tr><td>Lantern Pharma</td><td>RADR@ platform</td><td>Al-driven cancer drug repurposing and development</td><td>(Impact not specified in text)</td></tr><tr><td>XtalPi</td><td>ID4 (Intelligent Digital Drug Discovery and Development)</td><td>Reduced research time to PCC (e.g., Liver X receptor agonists) [12,14,38]</td><td>Reduced time by months to a year; Saved up to 90% experimental requirements [38]</td></tr><tr><td>DeepMind (AlphaFold)</td><td>Deep Learning for protein structure prediction</td><td>Revolutionized protein folding prediction [4,17]</td><td>Significantly improved protein structure modeling [27]</td></tr><tr><td>Broad Institute</td><td>Al for screening existing molecules</td><td>Discovered novel antibiotic Halicin [13]</td><td>Discovered antibiotic in record time[13]</td></tr><tr><td>Various Research Groups</td><td>VAES, GANS, RNNS, GNNs (e.g., GraphGMVAE, RELATION, DeepBlock) [9,21,44]</td><td>Generated novel scaffolds, potent inhibitors, molecules with controlled properties [2,9,21,44]</td><td>(Specific time/cost savings vary per study)</td></tr></table></body></html>  

The application of generative artificial intelligence (GenAI) is increasingly evident in drug discovery, with numerous companies and research groups showcasing tangible successes [2,4,18,34]. This progress is driven by various entities, including traditional pharmaceutical companies, AI-driven biotechnology firms, and major technology corporations [19]. Collaborations between these sectors, such as those between Sanofi and Baidu for mRNA-based vaccines, Pfizer and Google utilizing AlphaFold2, and partnerships involving AstraZeneca, Novartis, and Bayer with AI companies, underscore the growing integration of AI into conventional drug development pipelines [19,26]. Internet companies leverage their computational power and AI expertise, establishing dedicated teams or platforms like Baidu's PaddlePaddle, Huawei's medical intelligent body, and Tencent's cloud deep drug platform [19].​  

A key indicator of AI's impact is the progression of AI-designed drug candidates into clinical trials [2,4,17]. As of recent reports, approximately 15 AI-designed drugs have entered clinical evaluation stages globally [2]. Notable examples include Exscientia's DSP-1181 for obsessive-compulsive disorder and Insilico Medicine's INS018_055 (also referred to as ISM001-055) [2,4,6,11,15]. INS018_055, designed as a novel small molecule inhibitor targeting TNIK for the treatment of idiopathic pulmonary fibrosis (IPF), represents a significant milestone, being among the first AI-generated compounds to enter clinical Phase II trials [5,6,15]. The U.S. FDA has also granted Orphan Drug Designation to INS018_055 for IPF [4]. Other AI-discovered candidates in clinical development include Berg Health's novel anticancer drug BPM 31510 and Pharos iBio's PHI-101, a nextgeneration FLT3 inhibitor for cancer, developed using their Chemiverse platform [6,28]. Variational AI is also developing an AI-designed COVID-19 oral antiviral and a breast cancer treatment using its generative platform [4]. Recursion Pharmaceuticals employs image-based phenotypic analysis and AI to accelerate the development of drugs for rare diseases [28,31].​  

The impact of GenAI is quantifiable in reducing the time and cost associated with drug discovery [4]. For instance, Insilico Medicine reported that the discovery and preclinical development of INS018_055 took only 18 months and cost approximately $2 . 6 t o 2 . 7$ million, a significant reduction compared to the traditional timeline of around 4.5 years and tens of millions of dollars for this stage [5,11]. Similarly, the first AI-designed drug in immuno-oncology reached Phase I clinical trials in just 12 months, contrasting sharply with the typical 5-7 years for discovery [13]. Beyond clinical candidates, AI has accelerated the discovery of novel antibiotics like halicin through mining existing molecules in record time [13]. XtalPi's Intelligent Digital Drug Discovery and Development (ID4) platform, used in collaborations, has reduced the research time  

from target identification to preclinical candidate (PCC) by several months to a year, potentially saving up to $9 0 \%$ of experimental requirements [38].  

Several specific AI models and techniques underpin these successes. Insilico Medicine has been a pioneer in using generative adversarial networks (GANs) for de novo molecular design, generating novel structures with desired properties [9,34]. Their platforms, such as Chemistry42, utilize generative models like GENTRL (Generative Tensorial Reinforcement Learning) and adversarial autoencoders to propose candidate molecules with target-specific potential activity and favorable properties [6,11,18]. Other companies and research groups employ a variety of generative AI platforms and models, including Iktos's Makya and Ro5's De Novo Platform [18]. The RELATION model, a deep generative model, has been successfully applied to structure-based de novo design, yielding potent inhibitors for targets like AKT1 and CDK2 [21]. Variational autoencoders (VAEs), such as those used by Blaschke et al. for designing dopamine receptor type 2 antagonists and GraphGMVAE developed by You et al. for scaffold hopping, also demonstrate the utility of generative models in exploring chemical space and generating novel compounds with desired properties [9]. The integration of AI with robotic automation, as demonstrated by 晶泰 Technology, further enhances efficiency in drug and material science R&D [17].  

AI has proven particularly valuable in accelerating the discovery of antiviral drugs, as highlighted during the COVID-19 pandemic [2]. Companies like Exscientia rapidly screened potential antiviral candidates using AI platforms [28]. Specific AI models have been applied to target key SARS-CoV-2 proteins. Insilico Medicine developed chemical types for the SARS-CoV2 main protease using a GAN-based de novo design method [9]. Bung et al. employed transfer learning with a stacked RNN model to identify 31 promising lead compounds targeting the SARS-CoV-2 protease [9]. Chenthamarakshan et al. utilized the CLaSS method to design compounds predicted to bind preferentially to three key SARS-CoV-2 target proteins, identifying 3500 potential leads [9]. Beyond de novo design and screening, AI-driven network computation identified numerous drug repurposing opportunities, including for COVID-19 [13]. These applications demonstrate AI's potential to significantly accelerate the discovery of antiviral treatments, critical for responding to emerging infectious diseases [2].​  

In small molecule drug discovery, AI contributes through various workflows, including virtual screening, de novo design, and drug repurposing [18]. Virtual screening, historically one of the earliest applications of computational methods in drug discovery dating back to approaches like QSAR enhanced by deep learning in challenges like the Merck challenge in 2012 [8], is accelerated by AI platforms such as Tao Shu Bio's PLANET and COMET algorithms for rapid screening and reverse target identification [24]. Collaborations like Sanofi and Atomwise leverage AI for large-scale virtual screening campaigns [18]. De novo design using generative models, as discussed previously, is a core application, enabling the creation of entirely new molecular entities with desired properties like high binding affinity, drug-likeness, and low toxicity, exemplified by methods like DeepBlock which achieved an average drug-like score of 0.54 [44]. Drug repurposing, the identification of new uses for existing drugs, is also efficiently explored using AI by analyzing vast biological and chemical data through network computation [13,18]. Lantern Pharma, for instance, employs its RADR®️platform for AI-driven cancer drug repurposing and development [18].​  

Beyond target-specific design, AI frameworks are being developed for broader applicability in predicting molecular properties crucial for drug development. The NYAN framework incorporates a variational graph encoder and downstream surrogate models, demonstrating reusability and adaptability, particularly for tasks like toxicity prediction [40]. Its architecture facilitates leveraging learned molecular representations for various predictive tasks [40]. Case studies in drug formulation, such as predicting percolation thresholds in tablets and optimizing drug distribution in implants, further illustrate the versatility of generative AI beyond drug molecule design, enabling more efficient and controlled formulation development [25].​  

Overall, these case studies highlight the transformative potential of GenAI in expediting various stages of drug discovery and development, demonstrating success through clinical trial candidates, reduced timelines and costs, and the application of diverse AI methodologies across different therapeutic areas and drug modalities.  

# 11. Future Directions and Opportunities  

The integration of generative artificial intelligence (AI) is poised to transform drug design from a laborious "trial-and-error art" into a rapid and predictable "predictive science" [2]. This paradigm shift is expected to dramatically increase the efficiency of the drug discovery process, potentially shortening timelines and reducing costs [18,23]. Such gains in efficiency could enable researchers to pursue riskier, yet potentially groundbreaking, research projects due to a lower cost of failure [39]. The future of pharmaceutical research and development (R&D) is envisioned as a "closed-loop" system, where  

computational hypothesis generation is tightly integrated with rapid experimental validation [3,22]. This necessitates a profound convergence of engineering, computing, and biotechnology, leveraging advancements across AI, computing, biology, chemistry, and medicine to create synergistic solutions [1,3]. Projections, such as Gartner's prediction that over $3 0 \%$ of new drugs and materials will be discovered using generative AI by 2025, underscore the anticipated transformative impact of these technologies [4].  

Future advancements in generative AI for drug design will heavily rely on the development of more sophisticated and refined models [11]. This includes the creation of hybrid, multi-modal, and physics-informed AI models, designed to overcome current limitations and enhance prediction accuracy [26,35]. Multi-modal pre-training models, capable of integrating diverse data types like text and chemical information, hold significant promise, particularly in facilitating zerosample learning and extracting valuable insights from varied data formats [12,14,35]. Incorporating physical laws into datadriven algorithms is a crucial direction to improve model reliability, accuracy, and generalizability while reducing dependence on extensive datasets [12,14,35]. Moreover, integrating richer molecular details, such as 3D structural knowledge, is essential for improving the representation learning capabilities of models like variational graph encoders [40]. A key future direction is the development of models that can effectively incorporate and analyze multi-omics data to predict drug efficacy and safety with greater precision [3,14,22,35]. These advanced models are expected to capture complex biological relationships and improve the overall reliability of AI-driven predictions [26,35]. Concurrently, addressing the challenges of computational cost and complexity associated with modern deep learning approaches is necessary for their widespread adoption [8]. The development of accurate and interpretable AI models is also vital to build trust among researchers, regulators, clinicians, and patients [11,12].  

A major opportunity lies in the synergistic integration of generative AI with other cutting-edge technologies [1,2,15,16,17,26,27,28,35]. This includes high-throughput screening (HTS), CRISPR gene editing, synthetic biology, nanotechnology, robotics, automation, and microfluidics [1,2,15,16,17,26,27,28,35]. Specifically, combining generative AI with CRISPR and synthetic biology holds significant potential [8]. The integration with automation platforms is anticipated to facilitate the automated synthesis and experimental evaluation of proposed molecules, supporting the iterative, closedloop R&D cycle [3,21,22]. Combining AI with techniques such as DNA-encoded libraries (DEL) presents new research avenues, particularly for discovering hits against under-explored protein targets [20]. Integrating AI with structural biology data can lead to more accurate predictions of small molecule-protein interactions, guiding drug design [18]. Furthermore, the integration of quantum computing with AI is explored as a means to navigate vast chemical spaces more effectively [2,24]. Beyond the lab, seamless integration of AI systems with existing healthcare IT infrastructure, such as electronic health records (EHRs) and clinical decision support systems (CDSS), is crucial for real-world application and continuous system adaptation [42].​  

Generative AI is opening up significant opportunities in new areas of drug discovery and refining existing ones [16,17]. A key area of impact is the design of personalized medicines, tailoring treatments to individual patient profiles [9,15,16,17,18,22,26,27,28,33,35,37]. This includes AI-driven precision oncology, enabling rapid phenotypic analysis and realtime cancer cell management [27]. AI's potential extends to other critical areas such as drug repurposing and the development of therapies for rare diseases [16,17]. AI-powered analysis platforms can also enhance the understanding of drug mechanisms of action and facilitate the discovery of new therapeutic targets, including from sources like traditional medicine [5,31]. Beyond drug molecule design, generative AI can optimize drug formulation parameters, including drug load, particle distribution, and release performance [25]. In the long term, AI is expected to support a shift towards advanced therapeutic modalities like cell and gene therapies [27]. AI can also play a vital role in optimizing clinical trial design and operations, improving patient engagement, and supporting decentralized clinical trial models [3,35,37].  

Realizing these future opportunities necessitates addressing several critical factors and challenges. Strengthening data sharing practices, establishing unified data standards, and ensuring high data quality are foundational requirements [11,12,14,35]. Developing robust AI algorithms that perform accurately even under conditions of limited data is a significant future challenge [12]. A key focus for AI in the near future is to assist pharmaceutical companies in navigating the challenging transition from preclinical development to clinical translation, often referred to as the "valley of death" [17]. Successfully integrating AI systems into existing technological infrastructures and healthcare workflows poses significant hurdles, requiring comprehensive training and change management strategies for healthcare professionals [42]. Developing scalable and continuously learning AI systems capable of adapting to evolving clinical needs is also crucial [42]. Furthermore, fostering robust interdisciplinary collaborations between AI experts, chemists, biologists, computational scientists, and clinicians is paramount to translating technological potential into tangible therapeutic advancements [3,14].  

Building open ecosystems through close cooperation across technology, capital, research, and industry sectors will also be essential for platform development and innovation [17]. As AI becomes more embedded in healthcare, scientists and policymakers must proactively scrutinize the ethical implications for responsible clinical use [33]. Cooperative efforts, pooling resources and knowledge, can help develop scalable AI solutions with broad societal benefits [33].  

In conclusion, the future of generative AI in drug design is characterized by the development of more sophisticated models, deeper integration with experimental and automation technologies, and expansion into new application areas, fundamentally transforming the R&D landscape.  

# 12. Conclusion  

Generative AI has heralded significant advancements in drug design, fundamentally altering traditional paradigms [2,6,8,10,16,24,30,39]. AI techniques have dramatically improved the efficiency and success rates of drug research and development while simultaneously reducing associated costs [12,23,27]. Specific progress includes the impact of deep learning on generative model-based molecule design and the study of protein–ligand binding processes [21,36], the development of tools like PyRMD for automated ligand virtual screening [41], methods for generating molecules with controlled properties like DeepBlock [44], and approaches for predicting mechanisms of action using high-content imaging and deep learning [31]. AI is transforming the process from an empirical “trial-and-error art” to a more systematic “predictive science” [2].​  

The transformative potential of AI in revolutionizing drug discovery is widely acknowledged. It accelerates the overall process, substantially reduces costs, and improves the quality of drug candidates [2,6,10,15,30]. Generative AI, in particular, offers unprecedented opportunities by improving speed, productivity, quality, and sustainability across the R&D pipeline— from virtual assistants and molecule design to synthetic data generation and clinical trial optimization [1,5,9,14,22]. AIdriven projects have demonstrated significantly faster timelines, completing discovery to preclinical phases in durations notably shorter than the industry average [4]. Beyond speed and cost efficiency, AI promises to make drug development cheaper and more efficient, shorten the discovery phase, reduce failure rates, and pave the way for highly personalized medicine by providing insights into patient characteristics [13]. It is seen as a disruptive force reshaping the pharmaceutical industry and offering new solutions for disease treatment [17,27,28].  

Despite these significant advancements, key challenges and limitations persist that must be addressed to fully realize the potential of generative AI in drug design [1,9,26,35]. These challenges include improving the interpretability of complex models, managing computational costs, addressing issues related to data quality and availability, enhancing model complexity to capture biological nuance, and improving prediction confidence and mechanistic reasoning [8,10,11,24]. Furthermore, concerns regarding job displacement and regulatory frameworks need careful consideration [26]. Challenges specific to generated molecules, such as ensuring synthetic feasibility and favorable toxicity profiles, also remain critical [44]. Although significant progress has been made, as of 2024, AI-generated drugs had yet to receive market approval, highlighting the translational gap still being addressed [15].​  

Looking ahead, AI is expected to have the greatest impact by continuing to advance computational tools that challenge current pharmaceutical methods, enabling the de novo design and development of compounds from vast chemical databases [16]. Future efforts will likely focus on balancing predictive performance with interpretability and reducing training costs [8]. AI technologies are anticipated to be increasingly integrated throughout the entire medicinal product lifecycle, including diagnostic procedures, post-marketing safety evaluation, and the personalization of therapeutic approaches [42]. AI is poised to improve R&D efficiency significantly in drug discovery and preclinical research, potentially by over $5 0 \%$ , and increase the success rate of clinical trials [27]. AI-assisted drug screening, in particular, is expected to have a profound impact on the biomedical field as scientific knowledge advances [29]. The availability and quality of data will continue to be a key factor in the implementation and value realization of AI in biopharmaceuticals [17]. AI programs focusing on innovative data solutions and autonomous experimentation systems will further enhance research efficiency and change approaches to health and disease [33].​  

Realizing the full benefits of AI in drug discovery and development—and ultimately improving human health—necessitates continued research and development and, critically, robust interdisciplinary collaboration [1,2,6,14,15,19,22,26,28,35]. This involves collaborative efforts across various disciplines, economies, and societal levels [33]. Addressing the existing challenges requires sustained technological innovation and regulatory evolution [11,19]. Humans retain a central and  

indispensable role in the ethical and proper use of these technologies and the correct interpretation of generated evidence, underscoring the importance of training healthcare professionals [42].  

In conclusion, the confluence of generative AI and drug design presents a paradigm shift with immense potential to transform the discovery and development process, bringing the promise of more innovative, effective, safe, and standardized drugs closer to reality [19,23]. To harness this power for the benefit of human health, researchers, industry professionals, and policymakers must actively work together [15,16,22,28]. Pharmaceutical companies should proactively embrace this transformative change and develop strategies to fully leverage generative AI, promoting sustainable development and innovation [3,22,32]. Continued dedication to addressing challenges, fostering collaboration, ensuring ethical deployment, and promoting equitable access to AI technologies will be crucial in accelerating scientific discovery and delivering life-changing therapies to patients worldwide [33,42]. The future of drug design is inextricably linked with the advancement and responsible application of generative AI.​  

# References  

[1] 生成式AI：药物研发的生产力与科学突破 https://cloud.tencent.com/developer/article/2497954?policyId=1004   
[2] AI加速药物研发：从马拉松到预测科学 https://mp.weixin.qq.com/s?   
__biz $: =$ Mzg3OTE3NjA4Nw $\scriptstyle 1 = =$ &mid $\ c =$ 2247612258&idx $\mathop { : = }$ 2&sn $\ O =$ 543d7262eb135a34cfc904b0f708b3a5&chksm=ce80da6747a7a991   
c9f5dacd8208478e4ef40b1f683e632967530d68ec2441c8d016a8ac701c&scene=27   
[3] 生成式AI：驱动药物研发生产力与科学突破 https://cloud.tencent.com.cn/developer/article/2497954​   
[4] AI药物设计竞赛：FDA批准之路与未来展望 https://baijiahao.baidu.com/s?id $\ c =$ 1792189296356254829&wfr=spider&for=pc   
[5] AI加速新药研发落地，制药业进入“淘汰赛” https://baijiahao.baidu.com/s?id $=$ 1813759746337288851&wfr=spider&for=pc​   
[6] 人工智能技术在先导化合物发现与优化中的应用 https://mp.weixin.qq.com/s? _biz $\mathrel { \mathop : } =$ MjM5MjMwOTIyNA $\scriptstyle =$ &mid $\ c =$ 2651836632&idx $\vDash$ 2&sn $| =$ 013eb393ff7a9304bb0d2ad474e2b432&chksm $\mid =$ bc59548e4a5be67   
da47356f01742d27105f5cb6ab6ace6bc1a21e07771a6118c08bdef1a6eed&scene=27   
[7] AI/ML在药品开发中的应用：基于FDA讨论文件的分析 https://mp.weixin.qq.com/s? _biz $: =$ MzA5MzQzNTM5MA $\scriptstyle = =$ &mid $=$ 2658226996&idx $\varXi$ 1&sn $\mid =$ 1bd15c1648c86341114a45f95557d584&chksm $\mid =$ 8bd817a5bcaf9eb   
30784bf8e3fe9a07850b8f7a98439b284ecb50aa2529dedb651be3d3f6521&scene $^ { - 2 7 }$   
[8] 深度学习在计算化学中的应用、挑战与未来 https://blog.csdn.net/weixin_52812620/article/details/125956076   
[9] 生成式人工智能赋能药物发现：进展、挑战与未来 https://mp.weixin.qq.com/s? _biz $: =$ MjM5NTI3MDE0NQ $\scriptstyle 1 = =$ &mid=2653768409&idx $\mathrel { \mathop : }$ 1&sn=bae3fd491e599dfd24abc6479e0be70e&chksm $\mid =$ bc5142e253dac60   
eeb370488c0746866745e15d19bdb5489b25c3375933ff8f26e0a70aadb80&scene=27   
[10] 人工智能在药物设计中的应用：机遇与挑战 https://baijiahao.baidu.com/s?id $| =$ 1760521853749656016&wf $\Bumpeq$ spider&for=pc   
[11] 人工智能赋能新药研发：技术应用与未来展望 https://mp.weixin.qq.com/s? _biz $: =$ MzAwMDE3Njk0OA $\scriptstyle = =$ &mid=2648997997&idx $\mathop { : = }$ 1&sn=fee017570cb7b50a269c95c51e78c6c5&chksm=82fcc636b58b4f200   
bf8016ebef4faaa549faa56f37c47157a97425a586c827646b79b008b0a&scene=27   
[12] AI赋能药物研发：机遇、挑战与未来趋势 https://www.bio-equip.com/showarticle.asp?ID $\vDash$ 453142722   
[13] 人工智能赋能药物设计：迈向计算型精准医学 https://baijiahao.baidu.com/s?   
id=1729249351648100344&wfr=spider&for=pc​   
[14] AI 赋能药物研发：颠覆性变革与未来展望 https://mp.weixin.qq.com/s?   
__biz $: =$ MjM5NTI3MDE0NQ $\scriptstyle = =$ &mid $\circleddash$ 2653769433&idx $\vDash$ 1&sn=ec59383357662cf4562929b186ccaa1d&chksm $\mid =$ bc62b8d0175a138   
131a360e5757272ee4554894e9e1ff48a2513f0ea87c28a594a8cd67f4202&scene=27   
[15] AI 赋能药物研发：筛选技术新进展与MCE一站式平台 https://www.bio-equip.com/showarticle453139605.html   
[16] 深度学习在药物发现中的应用：机遇与挑战 https://mp.weixin.qq.com/s? biz=MzA4ODY4MDE0NA $\scriptstyle = =$ &mid=2247554448&idx=4&sn $\mid =$ 53a428173a5b26baabe56ce57f31eb0d&chksm $\mid =$ 9024262fa753af39   
7f380bf648bb3e66839fafceff0af9ea84c10701d5c723dfc5f1285f10d1&scene=27​   
[17] AI重塑药物研发：机遇、方向与价值 https://baijiahao.baidu.com/s?id $=$ 1821458066462198728&wfr=spider&for=pc   
[18] 人工智能赋能小分子药物开发：机遇与挑战 https://baijiahao.baidu.com/s?id $\mid =$ 1786396785451432293&wfr=spider&for=pc   
[19] 人工智能赋能药研：曙光已现，未来可期 http://www.yyjjb.com.cn/yyjjb/202401/202401101459265926_17624.shtml​   
[20] DEL $+ \mathsf { A l }$ 协同增强化学空间多样性，加速药物发现 https://hub.baai.ac.cn/view/40583   
[21] 人工智能辅助药物发现的新途径 https://mp.weixin.qq.com/s?   
__biz=MzUxNDAzMDUwMw $\scriptstyle = =$ &mid=2247518988&idx $\mathbf { \Psi } =$ 3&sn $\mathbf { \Psi } = \mathbf { \dot { \Psi } }$ 38183a637f77ed78d557be5be2141482&chksm=f94ee5c4ce396c   
d262ff4db55aeeeb1e2a15a68aafb3e08fc5c3f847a0a0e6bbaf33a28b5a6c&scene=27   
[22] 生成式AI：药物研发的生产力革命与科学突破 https://cloud.tencent.com/developer/article/2497954​   
[23] AI赋能制药：科技驱动医药研发新纪元 https://baijiahao.baidu.com/s?id $\ c =$ 1807722533300693094&wf $\Bumpeq$ spider&for=pc   
[24] AI赋能新药研发：机器学习驱动药物设计范式革命 https://baijiahao.baidu.com/s?   
id=1827181090541071877&wfr=spider&for=pc​   
[25] 生成式AI助力药物制剂开发：降本增效新方案 https://hub.baai.ac.cn/view/41710​   
[26] 人工智能在药物研发中的应用：挑战与未来 https://baijiahao.baidu.com/s?id $\mid =$ 1722273451729845476&wf $\scriptstyle \gamma =$ spider&for=pc   
[27] AI赋能制药产业转型升级 https://baijiahao.baidu.com/s?id $=$ 1828334267408703577&wfr=spider&for=pc   
[28] AI赋能生物医药：颠覆性技术革命加速药物发现 http://sunying.isenlin.cn/default.aspx?   
uid $\ c =$ 79BDC93422E6411083D2D69703E507A8&aid $\begin{array} { r } { { \bf \Pi } = \frac { { \bf \Pi } } { { \bf \Pi } } } \end{array}$ 3B51866C76BB4FB9B7FCE8D7B036CF2C   
[29] AI赋能药物研发：深度学习加速新药发现 https://baijiahao.baidu.com/s?id $=$ 1794556948122357122&wf $\mathit { \Theta } = \mathit { \Theta }$ spider&for=pc   
[30] AI in Drug Design and Discovery: A Comprehensive R https://link.springer.com/article/10.1007/s11030-021-10237-z​   
[31] AI驱动线粒体表型分析：加速药物发现与靶标识别 https://www.biomart.cn/news/16/3224724_0.htm​   
[32] 生成式AI：医药行业的效率革命 https://mp.weixin.qq.com/s?   
__biz $: =$ MzAxNzEwNjY2MA $\scriptstyle 1 = =$ &mid $\mathbf { \Psi } = \mathbf { \dot { \Psi } }$ 2651116835&idx=4&sn $=$ 1138937aa60b4eeaae24b4049cea7b9e&chksm $\mid =$ 81ab1672a905bfe9   
21b98d5845442bf503274d39819e25d58ba9ccb17dd902c5f98eff926c38&scene=27   
[33] AI Revolutionizes Health and Biology Research https://www.scijournal.org/articles/innovative-ai-techniques-propel  
health-and-biology-research-forward​   
[34] Insilico Medicine：人工智能药物开发领先者及专利布局 http://www.tip-lab.com/article/?   
uuid=e778cd4abe7f44cbaffafa78d7e8782b​   
[35] AI赋能新药研发：机遇与挑战 https://baijiahao.baidu.com/s?id $=$ 1822358028197301817&wfr=spider&for=pc​   
[36] Graph Convolutional Networks for Drug-Target Inter https://pubs.acs.org/doi/epdf/10.1021/acs.jcim.9b00628​   
[37] 机器学习赋能医药研发：应用与展望 http://baijiahao.baidu.com/s?id $\ c =$ 1664664175517200116&wfr=spider&for=pc   
[38] Singleron and XtalPi Collaborate on AI-Powered Dru https://www.singleronbio.com/about/detail-129.html​   
[39] AI赋能新药研发：潜力与局限 https://www.biodiscover.com/reaseach/735263.html​   
[40] NYAN Reusability: Variational Graph Encoders for M https://www.nature.com/articles/s42256-024-00923-6​   
[41] PyRMD: 全自动AI驱动的配体虚拟筛选新工具 https://pubs.acs.org/doi/10.1021/acs.jcim.1c00653​   
[42] AI for Pharmacological Therapies: Optimizing Benef https://www.frontiersin.org/journals/drug-safety-and  
regulation/articles/10.3389/fdsfr.2024.1356405/full​   
[43] ML Identifies BBB Penetration Substructures via Pa https://pubs.acs.org/doi/full/10.1021/acschemneuro.3c00840​   
[44] 人工智能助力靶向药物设计研究取得进展 https://www.nsfc.gov.cn/publish/portal0/tab1128/info93983.htm​  