# A Survey of Bias in EHR-Based AI Models

# 1 Abstract


The integration of Artificial Intelligence (AI) into healthcare, particularly through the analysis of Electronic Health Records (EHRs), has revolutionized the field by enabling more accurate diagnoses, personalized treatments, and efficient patient management. This survey paper focuses on the issue of bias in EHR-based AI models, providing a comprehensive overview of the current state of research, methodologies, and best practices for detecting and mitigating bias. The paper covers a range of topics, including comprehensive evaluation frameworks, collaborative and stakeholder approaches, systematic and quantitative methods, and fairness and bias mitigation techniques. Key findings include the importance of advanced large language models (LLMs) for grading and assessment, game-based consensus for fair causal structures, and the use of synthetic data and generative adversarial networks (GANs) to generate fair and representative datasets. The paper also highlights the need for robust, systematic approaches to bias mitigation and emphasizes the importance of interdisciplinary collaboration. By offering a detailed overview of the challenges and solutions in this field, this survey aims to serve as a valuable resource for researchers, practitioners, and policymakers working to advance the fairness and reliability of AI in healthcare.

# 2 Introduction
The integration of Artificial Intelligence (AI) into healthcare, particularly through the analysis of Electronic Health Records (EHRs), has revolutionized the field by enabling more accurate diagnoses, personalized treatments, and efficient patient management [1]. EHRs contain a wealth of data, including clinical notes, lab results, and patient demographics, which can be leveraged to improve healthcare outcomes [2]. However, the deployment of AI models in healthcare is not without challenges. One of the most significant concerns is the presence of bias in these models, which can lead to unfair and potentially harmful outcomes for certain patient groups [3]. Bias in EHR-based AI models can arise from various sources, including biased data collection, model training, and deployment processes. Addressing these biases is crucial for ensuring that AI systems in healthcare are fair, transparent, and trustworthy [4].

This survey paper focuses on the issue of bias in EHR-based AI models, providing a comprehensive overview of the current state of research, methodologies, and best practices for detecting and mitigating bias. The paper aims to bridge the gap between theoretical advancements and practical applications, offering insights that can inform the development of more equitable AI systems in healthcare [5]. By examining the multifaceted nature of bias in EHR data and the challenges it poses, this survey highlights the importance of interdisciplinary collaboration and the need for robust, systematic approaches to bias mitigation.

The paper begins by discussing comprehensive evaluation frameworks, which are essential for assessing the quality and consistency of AI models. These frameworks include advanced large language models (LLMs) for grading and assessment, governance frameworks for reinforcement learning, and adversarial training for security and defense. Each of these components plays a crucial role in ensuring that AI models are not only accurate but also secure and fair. For instance, LLMs can enhance the efficiency and accuracy of educational evaluations, while adversarial training can improve the robustness of models against manipulation.

Next, the paper delves into collaborative and stakeholder approaches, which are vital for building transparent and accountable AI systems. Game-based consensus for fair causal structures and hybrid human-AI approaches for transparency are explored in detail. These methods leverage the strengths of both human oversight and machine learning to ensure that AI models are not only accurate but also interpretable and aligned with ethical standards [6]. For example, game-based consensus mechanisms can balance the interests of different stakeholders, ensuring that causal models are fair and robust.

The paper also examines systematic and quantitative methods, which provide a rigorous foundation for bias detection and mitigation. Bayesian statistics for stakeholder participation, comparative risk-impact assessment frameworks, and systematic reviews for bias detection are discussed. These methods offer principled approaches to uncertainty quantification, risk evaluation, and continuous monitoring, which are essential for building trust in AI systems. For instance, Bayesian methods can integrate domain expertise and contextual knowledge, enhancing the relevance and effectiveness of AI models.

Finally, the paper explores fairness and bias mitigation techniques, including subgroup-specific and modular approaches, statistical and theoretical frameworks, and synthetic data and GANs [1]. These techniques aim to address the complex and intersecting biases that can arise in AI models, particularly in healthcare applications. Subgroup-specific discrimination aware ensembling, modular machine learning for explainability, and fairness modules with centroid fairness loss are highlighted as promising approaches for achieving fairer AI systems. Additionally, the paper discusses the use of synthetic data and GANs to generate fair and representative datasets, which can help mitigate biases in training and testing [7].

The contributions of this survey paper are multifaceted. It provides a comprehensive review of the current literature on bias in EHR-based AI models, synthesizing insights from various disciplines and methodologies. The paper also identifies gaps in the existing research and suggests directions for future work, emphasizing the need for interdisciplinary collaboration and the development of robust, systematic approaches to bias mitigation. By offering a detailed overview of the challenges and solutions in this field, this survey aims to serve as a valuable resource for researchers, practitioners, and policymakers working to advance the fairness and reliability of AI in healthcare [8].

# 3 Comprehensive Evaluation Frameworks

## 3.1 Structured Frameworks for Consistency and Quality

### 3.1.1 Advanced LLMs in Grading and Assessment
Advanced Large Language Models (LLMs) have emerged as powerful tools in the realm of grading and assessment, significantly enhancing the efficiency and accuracy of educational evaluations [9]. These models, such as GPT-3, GPT-4, and others, have demonstrated the ability to interpret and evaluate student responses with a high degree of precision, particularly in complex tasks like essay grading. By leveraging their sophisticated natural language processing capabilities, LLMs can assess not only the factual content of student responses but also their coherence, structure, and argumentation, thereby providing a more comprehensive evaluation compared to traditional automated systems. This has the potential to reduce the workload of educators, allowing them to focus on higher-level pedagogical activities.

However, the integration of LLMs in grading and assessment is not without challenges. One of the primary concerns is the vulnerability of these systems to adversarial gaming strategies, where students might exploit the model's patterns to receive higher grades without demonstrating genuine understanding [10]. Adversarial training methods have been proposed to mitigate this issue, aiming to make LLMs more robust against such tactics. These methods involve exposing the models to a diverse range of adversarial examples during training, thereby improving their ability to detect and resist manipulation. Additionally, the ethical implications of using AI in grading must be carefully considered, including issues of bias, transparency, and fairness [9]. Ensuring that LLMs do not perpetuate or exacerbate existing inequalities in the educational system is crucial for their responsible deployment.

Another critical aspect of using LLMs in grading and assessment is the development of effective prompt engineering and feedback mechanisms. Prompt engineering involves designing input prompts that elicit the most accurate and useful responses from the model, which is essential for reliable grading. Feedback mechanisms, on the other hand, are necessary to provide students with constructive criticism and guidance, helping them improve their learning outcomes. Recent research has explored the use of reinforcement learning with human feedback (RLHF) to refine LLMs' grading capabilities, ensuring that they align with educational standards and objectives. This approach involves iterative training cycles where human evaluators provide feedback on the model's performance, which is then used to fine-tune the system. Overall, the combination of advanced LLMs, robust adversarial training, and effective prompt engineering represents a promising direction for the future of automated grading and assessment [9].

### 3.1.2 Governance Frameworks for Reinforcement Learning
Governance frameworks for reinforcement learning (RL) are essential for ensuring that these systems operate within ethical, legal, and social boundaries. RL systems, by their nature, learn through interactions with their environment, making it crucial to establish robust governance mechanisms to guide their behavior and outcomes. These frameworks typically address key aspects such as accountability, transparency, and fairness. For instance, accountability in RL governance involves identifying the responsible parties for the system's actions, whether they be developers, users, or the AI itself. This is particularly important in high-stakes domains like healthcare and finance, where the consequences of RL decisions can have significant impacts on individuals and society.

Transparency is another critical component of RL governance, as it ensures that the decision-making processes of these systems are understandable and explainable. This is particularly challenging in RL, where the learning process can be opaque and the decision-making rationale complex. Governance frameworks often include mechanisms for logging and auditing the training and deployment phases of RL systems, allowing for retrospective analysis and accountability. Additionally, transparency can be enhanced through the use of explainable AI (XAI) techniques, which provide insights into the factors and data that influence RL decisions [11]. This is crucial for building trust and ensuring that the system's behavior aligns with ethical and legal standards.

Finally, fairness in RL governance involves ensuring that the system's decisions do not disproportionately disadvantage certain groups or individuals. This requires careful consideration of the data used to train the RL system, as biased or unrepresentative data can lead to unfair outcomes. Governance frameworks often include guidelines for data collection and preprocessing to minimize bias and promote inclusivity. Furthermore, these frameworks may incorporate regular audits and evaluations to monitor the system's performance and identify any emerging biases. By addressing these key aspects, governance frameworks for RL can help ensure that these powerful systems are used responsibly and ethically, contributing to their broader acceptance and integration into various domains.

### 3.1.3 Adversarial Training for Security and Defense
Adversarial training has emerged as a critical technique for enhancing the security and defense capabilities of machine learning models, particularly in the context of deep learning. This method involves exposing models to adversarial examples—inputs intentionally crafted to cause misclassification or erroneous behavior—during the training phase. By incorporating these adversarial examples, models can learn to recognize and resist such attacks, thereby improving their robustness. Adversarial training is particularly important for applications where the stakes are high, such as autonomous systems, financial transactions, and healthcare, where errors can have severe consequences.

The effectiveness of adversarial training lies in its ability to simulate a wide range of potential attack scenarios, allowing models to generalize better and become more resilient to unseen threats. Techniques such as Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and Carlini-Wagner (CW) attacks are commonly used to generate adversarial examples. These methods vary in complexity and computational cost, but they all aim to perturb the input data in a way that maximizes the model's error rate. By training on these perturbed examples, models can develop a more nuanced understanding of the feature space, leading to improved performance under adversarial conditions.

However, adversarial training is not without its challenges. One major issue is the trade-off between robustness and accuracy. Models trained with adversarial examples often exhibit a slight decrease in performance on clean data, as the additional noise introduced during training can make the model less confident in its predictions. Additionally, the computational overhead of generating and processing adversarial examples can be significant, especially for large-scale models. Despite these challenges, adversarial training remains a cornerstone of secure machine learning, driving ongoing research into more efficient and effective techniques for enhancing model robustness.

## 3.2 Collaborative and Stakeholder Approaches

### 3.2.1 Game-Based Consensus for Fair Causal Structures
Game-Based Consensus for Fair Causal Structures is a novel approach that leverages game theory to ensure fairness and transparency in the construction of causal models. This method involves multiple agents or stakeholders who interact through a series of strategic games to reach a consensus on the causal relationships within a given system. The primary goal is to create a causal structure that is not only accurate but also fair, reflecting the diverse perspectives and values of the participating entities. By integrating game theory, this approach can address the inherent biases and subjectivities that often arise in traditional, single-agent causal modeling.

In the context of fair causal structures, game-based consensus mechanisms are designed to balance the interests of different stakeholders, ensuring that no single entity can dominate the decision-making process. This is achieved through a series of iterative games where agents propose, challenge, and refine causal hypotheses. Each game round involves a set of rules that govern the interaction, such as the criteria for accepting or rejecting a proposed causal link. The iterative nature of these games allows for the gradual convergence towards a consensus that is both fair and robust. This process is particularly useful in domains where the stakes are high, such as healthcare, finance, and social policy, where the accuracy and fairness of causal models can have significant real-world implications [12].

The technical implementation of game-based consensus for fair causal structures involves a combination of algorithmic and human-in-the-loop components. On the algorithmic side, advanced machine learning techniques, such as reinforcement learning, are used to optimize the game dynamics and facilitate efficient convergence to a consensus. These algorithms help in automating the process of hypothesis generation and evaluation, reducing the cognitive burden on human participants. On the human side, domain experts and stakeholders play a crucial role in validating the causal hypotheses and ensuring that the final model aligns with real-world knowledge and ethical standards. This hybrid approach not only enhances the technical robustness of the causal models but also ensures that they are grounded in practical and ethical considerations.

### 3.2.2 Hybrid Human-AI Approaches for Transparency
Hybrid human-AI approaches in GIScience aim to enhance transparency by integrating human oversight and expertise with the computational capabilities of AI systems. These approaches leverage the strengths of both humans and machines to ensure that AI models are not only accurate but also interpretable and accountable. By incorporating human feedback and domain knowledge, hybrid systems can better address the complexities and nuances inherent in spatial data and analysis. For instance, in the context of open-source GIS applications, hybrid models can help validate and refine AI-generated outputs, ensuring that they align with local contexts and regulatory requirements. This collaborative approach not only improves the reliability of AI systems but also fosters trust among users and stakeholders.

One of the key mechanisms for achieving transparency in hybrid human-AI systems is through the use of explainable AI (XAI) techniques. XAI methods provide insights into the decision-making processes of AI models, allowing users to understand how specific outputs are generated [11]. In GIScience, this is particularly important for applications involving spatial reasoning and predictive modeling, where the stakes can be high. For example, in urban planning, hybrid systems can use XAI to explain the factors influencing land-use predictions, thereby enabling planners to make informed decisions. Additionally, by integrating real-time, context-specific warnings, these systems can alert users to potential biases or inaccuracies, promoting a more reflective and critical engagement with AI-generated content.

Another critical aspect of hybrid human-AI approaches is the role of human-in-the-loop (HITL) methodologies. HITL involves continuous human involvement in the AI development and deployment process, ensuring that models remain aligned with evolving user needs and ethical considerations. In GIScience, this can manifest through collaborative platforms where domain experts and end-users contribute to the training and validation of AI models. Such platforms not only enhance the transparency and accountability of AI systems but also facilitate the co-creation of knowledge and solutions. By fostering a culture of open collaboration and continuous improvement, hybrid human-AI approaches can significantly advance the field of GIScience, making it more responsive and responsible to the diverse challenges of the modern world.

### 3.2.3 Comparative Analysis of Human and Digital Labor
The comparative analysis of human and digital labor in the context of AI and robotics highlights significant differences in operational capacity, cost, and adaptability [13]. Human labor is characterized by its inherent limitations, such as the need for rest, sleep, and personal time, which restricts the average productive capacity to approximately one-third of the day. In contrast, digital labor, powered by AI and robotics, can operate almost continuously, offering a 24/7 availability that significantly reduces operational costs and increases efficiency [13]. This continuous operation capability is particularly advantageous in industries where consistent and high-volume output is crucial, such as manufacturing, data processing, and customer service.

However, the shift from human to digital labor is not without its challenges. The displacement of human jobs is a primary concern, as AI systems become increasingly capable of performing tasks that were traditionally the domain of human workers [13]. This transition not only affects the labor market but also has broader implications for economic and social structures. For instance, the loss of jobs can lead to increased unemployment, which may require substantial government intervention in the form of retraining programs, social welfare, and economic support to mitigate the adverse effects on affected individuals and communities. Additionally, the ethical implications of replacing human labor with AI, particularly in sectors that involve personal interactions and decision-making, raise questions about the preservation of human dignity and the value of human work.

Moreover, the environmental and energy implications of widespread automation must also be considered. While digital systems can operate continuously, they often require significant energy resources, which can contribute to increased carbon emissions and environmental degradation. The transition to digital labor, therefore, necessitates a balanced approach that considers both the economic benefits and the potential negative impacts on employment, social structures, and the environment. This holistic perspective is essential for developing policies and strategies that ensure a fair and sustainable transition to a more automated future.

## 3.3 Systematic and Quantitative Methods

### 3.3.1 Bayesian Statistics for Stakeholder Participation
Bayesian statistics offers a robust framework for integrating stakeholder participation in geospatial AI systems, particularly in the context of human-centered GIS. By leveraging prior distributions, Bayesian methods allow for the incorporation of domain expertise and contextual knowledge, which is crucial for ensuring that AI models are aligned with the needs and values of diverse stakeholders. This approach facilitates a more transparent and inclusive development process, where stakeholders can contribute their insights and experiences, thereby enhancing the model's relevance and effectiveness. For instance, in environmental monitoring applications, local communities can provide valuable prior information about ecological conditions, which can be integrated into the Bayesian model to improve predictive accuracy and decision-making.

Moreover, Bayesian methods are inherently adaptive, enabling continuous learning and updating as new data becomes available. This dynamic aspect is particularly beneficial in rapidly evolving geospatial contexts, where real-time data streams and emerging trends require frequent model adjustments. By updating the posterior distribution with new observations, Bayesian models can maintain their relevance and accuracy over time, thus supporting more informed and timely decision-making. This adaptability is crucial for applications such as urban planning and disaster response, where the ability to incorporate real-time data can significantly enhance the effectiveness of interventions and resource allocation.

Finally, Bayesian statistics provides a principled approach to uncertainty quantification, which is essential for building trust and accountability in AI systems. By explicitly modeling uncertainty, Bayesian methods offer a clear understanding of the confidence levels associated with model predictions, allowing stakeholders to make more informed decisions. This is particularly important in high-stakes applications, such as public health and environmental management, where the consequences of inaccurate predictions can be severe. Through the provision of posterior inference, Bayesian models not only deliver point estimates but also communicate the range of possible outcomes, thereby fostering a more nuanced and responsible use of AI in stakeholder-driven geospatial projects.

### 3.3.2 Comparative Risk-Impact Assessment Framework
The Comparative Risk-Impact Assessment Framework is designed to address the critical gap in the evaluation and enhancement of ISO standards' effectiveness in mitigating ethical risks across diverse global contexts [14]. This framework emerges from the recognition that a one-size-fits-all approach to standardization is insufficient, particularly in the rapidly evolving landscape of AI and geospatial technologies. By providing a systematic method, the framework aims to ensure that ISO standards are not only technically sound but also ethically robust, thereby fostering trust in AI systems and their applications [14].

The framework is structured around a multi-dimensional approach that integrates risk quantification, stakeholder engagement, and domain-specific risk taxonomies. Risk quantification involves identifying potential hazards and their associated harms, such as breaches of confidentiality, reliance on misinformation, and biases in decision-making [15]. This process is crucial for understanding the real-world consequences of AI systems and ensuring that the standards address these risks effectively. Stakeholder engagement is another key component, emphasizing the importance of involving domain experts, end-users, and affected communities in the development and evaluation of AI standards [16]. This participatory approach helps to ensure that the standards are not only technically feasible but also socially and ethically acceptable.

Finally, the framework advocates for the development of domain-specific risk taxonomies, recognizing that different application areas of AI, such as geospatial analysis and disaster response, have unique ethical and operational challenges. These taxonomies provide a structured way to categorize and prioritize risks, facilitating the creation of tailored standards that are more relevant and effective in specific contexts. By integrating these elements, the Comparative Risk-Impact Assessment Framework offers a comprehensive and flexible tool for enhancing the ethical and practical dimensions of AI standardization, ultimately contributing to the development of more trustworthy and responsible AI systems [14].

### 3.3.3 Systematic Review for Bias Detection and Mitigation
Systematic reviews for bias detection and mitigation in AI systems have become essential as the deployment of these technologies expands into critical domains such as healthcare, finance, and law enforcement [4]. These reviews aim to identify and address biases that can arise from data collection, model training, and deployment phases. Key methodologies include the use of fairness metrics, causal inference, and human-in-the-loop approaches [17]. Fairness metrics, such as demographic parity, equalized odds, and predictive parity, provide quantitative measures to assess bias [12]. However, these metrics often fail to capture the nuanced nature of bias, leading to the development of more sophisticated techniques like causal inference, which can trace the root causes of bias in data and models.

One prominent approach is the integration of human-in-the-loop (HITL) methods, which involve domain experts and affected communities in the bias detection and mitigation process. Tools like D-BIAS offer visual and interactive interfaces for preprocessing and debiasing algorithmic decision systems [18]. These tools enable users to explore variable interactions and their impact on model outcomes, facilitating a more transparent and collaborative approach to bias mitigation. HITL methods are particularly valuable in high-stakes applications where the stakes of biased decisions are high, such as in criminal justice and healthcare.

Despite these advancements, challenges remain in ensuring the robustness and reliability of bias detection and mitigation techniques. For instance, the dynamic nature of data and the evolving context of AI applications can introduce new forms of bias that existing methods may not capture. Moreover, the lack of standardized benchmarks and evaluation frameworks complicates the comparison and validation of different bias mitigation strategies. Future research should focus on developing adaptive and context-aware methods that can continuously monitor and adjust for bias, as well as fostering interdisciplinary collaboration to ensure that AI systems are fair, transparent, and accountable.

# 4 Fairness and Bias Mitigation Techniques

## 4.1 Subgroup-Specific and Modular Approaches

### 4.1.1 Subgroup-Specific Discrimination Aware Ensembling
Subgroup-specific Discrimination Aware Ensembling (SDAE) represents a novel approach to mitigating intersectional biases in machine learning models, particularly in healthcare applications [3]. SDAE leverages an ensemble of multimodal classifiers, each trained to focus on specific subgroups defined by sensitive attributes such as race, gender, and age [3]. By doing so, SDAE aims to address the limitations of traditional fairness methods that often fail to account for the compounded biases experienced by individuals belonging to multiple underrepresented groups. The core idea behind SDAE is to enhance the model's ability to make fair and accurate predictions for each subgroup, thereby reducing the overall unfairness in the system.

In the SDAE framework, pre-trained language models (LMs) such as MedBERT are utilized to extract rich, contextual representations from clinical notes and other textual data. These representations are then combined with other modalities, such as imaging data and structured electronic health records (EHRs), to form a comprehensive view of the patient's health condition [2]. Each classifier in the ensemble is specifically calibrated to handle the unique characteristics and challenges of the subgroup it represents. For instance, a classifier focused on female patients of a particular ethnic background might be adjusted to account for specific health disparities and biases observed in that subgroup. This tailored approach ensures that the model's predictions are not only accurate but also equitable across different demographic intersections.

To further enhance fairness, SDAE incorporates a post-processing step that fine-tunes the ensemble's output to minimize bias. This step involves adjusting the model's predictions based on subgroup-specific metrics, such as the false positive rate and false negative rate, to ensure that these rates are balanced across all subgroups. Additionally, SDAE employs a metric-aware unfairness indicator to systematically evaluate and monitor the level of unfairness in the model's predictions. Experimental results have shown that SDAE effectively reduces the compounded biases observed in intersectional subgroups, leading to more equitable and reliable healthcare outcomes. The approach highlights the importance of considering multiple sensitive attributes simultaneously and underscores the potential of ensemble methods in achieving fairer AI systems.

### 4.1.2 Modular Machine Learning for Explainability
Modular Machine Learning (MML) represents a significant advancement in the quest for more interpretable and fair AI systems, particularly in complex domains such as healthcare. MML decomposes the learning process into distinct modules, each responsible for a specific aspect of the model's functionality. This modularization facilitates a clearer understanding of how each component contributes to the final decision-making process, thereby enhancing explainability. For instance, in the context of medical diagnosis, a modular model might include separate modules for symptom analysis, patient history evaluation, and environmental factor assessment. Each module can be independently analyzed and validated, allowing for a more granular identification of biases or errors.

One of the key advantages of MML is its ability to promote fairness through modular representation and reasoning. By isolating the components that handle sensitive attributes, such as race or gender, MML enables targeted interventions to mitigate biases. For example, a module dedicated to processing demographic data can be designed with specific fairness constraints, ensuring that its outputs do not disproportionately affect certain groups. Additionally, the modular structure supports the integration of neuro-symbolic learning (NSL), which combines the strengths of neural networks and symbolic reasoning [19]. NSL enhances the transparency of the decision-making process by formalizing the inference steps, making it easier to audit and correct potential biases.

Moreover, MML's modular architecture facilitates the development of more robust and adaptable models. Each module can be updated or replaced independently, allowing the system to evolve in response to new data or changing regulatory requirements. This flexibility is particularly valuable in healthcare, where the rapid pace of medical discoveries necessitates continuous model refinement. By enabling the detection and correction of biases at the module level, MML not only improves the overall fairness of AI systems but also enhances their reliability and trustworthiness, making them more suitable for high-stakes applications such as personalized medicine and clinical decision support.

### 4.1.3 Fairness Module with Centroid Fairness Loss
The Fairness Module with Centroid Fairness Loss is a novel approach designed to address the challenge of reducing bias in pre-trained models without compromising their performance [20]. This module introduces a new loss function, Centroid Fairness, which aims to align the performance metrics of different subgroups within the population. The key idea is to ensure that the model's predictions for each subgroup are consistent with those of a reference group, thereby mitigating the biases that can arise from demographic or other sensitive attributes. By using centroid-based scores, the method simplifies the fairness objective, making it computationally efficient and compatible with modern fairness loss functions [20].

The Centroid Fairness Loss function operates by calculating the centroid of the feature representations for each subgroup and then aligning these centroids with the centroid of the reference group. This alignment is achieved through a distance metric that measures the discrepancy between the subgroup centroids and the reference group centroid. The loss function is designed to minimize this discrepancy, ensuring that the model's behavior is consistent across different subgroups. This approach is particularly advantageous in scenarios where retraining a large model from scratch is computationally expensive or impractical. The Fairness Module can be trained rapidly and integrated into existing models, providing a practical solution for bias mitigation [20].

Extensive experimental evaluations have demonstrated the effectiveness of the Fairness Module with Centroid Fairness Loss. In various benchmark datasets, the module has been shown to significantly reduce bias while maintaining or even improving the overall performance of the pre-trained models. This is a notable achievement, as it challenges the traditional trade-off between performance and fairness. The module's ability to align intra-group performance curves with those of the reference group ensures that the model's predictions are fair and equitable, making it a valuable tool for applications in healthcare, finance, and other critical domains where fairness is paramount.

## 4.2 Statistical and Theoretical Frameworks

### 4.2.1 Counterfactual Invariance for Medical Imaging
Counterfactual invariance in medical imaging aims to ensure that machine learning models' predictions remain consistent regardless of changes in sensitive attributes such as race, sex, or age. This concept is crucial because biases in medical image classification can lead to disparate outcomes, particularly in high-stakes applications like diagnosing diseases from chest X-rays or CT scans [21]. By achieving counterfactual invariance, models can make fairer and more reliable predictions, thereby reducing the risk of systemic biases affecting patient care.

To achieve counterfactual invariance, researchers have developed methods that involve generating counterfactual images where sensitive attributes are altered while maintaining the disease-related features [22]. One such approach involves using latent diffusion models with disentangled representations. These models can generate high-fidelity images where, for example, the race of a patient is changed without altering the presence or severity of a disease. This allows for a rigorous assessment of whether the model's predictions are influenced by the sensitive attribute. Experimental results on both synthetic and real-world datasets have shown that these methods can effectively align with the principles of counterfactual invariance, outperforming traditional baseline techniques.

However, implementing counterfactual invariance in medical imaging presents several challenges [22]. First, the generation of realistic counterfactual images requires sophisticated models capable of disentangling and manipulating specific attributes without degrading the quality of the image. Second, the evaluation of counterfactual invariance must be carefully designed to account for the complex interactions between sensitive attributes and disease markers [22]. Finally, the practical application of these methods in clinical settings necessitates thorough validation to ensure that the models do not compromise diagnostic accuracy while achieving fairness. Despite these challenges, the pursuit of counterfactual invariance represents a promising direction for mitigating biases in medical imaging and improving the fairness and reliability of AI-driven healthcare systems.

### 4.2.2 Statistical Fairness in AI Ethics
Statistical fairness in AI ethics is a central approach to addressing the distribution of benefits and harms in algorithmic decision-making, particularly in high-stakes domains like healthcare [17]. This approach involves quantifying and mitigating biases that arise from training data and model predictions. Commonly used fairness metrics include demographic parity, equalized odds, and predictive parity, each with its own strengths and limitations [12]. Demographic parity ensures that the probability of a positive outcome is the same across different demographic groups, while equalized odds require that true positive and false positive rates are equal across groups. Predictive parity, on the other hand, focuses on ensuring that the positive predictive value is consistent across groups. These metrics are crucial for identifying and correcting biases that can lead to disparate treatment and outcomes.

Despite the advancements in statistical fairness, the application of these metrics in complex, real-world scenarios remains challenging [17]. For instance, in medical image classification, biases related to race, sex, and age have been observed in deep learning models trained on chest X-rays, CT scans, and dermatology images [21]. These biases can result in underdiagnosis and undertreatment, particularly for marginalized or underrepresented groups. Existing fairness-enhancing techniques, such as constraint-based optimization and adversarial debiasing, often address single sensitive attributes and may degrade predictive performance [1]. This highlights the need for more sophisticated methods that can handle multi-attribute biases and maintain model utility.

Moreover, the issue of fairness extends beyond technical solutions and requires a holistic approach that considers the social and ethical implications of AI systems [23]. Recent policy discussions, such as those outlined in the European AI Act, emphasize the importance of preventing AI from perpetuating or exacerbating existing societal inequities [24]. This necessitates systematic bias analysis and audit processes to ensure accountability and mitigate unintended harms. Additionally, the integration of fairness into the design and deployment of AI systems must be guided by a deeper understanding of the meaning of fairness and its application in specific contexts [23]. This intersection of technical, social, and ethical considerations is essential for developing AI systems that are both effective and equitable [23].

### 4.2.3 Adversarial Perturbation and Social Network Analysis
Adversarial perturbation techniques have emerged as a powerful tool for analyzing and mitigating biases in AI models, particularly in the context of social network analysis. These techniques involve introducing small, carefully crafted perturbations to input data to observe how these changes affect the model's predictions. In the realm of social network analysis, adversarial perturbations can help identify and quantify biases that arise from the structure and content of social networks. For instance, by perturbing the attributes or connections of nodes within a network, researchers can assess how these changes influence the model's outputs, such as community detection, influence propagation, or link prediction. This approach not only aids in understanding the robustness of AI models but also highlights potential sources of bias that may not be apparent through traditional methods.

The application of adversarial perturbations in social network analysis has revealed several key insights. First, these perturbations have shown that AI models can exhibit racial and gender biases, often due to the underlying data distributions and the way these models are trained. For example, perturbing the racial or gender attributes of nodes in a social network can lead to significant changes in the model's predictions, indicating that the model may be overly sensitive to these attributes. This sensitivity can result in unfair outcomes, such as biased recommendations or unequal access to resources. Second, adversarial perturbations have also demonstrated that social network structures can amplify existing biases. By manipulating the connections between nodes, researchers have observed that certain groups may be disproportionately affected, leading to further marginalization and inequality.

To address these issues, researchers have proposed various strategies to mitigate the impact of adversarial perturbations and reduce biases in social network analysis. One approach involves developing more robust training methods that explicitly account for potential biases in the data. For example, incorporating fairness constraints into the training process can help ensure that the model's predictions are more equitable across different groups. Additionally, using techniques such as adversarial debiasing, where the model is trained to be invariant to specific sensitive attributes, can further enhance fairness [1]. Another strategy is to enhance the diversity and representativeness of the training data, which can help reduce the risk of model bias. By combining these methods, researchers aim to create more reliable and fair AI models for social network analysis, ultimately contributing to a more equitable and inclusive digital society.

## 4.3 Synthetic Data and GANs

### 4.3.1 Differentially Private Synthetic Data for Fairness
Differentially private synthetic data generation is a promising approach to address fairness issues in medical image classification tasks, where biases related to race, sex, and age have been widely observed [7]. This method involves creating synthetic datasets that preserve the statistical properties of the original data while ensuring that individual records cannot be traced back to specific patients, thus maintaining privacy [8]. By synthesizing data that are representative of diverse demographic groups, differentially private synthetic data can help mitigate the biases that arise from underrepresentation or misrepresentation in the original datasets [8]. This is particularly important in medical imaging, where the quality and representativeness of the training data significantly influence the performance and fairness of deep learning models [21].

The generation of differentially private synthetic data typically involves two key steps: data synthesis and privacy protection [23]. Data synthesis techniques, such as generative adversarial networks (GANs) and variational autoencoders (VAEs), are used to create synthetic data that closely mimic the distribution of the original data. To ensure differential privacy, noise is added to the synthetic data generation process, making it infeasible to infer the presence or absence of any individual in the original dataset [23]. This noise addition is carefully calibrated to balance privacy and utility, ensuring that the synthetic data remain useful for training machine learning models while protecting individual privacy. The effectiveness of this approach in preserving fairness is evaluated through various metrics, such as demographic parity and equalized odds, which assess the model's performance across different demographic groups [25].

Empirical studies have demonstrated that differentially private synthetic data can effectively reduce biases in medical image classification tasks [8]. For instance, when applied to chest X-ray and CT scan datasets, these techniques have shown improvements in model fairness, particularly in reducing disparities in predictive accuracy and false positive rates across different racial and gender groups [21]. However, challenges remain, such as the potential degradation of model performance due to the added noise and the need for sophisticated methods to ensure that the synthetic data accurately reflect the complex interactions between multiple sensitive attributes. Future research should focus on developing advanced synthesis and privacy mechanisms that can better address these challenges while maintaining the fairness and utility of the synthetic data [23].

### 4.3.2 Fairness in Synthetic Healthcare Data Generation
Fairness in synthetic healthcare data generation is a critical aspect of ensuring that AI-driven healthcare solutions do not perpetuate or exacerbate existing biases [8]. The generation of synthetic data involves creating realistic yet artificial datasets that can be used for training and testing machine learning models, particularly in scenarios where real data is scarce or sensitive [8]. However, if the synthetic data generation process itself is biased, it can lead to unfair outcomes in downstream applications, such as clinical decision support systems, patient risk stratification, and personalized treatment recommendations. This section explores the challenges and recent advancements in achieving fairness in synthetic healthcare data generation [7].

One of the primary challenges in generating fair synthetic healthcare data is the inherent biases present in the original datasets [7]. These biases can stem from various sources, including historical inequalities in healthcare access, socioeconomic factors, and systemic discrimination. For instance, if a dataset disproportionately represents certain demographic groups, the synthetic data generated from it may also reflect these imbalances. To address this, researchers have proposed methods that incorporate fairness constraints during the data generation process. Techniques such as adversarial training, where a discriminator is used to detect and penalize biased representations, and fairness-aware generative adversarial networks (GANs) have shown promise in mitigating these biases [1]. These methods aim to ensure that the synthetic data not only mimics the statistical properties of the real data but also adheres to fairness criteria.

Another important consideration is the evaluation of fairness in synthetic data [7]. Traditional fairness metrics, such as demographic parity, equalized odds, and predictive parity, are often applied to assess the fairness of machine learning models [25]. However, these metrics need to be adapted to the context of synthetic data generation [8]. For example, one approach is to compare the fairness metrics of the synthetic data with those of the original data to ensure that the synthetic data does not introduce or amplify existing biases. Additionally, the impact of synthetic data on downstream tasks must be evaluated to ensure that the fairness improvements translate into fairer model outcomes. This involves conducting extensive empirical studies across multiple real-world datasets to validate the effectiveness of fairness-aware synthetic data generation techniques [23].

### 4.3.3 FairGrad for Gradient Reconciliation
In addressing the challenge of optimizing predictive performance while ensuring fairness across multiple population dimensions, we propose FairGrad, a gradient reconciliation framework designed to dynamically balance these conflicting objectives [1]. Traditional approaches often require manual specification of fairness weights, which can be cumbersome and prone to suboptimal settings [26]. FairGrad overcomes this limitation by projecting conflicting gradients from different objectives onto mutually orthogonal hyperplanes [1]. This projection ensures that each objective's gradient contributes to the overall optimization direction without directly interfering with others, thereby maintaining a balanced optimization trajectory. The dynamic adjustment of gradients allows FairGrad to adaptively prioritize objectives based on the current state of the model, ensuring that both performance and fairness are optimized simultaneously.

The theoretical foundation of FairGrad is rooted in the concept of multi-objective optimization, where the goal is to find a Pareto-optimal solution that maximizes predictive accuracy while minimizing unfairness across various demographic groups. By decomposing the optimization problem into orthogonal components, FairGrad can effectively manage the trade-offs between these objectives. This approach is particularly useful in scenarios where the data distribution is skewed or where certain groups are underrepresented, as it ensures that the model's performance is not disproportionately influenced by dominant groups. The framework's ability to dynamically adjust the optimization trajectory also makes it robust to changes in the data distribution, which is a common issue in real-world applications where data can be highly dynamic and heterogeneous.

Empirical evaluations of FairGrad on a variety of datasets, including those with significant demographic imbalances, demonstrate its effectiveness in achieving both high predictive performance and multi-attribute fairness [1]. The results show that FairGrad can significantly reduce the performance gap between different demographic groups without a substantial loss in overall accuracy. This is achieved through the framework's ability to dynamically allocate optimization resources to underperforming groups, thereby mitigating the impact of data biases. The proposed method also provides a principled way to incorporate fairness into the training process, making it a valuable tool for developers and practitioners aiming to build more equitable machine learning systems [25].

# 5 EHR Data Representation and Analysis

## 5.1 Advanced Machine Learning Techniques

### 5.1.1 Transformer-Based Survival Models
Transformer-based survival models represent a significant advancement in the field of survival analysis, particularly when applied to electronic health records (EHRs) and other complex medical datasets. These models leverage the attention mechanism inherent in Transformer architectures to capture intricate temporal dependencies and interactions among clinical events, which are crucial for accurate survival predictions. Unlike traditional survival models that often rely on hand-crafted features and linear assumptions, Transformer-based models can automatically learn hierarchical representations of patient data, thereby improving the model's ability to handle high-dimensional and heterogeneous EHR data.

One of the key advantages of Transformer-based survival models is their capacity to incorporate time-varying covariates and handle irregularly spaced observations, which are common in EHRs. By using self-attention mechanisms, these models can dynamically weigh the importance of different clinical events and time points, allowing for a more nuanced understanding of patient trajectories. This dynamic weighting is particularly beneficial in scenarios where the timing and sequence of medical interventions significantly influence patient outcomes. Additionally, the ability to process sequences of varying lengths without the need for padding or truncation makes Transformer-based models highly flexible and suitable for diverse clinical applications.

Moreover, Transformer-based survival models offer enhanced interpretability through attention maps, which highlight the most influential clinical events and time periods contributing to the predicted survival outcomes. This interpretability is crucial for clinical decision-making, as it allows healthcare providers to understand the rationale behind the model's predictions and identify critical factors affecting patient prognosis. Despite these advantages, challenges remain, such as the computational complexity associated with training large-scale Transformer models and the need for robust regularization techniques to prevent overfitting. Nevertheless, ongoing research continues to address these issues, paving the way for more widespread adoption of Transformer-based survival models in clinical practice.

### 5.1.2 Semiparametric Estimators for Model Misspecification
Semiparametric estimators offer a robust approach to addressing model misspecification, a common issue in healthcare data analysis where the underlying data-generating process is often complex and not fully understood [27]. These estimators combine the flexibility of nonparametric methods with the efficiency of parametric models, allowing for the estimation of parameters of interest while making fewer assumptions about the functional form of the data. By leveraging the strengths of both paradigms, semiparametric estimators can provide more reliable and valid inferences, even when the model is not perfectly specified [27]. This is particularly important in healthcare applications, where the inclusion of unobserved confounders and the presence of selection bias can severely impact the validity of the results.

One of the key advantages of semiparametric estimators is their ability to converge to the true parameter values at a rate that is faster than that of fully nonparametric methods, while still maintaining robustness to model misspecification [27]. This is achieved through the use of influence functions, which characterize the sensitivity of the estimator to small perturbations in the data. Influence functions play a crucial role in semiparametric theory by providing a way to derive estimators that are asymptotically normal and efficient. These properties are essential for conducting valid statistical inference, such as hypothesis testing and confidence interval construction, which are fundamental in evaluating the effectiveness of healthcare interventions.

Moreover, semiparametric estimators can be designed to be doubly robust, meaning that they remain consistent if either the outcome model or the treatment assignment model is correctly specified. This double robustness is particularly valuable in settings where the true data-generating process is uncertain, as it provides a safeguard against model misspecification. Additionally, semiparametric methods can incorporate machine learning techniques to flexibly estimate nuisance parameters, such as propensity scores or outcome regressions, without sacrificing the statistical properties of the final estimator. This integration of machine learning with semiparametric theory enhances the practical utility of these estimators in complex healthcare datasets, where the relationships between variables are often nonlinear and high-dimensional.

### 5.1.3 VRNN-Based Adversarial Domain Separation
In the realm of medical data analysis, the challenge of domain adaptation (DA) is exacerbated by the heterogeneity of electronic health records (EHRs) across different healthcare systems. To address this, we propose a Variational Recurrent Neural Network (VRNN)-based Adversarial Domain Separation (VR-ADS) framework. This framework is designed to disentangle global latent representations that capture common patient characteristics across all systems from domain-specific representations that capture system-specific nuances. By doing so, VR-ADS aims to mitigate the impact of systematic biases present in EHRs from different sources, thereby enhancing the generalizability and fairness of predictive models.

The VR-ADS framework leverages the recurrent structure of VRNNs to model temporal dependencies in EHR data, which is crucial for tasks such as patient outcome prediction and disease progression monitoring. The adversarial component of VR-ADS ensures that the global latent representation is invariant to domain-specific characteristics by training a discriminator to distinguish between representations from different domains. This adversarial training forces the model to learn a shared representation that is robust to domain variations, while the domain-specific representations capture the unique aspects of each EHR system [28]. This separation is particularly important in medical applications where the data can vary significantly due to differences in patient populations, clinical practices, and data collection methods.

We validate the effectiveness of VR-ADS on real-world EHR datasets from two distinct medical systems, CCHS and Mayo. The results demonstrate that VR-ADS outperforms a DA framework that only unifies latent representations (ADU) in terms of both predictive accuracy and fairness metrics [28]. Specifically, VR-ADS shows improved performance in detecting septic patients 48 hours before the onset of sepsis, a critical task in clinical settings [28]. The ability to combine training data from multiple EHR systems not only addresses the issue of insufficient labeled data but also enhances the model's robustness and generalizability, making it a valuable tool for advancing predictive healthcare.

## 5.2 Causal and Fairness Algorithms

### 5.2.1 Causal Fairness in Treatment Allocation
Causal fairness in treatment allocation is a critical aspect of ensuring equitable healthcare delivery, particularly when leveraging electronic health records (EHRs) for decision-making [12]. The primary challenge lies in addressing the biases that can arise from the observational nature of EHR data, which often lacks the randomization found in clinical trials [29]. These biases can lead to unfair treatment allocation, where certain patient subgroups are systematically disadvantaged [3]. To tackle this issue, recent research has focused on integrating causal inference methods into the analysis of EHRs. By explicitly modeling the causal relationships between patient characteristics, treatments, and outcomes, these methods aim to identify and mitigate sources of bias, thereby promoting fairer treatment allocation [12].

One prominent approach in this domain is the use of Structural Causal Models (SCMs) to represent the underlying latent structure of EHR data [26]. SCMs allow researchers to formalize the causal relationships between variables, enabling a more precise assessment of treatment effects. For instance, an SCM can help identify and adjust for confounding factors that might otherwise distort the estimated treatment effects. This approach is particularly useful in longitudinal EHR data, where the temporal dynamics of patient interactions with the healthcare system can introduce complex dependencies. By leveraging SCMs, researchers can develop algorithms that not only estimate the causal effects of treatments but also ensure that these estimates are fair across different patient populations.

To operationalize these causal fairness principles, a two-stage model called the Fair Longitudinal Medical Deconfounder (FLMD) has been proposed [26]. The first stage of FLMD involves learning a deconfounded representation of the patient data, where the influence of confounding variables is minimized. This is achieved through techniques such as inverse probability weighting (IPW) or multiple imputation (MI), which help to balance the distribution of covariates across treatment groups. The second stage then uses these deconfounded representations to train a predictive model that can make fair treatment allocation decisions. By separating the deconfounding and prediction stages, FLMD ensures that the model's predictions are both accurate and fair, addressing the common trade-off between accuracy and fairness that plagues many traditional fairness methods [26]. This approach has shown promising results in improving the fairness of treatment allocation while maintaining or even enhancing predictive performance.

### 5.2.2 Fair Longitudinal Medical Deconfounder
The Fair Longitudinal Medical Deconfounder (FLMD) is a novel approach designed to address the challenges of confounding bias and missing data in longitudinal Electronic Health Records (EHR) [26]. FLMD leverages a deep generative model to capture unobserved confounders for each patient encounter, thereby providing a more accurate representation of the underlying patient state. This is particularly important in EHR data, where the non-random assignment of treatments and the presence of missing-not-at-random (MNAR) data can significantly bias the results of observational studies [30]. By modeling these unobserved confounders, FLMD aims to improve the validity of causal inferences derived from EHR data, which is crucial for making reliable predictions and understanding the effectiveness of different treatments [26].

In the first stage of FLMD, a deep generative model is trained to infer the latent confounders that influence both the observed clinical features and the treatment assignments. This stage involves constructing a Structural Causal Model (SCM) to represent the underlying causal relationships in the EHR data. The SCM is used to simulate the effects of unobserved confounders, allowing the model to adjust for these confounders during the learning process. This adjustment is critical for ensuring that the learned representations are not biased by the confounding factors that are common in observational EHR data. The second stage of FLMD focuses on debiasing the learned representations to ensure fairness across different patient subpopulations. This is achieved through a customized loss function that incorporates fairness constraints, such as the weighted reconstruction loss, which balances the accuracy of the model with the need to minimize disparities in outcomes across different patient groups.

The FLMD model is evaluated on the MIMIC-III dataset, a widely used benchmark for EHR analysis, to predict patient mortality across various subpopulations. The results demonstrate that FLMD outperforms both traditional deep learning models and common debiasing techniques in terms of fairness and prediction accuracy. This improvement is attributed to the model's ability to effectively handle the complex interplay between observed and unobserved confounders, as well as its capacity to mitigate selection bias and missing data issues. By addressing these challenges, FLMD provides a robust framework for learning fair and accurate representations from longitudinal EHR data, which can be applied to a range of clinical applications, including patient stratification and treatment recommendation [26].

### 5.2.3 Fair Patient Model with Autoencoder Architecture
In the realm of electronic health records (EHR), ensuring that patient representation models are both accurate and fair is of paramount importance [29]. The proposed Fair Patient Model (FPM) with an autoencoder architecture is designed to address the inherent biases that can arise from demographic and clinical data [2]. The autoencoder architecture in FPM is specifically tailored to learn a compact and robust representation of patient data that minimizes the impact of sensitive attributes, such as gender or race, while preserving the essential clinical information necessary for accurate predictions. This is achieved through a two-stage training process that first encodes the patient data into a lower-dimensional latent space and then decodes it back to the original feature space, ensuring that the learned representations are both informative and unbiased.

The FPM leverages a novel loss function that incorporates fairness constraints, such as demographic parity or equalized odds, to guide the learning process. By optimizing this loss function, the model is encouraged to generate representations that are not only predictive of clinical outcomes but also fair across different demographic groups. This approach is particularly significant in the context of EHR data, where missing values and non-random data collection can exacerbate existing biases [26]. The autoencoder architecture in FPM is further enhanced with a hierarchical attention mechanism, which allows the model to focus on the most relevant clinical features while down-weighting those that may introduce bias. This attention mechanism helps to ensure that the learned representations are not only fair but also interpretable, providing clinicians with insights into the factors driving the model's predictions.

Empirical evaluations of the FPM have demonstrated its effectiveness in generating fair and accurate patient representations [2]. In various downstream tasks, such as predicting patient mortality or readmission rates, the FPM has consistently outperformed both traditional deep learning models and common debiasing methods in terms of fairness metrics [2]. The model's ability to generalize well across different patient populations and clinical settings underscores its potential for improving the fairness and reliability of AI-driven healthcare applications [5]. By addressing the complex interplay between clinical features and sensitive attributes, the FPM represents a significant step forward in the development of equitable and trustworthy AI systems in healthcare.

## 5.3 Data Integration and Sensitivity Analysis

### 5.3.1 Integration of EHR and RCT Data
The integration of Electronic Health Records (EHR) and Randomized Controlled Trial (RCT) data represents a significant advancement in the field of clinical research, offering a unique opportunity to enhance the external validity of RCT findings [27]. EHR data, rich in longitudinal patient information, complements the controlled and standardized nature of RCT data, thereby providing a more comprehensive understanding of treatment effects in real-world settings. This integration allows researchers to leverage the extensive and diverse patient populations captured in EHRs, which can help in identifying subgroups that may benefit or be harmed by specific interventions, thus supporting the development of personalized medicine.

One of the primary challenges in integrating EHR and RCT data is the heterogeneity in data quality and structure. EHR data, while abundant, often suffer from issues such as missing values, inconsistent coding practices, and varying levels of detail across different healthcare providers [29]. These discrepancies can introduce biases and affect the reliability of the integrated analysis. To address these challenges, advanced data harmonization techniques and machine learning algorithms have been developed to preprocess and align EHR data with the structured and well-defined data from RCTs. For instance, natural language processing (NLP) techniques can be employed to extract and standardize clinical information from unstructured EHR notes, ensuring that the data are comparable and usable in conjunction with RCT data [31].

Another critical aspect of integrating EHR and RCT data is the need to account for the different data-generating processes inherent to each type of data. EHR data are typically observational and collected for clinical care, whereas RCT data are generated through carefully designed experimental protocols. This difference necessitates the development of sophisticated statistical methods that can adjust for confounding factors and selection biases, ensuring that the integrated analysis accurately reflects the true treatment effects. Techniques such as propensity score matching, inverse probability weighting, and causal inference methods have been adapted to handle these complexities, enabling researchers to draw robust conclusions from the combined data sources. The integration of EHR and RCT data not only enhances the generalizability of clinical trial findings but also supports the development of more nuanced and patient-centered treatment strategies.

### 5.3.2 Sensitivity Analysis for Non-Ignorable Missing Values
Sensitivity analysis for non-ignorable missing values (MNAR) is crucial in ensuring the robustness and reliability of statistical models, particularly in the medical domain where missing data are common and often not at random [30]. Unlike missing at random (MAR) or missing completely at random (MCAR) scenarios, MNAR implies that the probability of a value being missing depends on the unobserved data itself, complicating the analysis and interpretation of results [30]. Traditional methods for handling missing data, such as multiple imputation and maximum likelihood estimation, often assume MAR or MCAR, which can lead to biased estimates and incorrect inferences when these assumptions are violated.

To address the challenges posed by MNAR, sensitivity analysis frameworks have been developed to explore the impact of different missing data mechanisms on the analysis results [30]. These frameworks introduce sensitivity parameters that allow researchers to quantify the degree of departure from the MAR assumption. By varying these parameters, researchers can assess how sensitive their conclusions are to different assumptions about the missing data mechanism. This approach provides a more comprehensive understanding of the potential biases and uncertainties associated with the analysis, thereby enhancing the interpretability and validity of the results.

One key aspect of sensitivity analysis for MNAR is the ability to incorporate prior knowledge and expert judgment into the modeling process. This is particularly important in medical research, where domain experts can provide valuable insights into the reasons for missing data and the likely patterns of non-response. By integrating this knowledge, sensitivity analysis can be tailored to specific contexts and populations, leading to more accurate and relevant conclusions. Additionally, the use of simulation studies and synthetic data can help validate the performance of sensitivity analysis methods under various scenarios, ensuring that they are robust and reliable in real-world applications.

### 5.3.3 Patient2Vec with Hierarchical Attention
In the realm of patient representation learning, the Patient2Vec model with Hierarchical Attention (Patient2Vec-HA) stands out for its ability to capture nuanced and contextually relevant information from electronic health records (EHRs) [31]. Unlike traditional models that treat clinical events as independent entities, Patient2Vec-HA employs a hierarchical attention mechanism to weigh the importance of different clinical events at multiple levels. This hierarchical structure allows the model to focus on critical events and their temporal sequences, thereby enhancing the interpretability and accuracy of patient representations.

At the core of Patient2Vec-HA is the hierarchical attention mechanism, which operates at two levels: event-level and visit-level. At the event level, the model assigns attention scores to individual clinical events within a single visit, emphasizing those that are most relevant to the patient's health status. These scores are computed based on the event's features and its context within the visit. At the visit level, the model aggregates the weighted events to form a visit representation, which is then used to compute attention scores for each visit. This dual-level attention ensures that both the immediate and long-term clinical context are captured, providing a comprehensive view of the patient's medical history.

The effectiveness of Patient2Vec-HA in learning patient representations is further enhanced by its ability to handle the variability and sparsity of EHR data [31]. By dynamically adjusting the attention weights, the model can adapt to the unique patterns and irregularities present in each patient's record. This adaptability is crucial for tasks such as disease prediction and patient stratification, where the ability to discern subtle but significant patterns can significantly impact the outcome. The hierarchical attention mechanism not only improves the model's performance but also facilitates the interpretation of the learned representations, making it a valuable tool for both clinical decision-making and research.

# 6 Future Directions


The current landscape of research on bias in EHR-based AI models, while rich and multifaceted, still faces several limitations and gaps. One of the primary challenges is the lack of standardized evaluation frameworks for assessing the fairness and robustness of AI models across different healthcare settings and populations. Existing methods often focus on specific types of bias, such as demographic parity or equalized odds, but fail to capture the complex and intersecting nature of biases that can arise in real-world applications. Additionally, there is a need for more comprehensive datasets that are representative of diverse patient populations, as many current datasets are limited in scope and may not adequately reflect the full spectrum of healthcare experiences. Another gap is the limited integration of stakeholder perspectives, particularly from underrepresented groups, in the development and evaluation of AI models. This oversight can lead to the perpetuation of existing biases and the exclusion of critical insights that could improve model fairness and transparency.

To address these limitations, future research should focus on developing and validating comprehensive evaluation frameworks that can systematically assess the fairness and robustness of AI models across multiple dimensions. These frameworks should incorporate a wide range of fairness metrics, including intersectional fairness, to ensure that models perform equitably across different demographic and clinical subgroups. Additionally, there is a need for the creation and curation of large, diverse, and publicly available datasets that can serve as benchmarks for evaluating AI models. These datasets should be designed to reflect the complexity and heterogeneity of real-world healthcare settings, including the inclusion of data from underrepresented and marginalized populations. Furthermore, future research should prioritize the involvement of diverse stakeholders, including patients, healthcare providers, and community organizations, in the design, development, and evaluation of AI models. This participatory approach can help ensure that the models are not only technically sound but also ethically and socially responsible.

Another promising direction for future research is the exploration of advanced techniques for bias mitigation and fairness enhancement. For instance, the development of modular and explainable AI models, such as those based on modular machine learning (MML) and neuro-symbolic learning, can provide greater transparency and control over the decision-making process. These models can be designed to isolate and address specific sources of bias, such as those related to sensitive attributes like race, gender, and age. Additionally, the integration of causal inference methods into AI models can help identify and correct for confounding factors that may introduce bias. Techniques such as counterfactual invariance and causal fairness can be particularly valuable in ensuring that AI models make fair and accurate predictions across different patient populations. Furthermore, the use of synthetic data and generative adversarial networks (GANs) can help address issues of data scarcity and imbalance, providing a more robust and representative training environment for AI models.

The potential impact of the proposed future work is significant. By developing and validating comprehensive evaluation frameworks, researchers and practitioners can gain a more nuanced understanding of the biases and limitations of AI models in healthcare. This, in turn, can inform the development of more equitable and trustworthy AI systems that are better equipped to serve diverse patient populations. The creation of large, diverse, and publicly available datasets can foster innovation and collaboration, accelerating the advancement of AI in healthcare. The involvement of diverse stakeholders can ensure that AI models are aligned with the needs and values of the communities they serve, promoting greater acceptance and adoption. Finally, the exploration of advanced techniques for bias mitigation and fairness enhancement can lead to the development of more accurate, transparent, and fair AI models, ultimately improving healthcare outcomes and reducing health disparities.

# 7 Conclusion



This survey paper has provided a comprehensive overview of the current state of research on bias in Electronic Health Record (EHR)-based Artificial Intelligence (AI) models. The paper has explored various aspects of bias, including its sources, the methodologies for detecting and mitigating it, and the practical challenges in implementing these solutions. Key findings include the importance of comprehensive evaluation frameworks, collaborative and stakeholder approaches, systematic and quantitative methods, and fairness and bias mitigation techniques. Advanced methodologies such as subgroup-specific and modular approaches, statistical and theoretical frameworks, and the use of synthetic data and GANs have been highlighted as promising solutions for addressing the multifaceted nature of bias in AI models. The paper also emphasizes the critical role of interdisciplinary collaboration and the need for robust, systematic approaches to ensure that AI systems in healthcare are fair, transparent, and trustworthy.

The significance of this survey lies in its contribution to the growing body of knowledge on AI fairness in healthcare. By synthesizing insights from various disciplines and methodologies, this paper provides a valuable resource for researchers, practitioners, and policymakers. It highlights the importance of addressing bias in AI models to ensure that these systems do not perpetuate or exacerbate existing inequalities. The paper underscores the need for a holistic approach that integrates technical advancements with ethical considerations, stakeholder engagement, and regulatory frameworks. This comprehensive perspective is essential for building AI systems that are not only accurate and efficient but also equitable and trustworthy, thereby enhancing the overall quality and reliability of healthcare outcomes.

In conclusion, the challenges and opportunities in the field of bias mitigation in EHR-based AI models are significant. Future research should focus on developing more adaptive and context-aware methods that can continuously monitor and adjust for bias. Interdisciplinary collaboration, particularly between computer scientists, healthcare professionals, and social scientists, is crucial for addressing the complex and intersecting biases that can arise in AI models. Policymakers and regulatory bodies also have a vital role to play in ensuring that AI systems are developed and deployed responsibly. We call on the research community to continue advancing this field, with a focus on practical applications and real-world impact. By working together, we can build AI systems that truly serve the needs of all patients, promoting fairness, transparency, and trust in healthcare.

# References
[1] Balancing Fairness and Performance in Healthcare AI  A Gradient  Reconciliation Approach  
[2] Fair Patient Model  Mitigating Bias in the Patient Representation  Learned from the Electronic Healt  
[3] Fairness at Every Intersection  Uncovering and Mitigating Intersectional  Biases in Multimodal Clini  
[4] Enhancements for Developing a Comprehensive AI Fairness Assessment  Standard  
[5] Echoes of Biases  How Stigmatizing Language Affects AI Performance  
[6] AI Safety Should Prioritize the Future of Work  
[7] Bt-GAN  Generating Fair Synthetic Healthdata via Bias-transforming  Generative Adversarial Networks  
[8] A Case Study Exploring the Current Landscape of Synthetic Medical Record  Generation with Commercial  
[9] Integrating LLMs for Grading and Appeal Resolution in Computer Science  Education  
[10] Enhancing Security and Strengthening Defenses in Automated Short-Answer  Grading Systems  
[11] Towards responsible AI for education  Hybrid human-AI to confront the  Elephant in the room  
[12] Causal Fairness Assessment of Treatment Allocation with Electronic  Health Records  
[13] Exponential Shift  Humans Adapt to AI Economies  
[14] Enhancing Trust Through Standards  A Comparative Risk-Impact Framework  for Aligning ISO AI Standard  
[15] Understanding and Mitigating Risks of Generative AI in Financial  Services  
[16] Position  Bayesian Statistics Facilitates Stakeholder Participation in  Evaluation of Generative AI  
[17] Four Bottomless Errors and the Collapse of Statistical Fairness  
[18] FairPlay  A Collaborative Approach to Mitigate Bias in Datasets for  Improved AI Fairness  
[19] Modular Machine Learning  An Indispensable Path towards New-Generation  Large Language Models  
[20] Mitigating Bias in Facial Recognition Systems  Centroid Fairness Loss  Optimization  
[21] Evaluating and Mitigating Bias in AI-Based Medical Text Generation  
[22] AI Alignment in Medical Imaging  Unveiling Hidden Biases Through  Counterfactual Analysis  
[23] Quantitative Auditing of AI Fairness with Differentially Private  Synthetic Data  
[24] Fairness is in the details   Face Dataset Auditing  
[25] Discrimination-free Insurance Pricing with Privatized Sensitive  Attributes  
[26] A Counterfactual Fair Model for Longitudinal Electronic Health Records  via Deconfounder  
[27] Robust Causal Inference for EHR-based Studies of Point Exposures with  Missingness in Eligibility Cr  
[28] An Adversarial Domain Separation Framework for Septic Shock Early  Prediction Across EHR Systems  
[29] Combining Real-World and Randomized Control Trial Data Using  Data-Adaptive Weighting via the On-Tri  
[30] Sensitivity analysis for nonignorable missing values in blended analysis  framework  a study on the  
[31] Patient2Vec  A Personalized Interpretable Deep Representation of the  Longitudinal Electronic Health  