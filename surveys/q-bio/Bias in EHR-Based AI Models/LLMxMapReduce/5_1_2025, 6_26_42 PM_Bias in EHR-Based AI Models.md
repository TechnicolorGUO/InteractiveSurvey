# 5/1/2025, 6:26:42 PM_Bias in EHR-Based AI Models  

# 0. Bias in EHR-Based AI Models  

# 1. Introduction  

Artificial intelligence (AI) is increasingly being integrated into healthcare, leveraging vast amounts of electronic health record (EHR) data to develop models for tasks such as disease diagnosis, risk prediction, treatment planning, and patient management [7,14,15,18,20,28,30]. This application holds significant potential to enhance the speed, quality, and accessibility of medical care, potentially improving diagnostic accuracy, predicting patient outcomes, and increasing efficiency within healthcare systems [14,15,21,30]. The promise of medical AI includes the potential for greater objectivity and reproducibility, aiming to reduce provider biases and clinical inequities towards optimized and personalized care for all patients [20].​  

However, the development and deployment of AI models in healthcare are intrinsically linked to the data they are trained on, with EHRs serving as a primary source. This reliance introduces a significant risk: AI models can inherit and even amplify biases present in the training data, which often reflect historical and systemic inequalities in healthcare access and delivery [12,16,19,20]. Bias in medical AI can manifest as differential performance across various patient subgroups defined by characteristics such as race, ethnicity, gender, age, or geographic location [5,6,10,22,23,24,27]. This can lead to diagnostic inaccuracies, inequitable risk predictions, and ultimately exacerbate existing health disparities, particularly for historically marginalized or underserved populations [4,18,19,20].​  

Ensuring equitable and effective healthcare necessitates a proactive approach to identifying, understanding, and mitigating bias in AI systems [3,18,19,20]. The rise of ethical concerns surrounding AI in healthcare is prompting increased attention from researchers, policymakers, and regulatory bodies [1,2,13,17,18,20,32]. There is a growing recognition that failing to address bias risks the selective deployment of AI technologies, potentially benefiting certain populations while leaving others behind and worsening existing inequalities [3,4,18]. The ethical imperative is to balance the potential benefits of AI innovation with the responsibility to ensure fairness and equity across all patient groups [2,3,18]. This requires moving beyond traditional performance metrics to prioritize actual patient outcomes and equitable performance across subgroups, even when this conflicts with overall metrics [2,3].​  

Despite increasing awareness, the literature on bias detection and mitigation methods for medical AI algorithms remains nascent, lacking standardized reporting guidelines [20]. Understanding how biases occur and compound throughout the AI development pipeline is crucial for ensuring the equitable realization of medical AI's benefits [20].  

Given this critical context, the purpose of this survey is to provide a comprehensive examination of bias in AI models developed using EHR data. We will analyze the primary sources of bias inherent in EHR data and the AI development lifecycle, discuss the diverse impacts of such biases on model performance, clinical decision-making, and health equity, and review existing and proposed strategies for bias detection, mitigation, and reporting.​  

# 2. Types and Sources of Bias in EHR-Based AI Models  

The development and deployment of Artificial Intelligence (AI) models leveraging Electronic Health Records (EHRs) offer transformative potential for healthcare, yet they are profoundly challenged by the pervasive issue of bias. Bias in this context refers to systematic and unfair differences in predictions or outcomes that disadvantage certain patient groups relative to others [7,20]. Understanding the origins and manifestations of these biases is crucial for building equitable and trustworthy AI systems [16,26].  

![](images/8f1cad9e6daa3216abb9a9b03e7ca6512205eb3642c9dd952e778b96ca61a4e8.jpg)  

Bias is not monolithic; it arises from multiple sources and stages throughout the AI lifecycle, from data collection to model deployment and interaction [7,18,20]. These sources can be broadly categorized into Data Bias, Algorithmic Bias, Implementation and User Bias, and Societal Bias [14,15,16,20,26].  

Data Bias represents a foundational challenge, operating under the principle of "bias in, bias out" [7]. AI models learn from the data they are trained on, and if this data is flawed or unrepresentative, the models will inevitably perpetuate and potentially amplify existing inequalities and prejudices [12,26]. Key types of data bias include representation bias, selection and sampling biases, measurement bias, and label bias [7,22,26]. Representation bias, for instance, occurs when training datasets do not adequately reflect the diversity of the target patient population, particularly concerning race, ethnicity, age, or geographic location [5,6,7,11,22,23]. This leads to poor generalization and differential performance for underrepresented groups [33]. Measurement bias arises from inconsistencies in data collection, such as the racial bias observed in pulse oximetry or variations in demographic data recording [10,19]. A significant obstacle is the fragmentation of health data and privacy constraints, hindering the creation of comprehensive, diverse datasets needed for equitable training [14,15]. Furthermore, historical biases embedded within clinical records, reflecting past disparities in care quality and access, are directly transmitted to AI models trained on this data [18,26].​  

Algorithmic Bias originates from the choices made during the AI model development process itself [7,16]. This includes the algorithm's design, feature selection, optimization objectives, and training procedures [7]. Specific model architectures may inadvertently favor certain groups, while powerful models can overfit on spurious correlations present in biased data, particularly affecting underrepresented populations [3,20]. Feature selection bias can occur if features do not capture the problem accurately or if their relationship with outcomes varies across subgroups, such as using healthcare costs as a proxy for race [7,19]. Aggregation bias results from training a single model on pooled data from diverse groups without accounting for heterogeneity, often optimizing for the majority and creating a "fairness gap" [5,7,22,26]. Optimization for overall accuracy can lead models to rely on "demographic shortcuts," like using race or gender in medical image analysis, resulting in skewed predictions across groups [5,24]. Rigorous evaluation using appropriate fairness metrics is critical to identify these biases, but evaluation bias from unrepresentative test sets can mask actual disparities [26,27].​  

Implementation and User Bias manifest during the deployment and clinical application of AI models [7,12,20]. Deployment context bias arises when models are used in settings or populations different from their training data, leading to degraded, potentially biased performance [18,20,22]. Unequal access to AI tools or their misuse, such as biased treatment denial by insurers, directly impacts patient care [14]. Human cognitive biases significantly influence how users interact with AI. Automation bias leads healthcare professionals to over-rely on AI recommendations, while the "Eliza effect" causes unwarranted trust [7,12,29]. Conversely, dismissal bias (alarm fatigue) can lead to ignoring critical AI alerts [7]. Critically, real-world feedback loops can reinforce biases; if biased AI outputs are trusted and acted upon by clinicians, these actions can generate new data that solidifies the original biases in subsequent model training or refinement [7,18].​  

Societal Bias forms the bedrock upon which data, algorithmic, and implementation biases often build. Pre-existing societa inequalities related to race, ethnicity, gender, age, socioeconomic status, and geography are deeply embedded within healthcare systems, leading to disparities in access, quality of care, and health outcomes [17,18,21]. These systemic inequalities, including those stemming from historical discrimination and structural racism, directly influence data collection processes, clinical practices, and resource allocation, thereby embedding bias into the EHR data itself [7,10,19,27]. AI models trained on this data inevitably mirror and can even exaggerate these human and systemic biases [5,12,22,23,29]. Factors like the underrepresentation of certain racial groups in datasets, the use of racial corrections in clinical algorithms, gender stereotypes, ageism, and the impact of social determinants of health all reflect societal biases captured by AI [5,7,11,18,19,23,26]. Furthermore, intersectional bias highlights the compounding disadvantage faced by individuals belonging to multiple marginalized groups [9]. The lack of diversity among AI developers and within the healthcare workforce also contributes to perpetuating existing societal biases [4,7].​  

These various types of bias are not isolated but interact and compound across the AI pipeline, creating complex challenges [18,20]. Societal biases shape healthcare data (Data Bias), which influences algorithmic design and performance (Algorithmic Bias). Biased algorithms, in turn, are deployed and used by humans within existing healthcare workflows (Implementation and User Bias), potentially reinforcing initial societal inequities through feedback loops. Addressing bias in EHR-based AI requires a comprehensive approach that tackles these interconnected sources at every stage, from the foundational data to the real-world deployment and the underlying societal context.​  

# 2.1 Data Bias  

Data bias represents a fundamental challenge in the development and deployment of equitable AI models within healthcare, particularly those leveraging Electronic Health Records (EHRs). AI models frequently assimilate and reproduce biases inherent in their training data, subsequently reflecting and intensifying societal inequalities [17]. Unrepresentative or historically biased data serves as a primary source of bias in AI systems, capable of perpetuating and amplifying existing inequalities and prejudices [12].​  

<html><body><table><tr><td>Type of Data Bias</td><td>Description</td><td>Example/Manifestation in EHR-AI</td></tr><tr><td>Representation Bias</td><td>Training data doesn't reflect target population diversity.</td><td>Datasets predominantly from White patients for imaging; exclusion of multiracial groups.</td></tr><tr><td>Selection/Sampling Bias</td><td>Non-random sampling or exclusion of subgroups.</td><td>Geographic imbalance in datasets; excluding certain racial groups due to small sample size.</td></tr><tr><td>Measurement Bias</td><td>Inconsistencies/inaccuracies in data collection or processing.</td><td>Racial bias in pulse oximeter readings; inconsistent demographic data recording; data variability (equipment, protocols).</td></tr><tr><td>Label Bias</td><td>Inconsistent or inherently biased data labeling.</td><td>(Not explicitly detailed with EHR examples in text, but implied in human processes).</td></tr><tr><td>Imbalance Bias</td><td>Unequal sample sizes across groups or outcomes.</td><td>Underrepresentation of older adults or minority ethnic groups.</td></tr><tr><td>Historical Bias</td><td>Data reflects past/current systemic inequalities in care access/quality.</td><td>Data reflecting disparities in treatment/access for marginalized groups captured over time in EHRs.</td></tr><tr><td>Missing Data Bias</td><td>Absence of crucial data points (e.g., SDoH) or non- random missingness.</td><td>Lack of SDoH data; capturable but missing data.</td></tr></table></body></html>  

Several types of data bias contribute uniquely to unfair outcomes in EHR-based AI. Representation bias occurs when the training datasets inadequately represent all relevant groups or populations, hindering model generalization to different environments or patient demographics [7,22]. Selection and sampling biases similarly lead to unrepresentative data, arising from non-random sampling methods or the exclusion of specific subgroups [7,26]. Measurement bias stems from inconsistencies or inaccuracies in data collection and processing methods, affecting the true expression of the data being captured [7,22]. Label bias arises when data labeling is inconsistent or inherently biased [26]. Additionally, biases can manifest as imbalanced sample sizes, the presence of capturable but missing data, or the absence of crucial data points such as social determinants of health [20].​  

Examples from existing research vividly illustrate how these biases lead to poor generalization and differential performance for underrepresented groups. Representation and sampling biases are evident in the geographic distribution of patient data used for training diagnostic algorithms, where a significant majority of U.S. studies relied solely on data from a limited number of states, raising concerns about generalizability to other regions [6]. Race and ethnicity are particularly susceptible to representation bias; for instance, studies on colorectal cancer recurrence prediction have excluded multiracial or other subgroups due to small sample sizes [27]. Furthermore, the consistency of race/ethnicity identification in EHR data varies across different groups, with lower consistency observed for American Indian and Alaska Native populations, suggesting skewed representation [10]. Lack of diversity in genomics research contributes to disparities, as seen in differential biomarker expression for colorectal cancer in people with African ancestry or the misclassification of genetic test results for hypertrophic cardiomyopathy in patients with African or unspecified ancestry [19,25]. In medical imaging, AI models trained predominantly on datasets from specific demographics, such as predominantly White patients in pathology atlases or lightskinned individuals in dermatology datasets, exhibit reduced accuracy and skewed performance when applied to underrepresented groups [11,12,19]. AI age estimation models have shown decreased accuracy for older adults, likely due to their underrepresentation in training datasets [23]. Such skewed patient populations lead directly to differential performance, where AI models are less reliable or accurate for minority ethnic groups or medically underserved populations [4,18]. Even in medical imaging analysis, AI models can learn to use demographic attributes like race and gender as spurious correlations or computational shortcuts, leading to skewed diagnostic evaluations across different groups [5,24].  

Measurement bias also significantly impacts outcomes. A notable example is the racial bias in finger-clip pulse oximeter readings, where devices calibrated on light skin give falsely high readings for people with darker skin, delaying necessary oxygen treatment for Black patients with COVID [19]. Inconsistent data collection methods within healthcare systems can lead to inaccuracies in demographic identification [10]. Furthermore, data variability stemming from differences in equipment, protocols, or changes in patient populations over time and location constitutes a form of measurement bias [21]. These systematic discrepancies in care, data processing, and operational protocols contribute to "bias exhaust" – residual biases not directly tied to biological factors [3].​  

A significant challenge in mitigating data bias is the fragmentation of health data and the stringent privacy regulations governing its use [14,15]. This fragmentation and the associated restrictions impede the creation of large, comprehensive datasets that accurately reflect the full diversity of patient populations, which is crucial for training equitable AI tools [14,15,33]. Fragmented and non-representative data directly contribute to the development of skewed AI models [15].​  

Furthermore, historical biases embedded within clinical records perpetuate existing inequalities [18,26]. AI models trained on historical data reflect past prejudices, including disparities in treatment quality and access to care experienced by marginalized groups [18]. By learning from data generated within historically inequitable systems, AI risks replicating and amplifying these injustices [12]. The training data, being an abstraction of complex realities including sociocultural factors and healthcare delivery pathways, carries forward these historical disparities through processes like selection and imputation [3].​  

# 2.2 Algorithmic Bias  

Algorithmic bias represents a critical concern in the development and deployment of AI models within healthcare, particularly when utilizing Electronic Health Records (EHR) data. This form of bias arises not solely from the data itself, but from the specific choices made during the model development lifecycle, including the algorithm's conceptual design, the selection of features, the optimization strategies employed, and the training and validation processes [7,16]. Some model architectures may inherently favor certain patterns or demographic groups based on their design [18]. High-powered architectures, such as deep neural networks, can be particularly susceptible to overfitting on spurious correlations present in the training data, leading to reliance on superficial features that boost apparent performance but fail to generalize accurately, especially for underrepresented patient populations [3,20]. Furthermore, the use of pre-trained models or weights initialized on datasets that do not adequately represent the target clinical population can introduce or amplify existing biases [8].​  

The selection and representation of features play a significant role in perpetuating algorithmic bias [7]. Feature selection bias can occur when chosen features do not fully represent the underlying medical problem or when feature–outcome correlations vary inconsistently across different patient subgroups [7]. A notable example involves the use of proxy variables; an AI tool trained on healthcare costs, excluding explicit race data, inadvertently used cost as a proxy for race because the healthcare system historically spends more on certain demographic groups, leading to biased outcomes for others [19].​  

Aggregation bias is another prominent type of algorithmic bias, manifesting when data from diverse groups are improperly merged or when a "one-size-fits-all" model is developed without accounting for the heterogeneity within the data [7,22,26]. This approach often results in optimizing model performance primarily for the majority group, leading to a significant "fairness gap" and potentially worse performance for minority or marginalized populations [5,20,22].​  

Optimization techniques and the training process can also inadvertently perpetuate or exacerbate biases. When models are primarily optimized for overall accuracy (e.g., using Empirical Risk Minimization (ERM)), they may incorporate "demographic shortcuts" or spurious correlations to achieve high performance metrics, masking disparities in performance across subgroups [5]. Studies evaluating algorithms designed to enhance fairness during training, such as reweighting samples based on group or using adversarial methods, have demonstrated that models can still exhibit biased performance, evidenced by disparities in false negative rates across demographic attributes [24]. A stronger encoding of demographic information within the model's learned representations has been shown to correlate significantly with increased model unfairness [24].  

The manifestation of algorithmic bias necessitates rigorous evaluation. Algorithmic bias can lead to differential model performance across groups, such as facial recognition systems showing lower accuracy for individuals of color or melanoma detection models trained on predominantly White skin images missing diagnoses in Black patients [4]. AI algorithms can also learn and amplify human cognitive biases present in the training data, sometimes exaggerating them, as seen in age estimation models [23]. Evaluating algorithmic fairness requires careful consideration and selection of appropriate metrics [27]. Various statistical criteria exist for assessing fairness across groups, including equal calibration, equal discriminative ability (AUC), equal false-positive rates (FPRs) and false-negative rates (FNRs), and equal positive predictive values (PPVs) and negative predictive values (NPVs) [27]. Model evaluation serves as a crucial checkpoint for developers to identify biases introduced during training [20]. However, evaluation bias, arising from testing models on unrepresentative datasets, can lead to an inflated sense of confidence in the model's accuracy and fairness [26]. Continuous monitoring and evaluation are therefore essential procedures to maintain algorithmic fairness and ensure models remain accurate, safe, and effective over time [15,17]. Addressing algorithmic bias also involves exploring how algorithms themselves might be used to mitigate data biases and the importance of transparency in understanding bias manifestation [32].  

# 2.3 Implementation and User Bias  

Bias in AI models is not confined to data preparation or model development stages but can also be introduced or amplified during their deployment and subsequent use in clinical settings [7,12,20]. The real-world integration of AI models involves complex workflows and human-AI interaction, presenting multiple opportunities for bias manifestation [20].  

One critical aspect is deployment context bias, which occurs when AI systems are applied in clinical environments or on patient populations that differ significantly from the data used for their training and validation [18,20,22]. This mismatch can lead to degraded model performance, potentially affecting different patient groups disproportionately and thereby introducing bias [20]. Evaluating real-world outcomes and identifying performance disparities across relevant subgroups, especially considering intersectionality, is crucial but challenging [3].  

Bias can also arise from the workflow integration and how end-users interact with the AI system [20]. Disparities in healthcare providers' familiarity and training with AI tools can influence their implementation and interpretation, potentially leading to biased outcomes [18]. Effective integration requires early user involvement in the development process and sufficient training to ensure correct usage and foster acceptance [21]. Beyond integration challenges, unequal access to AI tools or their misuse, such as insurers unfairly denying care based on algorithmic outputs, can directly translate into disparities in patient care [14].  

Furthermore, human biases significantly influence the interpretation and use of AI predictions. A notable form is automation bias, where healthcare professionals over-rely on AI recommendations, potentially leading to incorrect judgments [7,29]. This over-reliance can stem from the perception that AI can analyze more data and identify complex correlations beyond human capacity [29]. The "Eliza effect" describes users' tendency to attribute unwarranted intelligence to AI, fostering overtrust and misunderstanding of limitations [12]. Conversely, dismissal bias, or alarm fatigue, occurs when users ignore or underestimate AI-generated alerts due to a history of false alarms, potentially harming patients [7]. These human cognitive tendencies underscore the importance of maintaining human oversight; clinicians must remain in control and critically evaluate AI outputs rather than blindly following them [14,19]. Vigilance includes questioning developers about the patient populations used for model training [19].​  

Finally, real-world feedback loops can reinforce existing biases [18]. If clinicians unconditionally trust and act upon biased AI recommendations, these decisions can become part of the data used to refine or retrain the algorithm, causing it to learn from and perpetuate its errors [7]. Similarly, post-deployment adjustments based on user feedback without considering the demographic diversity of the users providing that feedback can inadvertently introduce or amplify new biases [22]. Thus, the dynamic interplay between AI outputs and clinical actions creates a feedback mechanism that can solidify and disseminate biases within the healthcare system.  

# 2.4 Societal Bias  

Societal biases are deeply embedded within healthcare systems, manifesting as disparities in access to care, quality of treatment, and health outcomes across different population groups [17,18,21]. These disparities are influenced by a complex interplay of factors including race, ethnicity, gender, age, socioeconomic status (such as income and education), insurance status, and geographic location [17,18]. Historical discrimination and structural racism have created systemic inequalities that continue to affect healthcare utilization and outcomes, leading to biased healthcare datasets [8,10,11,19,27].​  

These pre-existing societal biases are subsequently captured and perpetuated within EHR data, which serves as the foundation for training many AI models in healthcare [15]. Data collection processes often suffer from exclusion bias, systematically omitting certain groups and resulting in datasets that inadequately represent the full diversity of the patient population [6,11,15,18]. Inconsistencies in demographic data recording, such as race and ethnicity identification, further reflect these systemic issues [10]. Moreover, EHR data inherently reflects clinical practices and resource allocation patterns that may themselves be influenced by implicit or systemic biases within the human healthcare workforce and institutional policies [7]. For instance, unfair allocation of medical resources can disproportionately affect uninsured individuals, underserved communities, and minority groups [7]. Predictive factors within EHRs may reflect social contingencies rather than purely biological causality, becoming brittle when applied in complex clinical settings [3].  

When AI models are trained on such biased data, they inevitably learn and perpetuate the existing inequalities [22,29]. AI systems can mirror and even exaggerate human biases present in the training data, leading to discriminatory AI outputs [5,12,23]. This includes bias related to culture, gender, age, and race [29]. For example, AI systems have been shown to underestimate the age of females and demonstrate decreased accuracy with older faces, mirroring societal tendencies [23]. AI trained on language data can reinforce gender stereotypes [12,22].  

Specific demographic and socioeconomic factors play a significant role. Race and ethnicity are strongly associated with healthcare utilization and outcomes, largely reflecting the impact of structural racism and inequalities [27]. Underrepresentation of certain racial groups in datasets leads to AI models that are less effective for these populations, thus perpetuating existing health disparities [5,11]. A stark example is the case of racial corrections applied in clinical algorithms, such as for estimating glomerular filtration rate, which can delay diagnosis and treatment for individuals from racial minority groups [19]. Similarly, gender biases can manifest as AI systems favoring male candidates or defaulting to male symptoms [26], or implicitly biasing treatment decisions, such as women with cirrhosis being less likely to receive liver transplants [7]. Ageism in AI leads to the marginalization and stereotyping of older individuals [26].​  

Beyond individual demographic traits, socioeconomic status influences an individual's capacity to access quality care and achieve favorable health outcomes [17]. Social determinants of health, including income, education, and living conditions, critically influence health outcomes and must be adequately represented in datasets to avoid biased predictions [18]. Geographic location correlates with health-related factors like lifestyle, diet, and environmental exposures [6]. A lack of representation from diverse geographic regions in training data can lead to inaccurate diagnoses for patients from underrepresented areas [6], contributing to pronounced discrepancies in health outcomes between different regions, such as between high-income and low- or medium-income countries [21]. Demographic variables may sometimes serve as proxies for underlying causal factors like socioeconomic status that vary geographically or temporally [24].  

Furthermore, the concept of intersectional bias highlights how fairness must be considered across multiple demographic dimensions simultaneously, recognizing that individuals belong to intersecting social groups that can experience compounded discrimination [9]. Other forms of societal bias reflected in AI include ableism, where systems may not accommodate disabilities [26]. The presence of human bias, both implicit and explicit, among AI developers and within the healthcare system itself is a major source of bias in medical AI [4,7]. This lack of diversity among those developing and deploying AI contributes to the perpetuation of existing biases [4]. Addressing societal bias in EHR-based AI models requires a multifaceted approach that acknowledges the deep roots of these inequalities in healthcare access, data collection, and algorithmic design.​  

# 3. Impact of Bias on Healthcare Outcomes and Disparities  

Biased AI models, particularly those leveraging Electronic Health Record (EHR) data, pose a significant threat to healthcare equity, critically exacerbating existing health disparities across patient populations [4,5,18]. The presence of bias leads to differential performance and biased recommendations, deviating from the objective of equitable healthcare [3].  

The impact of bias on healthcare outcomes can be quantified by examining disparities in performance metrics such as accuracy, sensitivity, specificity, False Negative Rate (FNR), False Positive Rate (FPR), Negative Predictive Value (NPV), and calibration across various demographic and social groups [1,24,27]. Studies reveal substantial performance deficits; for instance, recall rates for underrepresented racial groups in skin lesion detection AI models can drop significantly [12,20,26], while classification accuracy differences between racial groups in pathology AI can be substantial [11]. Age-related fairness gaps in imaging AI, measured by FNR disparities, have been observed to be as large as $3 0 \%$ , indicating a considerable impact on healthcare outcomes [24]. In colorectal cancer recurrence prediction, race-neutral models exhibit worse calibration and higher false-negative rates for minority subgroups compared to non-Hispanic White individuals [27].  

These performance disparities translate directly into tangible harms, leading to misdiagnosis, inappropriate treatment, and unequal access to care, particularly for underserved populations [2,5,17]. Specific examples abound across medical domains. In medical imaging, models designed for skin cancer detection show diminished accuracy on darker skin tones due to non-diverse training data [12,26], increasing misdiagnosis risk. Healthcare risk prediction algorithms have favored white patients over Black patients by erroneously using healthcare spending as a proxy for medical need, leading to inaccurate predictions and biased resource allocation [16,18,26]. This type of algorithmic bias has contributed to delays in necessary treatment, such as oxygen therapy for Black patients with COVID due to biased pulse oximeter readings [19], and potentially impacts pharmacogenomic testing outcomes for individuals with certain ancestries [19]. Furthermore, geographical biases in training data can lead to AI models that perform poorly for individuals from underrepresented regions, potentially resulting in incorrect diagnoses [6]. Inaccurate or inconsistent race/ethnicity data itself within EHRs can also precipitate differential treatment recommendations and adverse health outcomes for specific patient populations [10].  

Specific patient populations disproportionately impacted include racial and ethnic minorities, women, individuals with lower socioeconomic status, and those residing in geographically underrepresented areas [4,18,24,25,29]. Vulnerability is compounded by systemic factors such as racism, discrimination, pre-existing socioeconomic barriers, fragmented healthcare access, digital literacy disparities, and inconsistent data recording practices [10,20,25]. AI models trained on nondiverse datasets or using inappropriate proxies inherit and amplify biases, failing to generalize effectively to these groups [6,12,17]. Consequently, AI tools risk excluding individuals from beneficial applications [26] or leading to stigmatization and reduced access to care for disadvantaged citizens [30]. While AI offers potential in low-resource settings, existing barriers related to data, connectivity, and infrastructure can limit equitable access and performance [15,21].​  

Beyond the direct clinical impact, biased AI models erode trust in healthcare systems [4] and raise profound ethical concerns [29]. They contribute to discrimination, potentially reduce patient autonomy by influencing clinical decisionmaking [29], and compromise overall healthcare quality [3], ultimately undermining the goal of equitable and just healthcare for all [5]. Addressing these biases is critical to ensure that advancements in EHR-based AI contribute positively to public health without perpetuating or worsening existing inequities [4,18,25].  

# 3.1 Disparities in Diagnosis, Treatment, and Outcomes  

Bias embedded within EHR-based AI models frequently manifests as differential accuracy and biased recommendations, leading directly to unequal treatment and disparate health outcomes across various patient populations. This phenomenon represents a significant deviation from the objective of equitable healthcare [3].  

Specific examples highlight these disparities across different medical domains. In medical imaging, AI models designed for tasks such as skin cancer detection and prediction of melanoma demonstrate diminished performance when applied to images of lesions on darker skin tones compared to lighter ones [4,12,20,26]. This performance discrepancy is largely attributable to non-diverse training datasets that disproportionately feature lighter skin tones [26]. Studies quantifying the impact of imbalanced representation have shown substantial performance deficits, including recall rates as low as $2 5 \%$ for underrepresented racial groups in such models [20]. Similarly, AI models for pathological analysis, including subtyping breast and lung cancers and predicting IDH1 mutations in glioma, have exhibited notable differences in classification accuracy between white and Black patients, ranging from $3 . 7 \%$ to $1 6 \%$ [11]. Assessments of fairness in general medical imaging AI reveal "fairness gaps," indicating disparities in diagnostic accuracy across demographic groups defined by age, race, and sex [24]. These assessments, often employing metrics like differences in False Negative Rate (FNR) or False Positive Rate (FPR) across subgroups, have identified significant disparities, with reported age-related gaps reaching up to $3 0 \%$ [24].  

Beyond imaging, bias in risk prediction algorithms also contributes to disparities. A widely-used healthcare risk prediction algorithm in the United States exhibited a bias favoring white patients over Black patients [16,26]. This bias arose because the algorithm utilized healthcare spending as a proxy for medical need, a metric correlated with socioeconomic factors and race, leading to inaccurate predictions and unequal allocation of healthcare resources [16,26]. In predictive tasks like colorectal cancer recurrence prediction, simply omitting race and ethnicity data from models can detrimentally affect prediction accuracy for minority groups and contribute to health disparities through inappropriate care recommendations [27]. Conversely, explicitly incorporating race and ethnicity has been shown to improve predictive performance and algorithmic fairness for these groups [27]. Furthermore, geographical biases in training data can lead to AI models that perform poorly for individuals from underrepresented regions, potentially resulting in incorrect diagnoses [6]. Inaccurate identification stemming from biased race/ethnicity data itself within EHRs can also precipitate differential treatment recommendations and adverse health outcomes for specific patient populations [10].​  

Comparing these findings reveals common patterns of disparity, most notably pronounced racial bias affecting diagnosis and risk assessment across various clinical applications [4,5,11,16,20,26,27]. Age and sex biases are also identified in imaging AI fairness studies [24]. The root cause frequently involves biased or non-diverse training data that does not adequately represent the target patient population or the use of inappropriate proxy variables correlated with sensitive attributes [4,12,20,26].​  

The clinical significance of these disparities is substantial. Differential accuracy can lead to increased risks of misdiagnosis or delayed diagnosis for certain patient groups, such as individuals with darker skin tones in dermatology applications [4,26]. Biased recommendations result in inappropriate or suboptimal treatment strategies and unequal allocation of essential healthcare resources [5,6,11,12,27]. This can include delayed interventions or the withholding of necessary care based on algorithmically biased risk assessments [26]. Furthermore, insufficient representation in training data can lead to the exclusion of individuals from potentially beneficial AI-based clinical applications [26]. Ultimately, these disparities contribute to the exacerbation of existing health inequities and lead to worse health outcomes for affected populations [5,10,19,27]. The observation that Black men experience the highest age-adjusted prostate cancer mortality globally, potentially linked to underrepresentation in clinical trials, serves as a stark reminder of the real-world impact of such systemic disparities, which AI bias can compound [19]. Thus, biases in AI models directly undermine the goal of providing accurate, effective, and equitable healthcare for all patients.  

# 3.2 Impact on Specific Patient Populations and Underserved Groups  

<html><body><table><tr><td>Vulnerable Population Group</td><td>Examples of Impact/Contributing Factors</td></tr><tr><td>Racial and Ethnic Minorities</td><td>Poorer performance in imaging (darker skin tones); bias in risk algorithms (Black patients); lower accuracy in cancer prediction/subtyping.</td></tr><tr><td>Women</td><td>Lower accuracy in chest X-ray reading; increased risk of inaccurate diagnosis; potential implicit bias in treatment decisions.</td></tr><tr><td>Individuals with Lower SES</td><td>Receive fewer tests/meds; fragmented data across institutions; disparities in digital health literacy.</td></tr><tr><td>Older Adults</td><td>Decreased accuracy in age estimation models; potential marginalization/stereotyping (ageism).</td></tr><tr><td>Individuals in Underrepresented Geographies</td><td>Al models trained on limited regional data perform poorly; lack of data from low- resource settings/developing nations.</td></tr></table></body></html>  

<html><body><table><tr><td>Individualswith Disabilities</td><td>Systems may not accommodate needs (ableism).</td></tr><tr><td>Individuals with Intersecting Identities</td><td>Compounded disadvantage (e.g., Black women).</td></tr></table></body></html>  

Bias within AI models applied to healthcare, particularly those leveraging EHR data, demonstrates a disproportionate impact on specific patient populations and underserved groups, potentially exacerbating existing health inequities [4,18]. Studies have identified vulnerability across various dimensions, including race, ethnicity, gender, socioeconomic status, age, cultural background, and geographic location [24,29].  

Racial and ethnic minority groups frequently experience adverse outcomes from biased AI systems. For instance, AI models used for medical imaging and facial recognition often exhibit poorer performance on patients with darker skin tones due to training data predominantly composed of images from lighter-skinned individuals [12]. Similarly, AI models for detecting skin cancer and facial recognition algorithms for genetic disorders show reduced accuracy in these populations [12]. Bias in healthcare risk algorithms has been specifically noted to impact Black patients [16], leading to disparities such as delays in necessary oxygen treatment for Black patients with COVID due to oximeter inequity [19]. In cancer prediction and diagnosis, Black patients have shown lower accuracy rates in cancer subtyping and mutation prediction compared to white patients [11], and variations in colorectal cancer recurrence incidence predictions have been observed across racial and ethnic subgroups including Asian, Hawaiian or Pacific Islander, Black or African American, Hispanic, and non-Hispanic White patients [27]. Furthermore, hypertrophic cardiomyopathy genetic tests have demonstrated lower clinical yields for underrepresented minorities [19]. Inconsistent race/ethnicity data within EHRs, particularly affecting groups like American Indian and Alaska Native populations, can disproportionately compromise the accuracy of health research and interventions targeting these underserved groups [10]. Minority ethnic groups in the UK, who already face poorer health outcomes, are at risk of having these inequities worsened by biased AI [4].​  

Gender bias is also a documented concern. Chest X-ray reading algorithms trained predominantly on male patient data show significantly lower accuracy when applied to female patients [12]. Women, alongside people of color, are suggested to be more susceptible to receiving inaccurate diagnoses from biased AI models that may rely on demographic factors [5]. Biased performance, indicated by false negative rate (FNR) disparities, has been observed across different age groups, racial groups (White and Black), sex groups (male and female), and intersectional groups (e.g., White male and Black female) in various medical imaging tasks [24].  

Socioeconomic status (SES) acts as a compounding factor for vulnerability. Patients with low SES are documented to receive fewer diagnostic tests and medications for chronic diseases [20]. They are also more likely to seek care at multiple health institutions using disparate EHR systems, complicating data aggregation and potentially biasing models trained on fragmented records [20]. Furthermore, disparities in digital health literacy and the ability to self-report health outcomes can affect the data available for AI training and utilization for certain patient groups [20].  

Geographic location significantly impacts data availability and, consequently, AI model accuracy. The predominant source of medical datasets resides in developed nations, leading to a lack of diversity in documented diseases and patient demographics [17]. This concentration is evident in regions like Africa, where large-scale medical imaging datasets are notably absent [17]. Similarly, AI diagnostic tools trained on data primarily from specific regions within a country (e.g., California, Massachusetts, and New York) may perform less accurately when applied to populations in underrepresented states or regions, potentially reducing access to effective care for these groups [6]. While AI offers potential benefits in lowincome settings with limited access to specialists, such as enabling community health workers to provide cardiac prescreening [15,21], these regions often face barriers related to data availability, connectivity, and computing power, which can limit the equitable deployment and performance of AI tools [15].​  

The compounding factors that make these groups vulnerable stem primarily from data limitations and existing systemic inequities. AI models trained on unrepresentative datasets inherit and amplify biases present in the data, failing to generalize well to populations that are underrepresented [6,12,17]. This includes underrepresentation across demographic attributes, geographic locations, and disease variations [17]. Pre-existing socioeconomic barriers, fragmented healthcare access, digital divides, and inconsistent data recording practices further disadvantage vulnerable populations by affecting the quantity, quality, and structure of their data available for AI development [10,15,20]. Consequently, AI tools developed without explicit consideration for the unique barriers and characteristics of socially or economically marginalized  

populations may fail to address their healthcare needs equitably, potentially widening disparities in access, diagnosis, and treatment outcomes [4,18]. Addressing these compounding factors is crucial to ensure that advancements in EHR-based AI benefit all individuals equitably [25].​  

# 4. Mitigation Strategies for Bias  

Mitigating bias in AI models developed using Electronic Health Records (EHRs) is essential for ensuring equitable healthcare outcomes and requires a systematic approach across the entire AI lifecycle—from conceptualization to post-deployment monitoring [7].  

<html><body><table><tr><td>Strategy Category</td><td>Description</td><td>Key Techniques/Example S</td><td>Challenges</td></tr><tr><td>Data-Centric</td><td>Improve data quality, diversity, representativeness.</td><td>Diverse data collection, augmentation, resampling, preprocessing, using multiple sources, SDoH data, data quality checks.</td><td>Data quality issues, biased labeling, balancing utility/privacy, resource constraints.</td></tr><tr><td>Algorithm-Centric</td><td>Modify model training/algorithm design.</td><td>Fairness- constrained loss functions, re- weighting samples, statistical debiasing (adversarial),careful feature selection, architecture choices.</td><td>Balancing fairness vs. performance, acquiring sensitive attributes,limited generalizability of techniques.</td></tr><tr><td>Implementation/HuResponsible man-Centered</td><td>deployment and user interaction.</td><td>Human-in-the-loop, transparency/interpr etability (xAl), feedback mechanisms, multidisciplinary/div erse teams, user training, patient/community engagement, accessibility.</td><td>Integrating into workflows, overcoming user biases (automation/dismiss al), ensuring accountability, resources.</td></tr><tr><td>Evaluation & Monitoring</td><td>Assess and ensure equitable performance over time.</td><td>Fairness-aware metrics (FNR, FPR, calibration), subgroup analysis, bias audits, continuous monitoring (data/model drift), systematic pipelines, independent validation.</td><td>Insufficient metrics, lack of widespread continuous monitoring, complex logistics, defining "fairness".</td></tr></table></body></html>  

Mitigation strategies can be broadly categorized based on where they intervene in the AI pipeline: Data-Centric, AlgorithmCentric, Implementation/Human-Centered, and Evaluation/Monitoring [7]. Each category addresses specific sources of bias and employs distinct techniques to counteract them.​  

Data-centric approaches, focusing on improving the quality, diversity, and representativeness of training datasets, are foundational [7,16,18,20,26]. Techniques include diverse data collection, data augmentation, re-sampling, and robust preprocessing to handle imbalance and missing data [4,6,10,11,15,17,19,22,23,24]. However, ensuring high data quality and resolving issues like biased labeling remain challenging, alongside the critical need to balance data utility with privacy and access [7,10,15,16].  

Algorithm-centric interventions involve modifying the model training process and algorithms themselves [7,16,24,29]. Techniques encompass modifying loss functions with fairness constraints or re-weighting, using statistical debiasing methods like adversarial training, and carefully selecting model architectures and features [5,11,17,20,21,22,24,27,29]. Detecting bias algorithmically relies on specific fairness metrics and tools [7,12,16]. A key challenge and trade-off inherent i algorithm-centric approaches is balancing fairness metrics with overall model performance [8,9,24], alongside the difficulty of acquiring sensitive attributes necessary for some methods [17].  

Implementation and Human-Centered approaches focus on integrating AI models into clinical workflows responsibly and accountably, emphasizing human oversight and user-centric design [7]. Key strategies include implementing human-in-theloop systems to preserve clinical agency and decision-making [7,12,14,29], ensuring transparency and interpretability of AI outputs [7,12], and incorporating feedback mechanisms for continuous improvement and bias reporting [12,20]. A multidisciplinary approach involving diverse stakeholders, including patients and communities, is crucial for developing culturally competent and accessible tools [2,4,16,18,19,21]. Education and awareness are also vital to counter cognitive biases like automation and confirmation bias among users [7,12,22,29].​  

Evaluation and Monitoring constitute a critical, ongoing phase to assess and ensure equitable performance of deployed models [1,2,3,5,7,10,14,15,17,20,22,23]. Relying solely on traditional metrics like AUROC can mask disparities; thus, fairnessaware metrics such as disparate FPRs, FNRs, PPVs, NPVs, and calibration across subgroups are necessary [7,8,13,17,24,27]. Subgroup analysis, bias audits, and local evaluations on specific patient populations are crucial for identifying performance disparities [5,9,11,13,16,18,21,24]. Continuous monitoring is vital to detect model drift and emergent biases over time as data distributions change, requiring systematic pipelines, silent evaluation periods, and feedback loops [1,2,3,5,7,10,12,14,15,17,20,22,23]. Independent testing and transparent reporting are also highlighted as essential for trustworthiness [1,13,18].  

Implementing these mitigation strategies in real-world healthcare settings presents significant challenges. These include navigating complex data privacy and security requirements while enabling access to diverse data [4,15], establishing appropriate regulatory frameworks to ensure algorithm appropriateness for diverse groups and continuous monitoring [4,6,14], integrating human expertise effectively into AI workflows, overcoming resource constraints, and fostering widespread adoption and trust among clinicians and patients [19,29]. The inherent trade-offs between optimizing for fairness metrics and maintaining high overall model performance often require careful ethical consideration and contextspecific decisions [7,8,24,29]. Ultimately, addressing bias in EHR-based AI necessitates a comprehensive, multi-faceted approach that combines technical interventions with robust ethical guidelines, regulatory oversight, and continuous stakeholder engagement throughout the entire AI lifecycle [7,26].  

# 4.1 Data-Centric Approaches  

Data-centric approaches are recognized as fundamental strategies for minimizing bias in AI systems developed for healthcare applications [16,26]. These approaches focus on improving the quality and representativeness of the data used for training models, addressing biases inherent in the datasets themselves [7,20].  

A primary focus of data-centric bias mitigation is enhancing dataset representativeness. This involves actively recruiting underrepresented populations during data collection efforts [18] and ensuring datasets reflect the diversity of the target population across various dimensions, including race, ethnicity, sociodemographics, age, gender, and geographic location [6,7,20,22,23]. Maximizing the use of rich EHR data, including social determinants of health (SDoH) information, is also advocated for a more holistic understanding of patient conditions and to address health disparities [18,25]. Strategies such as utilizing multiple data sources, engaging in open science practices, and carefully selecting data cohorts (retrospective and prospective) are essential for mitigating representation, sampling, and selection biases [7]. Furthermore, enabling  

secure and wider access to health data is considered crucial for building more diverse and representative datasets that can improve AI accuracy and reduce bias [15]. Developing regulatory frameworks can also support ensuring algorithms are appropriate for diverse ethnic groups by emphasizing representative testing datasets [4].  

Data preprocessing techniques play a significant role in addressing data imbalance and enhancing diversity after collection. Prior to model development, it is recommended to review and characterize the dataset to identify areas of underrepresentation [20]. Statistical methods can be employed during preprocessing to account for data imbalance [20]. Specific data rebalancing methods, such as ReSample, have demonstrated effectiveness in reducing fairness gaps by addressing prevalence shifts during training [24]. Re-weighting or re-sampling datasets can also help balance representation, particularly for sensitive attributes like race/ethnicity [10]. Data augmentation, especially through generative approaches, is utilized to balance datasets across sensitive attributes by creating synthetic data for underrepresented groups or generating challenging cases to enhance model robustness [17]. Examples include generating images of darker skin tones to improve diversity in medical imaging datasets [19]. While data augmentation can facilitate both pre-training and fine-tuning [17], its effectiveness can sometimes be marginal, as noted in studies focusing on emphasizing underrepresented groups [11]. Data deduplication is another preprocessing step that can enhance dataset diversity and representativeness by eliminating near-duplicate entries [17]. Imputation methods, such as imputing race from tumor samples, can also be used to address missing demographic information and increase dataset diversity [19].​  

Despite the potential of data-centric approaches, ensuring high data quality and representativeness remains challenging. Data inaccuracies, inconsistencies, and missing values can introduce or exacerbate biases. Strategies to improve data quality include evaluating data accuracy and reliability [7] and optimizing healthcare data collection standards [10]. Standardized data collection, particularly in diverse or low-resource settings, is crucial for improving quality and comparability [21]. Biased labeling is another challenge; while not explicitly detailed with specific handling techniques in the provided digests, identifying biases in human-driven processes like labeling is key to understanding and mitigating their causes [16]. Preserving data utility and privacy while enhancing diversity and access presents a significant trade-off, requiring careful consideration and secure data handling practices [15]. Proactive measures, such as involving diverse users in the design process, can help identify potential data biases early on [12]. Overall, a comprehensive data-centric strategy involves not only sophisticated preprocessing techniques but also fundamental improvements in data collection, quality assurance, and access to build equitable AI models [7,20].​  

# 4.2 Algorithm-Centric Approaches  

Algorithm-centric interventions form a critical component in the comprehensive strategy to mitigate bias in AI models developed using Electronic Health Records (EHRs). These approaches focus on modifying the machine learning algorithms themselves, or the processes surrounding their training and validation, to promote fairer outcomes across different demographic or clinical subgroups [7,16,24,29]. Bias identification and mitigation should be actively integrated throughout the algorithm development and validation lifecycle, encompassing algorithm bias, validation bias, and representation bias [7].​  

A foundational aspect of algorithmic fairness is the ability to detect and measure bias. This involves employing specific fairness metrics and conducting analyses such as stratified subgroup analysis [7]. Counterfactual examples can also be considered to assess how prediction changes based on sensitive attributes [7]. Furthermore, researchers and developers can leverage dedicated tools and frameworks designed for identifying bias in AI models. Notable examples include IBM's AI Fairness 360 and Google's What-If Tool [12]. IBM's AI Fairness 360, for instance, is an open-source library providing a suite of metrics and algorithms to detect and reduce bias, particularly in unsupervised learning, allowing programmers to test models and datasets comprehensively [16].  

Several algorithmic techniques have been proposed and implemented to address bias during model training and development. One common strategy involves modifying the loss function used during optimization. This can include assigning higher weights to samples from underrepresented groups to incentivize the model to improve performance on these specific data points [20]. Alternatively, regularization terms can be added to the loss function that explicitly penalize disparities in prediction outcomes across different subgroups [20]. Integrating sensitive attributes directly into the loss function is another established approach to promote equitable outcomes across demographic groups [17,20]. However, acquiring sensitive attributes can present significant practical challenges [17].​  

Beyond loss function modifications, various statistical debiasing approaches have been explored. These include techniques like adversarial debiasing and Prejudice Regularization, which aim to make the model's predictions independent of sensitive attributes while maintaining predictive performance [20]. Debiasing robustness methods that specifically target and correct for "demographic shortcuts"—where models rely on sensitive attributes instead of clinically relevant features— have shown promise. Studies applying methods such as GroupDRO (Group Distributionally Robust Optimization) and DANN (Domain-Adversarial Neural Network) have demonstrated success in reducing disparities, such as closing the False Negative Rate (FNR) gap across groups while maintaining comparable Area Under the Receiver Operating Characteristic Curve (AUROC) scores [24]. Specifically, DANN methods that remove demographic information from the model's embeddings have been shown to lead to the lowest average out-of-distribution fairness gap [24]. Other related techniques like adversarial training can also be employed in algorithm development to mitigate bias [22].  

Algorithmic choices and model architecture also influence fairness. The use of self-supervised foundation models trained on vast datasets has been suggested as a method to reduce disparities by encoding richer, less biased representations of complex data like histology images [11]. Techniques to prevent overfitting, which can exacerbate biases by fitting noise or spurious correlations in biased datasets, are also relevant. For instance, Bayesian Neural Networks (BNNs) incorporating dropout layers can help reduce overfitting, particularly when dealing with smaller datasets [21]. Addressing data imbalance issues during training through techniques such as resampling, synthetic data generation, or cost-sensitive learning can further contribute to developing fairer algorithms [7].​  

Feature engineering plays a crucial role in enabling or mitigating algorithmic bias. Explicitly including sensitive attributes, such as race and ethnicity, as predictors or interaction terms within a model architecture can improve fairness and calibration for specific subgroups. For example, incorporating race and an interaction term between race and stage improved calibration for Black or African American patients in a colorectal cancer recurrence prediction model [27]. When direct acquisition of sensitive attributes is challenging, alternative methodologies like clustering-based data curation can offer potential proxy measures for use in fairness interventions [17]. Furthermore, the choice of algorithm must be suitable for the task's inherent fairness requirements, such as the need for fairness-aware algorithms capable of recognizing emotions across diverse cultural, gender, age, and racial groups without exhibiting bias [29]. Simple retraining of models has also been explored as an algorithm-centric approach [5], though its effectiveness may be limited when applied to patient populations different from the training data, underscoring the need for more robust interventions [5]. Innovative AI/ML approaches can also be applied to improve existing clinical models and address bias in specific tasks, potentially involving training on data from high-performing subgroups to enhance overall equitable outcomes [19].  

While algorithmic interventions offer powerful means to address bias, they often involve navigating complex trade-offs, particularly between fairness metrics and overall model accuracy. Although some techniques like GroupDRO and DANN have shown the potential to improve fairness metrics without significantly compromising accuracy [24], debiasing strategies are not universally effective and may not always translate to reduced bias in downstream clinical tasks [9]. The challenge of acquiring necessary sensitive attribute data [17] and the limited generalizability of some algorithmic interventions to diverse patient populations [5] highlight ongoing research challenges in this domain.  

# 4.3 Implementation and Human-Centered Approaches  

Mitigating bias in EHR-based AI models requires effective implementation strategies that prioritize human judgment and incorporate human-centered design principles. A fundamental element involves the clinical deployment phase, where models are integrated into real-world environments [7].  

A frequently advocated strategy is the human-in-the-loop (HITL) approach, ensuring that human experts, such as clinicians, review model predictions before final decisions are made [7,12].  

This approach preserves human agency in decision-making, preventing individuals from being deterred from questioning o deciding against AI recommendations, even if those decisions are influenced by factors like emotional state [14,29].  

Firms developing these tools have a responsibility to ensure that their products are safe, reliable, and accountable, maintaining human control [14].  

Clinicians, in turn, must accept accountability for understanding the potential for bias inherent in AI systems [19].  

A multidisciplinary approach is crucial, involving healthcare professionals, patients, social scientists, and ethicists to define legitimate subgroup differences and develop culturally competent AI systems [2,16].  

Diversifying the development workforce is also highlighted as important for ensuring solutions meet the needs of all users [4,16].  

Transparency and interpretability are key components of human-centered AI design [7,12].  

This includes using tools to enhance model interpretability [7] and developing transparent interfaces that clearly explain A reasoning [12].  

Setting realistic expectations by avoiding anthropomorphism is also essential [12].  

Furthermore, comprehensive documentation is a fundamental requirement for ethical implementation, detailing training data sources, testing methodologies, and deployment guidelines aimed at minimizing risk and bias [17].  

Transparency regarding the demographic distribution of training data and reporting model performance for relevant subgroups are necessary disclosures [7].  

Integrating feedback mechanisms is vital for continuous monitoring and verification of model outputs and performance in clinical settings, particularly concerning differences across sociodemographic factors [12,20].  

Tools like dashboards, visualizations, or notifications can enhance clinician awareness of potential AI limitations and biase [20].  

Incorporating feedback mechanisms allows users to report biased or confusing outputs [12].  

Culturally responsive approaches to assessment and care are advocated [25].  

Designing AI tools with accessibility in mind ensures they cater to the needs of socially and economically marginalized populations and do not perpetuate existing barriers to care [18].  

Engaging patients and local communities early in the design and development process ensures their preferences, values, and experiences are reflected, fostering wider acceptance and tailoring technologies to specific needs and constraints [18,19,21].  

Considering patient desires from the outset is deemed critical for overcoming historical data imbalances [19].  

Effective implementation also necessitates evaluating AI models on specific patient populations before widespread use [5].  

This human-centered approach helps ensure models do not produce inaccurate or biased results for certain groups, emphasizing the need for careful oversight and validation in real-world clinical settings [5].  

The public should be cautious of systems trained on narrow datasets, underscoring the importance of transparent training data characteristics [6].  

Education and awareness campaigns are necessary to sensitize developers, users, and stakeholders to the nuances of bias, promoting responsible technology use [22].  

Training and design principles play a significant role in countering automation bias and confirmation bias. By maintaining a human-in-the-loop, providing transparent explanations, and enabling users to question recommendations, the tendency for clinicians to blindly accept AI outputs (automation bias) is reduced [7,12,29].  

Similarly, providing subgroup performance data, feedback loops, and tools to highlight potential biases helps challenge pr existing beliefs and assumptions, mitigating confirmation bias [7,12,20].  

Deciding when automated decisions are appropriate versus when human involvement is necessary is also a key aspect of design that directly impacts the potential for these cognitive biases [16].  

# 4.4 Evaluation and Monitoring  

The reliable and equitable deployment of AI models in healthcare, particularly those based on Electronic Health Records (EHRs), necessitates a robust framework for evaluation and continuous monitoring. Traditional performance metrics, while essential, are often insufficient for identifying and quantifying biases that can lead to disparate outcomes across patient subgroups [7]. Metrics such as overall accuracy or Area Under the Receiver Operating Characteristic Curve (AUROC) may obscure performance disparities that exist among different demographic groups, resulting in seemingly high performance metrics masking significant inequities [7,8].  

To address these limitations, fairness-aware metrics have been developed and employed to assess bias. These include measures such as calibration intercept and slope, false-positive rates (FPRs), false-negative rates (FNRs), positive predictive values (PPVs), and negative predictive values (NPVs), often assessed at specific risk thresholds [13,24,27]. Fairness definitions like "equal opportunity," which focuses on minimizing disparities in FNR or FPR across demographic attributes, are crucial for evaluating equitable model behavior [24]. Furthermore, fairness evaluation encompasses both individual fairness, demanding consistent outputs for similar inputs, and group fairness, which evaluates performance across categories defined by sensitive attributes like race and gender [17]. The disparity in metrics like AUROC between protected subgroups can also serve as a direct measure for comparing fairness [8]. Reporting the specific metrics used to evaluate bias and discrimination is essential for transparency [13].  

Subgroup analysis and bias audits are critical methodologies for identifying disparities [24]. Evaluating model metrics for specific groups within the dataset allows researchers to determine if performance is consistent across different subgroups, thereby revealing potential biases [9,11,16]. Regular equity audits of AI systems can systematically identify and address the exclusion or underperformance for specific populations [5,18]. Local evaluations on specific patient populations are also recommended to ensure fairness and accuracy in the context of deployment [5,21]. Evaluation should ideally be performed not only on in-distribution data but also on out-of-distribution datasets to understand how fairness changes under distribution shifts, highlighting the need to mitigate both initial biases and those arising from covariate shift [24].  

Beyond initial validation, the need for continuous monitoring to detect model drift and emerging biases is paramount [1,2,3,5,7,10,14,15,17,20,22,23]. AI model results can change as the underlying data distributions evolve or new data is incorporated, potentially introducing or exacerbating biases over time [16]. Continuous monitoring of performance across different demographic groups is essential to detect and address such dynamic biases [11,23]. This ongoing process requires tracking changes in data distributions, performance, and bias metrics in the real clinical environment [2,7,20]. Despite its importance, continuous monitoring of deployed models for fairness is not yet widely practiced [17].​  

The importance of systematic evaluation pipelines and processes is increasingly recognized. This includes developing dashboards to track data distribution changes [2] and implementing mandatory silent evaluation periods where models are assessed in the background of clinical activity before full deployment, allowing for evaluation of safety, efficacy, and equity without impacting patient care [2,3]. Comprehensive monitoring systems should track model accuracy, user engagement, decision impact, and patient demographic information to identify biases and unfairness [7]. Utilizing diverse benchmark datasets and systematic evaluation tools are vital [17,22]. Implementing feedback mechanisms for users to report biased AI recommendations can also contribute to system improvement over time [12]. Independent third-party testing and validation against assurance standards are critical steps to determine algorithmic trustworthiness [1]. This systematic approach should encompass the entire lifecycle of model development and deployment, including rigorous post-market surveillance [3]. Healthcare institutions must manage data used for AI models as regulated products, adhering to guidelines to ensure algorithm accuracy and fairness over the long term [7]. Transparent reporting of all research findings, including negative results, is necessary to build a comprehensive and unbiased evidence base [13,18]. Regulators are increasingly expected to scrutinize training methods to prevent geographical or other data biases [6] and to fill regulatory gaps in monitoring for adverse events and algorithmic changes [14].  

# 5. Ethical and Regulatory Considerations  

The integration of AI models into healthcare, particularly those based on Electronic Health Records (EHRs), presents significant ethical and legal challenges, primarily stemming from the potential for bias to exacerbate existing health inequities.  

<html><body><table><tr><td>Ethical/Regulatory Aspect</td><td>Key Concepts/Challenges</td></tr><tr><td>Fairness</td><td>Justice,non-discrimination, equal opportunity, equal outcomes, balancing fairness definitions, equitable resource allocation, addressing systemic bias.</td></tr><tr><td>Beneficence/Nonmaleficence</td><td>Ensuring Al provides benefit without causing harm,avoiding misdiagnosis/unequal treatment, protecting vulnerable</td></tr><tr><td></td><td>populations.</td></tr></table></body></html>  

<html><body><table><tr><td>Patient Autonomy</td><td>Avoiding undue influence/manipulation (nudging),ensuring human oversight, informed consent for data use.</td></tr><tr><td>Privacy & Data Protection</td><td>Secure data handling, confidentiality, preventing misuse/stigmatization,informed consent, patient data ownership (loT context).</td></tr><tr><td>Accountability</td><td>Establishing clear responsibilities (developers,institutions,clinicians), oversight mechanisms,managing liability.</td></tr><tr><td>Transparency& Explainability</td><td>"Black box" challenge, necessity for documentation, disclosing limitations/biases, using xAl, transparent reporting of methods and performance.</td></tr><tr><td>Regulatory Frameworks</td><td>Keeping pace with innovation, regulatory gaps (bias,monitoring), mandating stratified evaluations,ensuringappropriateness for diverse groups, post-market surveillance.</td></tr></table></body></html>  

A foundational principle guiding responsible AI development and deployment is fairness, encompassing concepts of justice and non-discrimination [5,10,18]. Alongside fairness, other crucial ethical principles include beneficence, nonmaleficence, and patient autonomy [18]. Specific ethical concerns arise regarding patient privacy, data protection, and confidentiality, demanding robust data security measures, comprehensive informed consent processes, and safeguards against data misuse, stigmatization, and discrimination, especially when leveraging sensitive data such as genomics and social determinants of health (SDoH) [18,25,28,30]. The increasing use of medical devices integrated via the Internet of Things further accentuates the long-standing issue of data protection and ownership, with a growing consensus favoring patient ownership to enhance engagement and information sharing through data use agreements [30]. Developers and designers bear a moral obligation to prevent biased outcomes in critical healthcare applications that could lead to discrimination or unequal opportunities [12].​  

Complex ethical debates persist within the domain of fair AI in healthcare. One such debate concerns the balance between ensuring fairness in treatment allocation and achieving fairness in health outcomes, raising questions about whether sensitive attributes like race or ethnicity should be incorporated into prediction models and how fairness is defined depending on the clinical decision context and potential long-term consequences [27]. Another tension exists between the selective deployment of AI models only in patient subgroups where they perform optimally and the equitable deployment aiming for performance parity across all populations, even if it means "leveling down" overall effectiveness [2,3]. This highlights the inherent challenge of resource allocation in healthcare and the need for scrutiny of the basis for such decisions [3]. Furthermore, ethical considerations extend to the potential for AI systems, particularly those with emotional capabilities, to influence or manipulate patient or clinician decisions through practices like "nudging," raising profound questions about agency and where responsibility and liability reside when AI impacts decisions [29]. Incorporating diverse communities throughout the AI lifecycle is essential to address the unique needs and challenges faced by various populations [18].​  

Ensuring fairness, accountability, and transparency in AI-driven healthcare systems presents significant challenges. Fairness is compromised when models exhibit differential performance across demographic groups, including racial, ethnic, and geographic populations [4,6]. Accountability requires establishing clear responsibilities and oversight mechanisms for all stakeholders involved in the AI model lifecycle, from development and validation to deployment and monitoring [2,5,13,29]. Transparency is crucial but often hampered by the "black box" nature of complex models, necessitating clear documentation of model limitations, potential biases, and performance variations across different populations [2,15,28]. Specialized tools and platforms, including those for AI governance, responsible AI design, MLOps, LLMOps, and data governance, are being developed to promote adherence to ethical and legal standards, prevent biased outputs, and enhance transparency and accountability throughout the AI development and deployment process [26]. The development of trustworthy medical AI systems is guided by principles including fairness, universality, traceability, usability, robustness, and explainability [17].  

Regulatory authorities and legal frameworks play a critical role in addressing bias and establishing accountability for harmful outcomes in health AI. However, the pace of AI innovation often outstrips the capacity of governance systems, leading to regulatory gaps and challenges for authorities in assessing new tools [14,15,31]. Legislative frameworks are deemed crucial for enhancing AI system reliability in healthcare [17]. The European Union's AI Act represents a pioneering approach by classifying medical AI systems as "high-risk" and establishing specific provisions for General Purpose AI Models, including foundation models [17,28]. In the United States, nondiscrimination is mandated under the Affordable Care Act for AI-based patient support tools, requiring providers to exercise due diligence regarding protected features like race or sex and mitigate discrimination risks [19]. A dedicated task force has been established by the White House to incorporate equity principles into health AI technologies [19]. Despite these efforts, existing regulatory pathways, such as the FDA's Software as a Medical Device (SaMD) Action Plan, while regulating translation into clinical use and focusing on risk mitigation, reproducibility, and robustness, currently lack explicit regulations examining fairness or bias in medical AI outputs [20].  

Calls for more robust regulatory action are prevalent. Regulatory and policy agencies are urged to mandate demographicstratified evaluations of AI models as part of approval and deployment guidelines to ensure equitable benefits across patient groups [11]. A regulatory framework is needed to ensure algorithms are tested on and appropriate for minority ethnic groups to reduce dataset bias [4]. Regulatory oversight is also necessary for AI training methods to prevent harm to patients from underrepresented geographic areas, drawing parallels to historical oversights in clinical trials regarding diversity [6]. Regulators should provide concrete guidance on procedures to ensure AI use in healthcare adheres to principles such as fairness, appropriateness, validity, effectiveness, and safety (FAVES) [1]. Furthermore, regulators must consider the necessity for real-world performance monitoring, including detecting fairness degradation over time and under distribution shift, challenging the assumption of a single, universally fair model [24]. Legal frameworks must incorporate privacy-protective mechanisms, such as Certificates of Confidentiality, especially when handling sensitive data [25]. Addressing liability for healthcare systems and physicians using health AI is also a critical regulatory consideration [1]. Policymakers are encouraged to establish human rights-based and ethical AI frameworks that enforce inclusivity, accountability, and fairness, implementing regulatory oversight, regular audits, and independent verification and certification for sensitive applications [22]. Transparency and accountability are also critical in the context of AI use in research and publication, with authors held accountable for reporting AI use transparently [13].  

Transparency and explainability methods are pivotal in addressing bias and building trust in AI systems. The "black box" problem, where AI decision-making processes are opaque, necessitates approaches that make AI applications more understandable and human-interpretable [28]. Explainable AI (xAI) techniques aim to shed light on model behavior, which can help identify the sources of bias within datasets or algorithmic structures [28]. Transparent reporting of model limitations, potential biases, and performance variations across different populations is essential for building trust among clinicians, patients, and the public [2,13,15]. Trustworthiness, including explainability, is a fundamental principle guiding the development of reliable medical AI systems [17].​  

# 6. Case Studies and Examples  

<html><body><table><tr><td>Al System/Application Area</td><td>Type(s) of Bias Demonstrated</td><td>Impact/Manifestatio n</td><td>Key Contributing Factor(s)</td></tr><tr><td>Healthcare Risk Prediction</td><td>Algorithmic Bias (Proxy Variables), Data Bias (Historical/Societal)</td><td>Underestimating health needs of Black patients; delaying care; using spending as proxy for need.</td><td>Reliance on healthcare spending data (proxy for SES/Race), systemic inequalities reflected in data.</td></tr><tr><td>Skin Cancer Detection Al</td><td>Data Bias (Representation/Imb alance)</td><td>Reduced accuracy on dark skin tones;</td><td>Lack of diverse training data</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>misdiagnosis risk; exclusion of patients.</td><td>(predominantly light skin).</td></tr><tr><td>Medical Imaging (General)</td><td>Data Bias (Representation), Algorithmic Bias</td><td>Performance disparities by race, age, sex in subtyping/mutation prediction; lower accuracy for underrepresented groups.</td><td>Non-diverse training datasets, potential algorithmic choices/features favoring majority groups.</td></tr><tr><td>Pulse Oximeters</td><td>Measurement Bias (Hardware/Calibratio n)</td><td>Falsely high readings on dark skin,delaying oxygen treatment for Black patients.</td><td>Device calibration based on light skin.</td></tr><tr><td>Large Language Models (LLMs)</td><td>Data Bias (Societal/Historical)</td><td>Gender/cultural stereotypes; sexist content generation; biased associations with names/roles.</td><td>Training data reflecting societal biases (language corpora).</td></tr><tr><td>Al Hiring Tools (e.g., Amazon)</td><td>Data Bias (Historical/Societal)</td><td>Gender bias, penalizing female candidates.</td><td>Training data reflecting historical male dominance in industry.</td></tr><tr><td>Facial Recognition</td><td>Data Bias (Representation/Imb alance)</td><td>Lower accuracy for Black individuals.</td><td>Underrepresentation of Black individuals in training data.</td></tr></table></body></html>  

This section provides concrete illustrations of bias in AI systems—particularly those relevant to Electronic Health Records (EHRs) and healthcare more broadly—drawing insights from various case studies. These examples highlight specific types of bias, their origins, impact, and approaches to detection and mitigation, offering valuable lessons for researchers and practitioners.​  

A prominent category of bias manifests in healthcare risk prediction algorithms. The COMPAS system, used in US courts, demonstrated racial bias in predicting recidivism [31]. Similarly, healthcare risk-prediction algorithms used on millions of US citizens, such as Optum's tool or those analyzed by Obermeyer et al., have exhibited racial bias by underestimating the health needs of Black patients compared to White patients with similar chronic conditions [16,18,19,26]. This bias stemmed from the algorithms' reliance on healthcare spending as a proxy for medical need—a metric correlated with income and race, thus reflecting systemic socioeconomic disparities rather than actual health status [16,18,19,26]. The result is inaccurate predictions that disproportionately affect minority populations, potentially delaying access to necessary care. Another example, the Epic Sepsis Model (ESM), illustrated sample selection bias; after deployment, its performance significantly deteriorated compared to initial reports, with a substantial proportion of sepsis cases being missed and frequent false alarms generated. This highlights the challenge of generalizing performance beyond initial training data and underscores the importance of real-world validation [20].  

Bias in medical imaging AI is another critical area. Diagnostic tools for skin cancer have shown reduced accuracy for individuals with dark skin tones [4,26]. This disparity is primarily attributed to a lack of diversity in training datasets, which are often predominantly composed of images from White patients [4,19,26]. Such data imbalance can lead to misdiagnosis risks and exclude individuals with darker skin from benefiting from these AI-based applications [26]. Similarly, studies on AI models for diabetic retinopathy detection, breast cancer subtyping, lung cancer subtyping, and glioma IDH1 mutation prediction have revealed performance disparities stratified by race, emphasizing the pervasive nature of demographic bias in medical image analysis [5,8,11,13,24]. Early clinical trials for devices like the DermaSensor melanoma screening tool and the Tempus ECG-AF predominantly included White participants, underscoring the need for postmarket validation in diverse populations to ensure equitable performance [19]. The breast cancer context further underscores potential sex-based differences in clinical presentation and markers that necessitate distinct modeling approaches to avoid bias and predict outcomes accurately [3]. Moreover, class imbalance in datasets, as seen in melanoma prediction, can significantly affect model performance [20].​  

Bias is also prevalent in Large Language Models (LLMs) and general AI applications with societal impact. Investigations into LLMs have revealed gender and cultural biases, including stereotypical associations between gendered names and traditional roles, as well as the generation of sexist or misogynistic content, particularly in models not fine-tuned with human feedback [22]. For instance, British men were associated with varied occupations, while British women were disproportionately linked to stereotypical or controversial roles [22]. Google Translate exhibited gender bias by replicating stereotypes from its training data, prompting the implementation of gender-specific translation options and user feedback mechanisms as mitigation strategies [12]. Sanas's accent translation system, which aims to standardize accents, risks exacerbating racial biases by normalizing certain linguistic styles [16]. Beyond language, Amazon's AI hiring tool demonstrated gender bias by penalizing resumes containing "women's" due to historical data reflecting male dominance in tech, ultimately leading to its discontinuation [16,26]. Facebook's advertising platform allowed targeting based on gender, race, and religion, which perpetuated stereotypes in job advertisements and prompted the company to restrict such targeting in employment ads [16,26]. Facial recognition systems are also less accurate for Black individuals compared to White individuals—a bias linked to the underrepresentation of these groups in training datasets [4,33]. Similarly, AI age estimation tools not only mirror but sometimes exaggerate human age biases across different demographic groups [23].  

Bias extends to genetic risk prediction as well. Polygenic risk scores (PRS) for conditions such as schizophrenia or heritability estimates for substance use disorder exhibit differing associations or heritability across ancestral groups, thereby highlighting the need for diverse genomic datasets [25]. Furthermore, the deployment of AI diagnostic tools predominantly in well-resourced settings or among populations with insurance can lead to an unfair distribution of technology and exacerbate existing health inequities [18]. The influence of AI outputs can also subtly shift decision-making away from human experts, as illustrated by the hypothetical scenario in which a physician might be reluctant to question an AI’s suggestion [29]. Moreover, biased AI outputs can influence human decisions, as demonstrated in an experiment where AI recommendations in mental health emergencies led to racial and religious disparities in suggested interventions [26].​  

Lessons learned from these cases underscore several common pitfalls: reliance on historical data that reflects societal biases; the use of flawed proxy variables; inadequate diversity and representation in training datasets across demographic groups (race, gender, age, disability status); class imbalance; and insufficient real-world validation post-deployment [13,19,20,26]. Effective strategies to address these issues include increasing dataset diversity and representativeness, careful selection and validation of features (avoiding biased proxies), implementing bias detection methods during both development and post-deployment, conducting postmarket surveillance in diverse populations, incorporating fairness metrics alongside performance metrics, and adopting transparent AI practices [12,19,26]. A human-centered approach, which considers the impact on diverse user groups and incorporates feedback mechanisms, is also crucial [12,21].  

These case studies collectively provide actionable guidance and emphasize the necessity of proactive bias assessment and mitigation throughout the AI lifecycle to ensure equitable and trustworthy applications.  

# 7. Future Directions and Open Challenges  

<html><body><table><tr><td>Category</td><td>Future Directions /Open Challenges</td></tr><tr><td>Data</td><td>Collect larger, more diverse/representative datasets; optimize data collection standards; improve sensitive data accuracy; refine data harmonization; integrate SDoH/biobank data.</td></tr><tr><td>Methodology</td><td>Develop robust bias identification/mitigation tailored for clinical Al; investigate specific modelingapproaches (interaction terms,stratification);address intersectional bias; build robustness to</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>distribution/domain shifts; understand underlying bias causes.</td></tr><tr><td>Evaluation& Monitoring</td><td>Clinical validation in independent datasets; develop metrics focused on actual patient outcomes; continuous monitoring for data/bias drift;model recalibration strategies; longitudinal studies; robust assurance mechanisms (labs).</td></tr><tr><td>Implementation& Deployment</td><td>Navigate practical deployment hurdles (esp. low-income settings); address resistance to workflow changes; user-centered design; ensuring equitable access/use.</td></tr><tr><td>Policy & Regulation</td><td>Support equitable practices; minimum global standards; accelerate legislation/guidelines; address biases in regulation/insurance frameworks.</td></tr><tr><td>Collaboration & Education</td><td>Foster interdisciplinary partnership; increase diversity in developer/leadership; integrate Al ethics into medical training; enhance public understanding/participation.</td></tr></table></body></html>  

Ensuring equity in EHR-based AI models necessitates addressing several critical areas for future research and development, synthesizing insights from the challenges identified across the field [3,7,16,17,20,26,28,29,30,31]. A foundational challenge lies in the data itself. Future efforts must focus on collecting significantly larger, more diverse, and representative datasets that accurately reflect the heterogeneity of patient populations, including sufficient representation of various racial, ethnic, geographic, and underserved groups [2,6,19,20]. This involves optimizing healthcare data collection standards and improving the accuracy of sensitive demographic information, particularly for populations currently underrepresented [10]. Furthermore, developing refined data harmonization strategies is crucial given the variability in EHR data across institutions [25]. Exploring mechanisms to combine disparate data sources, such as EHR, Social Determinants of Health (SDoH) data, and biobank data, could provide a more comprehensive view for analysis [25]. However, creating and maintaining these extensive and diverse datasets presents significant financial and resource challenges [6].  

Methodological advancements are equally vital. Research should explore more robust techniques for identifying and mitigating bias, specifically tailored for the nuances of clinical decision-based AI [9,20]. This includes investigating specific modeling approaches such as interaction term models or race-stratified models using larger subgroups to enable more flexible and equitable representations [27]. Addressing intersecting sources of bias, including the complex interplay of demographic factors like gender and race, requires dedicated intersectional analysis and potentially holistic solutions beyond purely technological fixes [20,22]. Building AI models that are robust to distribution and domain shifts encountered in real-world deployment is a significant technical challenge, potentially requiring methods that incorporate prior knowledge of potential shifts or facilitate active data collection for specific groups [24]. Further research is also needed to understand the underlying causes of bias in AI models, examining the roles of training data characteristics and algorithmic design [23]. Emerging technologies, such as multi-modal foundation models that integrate various data types like genomics and EHRs, show promise in reducing bias but require further refinement and investigation, especially concerning their performance on underrepresented demographic groups [11].  

Rigorous evaluation, validation, and ongoing monitoring are paramount. A core challenge is the clinical validation of AI tools in external, independent datasets, moving beyond retrospective studies which are prone to selection and spectrum bias [30]. Developing improved metrics that focus on actual patient outcomes in authentic deployment contexts is essential for assessing true performance and impact [3]. Furthermore, continuous monitoring for data shift and bias drift over time, along with implementing strategies for model recalibration and retraining to maintain fairness across diverse data distributions, represents a crucial area for future work [3,5]. Longitudinal studies are necessary to evaluate the long-term impact and equity of deployed AI systems [20]. Establishing robust assurance mechanisms, potentially through networks of health AI assurance labs, is a complex undertaking that needs experimentation with diverse approaches to account for local contexts and health system inequities [1].  

The successful and equitable integration of AI into healthcare necessitates addressing implementation and deployment challenges. This includes navigating the practical hurdles of deploying AI in diverse settings, particularly in low-income contexts facing constraints in reliability, operational processes, data accessibility, and financial viability [21]. It also involves addressing potential resistance from patients and healthcare professionals to the redesign of workflows necessary for efficient AI utilization [14], emphasizing the need for user-centred design [21].  

Addressing bias fundamentally requires coordinated action across multiple domains. Policymakers play a crucial role in supporting equitable AI practices, including the development of minimum global standards for AI regulation [15,17]. The formulation of legislation, regulations, and practical guidelines needs to accelerate to keep pace with AI development [7]. Discourse on intersecting sources of bias within the broader healthcare system, such as biases embedded in regulation and insurance frameworks, is also critical [20].  

Finally, fostering a culture of fairness and equity demands robust interdisciplinary collaboration. This requires close partnership among AI researchers, healthcare professionals, ethicists, legal and social experts, policymakers, and patient advocacy groups [7,17]. Increasing diversity within the AI developer community and at strategic leadership levels within healthcare and government is essential [4,7]. Furthermore, integrating AI bias and ethical considerations into medical training curricula will empower healthcare professionals to better understand and critically evaluate these technologies in clinical practice [7]. Increased public understanding of AI risks and the creation of infrastructure to facilitate public participation in AI debates are also necessary steps towards responsible AI development [31]. Continued, transparent discussion on the creation and use of AI technologies in healthcare is paramount [4].  

# 8. Conclusion  

This survey has provided a comprehensive overview of bias in EHR-based AI models, exploring its myriad forms, sources, and significant impacts on healthcare delivery and outcomes. As discussed, bias can manifest throughout the entire AI development pipeline, from data collection and annotation to model development, evaluation, implementation, and even publication [16,20]. Key sources include incomplete or non-representative datasets lacking diversity across demographic, geographic, and socioeconomic dimensions [6,11,23], data quality issues inherent in EHRs [10], the encoding of demographic attributes as model "shortcuts" [24], and the socio-technical architectures and data capture mechanisms underlying AI systems [31].​  

The impact of such biases is profound and directly threatens the equitable application of AI in medicine. Biased models can lead to diagnostic inaccuracies [5,23], inappropriate care recommendations [27], and the exacerbation of existing health disparities, particularly for minority and underserved populations [4,7,19,20]. Furthermore, model performance can degrade significantly under real-world distribution shifts, potentially undermining clinical efficacy [24].  

Addressing these biases is paramount and increasingly feasible with dedicated methods and metrics [7,20]. Mitigation strategies often involve improving data diversity [6,11], enhancing training protocols [23], and employing fairness-aware techniques [7,16,26]. However, challenges remain, as simple approaches like omitting sensitive attributes may inadvertently harm subgroup accuracy [27], and current debiasing strategies may not fully address complex issues like intersectional bias while preserving predictive power [9]. The debate between selective and equitable deployment necessitates improved evaluation practices that prioritize actual patient outcomes over conventional benchmarks [2,3].  

The imperative of fairness and equity in AI-driven healthcare cannot be overstated [7,18,20]. Ensuring that AI technologies benefit all populations equitably is not merely a technical challenge but a fundamental ethical requirement for maintaining medical ethical standards and promoting healthcare equity [7]. AI holds immense promise for transforming healthcare [14,28], but this potential can only be fully realized if biases are effectively identified and mitigated [4,7].  

Achieving equitable AI in healthcare requires a concerted and multi-faceted effort. This includes continued rigorous research into understanding bias mechanisms and developing more effective and robust mitigation strategies, particularly for challenging areas like distribution shifts and intersectional fairness [9,24]. Enhanced collaboration is essential, bridging government, healthcare institutions, technology developers, and the public to shape the creation, implementation, and operation of health AI [1,4]. Policy development must address regulatory challenges [15], promote transparency, accountability [13], and ensure socio-technical architectures support equitable data use [31]. Finally, responsible implementation is critical, demanding continuous monitoring and evaluation of AI models in real-world clinical settings to detect fairness degradation [15,19,24], implement effective integration strategies [21], prioritize ethical deployment principles [2], and foster vigilance and empathy in the AI development process [12]. Only through a comprehensive and proactive approach involving all stakeholders can AI truly become a powerful force for reducing healthcare disparities and promoting a more equitable and efficient healthcare system for all [5,7].  

# References  

[1] A Nationwide Network for Health AI Assurance Labs: https://jamanetwork.com/journals/jama/fullarticle/2813425   
[2] 医疗AI：有缺陷指标下的伦理困境与公平挑战 https://www.vbdata.cn/intelDetail/585707   
[3] Healthcare AI: Ethical Dilemmas and Flawed Metrics https://link.springer.com/article/10.1038/s41746-024-01242-1​   
[4] AI Risks Worsening Health Inequities for UK Ethnic https://www.imperial.ac.uk/news/230413/ai-could-worsen-health  
inequities-uks/   
[5] AI医学图像分析偏见：MIT研究揭示问题与解决方案 https://www.forwardpathway.com/108607​   
[6] Geographic Bias Threatens Medical AI Accuracy https://hai.stanford.edu/news/geographic-bias-medical-ai-tools​   
[7] 人工智能医疗偏差的识别与缓解策略综述 https://www.ebiotrade.com/newsf/2025-3/20250313073922801.htm​   
[8] Positive-Sum Fairness in Medical AI for Chest Radi https://hackernoon.com/new-study-shows-how-positive-sum-fairness   
impacts-medical-ai-models-in-chest-radiography​   
[9] Benchmarking Intersectional Bias in NLP Models https://www.aminer.cn/pub/634d80f190e50fcafd4ef3d3​   
[10] EHR与ACS数据对比：种族信息一致性及对健康差异研究的影响 https://www.ebiotrade.com/newsf/2025-   
4/20250423055939469.htm​   
[11] Foundation Models Reduce Bias in Pathology AI, Enh https://medicalxpress.com/news/2024-04-bias-pathology-ai  
algorithms-accuracy.html​   
[12] Mitigating Bias & the Eliza Effect: A UX Guide to  https://8thlight.com/insights/how-address-mitigate-bias-ai​   
[13] JAMA Network Guidance: Reporting AI Use in Researc https://jamanetwork.com/journals/jama/fullarticle/2816213​   
[14] AI医生：未来医疗的希望与挑战 https://roll.sohu.com/a/774715065_121124372​   
[15] AI医生时代：机遇与挑战 https://baijiahao.baidu.com/s?id $=$ 1802732114127325813&wfr=spider&for=pc​   
[16] 人工智能偏见：定义、类型、案例和解决办法 https://www.wbolt.com/ai-bias.html​   
[17] Fair Foundation Models for Medical Image Analysis: https://arxiv.org/html/2502.16841v1​   
[18] AI in Public Health: Equity and Ethical Considerat https://www.cdc.gov/pcd/issues/2024/24_0245.htm​   
[19] AI医疗诊断或加剧不平等，专家呼吁警惕偏见 https://www.vbdata.cn/intelDetail/220826​   
[20] Bias in Medical AI: Implications for Equitable Cli https://journals.plos.org/digitalhealth/article?   
id=10.1371/journal.pdig.0000651​   
[21] AI-Driven Cardiac Auscultation in Low-Income Setti https://journals.plos.org/digitalhealth/article?   
id=10.1371/journal.pdig.0000437​   
[22] Gender Bias in Large Language Models: An Investiga https://mip.sgpjbg.com/baogao/163355.html   
[23] AI Age Estimation Mirrors and Exaggerates Human Bi https://www.nature.com/articles/s41598-022-27009-w​   
[24] Fair Medical Imaging AI: Limits in Real-World Gene https://link.springer.com/article/10.1038/s41591-024-03113-4   
[25] ELSI Lens on Genomics, RWD, and Personalized Menta   
https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2024.1444084/full​   
[26] AI Bias: Examples, Types, and Fixes for 2025 https://research.aimultiple.com/ai-bias/​   
[28] AI Solutions for Global Health Challenges in the D https://www.frontiersin.org/journals/public  
health/articles/10.3389/fpubh.2023.1328918/full​  