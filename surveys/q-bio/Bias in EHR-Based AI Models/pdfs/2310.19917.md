# Review  

# Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models  

Feng Chen, $\mathbf { M S } ^ { * , 1 , 2 }$ , Liqin Wang, PhD1,3, Julie Hong, ${ \mathsf { H } } { \mathsf { S } } ^ { 4 }$ , Jiaqi Jiang, MS1, Li Zhou, MD, PhD1,3  

1Department of Biomedical Informatics, Harvard Medical School, Boston, MA 02115, United States, 2Department of Biomedical Informatics and Health Education, University of Washington, Seattle, WA 98105, United States, 3Division of General Internal Medicine and Primary Care, Brigham and Women’s Hospital, Boston, MA 02115, United States, 4Wellesley High School, Wellesley, MA 02481, United States Corresponding author: Feng Chen, MS, University of Washington, 4751 12th Ave NE, Apt 306, Seattle, WA 98105, United States (fengc9@uw.edu)  

# Abstract  

Objectives: Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data.  

Materials and Methods: We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Metaanalyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed met­ rics for bias assessment.  

Results: Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selec­ tion, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strat­ egies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting.  

Discussion: This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standar dized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.  

Key words: bias; artificial intelligence; deep learning; electronic health record; scoping review.  

# Background and significance  

The rapid advancement of artificial intelligence (AI) in healthcare, particularly through the utilization of large-scale, real-world electronic health records (EHRs) data, has revolu­ tionized medical research and clinical decision-making (CDS). Especially in the past decade, EHRs have become increasingly mature and widespread, providing a vast and rich source of data for AI development.1 Unlike AI applica­ tions aimed at enhancing the broader healthcare ecosystem, such as public health surveillance, drug discovery, and health­ care operations,2 AI models built from EHR data are more intimately connected with individual patient records and the CDS processes.3 EHRs, encompassing diverse patient data, including demographics, lab results, diagnoses, and treat­ ments, are uniquely suited for developing data-driven AI models for predictive analytics with applications ranging from risk identification and disease progression prediction, and outcome forecasting.5  

However, the integration of AI with EHR data, while cru cial for advancing CDS and medical research, confronts challenges from biases inherent in EHR data and AI models. These challenges include inconsistencies in documentation, variations in data quality, and model inaccuracies.6,7 Such inaccuracies, manifesting as analytical errors and skewed out­ comes often due to disproportionate dataset representation (eg, overrepresentation or underrepresentation of certain patient demographics, health conditions, or treatment types in the training data) can lead to differential performance across patient subgroups, potentially exacerbating healthcare disparities.8,9 Biases can emerge at various stages of the model development lifecycle,10–12 underlining the importance of developing and implementing strategies for effective detec­ tion, mitigation, and evaluation of biases.6,7 Tailoring these strategies to the unique characteristics of EHR data and AI models is essential for ensuring their efficient and equitable application, thereby facilitating successful AI integration in healthcare.13  

While a few scoping reviews have been conducted to understand bias in broader medical AI,14,15 a focused review on AI models derived from EHR data is notably absent. This gap underscores the need for a systematic review to identify, summarize, and propose strategies for managing these biases. This study aims to bridge this gap by analyzing existing research on AI biases within EHR-based models.  

# Objective  

This study aims to systematically review and synthesize the current literature on bias in AI models built from EHR data, focusing on the identification, evaluation, and mitigation strategies for major types of biases across the model develop­ ment cycle. The goal is to enhance understanding of AI bias management within EHR-based applications and highlight research directions to reduce potential AI impacts on health­ care disparities.  

# Materials and methods  

# Data sources and searches  

This review was conducted in compliance with the 2021 PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) guidelines, as illustrated in Figure 1. We conducted systematic searches on 3 relevant publication data­ bases (PubMed/MEDLINE, Web of Science, and the Institute of Electrical and Electronics Engineers) to retrieve articles published between January 1, 2010, and December 17, 2023. Search queries used for individual databases are available in Table S1.  

# Inclusion and exclusion criteria  

We included articles that: (1) were written in English; (2) con­ tained metadata (authors, title, publication year) and full text; (3) were published between January 1, 2010 and December 17, 2023; (4) focused on EHR-based AI models (ie, EHR data were used for model training, testing, and validation); and (5) evaluated bias, clearly describing its impact on healthcare dis­ parity and detailing bias handling approaches. Studies centered on imaging and device-related data were excluded due to the distinct nature of potential biases that arise from the format, collection, utilization, and interpretation of data between EHRs and medical images/devices.  

# Article screening process  

We retrieved titles and abstracts from 3 databases and removed the duplicates. Each of the remaining titles and abstracts was then screened by at least 2 reviewers (Feng Chen, Julie Hong, Jiaqi Jiang). The full-text screening was conducted by Feng Chen, Liqin Wang, and Julie Hong with each article reviewed by at least 2 reviewers independently. Any inconsistencies among reviewers were resolved through team meetings for consensus.  

# Data extraction  

Data was initially extracted by Feng Chen and Jiaqi Jiang and reviewed by Liqin Wang and Li Zhou. We thoroughly exam­ ined the full text of the remaining articles to extract pertinent components for our analysis. The data extraction process cov­ ered 3 key categories: bibliographic data, information related to the AI model, and specifics of bias and fairness. Bibliographic data included the title, authors, and year of publication of the study. Information related to the AI model comprised the source of EHR data, sample size, and the primary objectives and tasks of the AI models. Lastly, bias/fairness-specific information included the types of bias reported in each study, the strategies employed to detect or mitigate the bias, evaluation metrics used to measure the biases.  

# Categorization of bias types  

The categorization of bias types in EHR-based AI models involved a structured 2-step process. First, we examined the biases reported in the selected studies to reflect the current understanding of biases in AI research. This initial analysis provided a direct insight into the prevalent issues within the field. Second, we expanded our scope by integrating insights from the broader literature on healthcare AI, including works beyond our systematic review. Our analyses were further enhanced by consolidating established bias risk assessment tools, such as ROBINS-I,16 ROBINS-E,17 and PROBAST,18 and relevant review articles, such as Mehrabi et $\mathsf { a l } ^ { 1 9 }$ which categorized potential biases in AI applications. By combining these methodologies, we defined and categorized the major types of bias present in EHR-based AI models, ensuring a thorough understanding of the bias landscape in this domain.  

# Bias analyses workflow construction  

We created a framework, illustrated in Figure 2, to categorize and examine methods for addressing bias in AI model devel­ opment. This framework identifies the major potential biases (examined from the above step) that may arise during 3 key stages: data collection and preparation, model training and testing, and model deployment. For each stage, we identified specific types of bias and reviewed targeted mitigation strat­ egies. These strategies are classified into preprocessing, inprocessing, and post-processing methods, corresponding to the respective stages of AI model development, ensuring a structured approach to bias management throughout the life­ cycle of AI models.20,21  

# Results  

This section outlines the major types of bias identified through our literature review, provides definitions and explanations for each, and then presents our detailed analyses of the studies included in the review.  

# Types of bias  

We identified 6 primary types of bias potentially present in the development of EHR-based AI models:  

Implicit bias: Also known as prejudice bias, implicit bias occurs automatically and unintentionally.22 It often stems from preexisting bias within the data (such as stereotypes and flawed case assumptions) which may occur in the data collection and data preprocessing steps. Utilizing biased data inevitably leads to biased outcomes. Racial bias, gen­ der bias, and age bias fall under implicit bias.  

Selection bias: This type of bias, also known as sampling bias or population bias, occurs when individuals, groups, or data used in analysis are not properly randomized dur­ ing data preparation.23,24 For instance, if an AI model is used to predict the mortality rate of patients with sepsis across the US but is only trained by data from a single hos­ pital in a specific geographical area, it may not generalize well to a broader population, leading to skewed and inac­ curate predictions.  

![](images/a466d785d7413b457a61df5ff29628b0328e738eb65c99843184a69060a43c5b.jpg)  
Figure 1. PRISMA flow diagram.  

Measurement bias: This bias usually arises during the data collection stage of a study, often due to inaccuracies or incom­ pleteness in data entries from clinicians or clinical devices.25 Incorrect or biased labels resulting from coding errors or sub­ jective interpretations by annotators can also impact the per­ formance and validity of the machine learning models.  

Confounding bias: Also known as association bias, con­ founding bias is a systematic distortion between an exposure and health outcome by extraneous factors.24 Confounding bias could be introduced when collecting data as well as training models. For example, in a study predicting the read­ mission of patients, there could be confounding bias in the data related to socioeconomic status, as people with lower socioeconomic status may have limited access to healthcare resources making them more likely to have worse medical conditions. In this scenario, socioeconomic status is related to both the input medical conditions and the model predictions. Algorithmic bias: This form of bias arises when the intrin­ sic properties of a model and/or its training algorithm cre­ ate or amplify the bias in the training data. Algorithmic bias can create or amplify bias due to various factors, including imbalanced or misrepresentative training data, improper assumptions made by the model, lack of regula­ tion in model processing, and so on.7 For instance, linear regression models performing prediction on clinical data with complex features failing to meet the Gaussian distri­ bution assumption might cause bias.  

Temporal bias: This bias occurs when sociocultural preju­ dices and beliefs are systematically reflected,26 especially when historical or longitudinal data is used to train mod­ els.19 Such data likely embodies different healthcare prac­ tice patterns, outdated treatment and test records, differ­ ent disease progression stages, and obsolete data recording processes that may negatively impact the performance on current data. Temporal bias could be introduced at any stage of AI application development.  

# Article selection and screening results  

Figure 1 outlines the article selection and screening process. We initially identified 450 articles, of which 92 were dupli cates, leaving 358 articles. Upon abstract screening, 232 papers were excluded for not meeting our review criteria, including: review papers $( n = 4 2 )$ ), perspective articles $( n = 3 6 )$ ), off-topic articles $( n = 1 0 8 )$ ), qualitative studies $( n = 3 5 )$ ), abstract-only $( n = 9 )$ , and preprint articles $( n = 3 )$ . Additionally, 8 articles were excluded due to unavailability of the full text. 117 studies underwent full-text review, and 97 were further excluded, due to using non-EHR data $( n = 1 6 )$ , lack of clear methods to detect or mitigate bias in AI models $( n = 6 6$ ), and failure to directly evaluate bias impact $( n = 1 4 )$ ). One paper was retracted before we finalized this review. Ultimately, 20 articles were included in the final analysis.  

![](images/a162e285b36af884dc67f854725606b4fd150adb2bed43d631331c1e56e81ee7.jpg)  
Figure 2. Bias handling workflow in artificial intelligence (AI) model development. The pipeline for the AI applications is shown in the blue box. Possible types of bias that could be introduced in each step are shown in the orange box, while possible handling approaches during each step of the model development are shown in green. Preprocessing handling methods refer to analyzing and adjusting the existing data set to preempt biases resulting from inadequate data during data collection and preparation. In-processing handling methods aim to handle bias during the model training and testing to avoid bias during training and eliminate bias from input data. Postprocessing handling methods account for handling model biases by interpreting or adjusting model outputs and correctly making use of the result. Approaches marked with  were approaches that were not covered by papers in this review.  

# Research trends over time  

The publication trends over the years are shown in Figure S1. All the final included articles were published after 2014. Nota­ bly, more than half of the papers (14 out of 20) were published in 2022 and 2023, indicating a growing interest in emphasiz­ ing bias and fairness when using EHR data in recent years.  

# Tasks of research  

To understand how bias might have influenced AI applica­ tions, we categorized the principal tasks of the AI models in each study, as detailed in Table 1. These EHR-based AI predictive tasks include diseases’ diagnosis or risk predic­ tion,24,73–23–1 treatmoertnatliteyffectoror sdiusrevaisveal propgredsiscitoinon,p5r,3e6d–i3c9­ medication or test usage prediction, diseases’ risk-association prediction,40,41 health status classification,42 and EHR miss­ ingness imputation.43,44  

# Datasets  

The datasets used in these studies were varied. Only 1 study utilized a commercial EHR dataset called Flatiron Health database.43 The other studies used datasets derived from individual hospitals or integrated healthcare delivery systems. The size of the datasets varied drastically, with the largest dataset being over 1000 times larger than the smallest one. For example, Juhn’s study enrolled a population of 555,27 while Wolk’s research was on a dataset of over $6 0 0 \ 0 0 0$ patients.28 Five studies $( 2 5 \% )$ used 2 different EHR datasets for validation and evaluation.29,31,37,41,44 Three studies pub­ lished in $2 0 2 3 ^ { 3 7 , 4 1 , 4 4 }$ indicate an increasing trend in address­ ing the generalizability and robustness of bias handling.  

# Bias evaluation metrics  

The studies included in this review employed a diverse array of metrics to assess bias, as detailed in Tables 2 and 3, with definitions described in Table 4. Eight studies $( 4 0 \% ) ^ { 4 , 5 , 2 8 , 2 9 , 3 2 , 3 3 , 3 7 , 4 2 }$ applied only performance metrics, such as sensitivity, specificity, accuracy, and area under receiver operating characteristic curve (AUROC), mean squared error (MSE). The remaining 12 $( 6 0 \% )$ employed fairness metrics and all of them focused on group fairness which tests for some form of statistical parity (eg, between positive outcomes, or errors) for members of different pro­ tected groups.45 The classification of group-based fairness metrics20 has 3 criteria: independence, separation, and suffi­ ciency. Among the studies using fairness metrics, 330,34,40 applied parity-based metrics to measure the independence of the models from group information, which directly looks at how independent the predicted value among different groups. Nine27,30,31,34–36,38–40 utilized confusion matrix-based met­ rics to measure the separation, which is the independence of the predicted value and groups conditional on true value. One used calibration-based metrics to measure the suffi­ ciency, which is the independence of the true value and groups conditional for a given predicted value.31 Two used score-based metrics to directly compare the predicted value.41,43  

Table 1. Summary of the topics and datasets of those reviewed articles.   


<html><body><table><tr><td>Article</td><td>Task for AI model</td><td>Data sources</td><td>Sample size</td></tr><tr><td>Karlsson et al32</td><td>Prediction of adverse drug events</td><td>Stockholm Electronic Patient Record Corpus, Sweden</td><td>N/A</td></tr><tr><td>Hee et al</td><td>Mortality prediction of 9 common diseases</td><td>MIMIC-III, MA, United States</td><td>40000</td></tr><tr><td> Zhu et al33</td><td>Prediction of hospital re-admission rate</td><td>Over 10 South Florida regional hospitals,FL, United States</td><td>92 827</td></tr><tr><td>Huda et al42</td><td>Classification of the health condition or ability</td><td>Prospective pilgrims in Purworejo,Indonesia</td><td>2425</td></tr><tr><td>Allen et al36</td><td>Mortality prediction</td><td>MIMIC-III,MA,United States</td><td>28 460</td></tr><tr><td>Khoshnevisan et al29</td><td>Early detection of septic shock</td><td>Christiana Care Health System; Mayo Clinic, United States</td><td>61 848; 74 463a</td></tr><tr><td> Juhn et al27</td><td>Risk prediction of asthma exacerbation</td><td>Mayo Clinic,United States</td><td>555</td></tr><tr><td>Wolk et al28</td><td>Risk prediction of influenza complications</td><td>Geisinger,PA, United States</td><td>604389</td></tr><tr><td>Getz et al43</td><td>Missingness imputation of metastatic urothelial carcinoma patients'EHR</td><td>Flatiron Health database,United States</td><td>3858</td></tr><tr><td>Meng et al38</td><td>Mortality prediction for ICU admitted patients</td><td>MIMIC-IV,MA, United States</td><td>43005</td></tr><tr><td> Wang et al39</td><td>Mortality prediction for sepsis patients</td><td>MIMIC-III, MA, United </td><td>11 719</td></tr><tr><td>Roosli et al31</td><td>In-hospital mortality risk prediction</td><td>MIMIC-III; STARR, United States</td><td>18 094; 6066</td></tr><tr><td>Jiang et al37</td><td>Mortality prediction of respiratory system diseases</td><td>MIMIC-IV; eICU, United States</td><td>11 167; 35 175</td></tr><tr><td>Li et al40</td><td>Risk factors prediction of post-liver transplant</td><td>Organ Procurement and Transplantation Net- work,United States</td><td>160 360</td></tr><tr><td>Lee et al4</td><td>Disease progression prediction of glaucoma</td><td>Department of Ophthalmology in NYU Lan- gone Health,NY, United States</td><td>785</td></tr><tr><td>Li et al30</td><td>Risk prediction of cardiovascular disease</td><td>Vanderbilt University Medical Center,TN, United States</td><td>109 490</td></tr><tr><td>Davoudi et al34</td><td>Pain level prediction of acute postoperative pain</td><td>University of Florida Health System/Shands Hospital,FL,United States</td><td>14 263</td></tr><tr><td>Cui et al41 Yin et al44</td><td>Bipartite ranking for coronary heart disease</td><td>MIMIC-III; eICU, United States</td><td>21 139;17 402</td></tr><tr><td></td><td>Missingness imputation of temporal EHR data</td><td>MIMIC-III; eICU, United States</td><td>13 112;10 162</td></tr><tr><td>Raza and Bashir35</td><td>ICU readmission prediction</td><td>MIMIC-III,MA, United States</td><td>6500</td></tr></table></body></html>

a Khoshnevisan et al, Ro€€osli et al, Jiang et al, Cui et al, and Yin et al had data from 2 dataset with different sample size  

# Bias detection and mitigation characteristics  

Of the 20 studies analyzed in this review, 11 $( 5 5 \% )$ pertained to implicit bias,4,27,28,30–32,35,36,38–40 6 $( 3 0 \% )$ pertained to selection bias,5,33,40,42–44 6 $( 3 0 \% )$ to algorithmic bias,28,31,34,38,39,41 1 to confounding bias,29 1 to measure­ ment bias,29 and 1 to temporal bias.37 Six studies $( 3 0 \% )$ cov­ ered 2 types of bias,28,29,31,38–40 while the rest focused on a single type of bias.  

Of the 20 studies analyzed in this review, 5 $( 2 5 \% ) ^ { 2 7 , 3 1 , 3 5 , 3 8 , 3 9 }$ only detected the presence of bias in AI models by introducing quantitative measurement to identify and explain source of bias, as listed in Table 2. The other 15 (75%)4,5,27–30,33,34,36,37,40–44 sought to mitigate biases. Among these mitigation approaches, 12 out of 15 $( 8 0 \% )$ reported improved performance after bias mitigation. In con­ trast, 2 studies $( 1 3 . 3 \% )$ observed that performance remained largely unchanged after bias mitigation,34,43 and 1 $( 6 . 7 \% )$ found performance variability based on the evaluation met­ rics used.32 Bias handling approaches and mitigation results are summarized in Table 3.  

Figure 2 demonstrates the EHR-based AI application development stages, potential biases at each stage and corre­ sponding bias mitigation approaches. When aligning the included articles with each stage, the details of each study’s approach were described in Table 3. The bias mitigation methods of 11 studies (73.3%)5,28–30,32,33,36,37,42–44 fell within the preprocessing step, using approaches including resampling, reweighting, transformation, relabeling, and blinding. Three studies $( 2 0 \% ) ^ { 4 , 3 4 , 4 0 }$ developed in-processing approaches including transfer learning and reweighting. Only 1 study $( 6 . 7 \% )$ applied a postprocessing bias mitigation method using transformation to identify bias from the model application.41  

# Discussion  

AI’s growing role in healthcare has become particularly evi­ dent due to its unprecedented capability to harness EHR data for various tasks, such as mortality prediction, disease pro­ gression prediction, and risk assessment.46 On the other hand, AI models may exhibit biases, and if left unaddressed, they have the potential to exacerbate healthcare disparities. There is an urgent need to thoroughly evaluate possible biases produced by AI models,47 and assess the fairness of AI meth­ ods to reduce healthcare disparities.48,49 In this systematic review, we have comprehensively identified and scrutinized potential biases in AI models developed from EHR data, revealing their potential influence on healthcare disparities. Our findings underline 6 major types of bias, with a notable focus on implicit and selection biases, and present an array of methods for their detection and mitigation. The study empha­ sizes the critical need for standardized, generalizable, and interpretable frameworks to ensure fairness in healthcare AI. By highlighting these issues, this review advocates for action­ able strategies to promote equity in healthcare outcomes by improving the fairness of EHR-based AI models.  

# Major types of bias in AI models using EHR data  

In this review, all 6 types of bias have been covered by at least 1 study, with most focused on implicit and selection biases. In recent years, more studies have explored confounding and algorithmic biases, which require a deeper understanding of data and models used in AI applications. Although confound­ ing and algorithmic biases have been extensively studied in the fields of computer science and statistics,50–52 their distinct impact on health disparities has not been thoroughly explored when applied to EHR data. This research gap underscores the need for a focused investigation into how these biases specifically impact health disparities within the context of EHR data analytics. While there is only 1 paper in this review37 focused on temporal bias, other possible approaches to mitigate temporal bias might be feasible, including resampling data, transforming time-related fea­ tures, and retraining the model or updating the learning rate during training to better learn the time-dependent data.53,54 No study has yet developed a comprehensive pipeline to address more than 2 types of bias simultaneously. While pre­ vious research has concentrated on identifying and mitigating specific biases, future research may aim to tackle multiple biases concurrently throughout the model development process.  

Table 2. Paper summary for bias detection approaches.   


<html><body><table><tr><td>Article</td><td>Bias type</td><td>Evaluation metrics</td><td>Bias detection method</td><td>Bias detection summary</td></tr><tr><td> Juhn et al27</td><td>Implicit bias</td><td>Accuracy, equal opportunity, equal odds, predictive equity</td><td>HOUSES is applied as a new feature,an individual-level SES measure based on 4 real property data variables of an individual housing unit after principal compo-</td><td>Asthmatic children with lower SES have larger balanced error rate than those with higher SES and has a higher proportion of missing infor- mation relevant to asthma care.</td></tr><tr><td>Roosli et al31</td><td>Implicit bias; algorithmic bias</td><td>Statistical parity, calibration-in-the-large</td><td>nent factor analysis. Statistical parity based on AUROC and AUPRC,and calibration are measured for 3 steps of model vali- dation,respectively.</td><td>The predictive model is found hard to classify minority class instances correctly and fairly.</td></tr><tr><td>Wang et al39</td><td>Implicit bias; algorithmic bias</td><td>Statistical parity</td><td>The AUROC differences are com- pared between the entire cohort and those on the subpopulations by per- mutation tests.</td><td>Performance decreases for mortality prediction for minority racial and socioeconomic groups.</td></tr><tr><td>Meng et al38</td><td>Implicit bias; algorithmic bias</td><td>Statistical parity</td><td>AUROC for models and feature importance across different groups and the whole are compared.</td><td>Performance differences exist among different groups and models rely on different racial attributes across groups.</td></tr><tr><td>Raza et al35</td><td>Implicit bias</td><td>Equal opportunity, predictive equity,proportion parity, false negative rate parity</td><td>Confusion metrics are calculated and compared for each subgroup after training on the whole data.</td><td>Disparity based on 4 fairness evalu- ation,disparity among different socioeconomic groups is observed in the readmission prediction.</td></tr></table></body></html>

Abbreviations: AUPRC $\ c =$ area under precision-recall (PR) curve, AUROC $\ b =$ area under the receiver operating characteristic curve, HOUSE index $\ b =$ HOUsing-based SocioEconomic Status measure, ${ \mathrm { S E S } } =$ socio-economic status.  

# Bias evaluation and fairness assessment  

The bias evaluation methods in the reviewed studies highlight a methodological disparity, with approximately half employ­ ing fairness metrics to assess biases among groups. The rest relied on general performance metrics like accuracy, sensitiv­ ity, and specificity, which, while useful for evaluating overall model performance, may not detect subtle but significant dis­ parities between groups.55 This oversight can mask the differ­ ential impacts of AI models on diverse patient populations, inadvertently overlooking potential biases. Fairness metrics, designed to evaluate the equitable distribution of AI out­ comes across demographic or clinical groups, are crucial for uncovering these disparities. As the field progresses, integrat­ ing fairness metrics with traditional performance metrics is vital to cultivate AI models that are both effective, and fair, ultimately supporting the goal of equitable healthcare through AI in EHR data analysis.  

# The role of bias detection  

Bias detection in this review primarily relied on analyzing performance differences among groups, with 1 study intro­ ducing the HOUSE index27 as an innovative feature to better capture socioeconomic status and related biased features in the original data. These methods can effectively reveal implicit and algorithmic biases within AI models. Yet, their capacity to uncover hidden data patterns leading to bias remains limited. Two recent studies in $2 0 2 3 ^ { 5 6 , 5 7 }$ that aimed to develop AI methods that directly detect and elucidate more nuanced bias in EHR data. Although these studies were not evaluated within AI models and hence were not included in this review, they signify a promising direction for the devel­ opment of new technologies to detect more complex and nuanced biases in EHR data.  

# Navigating bias mitigation approaches  

Bias mitigation approaches in this review are categorized into 3 stages: preprocessing, in-processing, and postprocessing. Among these, preprocessing approaches are the most used approach to handle bias. These methods offer tangible, earlystage bias mitigation and might be readily adapted to clinical workflows. Resampling and reweighting, 2 major methods in this category, effectively address class or group imbalance by modifying the distribution of the training data. Other prepro­ cessing strategies covered by this review include transforma­ tion to recover missing data, relabeling, and domain adaptation. Integrating multiple preprocessing approaches within a single pipeline to effectively address various bias types presents an opportunity for future research. However, they might be less effective in addressing feature correlations and may even lead to data loss. Thus, they are limited in miti­ gating confounding, algorithmic, and temporal bias.  

<html><body><table><tr><td colspan="6">Table 3.Paper summary for bias mitigation approaches by bias type.</td></tr><tr><td>Article</td><td>Bias handling approach</td><td>Evaluation metrics</td><td>Bias research question</td><td>Bias handling method</td><td>Bias handling result</td></tr><tr><td>Implicit bias Allen et al36</td><td>Preprocessing:</td><td>Equal opportunity</td><td>Racial bias exists between White and non-</td><td>Individual training examples are given</td><td>The model reduces racial bias and improves accuracy</td></tr><tr><td></td><td>reweighting (reweight based on probability)</td><td></td><td>White racial groups in the early warning and mortality scoring systems. The data sparsity creates</td><td>weights based on mor- tality status and race within each age strata using a reweighting scheme. In the first resampling</td><td>comparing with existing mortality score predictors. Both approaches mitigate</td></tr><tr><td>Karlsson et al32</td><td>Preprocessing: resampling</td><td>F1-score, AUROC,MSE</td><td>class bias and causes the prediction result to favor the majority class.</td><td>approach,m new fea- tures are resampled a maximum of n times if no informative feature is found, while in the sec- ond resampling approach i feature is resampled until an infor- mative feature is found or until there are no</td><td>class bias. The suggested choice of approach to han- dle sparsity is highly dependent on the evalua- tion metrics.</td></tr><tr><td>Lee et al4</td><td>In-processing: transfer learning</td><td>MAE,MSE</td><td>Imbalanced ophthalmic clinical data arises healthcare disparities among racial groups.</td><td>more features. Two-step transferred learning is used: first is transferring the infor- mation from source domain by fitting the linear model on the combined set and then correcting the bias by fitting the contrast solely</td><td>Compared with conven- tional approaches,transfer learning achieves better performance in MAE and MSE.</td></tr><tr><td>Li et al30</td><td>Preprocessing: resampling, blinding</td><td>Equal opportu- nity,statistical parity</td><td>Risk assessment predic- tive models can be biased because of sys- tematic bias in the train- ing data.</td><td>on the target domain. Three different approaches are used: removing protected attributes,resampling the imbalanced training dataset by sample size, and resampling by the</td><td>Removing protected attrib- utes and resampling by sample size didn't signifi- cantly reduce the bias. Resampling by case pro- portion reduced the EOD</td></tr><tr><td>Selection bias Zhu et al33</td><td>Preprocessing:</td><td>AUROC</td><td>The population is highly</td><td>proportion of people with CVD outcomes. Localize sampling that uses LDA embedding</td><td>and DI for gender groups but slightly reduced accu- racy in many cases. Localized sampling helps to</td></tr><tr><td>Huda42</td><td>resampling (localize sampling) Preprocessing:</td><td>Accuracy</td><td>skewed and biased for model because re-admis- sion patients is a small proportion. Imbalanced datasets</td><td>assesses the locality of instances and allows the sampling process to bias to such instances. SMOTE oversamples</td><td>solve the sample imbalance issue for effective hospital re-admission prediction. The combination of</td></tr><tr><td></td><td>resampling (SMOTE+ Neu- ral Network)</td><td></td><td>cause bias in Istitaah classification system.</td><td>minority classes by cre- ating a set of a new instance from minority class by interpolating several minority classes instances around the existing samples.</td><td>SMOTE and Neural Net- work gains the balanced dataset and was the most accurate in classification.</td></tr><tr><td>Hee et al5</td><td>Preprocessing: resampling (strati- fied random sampling)</td><td>AUROC, accuracy</td><td>Data quality assurance methods are needed to reduce bias when reus- ing clinical data for mortality prediction.</td><td>CDQA and MDQA identify most relevant variables to conduct stratified random sampling.</td><td>CDQA and MDQA stratify sampled inputs and improve predictive results in AUC and accuracy.</td></tr><tr><td>Getz et al43</td><td>Preprocessing: transformation (multiple imputation)</td><td>Percent bias</td><td>Missingness not at ran- dom may cause bias in machine learning model.</td><td>Missing values are imputed using multiple imputation using chain equation, random forest and denoising autoen-</td><td>Denoising autoencoders does not outperform the traditional multiple impu- tation methods.</td></tr></table></body></html>  

Table 3. (continued)   


<html><body><table><tr><td>Article</td><td>Bias handling approach</td><td>Evaluation metrics</td><td>Bias research question</td><td>Bias handling method</td><td>Bias handling result</td></tr><tr><td>Yin et al44</td><td>Preprocessing: transformation (data imputation)</td><td>AUPRC,MAE</td><td>Traditional models typi- cally condition their model predictions on the partial observations and observational bias</td><td>The proposed model uses 3 subnetworks to impute missing data by propensity score adjustment.</td><td>The model outperforms current methods in binary data imputation,disease progression modeling,and mortality prediction.</td></tr><tr><td>Algorithmic bias Davoudi et al34</td><td>In-processing: reweighting</td><td>Equal opportu- nity, predictive</td><td>performance. Predictive models sys- tematically predict an outcome more likely for</td><td>The weights of observa- tions in each attribute- outcome combination in</td><td>Reweighing the prediction models based on each pro- tected attribute reduce the</td></tr><tr><td>Cui et al41</td><td>Post-processing: transformation (xOrder)</td><td>xAUC</td><td>some socioeconomic groups. Ranking models that rank positive instances higher than negative ones with poor fairness</td><td>adjusted. The framework utilizes dynamic programming for adjusting ranking scores to optimize the ordering by minimizing</td><td>introduced bias in some other cases where there was no bias. The framework consis- tently achieves a better bal- ance between the algorithm utility and ranking fairness</td></tr><tr><td>Temporal bias</td><td></td><td></td><td>protected groups. AUROC,AUPRC， Temporal bias can be</td><td>an objective comprising a weighted sum of algo- rithm utility loss and ranking disparity. The model aligned</td><td>with different metrics.</td></tr><tr><td></td><td>Preprocessing: transformation (time alignment)</td><td>F1-score</td><td>introduced to longitudi- nal EHR data when treating patients with different disease pro- gression state.</td><td>patients'timeline to a shared timeline not sim- ply treating hospital/ ICU admission time as the“time-zero.”</td><td>This time-alignment regis- tration method enhances mortality prediction with at least a 1%-2% increase in evaluation metrics.</td></tr><tr><td colspan="2">Implicit bias and selection bias In-processing: reweighting Li et al40</td><td>AUROC,AUPRC, statistical parity,</td><td>Fairness discrepancy exists among sensitive attributes in predicting post-liver transplant risk</td><td>Used fairness metrics including demographic disparity and equalize odds to measure dispar-</td><td>The algorithm reduced fair- ness disparity among all sensitive attributes and achieve task balance while</td></tr><tr><td></td><td>reweighting)</td><td></td><td>factors.</td><td>ity and optimize through iteration and reweight to balance the tasks in the loss function.</td><td>maintaining accuracy.</td></tr><tr><td colspan="8">Wolk et al28 Preprocessing: reweighting</td></tr><tr><td></td><td>(inverse propensity score weighting) Confounding bias and measurement bias</td><td></td><td>tivity for White than for Black individuals. General scoring systems</td><td>rect for over-estimation on unvaccinated due to unrelated complications. The model is a VRNN-</td><td>model and mitigate bias among different populations. The model outperforms</td></tr><tr><td colspan="8">Khoshnevisan and Preprocessing: relabeling (domain Chi29</td></tr><tr><td></td><td>adaptation)</td><td>specificity,F1- score</td><td>sensitivity and specificity for identifying high risk septic shock patients due to covariate shift</td><td>Domain Separation con- struction, which sepa- rates 1 global-shared representation for all domains from local</td><td>adaptation methods to address both covariate shift and systematic bias.</td></tr></table></body></html>

Abbreviations: $\mathrm { { C D Q A } = }$ contextual data quality assurance, $\mathrm { C V D = }$ cardiovascular disease, $\mathrm { D I = }$ disparity impact, $\mathrm { E O D = }$ equal opportunity difference, Gflu-CxFlag $\ c =$ Geisinger flu-complications flag, $\mathrm { { I C U } = }$ intensive care unit, $\mathrm { \ L D A = }$ latent Dirichlet allocation, $\mathbf { \bar { M A E } = }$ mean absolute error, $\dot { \mathrm { M D Q A } } =$ mutual data quality assurance, $\mathbf { M S E } =$ mean squared error, $\mathrm { S M O T E } =$ synthetic minority oversampling technique, ${ \mathrm { V R N N } } =$ variational recurrent neural network, $\mathbf { x R O C } = \mathbf { \dot { C } R O S S }$ -area under the cross-operating characteristic curve.  

In-processing approaches offer dynamic strategies for bias mitigation during model training. Reweighting could dynami­ cally reduce bias by changing the importance given to differ­ ent training examples to focus more on the underrepresented classes or less on the overrepresented ones, helping to ensure that the model does not simply learn to predict the majority class or the bias present in the training data. Transfer learn­ ing, as proposed by Lee et al,4 is particularly useful to miti­ gate bias with limited data, as this approach leverages models pre-trained on large datasets to enhance performance. Besides these, several other in-processing approaches, proposed for machine learning models, might also be feasible, including constraint optimization to impose fairness constraints during the learning,51 regularization to avoid overfitting,21,58,59 and adversarial learning to determine the fairness of the train­ ing.60–62  

Table 4. Evaluation metrics covered in reviewed articles.   


<html><body><table><tr><td>Term</td><td>Description</td></tr><tr><td colspan="2">Performance metrics</td></tr><tr><td>AUROC</td><td>Assessing the overall performance by measuring the area under the probability curve that plots the TPR against FPR at</td></tr><tr><td>Accuracy</td><td>various threshold. Measuring the correct proportion of pre- dicted results.</td></tr><tr><td>F1-score</td><td>Measuring both the PPV and TPR.</td></tr><tr><td>Sensitivity</td><td>Same as the TPR,which measures the probability that the test assigns a diseased</td></tr><tr><td>Specificity</td><td>individual as positive. Same as the FNR of the model.</td></tr><tr><td>MAE</td><td>Measuring the average of the absolute dif- ference between the predicted values and the actual values.</td></tr><tr><td colspan="2">Fairness metrics</td></tr><tr><td>Calibration-based metrics</td><td>Measuring the diference in average pre-</td></tr><tr><td>Calibration-in-the-large</td><td>dicted and observed risk in each group.</td></tr><tr><td colspan="2">Score-based metrics Percent bias</td></tr><tr><td></td><td>Measuring the average tendency of the pre- dicted values to be larger or smaller than actual ones.</td></tr><tr><td>xAUC</td><td>A cross-group evaluation based on AUC that calculates how well the model distin- guishes between positive examples in</td></tr><tr><td></td><td>group A and negative examples in group B and vice versa.</td></tr><tr><td colspan="2">Confusion Matrix-based metrics Equal odds</td></tr><tr><td></td><td>Measuring both TPR and FPR across groups,which is the percentage of actual negatives that are predicted as positive.</td></tr><tr><td>Equal opportunity Predictive equality</td><td>Representing the equal TPR across groups. Also known as predictive parity,which</td></tr><tr><td></td><td>measures the difference of FPR across dif- ferent groups.</td></tr><tr><td>Proportion parity</td><td>Measuring whether each group is repre- sented proportionally to its share of the population.</td></tr><tr><td>False negative rate parity</td><td>Measuring the FNR across different groups.</td></tr><tr><td>Parity-based metrics</td><td>Also known as disparate impact or demo-</td></tr><tr><td>Statistical parity</td><td>graphic parity that measures the difference</td></tr><tr><td></td><td>in probabilities of a positive outcome</td></tr></table></body></html>

Abbreviations: $\mathrm { A U R O C = }$ area under the receiver operating characteristic curve, $\mathrm { F N R } =$ false negative rate, $\mathrm { { F P R } = }$ false positive rate, $\mathbf { \bar { M A E } } =$ mean absolute value, $\mathrm { M S E } \bar { = }$ mean squared error, $\mathrm { \Delta N P V = }$ negative predictive value, $\mathrm { P P V } =$ positive predictive value, $\mathrm { T P R } =$ true positive rate, $\mathbf { x A U C } =$ CROSS-area under the cross-operating characteristic curve.  

In this review, only 1 study41 employed postprocessing methods to adjust model output, an area currently underutil­ ized. A previous survey20 pointed out several other possible approaches to mitigate bias in machine learning models, including applying calibration to correct bias in the predicted probabilities,63–65 and choosing proper thresholds to make decisions.66–68 Postprocessing methods present a flexible and adaptable way to assess fairness, especially in black-box AI models. Future research should focus on robust postprocess­ ing methods to effectively detect, mitigate, and clarify bias, which could greatly contribute to fairness in EHR-related AI applications.  

# Research challenges and future directions  

Addressing bias in AI models for healthcare purposes is a rel­ atively recent research focus: there is a limited body of research directly dedicated to this crucial subject. A pressing need exists for establishing comprehensive guidelines for the use of evaluation metrics in AI models built from EHR data. These should include fairness evaluations to mitigate dispar­ ities among groups, alongside traditional performance met­ rics. Also, studies covered in this review all focused on group fairness.69 Nevertheless, considering personalized healthcare using EHR data, future research efforts may focus more on developing and evaluation AI models which are fair to indi­ viduals.45 None of these AI applications reviewed have been implemented in clinical settings. Further assessment is required to evaluate their practical use. Future research should focus on the following key areas to improve bias man­ agement in healthcare AI models:  

1) Data quality examination: Ensuring the quality of EHR data is crucial for both clinical research and healthcare applications that might have a direct impact on patient care. Further research should continue to focus on the methods for improving the data quality like data cleaning, sampling and missingness imputation discussed in this review. This is crucial to reduce bias stemming from the quality of EHR data.   
2) Bias detection and mitigation pipeline: Future research should attempt to develop a comprehensive pipeline that detects and mitigates multiple biases at each step of AI application development. Bias handling approaches should be integrated at each stage of the model develop­ ment process, including data collection and preparation, model training and testing, model deployment, and result interpretation.   
3) Explainable bias for AI applications: There is a need for more transparent AI models built from EHR data that can explain how and why biases occur. Approaches include statistics analysis, feature importance assessment, and eth­ ical fairness considerations. Improving the interpretability will help to build fairer and more understandable clinical AI models for patients’ care.   
4) Validation of AI model: Validating AI models on diverse datasets is crucial to ensure their generalizability and robustness. This includes developing applications that are adaptable to a broader range of communities on multiple datasets and validating the influence of models in clinical environments.  

# Limitations  

First, despite comprehensive searches across 3 major data­ bases, the limitations inherent in our keyword search strategy may have omitted potentially relevant papers. Second, the rel­ atively modest number of papers included could limit the breadth of our analysis, which is likely due to the nascent stage of research within the scope specified by this review. Nonetheless, the detailed examination of each paper offers valuable insights into the current state of methodologies and their inherent constraints. Third, the interpretations and sum­ marizations within this review are based on the authors’ knowledge and understanding of the focused research area, introducing an element of subjectivity. Lastly, it is important to note that none of the models discussed in this review were tested in actual clinical settings, and as such, the impact of their bias handling methods on clinical outcomes has not yet been rigorously measured. The translation of these models into practice may present unforeseen challenges and learning opportunities that can only be identified through real-world application and evaluation.  

# Conclusion  

Bias in EHR-derived AI models has gained increasing atten­ tion by the research community. This review highlights cur­ rent achievements and emphasizes the need for more in-depth research focused on developing standardized, generalizable and interpretable methods for detecting, mitigating and eval­ uating bias in AI models. As AI’s presence in healthcare grows, it becomes critical to ensure that these technologies are equitable, thereby minimizing the risk of healthcare dis­ parity due to biases. Continued efforts in this area are essen­ tial for improving the healthcare equity and maximizing the benefits of AI for healthcare.  

# Author contributions  

The following authors contributed to the following tasks: Li Zhou (Idea conception); Jiaqi Jiang, Liqin Wang, Feng Chen (development of database queries); Feng Chen, Jiaqi Jiang (article collection); Feng Chen, Jiaqi Jiang, Liqin Wang (article annotation); Feng Chen, Liqin Wang, Li Zhou (devel­ opment of analysis methods); Feng Chen, Jiaqi Jiang (data analysis); Feng Chen, Jiaqi Jiang, Liqin Wang, Li Zhou (manuscript writing); and Feng Chen, Liqin Wang, Li Zhou (manuscript revision).  

# Supplementary material  

Supplementary material is available at Journal of the Ameri­ can Medical Informatics Association online.  

# Funding  

This work was supported by the National Library of Medi­ cine under Grant No. 1R01LM014239.  

# Conflicts of interest  

None declared.  

# Data availability  

The data underlying this article are available in the article and in its online supplementary material.  

# References  

1. Adler-Milstein J, Jha AK. HITECH act drove large gains in hospi­ tal electronic health record adoption. Health Aff (Millwood).   
2017;36(8):1416-1422.   
2. Topol EJ. High-performance medicine: the convergence of human and artificial intelligence. Nat Med. 2019;25(1):44-56. cine. 2019;46:27-29.   
4. Lee T, Wollstein G, Madu CT, et al. Reducing ophthalmic health disparities through transfer learning: a novel application to over­ come data inequality. Transl Vis Sci Technol. 2023;12(12):2.   
5. Hee K. Is data quality enough for a clinical decision?: apply machine learning and avoid bias. In: 2017 IEEE International Conference on Big Data (Big Data). IEEE; 2017:2612-2619.   
6. Vokinger KN, Feuerriegel S, Kesselheim AS. Mitigating bias in machine learning for medicine. Commun Med (Lond). 2021;1(1):25.   
7. Norori N, Hu Q, Aellen FM, et al. Addressing bias in big data and AI for health care: a call for open science. Patterns. 2021;2 (10):100347.   
08. Mikołajczyk-Bareła A, Grochowski MA. Survey on bias in machine learning research. arXiv preprint arXiv:2308.11254. 2023. 9. Ferrante RLMER E. Addressing fairness in artificial intelligence for medical imaging. Nat Commun. 2022;13(1):4581.   
10. Abul-Husn NS, Kenny EE. Personalized medicine and the power of electronic health records. Cell. 2019;177(1):58-69.   
11. Cesare N, Were LP. A multi-step approach to managing missing data in time and patient variant electronic health records. BMC Res Notes. 2022;15(1):64.   
12. Rajpurkar P, Chen E, Banerjee O, Topol EJ. AI in health and medi­ cine. Nat Med. 2022;28(1):31-38.   
13. Wiens J, Saria S, Sendak M, et al. Do no harm: a roadmap for responsible machine learning for health care. Nat Med. 2019;25 (9):1337-1340.   
14. Huang J, Galal G, Etemadi M, Vaidyanathan M. Evaluation and mitigation of racial bias in clinical machine learning models: scop­ ing review. JMIR Med Inform. 2022;10(5):e36388.   
15. Celi LA, Cellini J, Charpignon M-L, et al. Sources of bias in artifi­ cial intelligence that perpetuate healthcare disparities—a global review. PLOS Digital Health. 2022;1(3):e0000022.   
16. Sterne JA, Hernan MA, Reeves BC, et al. ROBINS-I: a tool for assessing risk of bias in non-randomised studies of interventions. BMJ. 2016;355:i4919.   
17. Bero L, Chartres N, Diong J, et al. The risk of bias in observational studies of exposures (ROBINS-E) tool: concerns arising from application to observational studies of exposures. Syst Rev. 2018;7(1):242.   
18. Wolff RF, Moons KG, Riley RD, et al. PROBAST: a tool to assess the risk of bias and applicability of prediction model studies. Ann Int Med. 2019;170(1):51-58.   
19. Mehrabi N, Morstatter F, Saxena N, et al. A survey on bias and fairness in machine learning. ACM Comput Surv. 2021;54(6):1-35.   
20. Caton S, Haas C. Fairness in machine learning: a survey. ACM Comput Surv. 2020.   
21. Aghaei S, Azizi MJ, Vayanos P. Learning optimal and fair decision trees for non-discriminative decision-making. In: Proceedings of the 33rd AAAI Conference on Artificial Intelligence and 31st Innovative Applications of Artificial Intelligence Conference and 9th AAAI Symposium on Educational Advances in Artificial Intel­ ligence. AAAI Press; 2019:175.   
22. FitzGerald C, Hurst S. Implicit bias in healthcare professionals: a systematic review. BMC Med Ethics. 2017;18(1):19.   
23. Johnson LC, Beaton R, Murphy S, Pike K. Sampling bias and other methodological threats to the validity of health survey research. Int J Stress Manag. 2000;7(4):247-267.   
24. Haneuse S. Distinguishing selection bias and confounding bias in comparative effectiveness research. Med Care. 2016;54(4):e23-e29.   
25. Oort FJ, Visser MR, Sprangers MA. Formal definitions of meas­ urement bias and explanation bias clarify measurement and con­ ceptual perspectives on response shift. $J$ Clin Epidemiol. 2009;62 (11):1126-1137.   
26. Belenguer L. AI bias: exploring discriminatory algorithmic decision-making models and the application of possible machineEthics. 2022;2(4):771-787.   
27. Juhn YJ, Ryu E, Wi C-I, et al. Assessing socioeconomic bias in machine learning algorithms in health care: a case study of the HOUSES index. J Am Med Inform Assoc. 2022;29(7):1142-1151.   
28. Wolk DM, Lanyado A, Tice AM, et al. Prediction of influenza complications: development and validation of a machine learning prediction model to improve and expand the identification of vaccine-hesitant patients at risk of severe influenza complications. $J$ Clin Med. 2022;11(15):4342.   
29. Khoshnevisan F, Chi M. An adversarial domain separation framework for septic shock early prediction across EHR systems. In: 2020 IEEE International Conference on Big Data (Big Data). IEEE; 2020:64-73.   
30. Li F, Wu P, Ong HH, et al. Evaluating and mitigating bias in machine learning models for cardiovascular disease prediction. J Biomed Inform. 2023;138:104294.   
31. Ro€o€sli E, Bozkurt S, Hernandez-Boussard T. Peeking into a black box, the fairness and generalizability of a MIMIC-III benchmark­ ing model. Sci Data. 2022;9(1):24.   
32. Karlsson I, Bostro€m H, eds. Handling sparsity with random forests when predicting adverse drug events from electronic health records. In: 2014 IEEE International Conference on Healthcare Informatics. IEEE; 2014.   
33. Zhu X, Hurtado J, Tao H. Localized sampling for hospital readmission prediction with imbalanced sample distributions. In: International Joint Conference on Neural Networks (IJCNN). IEEE; 2017:4571-4578.   
34. Davoudi A, Sajdeya R, Ison R, et al. Fairness in the prediction of acute postoperative pain using machine learning models. Front Digit Health. 2022;4:970281.   
35. Raza S, Bashir SR, eds. Auditing ICU readmission rates in an clini­ cal database: an analysis of risk factors and clinical outcomes. In: 2023 IEEE 11th International Conference on Healthcare Infor­ matics (ICHI). IEEE; 2023.   
36. Allen A, Mataraso S, Siefkas A, et al. A racially unbiased, machine learning approach to prediction of mortality: algorithm develop­ ment study. JMIR Public Health Surveill. 2020;6(4):e22400.   
37. Jiang S, Han R, Chakrabarty K, et al. Timeline registration for electronic health records. AMIA Summits on Transl Sci Proc. 2023;2023:291.   
38. Meng C, Trinh L, Xu N, et al. Interpretability and fairness evalua­ tion of deep learning models on MIMIC-IV dataset. Sci Rep. 2022;12(1):7166.   
39. Wang H, Li Y, Naidech A, Luo Y. Comparison between machine learning methods for mortality prediction for sepsis patients with different social determinants. BMC Med Inform Decis Mak. 2022;22(Suppl 2):156-113.   
40. Li C, Jiang X, Zhang K. A transformer-based deep learning approach for fairly predicting post-liver transplant risk factors. J Biomed Inform. 2024;149:104545.   
41. Cui S, Pan W, Zhang C, Wang F. Bipartite ranking fairness through a model agnostic ordering adjustment. IEEE Trans Pat­ tern Anal Mach Intell. 2023;45(11):13235-13249.   
42. Huda NM. Design of Istitaah classification system based on machine learning using imbalanced dataset. In: 7th International Conference on Cyber and IT Service Management (CITSM). Vol. 7. IEEE; 2019:1-6.   
43. Getz K, Hubbard RA, Linn KA. Performance of multiple imputa­ tion using modern machine learning methods in electronic health records data. Epidemiology. 2022;34(2):206-215.   
44. Yin K, Qian D, Cheung WK. PATNet: propensity-adjusted tempo­ ral network for joint imputation and prediction using binary EHRs with observation bias. IEEE Trans Knowl Data Eng. 2023;1-14.   
45. Dwork C, Hardt M, Pitassi T, et al., eds. Fairness through aware­ ness. In: Proceedings of the 3rd Innovations in Theoretical Com­ puter Science Conference. 2012:214-226.   
46. Rajkomar A, Oren E, Chen K, et al. Scalable and accurate deep learning with electronic health records. NPJ Digit Med. 2018;1 (1):18.   
47. Shachar C, Gerke S. Prevention of bias and discrimination in clini­ cal practice algorithms. JAMA. 2023;329(4):283-284.   
48. Ntoutsi E, Fafalios P, Gadiraju U. Bias in data-driven artificial intelligence systems—an introductory survey. Wiley Interdiscip Rev: Data Min Knowl Discov. 2020;10(3):e1356.   
49. Gianfrancesco MA, Tamang S, Yazdany J, Schmajuk G. Potential biases in machine learning algorithms using electronic health record data. JAMA Intern Med. 2018;178(11):1544-1547.   
50. Weberpals J, Becker T, Davies J, et al. Deep learning-based pro­ pensity scores for confounding control in comparative effective­ ness research: a large-scale, real-world data study. Epidemiology. 2021;32(3):378-388.   
51. Mi X, Tighe PJ, Zou F, Zou B. 2020. A deep learning semiparametric regression for adjusting complex confounding structures. Ann Appl Stat. 2021;15(3):1086-1100.   
52. Hayakawa T, Nagashima T, Akimoto H, et al. Benzodiazepinerelated dementia risks and protopathic biases revealed by multiplekernel learning with electronic medical records. Digit Health. 2023;9:20552076231178577.   
53. Vela D, Sharp A, Zhang R, et al. Temporal quality degradation in AI models. Sci Rep. 2022;12(1):11654.   
54. Yuan W, Beaulieu-Jones BK, Yu K-H, et al. Temporal bias in casecontrol design: preventing reliable predictions of the future. Nat Commun. 2021;12(1):1107.   
55. Fletcher RR, Nakeshimana A, Olubeko O. Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health. Front Artif Intell. 2020;3:561802.   
56. Jun I, Ser S, Xu J, et al., eds. Quantification of racial disparity on uri­ nary tract infection recurrence and treatment resistance in Florida using algorithmic fairness methods. In: 2023 IEEE 11th Interna­ tional Conference on Healthcare Informatics (ICHI). IEEE; 2023.   
57. Witting C, Azizi Z, Gomez SE, et al. Natural language processing to identify reasons for sex disparity in statin prescriptions. Am $J$ Prev Cardiol. 2023;14:100496.   
58. Berk R, Heidari H, Jabbari S, et al. A convex framework for fair regression. arXiv preprint arXiv:1706.02409. 2017.   
59. Feldman M, Friedler SA, Moeller J, et al. Certifying and removing disparate impact. In: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Min­ ing. 2015:259-268.   
60. Beutel A, Chen J, Zhao Z, Chi EH. Data decisions and theoretical implications when adversarially learning fair representations. ArXiv, abs/1707.00075. 2017.   
61. Celis LE, Keswani V. Improved adversarial learning for fair classi­ fication. arXiv preprint arXiv:1901.10443. 2019.   
62. Edwards H, Storkey AJ. Censoring representations with an adver­ sary. arXiv preprint arXiv:1511.05897. 2015.   
63. Hebert-Johnson U, Kim M, Reingold O, Rothblum G, eds. Multi­ calibration: calibration for the (computationally-identifiable) masses. In: International Conference on Machine Learning. PMLR; 2018:1939-1948.   
64. Liu LT, Simchowitz M, Hardt M, eds. The implicit fairness crite­ rion of unconstrained learning. In: International Conference on Machine Learning. PMLR; 2019:4051-4060.   
65. Liu Y, Radanovic G, Dimitrakakis C, et al. Calibrated fairness in bandits. arXiv preprint arXiv:1707.01875. 2017.   
66. Hardt M, Price E, Srebro N. Equality of opportunity in supervised learning. In: Proceedings of the 30th International Conference on Neural Information Processing Systems. Curran Associates Inc.; 2016:3323-3331.   
67. Iosifidis V, Fetahu B, Ntoutsi E. FAE: a fairness-aware ensemble framework. In: 2019 IEEE International Conference on Big Data (Big Data). IEEE; 2019:1375-1380.   
68. Valera I, Singla AK, Rodriguez MG. Enhancing the accuracy and fairness of human decision making. Adv Neural Inf Process Syst. 2018;31:1769-1778.   
69. Castelnovo A, Crupi R, Greco G, et al. A clarification of the nuan­ ces in the fairness metrics landscape. Sci Rep. 2022;12(1):4209.  