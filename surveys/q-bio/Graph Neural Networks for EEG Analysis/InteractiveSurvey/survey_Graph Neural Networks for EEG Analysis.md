# A Survey of Graph Neural Networks for EEG Analysis

# 1 Abstract


Electroencephalography (EEG) has long been a cornerstone in neuroscience, providing valuable insights into brain function and activity. This survey paper focuses on the application of Graph Neural Networks (GNNs) in EEG analysis, highlighting their potential in modeling the complex spatiotemporal relationships within brain signals. The survey covers a wide range of topics, including optimization and efficiency in EEG signal classification, advanced graph construction and feature extraction techniques, and the challenges of robustness and interpretability in GNN models. Key findings include the effectiveness of iterative pruning and mutual information for spatial-temporal filtering, the benefits of lightweight adapters for pre-trained models, and the potential of multi-scale temporal convolution and reinforcement learning for dynamic classification tasks. Additionally, the survey explores the practical implications of GNNs in real-world motor tasks and clinical applications, such as stroke severity prediction and Alzheimer's disease diagnosis. By synthesizing and analyzing current research, this survey provides a comprehensive resource for researchers and practitioners, identifying key trends and outlining promising directions for future research in the field of EEG analysis.

# 2 Introduction
Electroencephalography (EEG) has long been a cornerstone in the field of neuroscience, providing valuable insights into brain function and activity [1]. The non-invasive nature of EEG, coupled with its high temporal resolution, makes it an indispensable tool for studying various cognitive and physiological processes [2]. Over the years, advances in signal processing and machine learning have significantly enhanced the ability to analyze and interpret EEG data [2]. However, the complex and dynamic nature of brain signals presents unique challenges, particularly in tasks such as motor imagery classification, emotion recognition, and disease diagnosis [3]. Traditional methods often struggle to capture the intricate spatiotemporal patterns inherent in EEG data, necessitating the development of more sophisticated analytical tools [4].

This survey paper focuses on the application of Graph Neural Networks (GNNs) in EEG analysis [5]. GNNs, a class of deep learning models designed to operate on graph-structured data, have shown remarkable potential in various domains, including social networks, molecular chemistry, and computer vision [6]. In the context of EEG, GNNs can effectively model the spatial and temporal relationships between different brain regions, making them a natural fit for tasks that require a deep understanding of brain connectivity and dynamics [7]. This survey aims to provide a comprehensive overview of the current state of GNNs in EEG analysis, highlighting their applications, methodologies, and future directions [5].

The survey begins by exploring the optimization and efficiency in EEG signal classification, a critical aspect of GNN-based models [8]. Techniques such as iterative pruning and mutual information for spatial-temporal filtering are discussed, emphasizing their roles in enhancing the performance and interpretability of GNNs. These methods not only reduce computational complexity but also improve the model's ability to capture the essential features of brain activity. Additionally, the use of lightweight adapters for pre-trained models is examined, showcasing how these adapters can efficiently fine-tune large models to specific tasks with minimal additional training.

Next, the survey delves into advanced graph construction and feature extraction techniques. Multi-scale temporal convolution and reinforcement learning for dynamic classification are highlighted as key methods for capturing the complex temporal dynamics of EEG signals. These approaches facilitate the extraction of hierarchical temporal features and enable the model to adaptively classify brain signals over time, which is particularly useful in dynamic tasks such as motor imagery [9]. Furthermore, the application of GNNs in real-world motor tasks, such as billiard games in virtual reality environments, is discussed, demonstrating the practical implications of these models in understanding and enhancing motor learning and planning [9].

The survey also addresses the challenges of robustness and interpretability in GNN models. Over-smoothing mitigation and attention mechanisms are presented as effective strategies to maintain the distinctiveness of node representations and enhance feature perception, respectively. The use of GNNExplainer for model validation is also covered, illustrating how this method can provide insights into the critical brain regions and connections that influence the model's predictions. These techniques are essential for ensuring the reliability and interpretability of GNN models in EEG analysis [4].

Finally, the survey highlights the contributions of this work. By synthesizing and analyzing a wide range of research on GNNs for EEG analysis, this survey provides a comprehensive resource for researchers and practitioners in the field [5]. It identifies key trends, discusses the strengths and limitations of current approaches, and outlines promising directions for future research. The insights and methodologies presented in this survey are expected to advance the field of EEG analysis, leading to more accurate and interpretable models that can have significant impacts in clinical and cognitive neuroscience [4].

# 3 GNNs for EEG Classification and Motor Tasks

## 3.1 Optimization and Efficiency in EEG Signal Classification

### 3.1.1 Iterative Pruning for Efficient Classification
Iterative pruning has emerged as a powerful technique to enhance the efficiency and effectiveness of classification models, particularly in the context of EEG data analysis. This method involves systematically removing less important connections or features from the model, leading to a more compact and computationally efficient structure. In the realm of EEG classification, iterative pruning is often applied to Graph Neural Networks (GNNs) to optimize the adjacency matrix, which represents the connectivity between different brain regions [10]. By iteratively pruning the adjacency matrix, the method aims to identify and retain only the most significant connections, thereby improving the model's ability to capture the essential spatial-temporal dynamics of brain activity.

The process of iterative pruning typically begins with an initial dense adjacency matrix, where all possible connections between EEG channels are considered [10]. During each iteration, the model is trained, and the importance of each connection is evaluated based on its contribution to the classification performance. Connections with low importance scores are then pruned, and the process is repeated until a sparse adjacency matrix is obtained. This sparse matrix not only reduces the computational complexity of the model but also enhances its interpretability by highlighting the most relevant brain regions and their interactions. The iterative nature of this process ensures that the model gradually refines its structure, leading to a more robust and efficient representation of the underlying brain activity.

Moreover, iterative pruning in the context of EEG classification has been shown to improve the model's performance on various tasks, such as motor imagery classification and emotion recognition. By focusing on the most salient features and connections, the method helps to mitigate overfitting, a common issue in EEG data analysis due to the limited availability of labeled data. Additionally, the reduced computational demand of the pruned model makes it more suitable for real-time applications, such as brain-computer interfaces (BCIs) and neurofeedback systems. Overall, iterative pruning represents a promising approach to enhancing the efficiency and effectiveness of GNN-based classifiers in EEG data analysis, paving the way for more advanced and practical applications in the field [8].

### 3.1.2 Mutual Information for Spatial-Temporal Filtering
Mutual Information for Spatial-Temporal Filtering leverages the statistical dependence between EEG channels to construct a more robust and informative adjacency matrix for graph convolutional networks (GCNs) [11]. Unlike traditional methods that rely on fixed or manually defined adjacency matrices, mutual information captures the dynamic and non-linear relationships between different brain regions, providing a more accurate representation of the brain's functional connectivity. This approach is particularly useful in motor imagery (MI) tasks, where the inter-channel dependencies can vary significantly across different cognitive states and individuals [10].

In the proposed MutualGraphNet model, mutual information is computed between pairs of EEG channels to determine the strength of their association. This information is then used to construct a weighted adjacency matrix, where the weights represent the degree of mutual information between channels. The resulting graph structure is more adaptive and can better reflect the underlying neural dynamics, leading to improved feature extraction and classification performance [12]. The mutual information-based adjacency matrix is integrated into the GCN framework, allowing the model to learn both spatial and temporal features simultaneously. This dual learning mechanism enhances the model's ability to capture the intricate patterns in EEG data, which are crucial for accurate MI classification [13].

The effectiveness of using mutual information for spatial-temporal filtering is demonstrated through extensive experiments on MI datasets. The MutualGraphNet model outperforms conventional methods that use static or heuristic-based adjacency matrices, achieving higher accuracy and robustness. By incorporating mutual information, the model can adapt to the individual variability in EEG signals, making it a promising approach for real-world applications in brain-computer interfaces (BCIs) and neurological disorder diagnosis. The ability to dynamically adjust the adjacency matrix based on the mutual information between channels also opens new avenues for personalized EEG analysis and the development of more sophisticated cognitive neuroscience tools [10].

### 3.1.3 Lightweight Adapters for Pre-Trained Models
Lightweight adapters for pre-trained models represent a parameter-efficient fine-tuning (PEFT) approach that significantly reduces the computational and data requirements for adapting large pre-trained models to specific downstream tasks [1]. In this method, a lightweight module, referred to as an adapter, is inserted into the pre-trained backbone model. The adapter is then fine-tuned while the backbone remains frozen, allowing the model to learn task-specific features without retraining the entire network. This approach is particularly advantageous in the context of EEG data, where the pre-trained model, often a deep neural network, has already captured the temporal characteristics of the signals [1]. By adding an adapter, the model can efficiently learn spatial features and adapt to new tasks, such as motor imagery classification, with minimal additional training.

The effectiveness of lightweight adapters is demonstrated through their application in various EEG classification tasks. For instance, in motor imagery classification, where the goal is to distinguish between different mental tasks based on EEG signals, lightweight adapters have shown significant improvements in both accuracy and runtime compared to fully fine-tuned models. This is particularly important in real-world applications where computational resources are limited. The adapter module, typically consisting of a few layers with a small number of parameters, can be fine-tuned using a limited amount of labeled data, making it a practical solution for scenarios where data collection is costly or difficult. Moreover, the modular nature of adapters allows for easy integration with existing pre-trained models, facilitating rapid prototyping and deployment.

In the context of EEG data, lightweight adapters have been successfully applied to enhance the performance of pre-trained models like BENDR. BENDR, which is pre-trained using stacked convolutional layers, captures the temporal dynamics of EEG signals effectively [1]. By adding a lightweight adapter, such as a GraphAdapter, the model can further refine its spatial understanding of the EEG data. This combination leverages the strengths of both the pre-trained model and the adapter, resulting in a more robust and adaptable system. The GraphAdapter, in particular, integrates the graph structure of EEG electrode placements, enabling the model to better capture the spatial relationships between different brain regions [14]. This approach not only improves classification accuracy but also provides insights into the neural mechanisms underlying the tasks, making it a valuable tool for both research and clinical applications.

## 3.2 Advanced Graph Construction and Feature Extraction

### 3.2.1 Multi-Scale Temporal Convolution for Brain Activities
Multi-scale temporal convolution has emerged as a crucial technique for enhancing the representation of temporal dynamics in EEG signals, particularly in the context of brain activities [15]. By employing multiple 1D convolutional kernels of varying sizes, this approach can capture both short-term and long-term dependencies within the EEG time series [11]. Each kernel size corresponds to a specific temporal scale, allowing the model to simultaneously analyze the signal at different resolutions. This multi-scale analysis is particularly important for EEG data, which exhibits complex temporal patterns across various time scales, from fast oscillations to slower modulations [5].

The integration of a kernel-level attentive fusion layer further refines the temporal representations by dynamically weighting the outputs of different convolutional kernels. This attention mechanism enables the model to focus on the most relevant temporal scales for the task at hand, thereby improving the interpretability and effectiveness of the learned features. Additionally, the attentive fusion layer helps mitigate the issue of overfitting, which is common in deep learning models when dealing with high-dimensional and noisy EEG data. By adaptively selecting the most informative temporal scales, the model can generalize better to unseen data, making it more robust and reliable for practical applications.

Moreover, the multi-scale temporal convolution approach facilitates the extraction of hierarchical temporal features, which are essential for understanding the intricate temporal dynamics of brain activities. These features serve as a strong foundation for subsequent graph neural network (GNN) layers, which are responsible for modeling the spatial relationships between different brain regions [5]. By combining the strengths of multi-scale temporal convolution with GNNs, the proposed framework can effectively capture both the temporal and spatial aspects of EEG data, leading to improved performance in tasks such as motor imagery classification and cognitive state monitoring [15]. This integrated approach not only enhances the model's ability to represent complex brain activities but also provides a more comprehensive understanding of the underlying neural mechanisms.

### 3.2.2 Reinforcement Learning for Dynamic Classification
Reinforcement Learning (RL) has emerged as a powerful paradigm for dynamic classification tasks, particularly in scenarios where the data and the environment are non-stationary [3]. In the context of EEG data, RL can be leveraged to adaptively classify brain signals over time, capturing the evolving nature of neural activity. The core idea is to train an RL agent to make sequential decisions, such as whether to classify a given time point or to skip it, based on the current state of the environment and the agent's past experiences. This approach is particularly useful in motor imagery (MI) tasks, where the brain's response to motor commands can vary significantly over time, and the ability to dynamically adjust the classification strategy can lead to more accurate and robust models [16].

The integration of RL with Graph Neural Networks (GNNs) has shown promising results in this domain [6]. GNNs are adept at modeling the complex spatial and temporal dependencies in EEG data, making them a natural fit for RL-based dynamic classification [5]. The GNNs can extract high-level features from the EEG signals, which are then used by the RL agent to make informed decisions. For instance, the EEG RL-Net approach combines GNN features with RL to determine the optimal time points for classification, thereby improving the overall accuracy and efficiency of the system [3]. This method not only enhances the performance of the classifier but also reduces the computational burden by avoiding unnecessary classifications at every time point.

Moreover, the use of RL in dynamic classification tasks allows for the incorporation of feedback mechanisms, which can further refine the model's performance over time. By continuously learning from the outcomes of its decisions, the RL agent can adapt to changes in the data distribution and improve its classification accuracy. This adaptability is crucial in real-world applications, such as brain-computer interfaces (BCIs), where the user's brain activity can change due to various factors, including fatigue, attention, and learning [1]. The combination of RL and GNNs thus provides a flexible and robust framework for dynamic EEG classification, paving the way for more advanced and personalized neurotechnological applications [5].

### 3.2.3 Real-World Application in Motor Tasks
In the realm of real-world motor tasks, the application of advanced computational models, particularly Graph Neural Networks (GNNs), has opened new avenues for understanding and enhancing motor learning and planning. A notable example is the use of GNNs to analyze EEG data collected during a billiard task performed in an Embodied Virtual Reality (VR) environment [9]. This setup allows for the collection of high-fidelity brain activity data while subjects engage in a complex, dynamic motor task. The GNN model is designed to predict the success or failure of the previous trial, leveraging the temporal and spatial patterns of neural activity captured by the EEG electrodes [5]. By mapping the electrode space to the corresponding regions of the cerebral cortex, the model can infer the co-activation of different brain regions, providing insights into the neural mechanisms underlying motor performance.

The integration of GNNs with real-world motor tasks represents a significant advancement over traditional machine learning approaches, which often rely on manually defined features and are limited in their ability to capture the intricate spatiotemporal dynamics of brain activity [9]. GNNs, by contrast, can automatically learn and represent the complex interactions between different brain regions, making them well-suited for tasks involving dynamic and interconnected neural processes [7]. This approach not only enhances the accuracy of predictions but also provides a deeper understanding of the neural correlates of motor learning and planning. For instance, the model can identify specific patterns of neural activation that are predictive of successful motor outcomes, which can inform the development of more effective training protocols and rehabilitation strategies.

Moreover, the insights gained from studying brain patterns during complex motor tasks have broader implications for cognitive neuroscience [9]. Understanding how the brain processes and executes motor actions can shed light on fundamental aspects of human cognition, such as decision-making, strategy formation, and adaptive learning. These findings can be applied to a variety of fields, including the design of more intuitive and responsive brain-computer interfaces (BCIs), the development of personalized rehabilitation programs, and the enhancement of human-machine interaction systems [16]. By bridging the gap between theoretical neuroscience and practical applications, this research paves the way for innovative solutions that can significantly improve the quality of life for individuals with motor impairments and enhance the capabilities of healthy individuals in various domains.

## 3.3 Robustness and Interpretability in GNN Models

### 3.3.1 Over-Smoothing Mitigation in Deep Graph Convolutions
Over-smoothing is a critical issue in deep graph convolutions, where nodes become indistinguishable as the depth of the network increases, leading to degraded performance. This phenomenon occurs because repeated graph convolutions aggregate information from increasingly distant neighbors, causing the node representations to converge to similar values. To mitigate over-smoothing, several strategies have been proposed, focusing on modifying the architecture, regularization techniques, or the graph structure itself. One approach involves designing architectures that limit the receptive field of each node, ensuring that information is aggregated from a controlled neighborhood. For example, GraphSAGE employs a sampling mechanism to restrict the number of neighbors considered during each convolution, thereby preventing the dilution of node-specific features.

Another effective strategy is the introduction of skip connections or residual blocks, which allow the model to retain node-specific information across layers. This technique helps maintain the distinctiveness of node representations even in deeper networks. Additionally, some methods incorporate normalization techniques, such as batch normalization or layer normalization, to stabilize the training process and prevent the node representations from becoming too homogeneous. These normalization techniques help in maintaining the variance of node features, which is crucial for preserving the unique characteristics of each node.

Finally, recent advancements have explored the use of adaptive or learnable graph structures to dynamically adjust the graph topology during training. This approach can help in maintaining the heterogeneity of node representations by adapting the graph connections based on the current state of the network. For instance, methods like DeepSAGE leverage the theory of Markov chains to measure and control the severity of over-smoothing, dynamically adjusting the graph structure to ensure that node representations remain distinct. These adaptive methods offer a promising direction for mitigating over-smoothing and improving the performance of deep graph convolutions in various applications, including EEG data analysis [5].

### 3.3.2 Attention Mechanisms for Enhanced Feature Perception
Attention mechanisms have emerged as a critical component in enhancing the feature perception capabilities of Graph Neural Networks (GNNs) in EEG classification tasks [4]. By dynamically weighting the importance of different nodes and edges, attention mechanisms enable the model to focus on the most relevant features, thereby improving the interpretability and performance of the GNN. In the context of EEG data, where the spatiotemporal dynamics are complex and highly variable, attention mechanisms help in capturing the intricate patterns of neural activity that are crucial for tasks such as motor imagery recognition, emotion classification, and cognitive state monitoring.

The application of attention mechanisms in GNNs for EEG classification involves several key approaches [5]. One common method is the use of self-attention mechanisms, where the attention weights are computed based on the similarity between node features. This allows the model to adaptively focus on the most informative regions of the brain, such as specific electrode placements or functional connections, during the message-passing phase. Another approach is the use of graph attention networks (GATs), which extend the self-attention mechanism to the graph structure, enabling the model to learn the importance of different edges and nodes simultaneously. These mechanisms not only enhance the model's ability to capture local and global dependencies but also provide insights into the underlying neural processes.

Furthermore, attention mechanisms in GNNs for EEG classification have been integrated with other advanced techniques to further improve feature perception [5]. For instance, hierarchical attention mechanisms have been proposed to capture multi-scale features, allowing the model to focus on both fine-grained and coarse-grained aspects of the EEG data. Additionally, the combination of attention mechanisms with recurrent neural networks (RNNs) or long short-term memory (LSTM) units has been shown to effectively handle the temporal dynamics of EEG signals, making the model more robust to variations in the data [5]. These integrations highlight the versatility and effectiveness of attention mechanisms in enhancing the feature perception capabilities of GNNs for EEG classification tasks.

### 3.3.3 Model Validation with GNNExplainer
In the context of EEG data classification, the validation of Graph Neural Network (GNN) models is crucial for ensuring the reliability and interpretability of the predictions [5]. GNNExplainer, a method designed to explain the predictions of GNNs, plays a pivotal role in this process. By identifying the most influential subgraph components, GNNExplainer helps in understanding which parts of the EEG data are most relevant to the model's decision-making process [17]. This is particularly important in EEG studies, where the underlying neural mechanisms are complex and not fully understood [2]. The application of GNNExplainer in this domain not only enhances the model's transparency but also provides neurobiological insights that can be cross-validated with existing literature.

The process of using GNNExplainer for model validation involves several steps. Initially, the GNN model is trained on a graph-structured EEG dataset, where each node represents an electrode and the edges capture the functional connectivity between them [18]. Once the model is trained, GNNExplainer is applied to a specific prediction to identify the subset of nodes, edges, and node features that are most influential in the model's decision [17]. This subset, often visualized as a subgraph, highlights the critical brain regions and connections that the model relies on for classification. For instance, in the context of motor learning and planning, GNNExplainer might reveal that the model focuses on the connectivity between the frontal and parietal lobes, which are known to be involved in motor control and planning.

The validation of these explanations is performed by comparing the identified subgraphs with known neurobiological findings. For example, if GNNExplainer indicates a strong reliance on the connectivity between the frontal and parietal lobes, this can be corroborated with studies that have shown the importance of these regions in motor tasks. This cross-validation not only strengthens the credibility of the GNN model but also provides a deeper understanding of the neural mechanisms underlying the task. Furthermore, the ability to visualize and interpret these subgraphs can guide future experimental designs and hypothesis testing in neuroscience, thereby bridging the gap between machine learning models and neurobiological research.

# 4 GNNs for Clinical Neuroscience and Disease Diagnosis

## 4.1 Adaptive and Federated Learning for EEG Analysis

### 4.1.1 Progressive Attention for Critical Channel Identification
Progressive Attention for Critical Channel Identification leverages the hierarchical and dynamic nature of brain networks to identify the most relevant EEG channels for stroke severity prediction. This method employs a multi-stage attention mechanism that progressively refines the focus on critical channels, thereby enhancing the model's interpretability and accuracy. The initial stage involves a global attention mechanism that evaluates the entire brain network to identify broad patterns and regions of interest. This global assessment is crucial for understanding the overall impact of stroke on brain connectivity and function, providing a foundational layer for subsequent detailed analysis.

In the second stage, the attention mechanism narrows its focus to specific regions identified in the first stage, allowing for a more detailed examination of local network properties. This stage is designed to capture the nuanced changes in connectivity that are indicative of stroke-related impairments. By concentrating on these regions, the model can more accurately predict the clinical severity score according to the NIH Stroke Scale (NIHSS) [19]. The progressive nature of this approach ensures that the model does not miss critical information while also avoiding overfitting to noise or irrelevant features.

Finally, the third stage of the progressive attention mechanism further refines the selection by identifying individual EEG channels that are most predictive of stroke severity. This fine-grained analysis is essential for clinical application, as it provides specific insights into the brain regions most affected by stroke. The model's ability to highlight these critical channels aligns with clinical neuroscience literature, reinforcing the biological plausibility of the findings. This multi-stage approach not only improves the model's predictive performance but also enhances its interpretability, making it a valuable tool for both research and clinical practice.

### 4.1.2 Federated Learning for Secure Data Integration
Federated Learning (FL) represents a significant advancement in the secure integration of data from multiple sources, particularly in the healthcare domain, where data privacy and security are paramount [19]. By allowing multiple institutions to collaboratively train machine learning models without the need to share raw data, FL addresses the critical issue of data silos and regulatory constraints that often hinder multi-institutional research. In the context of neurological studies, such as those involving EEG data for stroke and Alzheimer's disease (AD) research, FL enables the aggregation of diverse datasets, thereby improving the robustness and generalizability of the models. This is particularly important given the high variability in EEG signals across different subjects, which can be mitigated by training on a larger, more diverse dataset [5].

The implementation of FL in neurological research typically involves the use of advanced communication protocols, such as Message Queuing Telemetry Transport (MQTT), to efficiently and securely transmit model updates between participating institutions. MQTT is chosen for its lightweight nature and ability to handle asynchronous communication, which is essential for coordinating the training process across distributed clients. To manage the statistical heterogeneity of multi-institutional EEG data, FL algorithms such as Federated Averaging (FedAvg) and SCAFFOLD are employed. FedAvg is a straightforward and widely used method that averages the model updates from all clients, while SCAFFOLD introduces a control variate to correct for client drift, which is particularly useful in non-iid data distributions. Both algorithms are evaluated in the context of EEG data to determine their effectiveness in handling the unique challenges of neurological data [5].

In addition to the technical aspects of FL, the integration of graph-based models, such as Graph Neural Networks (GNNs), further enhances the capabilities of FL in neurological research [7]. GNNs are particularly well-suited for EEG data due to their ability to capture the spatial and temporal relationships between brain regions, which are crucial for understanding complex neurological conditions [5]. By combining FL with GNNs, researchers can develop more accurate and interpretable models that not only predict clinical outcomes but also provide insights into the underlying neurological mechanisms. This approach not only advances the state of the art in neurological research but also paves the way for more personalized and effective treatments.

### 4.1.3 Resolution-Based Feature Extraction for AD Classification
Resolution-based feature extraction is a critical step in the classification of Alzheimer's Disease (AD) using electroencephalogram (EEG) data [20]. This method leverages the multi-dimensional nature of EEG signals, which include spatial, spectral, and temporal components, to extract meaningful features that can distinguish between AD patients and healthy controls (HC). The spatial resolution, in particular, plays a pivotal role in capturing the topological structure of brain activity, which is often disrupted in AD. By adjusting the spatial resolution, researchers can focus on specific brain regions or networks that are most relevant to the disease, thereby improving the discriminative power of the features.

In our approach, we systematically vary the resolution settings across these dimensions to optimize the feature extraction process. For instance, increasing the spatial resolution can help in identifying localized changes in brain activity, which are indicative of AD pathology. Conversely, reducing the spatial resolution can reveal broader network-level alterations that are characteristic of the disease. Similarly, the spectral and temporal resolutions are adjusted to capture the dynamic and frequency-specific aspects of EEG signals. This balanced approach ensures that the extracted features are both comprehensive and specific, providing a robust basis for subsequent classification tasks. The feature tensors obtained through this process are then fed into a support vector machine (SVM) classifier, which evaluates the classification performance based on different resolution configurations.

To further enhance the effectiveness of resolution-based feature extraction, we employ graph pooling techniques to maintain the integrity of brain network structures during the downsampling process. This is particularly important in AD classification, where the preservation of graph clusters can provide valuable insights into the disease's impact on brain connectivity. By dynamically constructing and refining the brain graph, our model can adapt to the unique characteristics of each subject, thereby mitigating the effects of inter-subject variability. The resulting feature representations are not only more discriminative but also more interpretable, allowing clinicians to gain a deeper understanding of the underlying neurological changes associated with AD.

## 4.2 Explainable Models for Disease Prediction

### 4.2.1 Gated Graph Convolutions for AD Diagnosis
Gated Graph Convolutions (GGCs) represent a powerful tool for Alzheimer's Disease (AD) diagnosis, particularly when applied to EEG data, which captures the brain's electrical activity with high temporal resolution. Unlike traditional methods that rely heavily on costly and invasive procedures, GGCs can process complex graph-structured data, making them suitable for EEG-based AD classification [3]. The key advantage of GGCs lies in their ability to dynamically update node features and propagate information across the graph, thereby capturing both local and global patterns in the brain's connectivity. This dynamic updating mechanism is crucial for identifying subtle changes in brain activity that may indicate the onset of AD.

In our proposed model, we construct a graph where nodes represent EEG electrodes, and edges capture the functional connectivity between them [5]. The node features are derived from the EEG signals, including spectral and temporal information, which are essential for characterizing brain activity [12]. The GGC layers then iteratively update these node features by aggregating information from neighboring nodes, allowing the model to learn hierarchical representations of the brain's functional connectivity. This process is particularly beneficial for AD diagnosis, as it can highlight regions of the brain that show abnormal connectivity patterns, which are often associated with the disease. By integrating a gating mechanism, the model can selectively focus on relevant features and ignore noise, enhancing its robustness and interpretability.

To further improve the model's performance, we incorporate a clustering-based node pooling mechanism that coarsens the brain graph, effectively localizing the brain regions most relevant to AD [12]. This not only reduces the computational complexity but also aids in the interpretability of the model's predictions. The learned brain graphs provide valuable insights into the spatial distribution of abnormal connectivity, which can be correlated with known AD biomarkers. Our experimental results on two routine EEG AD datasets demonstrate that the proposed GGC-based model achieves competitive classification performance while offering interpretable predictions, making it a promising tool for early AD diagnosis and monitoring.

### 4.2.2 Spiking Neural Networks for Anticipatory Brain Activity
Spiking Neural Networks (SNNs) have emerged as a powerful computational framework for modeling anticipatory brain activity, offering a biologically plausible alternative to traditional artificial neural networks. Unlike conventional neural networks that use continuous activation functions, SNNs mimic the spiking behavior of biological neurons, which communicate through discrete, asynchronous events. This spiking mechanism allows SNNs to efficiently process and transmit information with low energy consumption, making them particularly suitable for real-time and resource-constrained applications. In the context of anticipatory brain activity, SNNs can simulate the dynamic and temporally precise interactions within neural circuits, providing insights into how the brain predicts and prepares for future events.

Recent advances in SNN architectures have enabled the development of sophisticated models that can capture the intricate temporal dynamics of neural signals. For instance, Temporal Coding SNNs (TC-SNNs) utilize the precise timing of spikes to encode and decode information, which is crucial for tasks requiring fine-grained temporal resolution, such as predicting motor actions or sensory inputs. These models can be trained using supervised learning techniques, such as backpropagation through time, or unsupervised learning methods, like spike-timing-dependent plasticity (STDP), which mimics the synaptic learning rules observed in biological neural systems. By leveraging these mechanisms, SNNs can effectively model the anticipatory processes that underlie cognitive functions, including attention, decision-making, and motor planning.

Moreover, SNNs have been integrated with other neurocomputational models to enhance their predictive capabilities. For example, hybrid models combining SNNs with recurrent neural networks (RNNs) or long short-term memory (LSTM) networks have been developed to better capture the complex temporal dependencies in neural data. These hybrid models can simulate the hierarchical and modular organization of the brain, where different regions interact to form coherent representations of the environment. Additionally, the use of neuromorphic hardware, which is designed to emulate the structure and function of biological neural networks, has further advanced the practical applications of SNNs in modeling anticipatory brain activity. Neuromorphic systems can perform computations in parallel and in real-time, making them ideal for implementing large-scale SNN models that can simulate the brain's predictive mechanisms with high fidelity.

### 4.2.3 Graph Attention Models for Stroke Impairment Quantification
Graph Attention Models (GAMs) have emerged as a powerful tool for quantifying stroke impairment by leveraging the structural and functional connectivity of the brain [21]. These models, specifically designed to process graph-structured data, are adept at capturing the intricate relationships between different brain regions, which are crucial for understanding the impact of stroke on neural networks [21]. In the context of stroke impairment quantification, GAMs are employed to predict the clinical severity score according to the NIH Stroke Scale (NIHSS), a standardized tool used to assess the extent of neurological deficits following a stroke [19]. By integrating EEG connectivity data into a graph structure, GAMs can identify key brain regions and connections that are most relevant to the prediction of stroke severity, thereby enhancing the model's interpretability and clinical utility [21].

The effectiveness of GAMs in stroke impairment quantification is largely attributed to their ability to dynamically assign attention weights to different nodes and edges in the brain graph. This attention mechanism allows the model to focus on the most salient features, such as specific brain regions or connectivity patterns, that are indicative of stroke severity. For instance, the model can highlight significant neurological connections that are consistent with known stroke recovery mechanisms, such as the reorganization of motor and sensory networks. This not only improves the accuracy of the predictions but also provides valuable insights into the underlying neurological processes, which can inform personalized treatment strategies and rehabilitation plans.

Moreover, the application of GAMs in stroke impairment quantification has revealed critical insights into how stroke affects brain network properties, such as small-worldness, efficiency, and modularity. These properties are essential for understanding the brain's adaptive mechanisms during recovery and can help in identifying patients who are more likely to benefit from specific interventions. The use of GAMs in this context also addresses the challenge of variability among subjects by learning robust representations that generalize across different patient populations. By integrating these models into clinical workflows, healthcare providers can make more informed decisions, ultimately leading to improved patient outcomes and a better understanding of the complex dynamics of post-stroke recovery.

## 4.3 Advanced Techniques for Early Diagnosis

### 4.3.1 Machine Learning for Subtle Brain Changes
Machine learning techniques, particularly those leveraging graph neural networks (GNNs), have emerged as powerful tools for detecting subtle brain changes, especially in the context of neurological disorders such as Alzheimer's Disease (AD) and stroke [12]. These methods are capable of analyzing complex, graph-structured data derived from electroencephalography (EEG) and other neuroimaging modalities, providing deeper insights into the underlying neural mechanisms [5]. GNNs, with their ability to model interactions between nodes in a graph, offer a natural fit for brain network analysis, where nodes represent brain regions and edges represent functional or structural connections [16].

One of the key challenges in identifying subtle brain changes is the variability in brain signals across individuals, which can obscure the detection of meaningful patterns. Advanced machine learning algorithms, including graph attention models, have been developed to address this issue by adaptively weighting the importance of different brain regions and connections [12]. For instance, in the context of AD, GNNs can dynamically construct brain graphs from EEG data, enhancing the representation of node features and localizing the brain regions most relevant to the disease [18]. This not only improves the accuracy of AD classification but also provides interpretable insights that align with clinical neuroscience findings, such as the involvement of specific neural pathways in disease progression.

Moreover, the integration of machine learning with clinical neuroscience has led to the development of explainable models that can predict clinical severity scores, such as the NIH Stroke Scale (NIHSS), and identify key brain regions involved in stroke recovery [21]. By employing graph attention mechanisms, these models can provide detailed explanations for their predictions, which is crucial for clinical decision-making. The use of clustering-based node pooling further enhances the interpretability of these models by coarsening the brain graph and focusing on the most significant regions. Overall, the application of machine learning, particularly GNNs, to subtle brain changes represents a promising direction for advancing our understanding of neurological disorders and improving patient outcomes.

### 4.3.2 Clustering-Based Node Pooling for Brain Region Localization
Clustering-based node pooling in graph neural networks (GNNs) has emerged as a powerful technique for brain region localization, particularly in the context of neurological disorders such as stroke and Alzheimer's disease (AD) [12]. This method involves the hierarchical reduction of the graph's node set by clustering nodes into communities or regions, which helps in preserving the most relevant structural and functional information while reducing computational complexity. By leveraging clustering algorithms, such as spectral clustering or community detection methods, the brain graph can be coarsened into a more manageable representation that retains the essential connectivity patterns and functional modules.

The clustering-based node pooling mechanism is particularly advantageous in the analysis of brain graphs derived from EEG or fMRI data, where the high dimensionality and sparsity of the data can pose significant challenges [5]. In this approach, nodes representing brain regions are grouped into clusters based on their similarity in functional connectivity or anatomical proximity [7]. This process not only reduces the graph's complexity but also enhances the interpretability of the model by highlighting the most salient brain regions involved in specific neurological functions or dysfunctions. For instance, in the context of stroke, clustering-based node pooling can help identify the key brain regions that are most affected by ischemic changes and contribute to the overall impairment as measured by clinical scales such as the NIHSS.

Furthermore, the adaptability of clustering-based node pooling to different levels of granularity allows for a more nuanced analysis of brain networks. By adjusting the clustering parameters, researchers can explore the brain's hierarchical organization at various scales, from local microcircuits to large-scale functional networks. This flexibility is crucial for understanding the dynamic reorganization of brain connectivity following neurological insults and for developing more accurate and personalized models of brain function and dysfunction. The ability to dynamically adjust the level of detail in the graph representation also facilitates the integration of multi-modal data, such as combining EEG and fMRI, to provide a more comprehensive view of brain activity and connectivity [5].

### 4.3.3 High Temporal Resolution for Predictive Modeling
High temporal resolution is crucial in predictive modeling, particularly when dealing with dynamic processes such as brain activity in neurological disorders like Alzheimer's Disease (AD) and stroke. EEG data, characterized by its high temporal resolution, captures the rapid changes in brain electrical activity, making it an invaluable resource for understanding the dynamics of these conditions [1]. The fast-changing nature of EEG signals allows for the detection of subtle variations in brain activity that may be indicative of disease progression or response to treatment. However, the sheer volume and complexity of EEG data pose significant challenges in terms of data processing and feature extraction [2].

To address these challenges, recent advances in graph neural networks (GNNs) have been leveraged to integrate spatial and temporal information effectively [6]. Gated graph convolutions, for instance, enable the model to capture both local and global dependencies in the graph structure, thereby enhancing the model's ability to identify relevant patterns in EEG data [5]. By modifying the feature resolution along spatial, spectral, and temporal dimensions, researchers can optimize the balance between these aspects to improve classification performance. For example, adjusting the spatial resolution using graph pooling techniques helps preserve important graph clusters while reducing computational complexity [20].

In our experimental setup, we conducted resolution-based feature extraction on two routine EEG AD datasets, classifying the resulting feature tensors using a support vector machine (SVM) [20]. This approach allowed us to evaluate the impact of different resolution configurations on classification performance, ultimately identifying the optimal balance of dimensions for AD classification. The results demonstrated that high temporal resolution, combined with appropriate spatial and spectral resolutions, significantly enhances the model's predictive accuracy. Furthermore, the use of edge-based explanation methods, such as EdgeSHAPer, provides interpretable insights into the model's decision-making process, fostering trust and transparency in clinical applications.

# 5 GNNs for Emotion Recognition and Depression Detection

## 5.1 Comprehensive Frameworks for Emotion Recognition

### 5.1.1 Node-Level and Edge-Level GNNs for Brain Region Dependencies
Node-Level and Edge-Level Graph Neural Networks (GNNs) have emerged as powerful tools for modeling the complex dependencies among brain regions in EEG-based applications, particularly in depression detection and emotion recognition [5]. These models leverage the inherent graph structure of EEG data, where each EEG channel represents a node and the connections between channels form the edges [5]. By capturing both local and global dependencies, these GNNs can effectively model the intricate interactions within and between brain regions [7]. Node-level GNNs focus on extracting features from individual nodes, emphasizing the unique characteristics of each EEG channel [8]. This approach is crucial for identifying specific patterns of brain activity that are indicative of particular emotional states or neurological conditions. For instance, in the context of depression detection, node-level GNNs can help identify common depression-related patterns across individuals while also accounting for individual differences through personalized feature extraction [22].

Edge-level GNNs, on the other hand, concentrate on the relationships between nodes, capturing the dynamic and functional connectivity of the brain. These models are particularly useful for understanding how different brain regions interact during various cognitive and emotional processes. By modeling the edges, edge-level GNNs can reveal the underlying network topology and functional connectivity patterns that are critical for emotion recognition and other cognitive tasks [23]. For example, the use of edge-level GNNs in EEG-based emotion recognition has shown significant improvements in classification accuracy by capturing the dynamic changes in brain connectivity that occur during different emotional states [23]. The integration of both node-level and edge-level GNNs in a hybrid architecture allows for a more comprehensive representation of the brain's complex dynamics, enhancing the model's ability to generalize across different subjects and tasks [7].

Recent advancements in GNNs for EEG data have further refined the ability to model brain region dependencies by incorporating multi-level graph contrastive learning and hierarchical information extraction [24]. Multi-level graph contrastive learning, as seen in the Dual-Branch Graph Neural Network (DB-GNN), combines node-level and graph-level contrastive learning to improve the model's robustness and feature representation. This approach not only enhances the model's ability to perceive both local and global features but also reduces the risk of overfitting. Additionally, the integration of graph pooling and unpooling modules, as in the Graph Pooling and Unpooling Module (GPUM), enables the extraction of hierarchical information from individualized graph connections, further enriching the model's representation capabilities. These advancements highlight the potential of GNNs in advancing our understanding of brain function and improving the accuracy of EEG-based diagnostic and recognition systems [5].

### 5.1.2 Regularized GNNs for Cross-Subject Robustness
Regularized Graph Neural Networks (RGNNs) have emerged as a promising approach to address the challenges of cross-subject robustness in EEG-based emotion recognition [25]. The inherent variability in EEG signals across subjects poses a significant barrier to the generalization of models, particularly in scenarios where data from new subjects are encountered [1]. RGNNs tackle this issue by incorporating regularization techniques that aim to minimize the domain discrepancies between source and target subjects, thereby enhancing the model's ability to generalize across different subjects. One such technique is the Node-wise Domain Adversarial Training (NodeDAT), which extends traditional domain adversarial training by applying it at the node level. This finer-grained regularization helps in aligning the feature distributions of individual EEG channels across subjects, thus improving the model's robustness to inter-subject variations.

In addition to NodeDAT, another key regularizer proposed in the context of RGNNs is Emotion-aware Distribution Learning (EmotionDL). EmotionDL focuses on preserving the emotional content of the EEG signals during the domain adaptation process. By explicitly modeling the emotional distribution of the data, EmotionDL ensures that the learned features not only generalize well across subjects but also maintain the emotional discriminability necessary for accurate emotion recognition. The combination of NodeDAT and EmotionDL in RGNNs has been shown to significantly enhance the model's performance in cross-subject scenarios, outperforming traditional methods that rely solely on global feature alignment. This dual-regularization approach not only mitigates the impact of noisy labels and domain shifts but also leverages the emotional context to improve the robustness and reliability of the model.

To further enhance the robustness of RGNNs in cross-subject settings, recent studies have also explored the integration of multi-level graph contrastive learning. This approach leverages both graph-level and node-level contrastive learning to constrain the learning architecture, thereby enabling the model to collaboratively perceive both global and local features of the brain networks. By incorporating this multi-level contrastive learning, RGNNs can better capture the hierarchical structure of EEG data, which is crucial for understanding the complex inter-channel relationships in brain signals. This, in turn, leads to more stable and reliable performance across different subjects, making RGNNs a powerful tool for EEG-based emotion recognition in real-world applications.

### 5.1.3 Transferable Attention for Domain-Invariant Representations
Transferable attention mechanisms play a crucial role in generating domain-invariant representations, particularly in the context of EEG-based emotion recognition. By leveraging self-attention, these mechanisms enable the model to focus on relevant features while suppressing noise and irrelevant information [26]. This is achieved through a node-wise domain adversarial training (NodeDAT) method, which regularizes the graph neural network (GNN) to minimize domain discrepancies between the source and target domains at a finer granularity. Unlike traditional domain adversarial training, NodeDAT specifically targets the feature representations of individual nodes, ensuring that the model learns robust and transferable features across different subjects and conditions.

The transferable attention mechanism is integrated into the GNN architecture to enhance its ability to capture hierarchical and individualized information from EEG signals. This integration is facilitated by a graph pooling and unpooling module (GPUM), which automatically aggregates EEG channels into brain regions, capturing the functional correlations between them. The self-attention mechanism within the GPUM allows the model to dynamically adjust its focus on different brain regions, thereby improving the representation of domain-invariant features. This dynamic adjustment is crucial for handling the variability in EEG signals across different individuals and experimental setups.

To further enhance the model's generalization capability, the transferable attention mechanism quantifies the transferability of each channel using an entropy function based on the output of a node-wise domain classifier. This quantification ensures that the model prioritizes channels with high transferability, which are less likely to be affected by inter-domain differences. By focusing on these channels, the model can effectively align the representations extracted from the whole EEG signal across different domains, leading to improved accuracy in cross-subject and cross-session classification tasks. This approach not only enhances the robustness of the model but also provides a principled way to handle the inherent variability in EEG data [2].

## 5.2 Advanced Architectures for Enhanced Accuracy

### 5.2.1 Music-Induced Emotional States for Improved Recognition
Music-induced emotional states have emerged as a powerful tool for enhancing the accuracy and robustness of emotion recognition systems, particularly those utilizing electroencephalogram (EEG) data. The MEEG dataset, an advanced multi-modal EEG emotion dataset, leverages diverse musical stimuli to induce a wide range of emotional states, thereby providing a richer and more varied set of data for training and testing emotion recognition models [15]. Compared to the DEAP dataset, which primarily uses audio-visual stimuli, the MEEG dataset demonstrates superior performance in terms of model accuracy (ACC) and F1 scores, highlighting the effectiveness of music in eliciting distinct emotional responses [15].

The AT-DGNN framework, which integrates a sliding window technique, an attention mechanism, and a multi-layer dynamic graph neural network (LGGNet), is specifically designed to explore the intricate connections within and between brain functional areas during music-induced emotional states [15]. This framework not only captures the temporal dynamics of EEG signals but also emphasizes the importance of spatial relationships among different brain regions [9]. By focusing on the pre-frontal, parietal, and occipital regions, which are known to be highly active during emotional processing, the AT-DGNN framework achieves superior performance over state-of-the-art models in most experimental settings. The attention mechanism further enhances the model's ability to identify and weigh the most relevant features, contributing to more accurate emotion classification.

Moreover, the integration of music-induced emotional states into EEG-based emotion recognition offers a unique opportunity to study the interplay between different frequency components, such as within-frequency coherence (WFC) and cross-frequency coupling (CFC). While WFC has been extensively studied for its role in emotional processing, the simultaneous exploration of both WFC and CFC can provide a more comprehensive understanding of the neural mechanisms underlying emotion [26]. This integrated approach not only improves the recognition accuracy of emotional states but also sheds light on the complex interactions between various brain regions during emotional experiences, paving the way for more sophisticated and nuanced emotion recognition systems [25].

### 5.2.2 Dual-Branch GNNs for Frequency Coupling
In the context of frequency coupling, Dual-Branch Graph Neural Networks (DB-GNNs) offer a sophisticated approach to modeling the complex interactions within EEG signals [5]. The DB-GNN architecture is designed to integrate the strengths of both fixed and instance-adaptive graph connections, thereby enhancing the detection of depression [22]. The Common Graph Neural Network (CGNN) branch utilizes a fixed graph structure to capture the common patterns across multiple depressed patients, ensuring a stable and consistent representation of the underlying neural dynamics. This fixed structure is crucial for identifying the generalizable features that are indicative of depression, such as specific frequency bands and their interactions.

Conversely, the Individualized Graph Neural Network (IGNN) branch dynamically constructs graph connections based on the unique characteristics of each patient's EEG data [16]. By adaptively learning the graph structure, the IGNN can capture the individual differences in brain activity, which are often critical for accurate diagnosis. This adaptive approach allows the model to account for variations in neural connectivity patterns that may not be evident in a fixed graph structure. The dynamic construction of graph connections in the IGNN is particularly useful for identifying subtle changes in frequency coupling that are specific to individual patients, thereby improving the model's sensitivity and specificity.

The integration of these two branches in the DB-GNN framework provides a comprehensive approach to modeling frequency coupling in EEG signals. The CGNN branch ensures that the model can generalize well across different patients, while the IGNN branch allows for personalized detection of depression [22]. This dual-branch architecture not only enhances the model's ability to capture both common and individualized patterns but also facilitates a more nuanced understanding of the neural mechanisms underlying depression. By leveraging the strengths of both fixed and adaptive graph connections, the DB-GNN offers a robust solution for improving the accuracy and reliability of depression detection using EEG data [22].

### 5.2.3 Prior Information in Graph Transformers
In the context of integrating prior information into Graph Transformers, the Prior Information-based Graph Transformer Module (PiGTM) has emerged as a significant advancement. This module aims to enhance the global feature perception capability of the Depression Brain Graph Neural Network (DB-GNN) by leveraging a Transformer architecture [1]. The Transformer is employed to enable a collaborative perception of global information across all Weighted Functional Connectivity (WFC) and Cross-Frequency Coupling (CFC) brain networks [26]. By integrating prior coupling information of brain networks into the self-attention mechanism, PiGTM mitigates the risk of overfitting, which is a common issue in deep learning models with limited training data. This integration ensures that the model can effectively generalize to unseen data while capturing the intricate and dynamic patterns of brain connectivity associated with depression [22].

The integration of prior information in PiGTM is achieved through a carefully designed self-attention mechanism that incorporates known biological and functional connectivity patterns. This approach not only enhances the model's ability to capture long-range dependencies and global context but also ensures that the learned representations are biologically plausible. The prior coupling information, derived from neuroscientific studies, serves as a regularizer that guides the attention mechanism to focus on relevant and meaningful connections within the brain network. This is particularly important in the context of depression, where both common and individualized abnormal patterns in brain networks have been observed [22]. By leveraging this prior knowledge, PiGTM can better distinguish between these patterns, leading to improved accuracy in depression detection.

Furthermore, the use of prior information in PiGTM addresses the limitations of traditional GNN models that often rely on a single type of graph connection, such as functional connectivity or anatomical proximity. By incorporating multiple types of connectivity information, PiGTM can capture a more comprehensive and nuanced representation of the brain's functional and structural organization. This multi-faceted approach is crucial for understanding the complex and heterogeneous nature of depression, which involves both common and individualized abnormalities in brain networks [22]. The enhanced feature extraction capabilities of PiGTM, combined with its robustness to overfitting, make it a promising tool for advancing the field of depression detection and neurological research.

## 5.3 Hybrid Models for Depression Detection

### 5.3.1 Common and Individualized Patterns in EEG Data
In the analysis of EEG data, the identification of both common and individualized patterns is crucial for understanding the neural mechanisms underlying various cognitive and affective states, particularly in conditions such as depression. Common patterns, often observed across multiple individuals, include abnormal activity in specific brain regions, such as the frontal lobe, where depressed patients exhibit distinct connectivity differences compared to healthy individuals [22]. These common patterns are typically characterized by altered functional connectivity, where the frontal lobe channels in depression patients tend to be partitioned into different regions in the region-level graph, while those in healthy individuals cluster within the same region [22]. This hierarchical structure provides a foundational understanding of the neural correlates of depression, highlighting the importance of frontal lobe connectivity in the disorder.

However, the presence of individualized patterns in EEG data adds a layer of complexity to the analysis. Individualized patterns reflect the unique neural signatures of each subject, which can vary significantly even among individuals with the same diagnosis. These patterns are often revealed through the analysis of brain networks at multiple levels, including within-frequency and cross-frequency coupling [26]. For instance, the within-frequency coupling brain networks constructed from multi-channel EEG signals in different frequency bands (e.g., alpha, beta) can differ markedly between individuals, reflecting variations in neural oscillatory activity [26]. Similarly, cross-frequency coupling, which examines the interactions between different frequency bands, can capture individualized patterns that are not apparent in single-band analyses. The integration of these individualized patterns with common patterns is essential for developing more personalized and accurate models of brain function and dysfunction.

To effectively capture both common and individualized patterns, recent approaches have leveraged advanced graph-based methods and deep learning techniques. For example, the use of graph convolutional networks (GCNs) and hierarchical graph neural networks (HGNs) has shown promise in modeling the spatial and temporal dynamics of EEG signals [5]. These methods not only account for the topological structure of EEG channels but also integrate hierarchical information, such as the clustering of channels into brain regions, to enhance the representation of neural activity. Additionally, unpooling operations in these networks help to integrate the hierarchical information with the original features, providing a more comprehensive understanding of the brain's functional architecture [22]. By combining these advanced techniques, researchers can better identify and characterize the complex and nuanced patterns in EEG data, ultimately leading to more effective diagnostic and therapeutic strategies [5].

### 5.3.2 Hierarchical Information Extraction with Graph Pooling
Hierarchical information extraction in graph neural networks (GNNs) for EEG-based applications, particularly in depression detection, is significantly enhanced through the integration of graph pooling mechanisms [22]. Graph pooling operations enable the automatic aggregation of EEG channels into higher-level brain regions, thereby facilitating the extraction of both local and global features [7]. This hierarchical structure is crucial for understanding the complex interactions within the brain, especially in conditions like depression, where regional connectivity patterns differ markedly between affected individuals and healthy controls. By leveraging graph pooling, the model can dynamically adjust to the varying scales of brain networks, capturing the nuanced differences in connectivity that are indicative of depressive states [22].

The graph pooling and unpooling module (GPUM) embedded within the individualized graph neural network (IGNN) plays a pivotal role in this hierarchical extraction process. This module not only aggregates information from individual EEG channels but also reconstructs the detailed structure at finer resolutions when necessary. This bidirectional flow of information ensures that the model retains the essential details while also benefiting from the higher-level abstractions provided by the pooled representations. Such a dual-branch approach helps mitigate the risk of overfitting, a common issue in GNNs due to overparameterization, by balancing the local and global information. This balance is particularly important in the context of EEG data, where the spatial and temporal dynamics of brain activity must be accurately captured to distinguish between different emotional or pathological states [25].

Moreover, the hierarchical structure derived from graph pooling allows for the identification of distinct patterns in brain connectivity that are characteristic of depression [22]. For instance, EEG channels in the frontal lobe of depressed individuals often exhibit a more fragmented connectivity pattern, with channels being partitioned into different regions, as opposed to the more cohesive clustering observed in healthy individuals. By capturing these hierarchical differences, the model can more accurately detect and classify depressive states, thereby improving the overall reliability and effectiveness of depression detection systems. This hierarchical approach not only enhances the model's interpretability but also provides a robust framework for integrating multi-scale information, making it a valuable tool in the field of neuroinformatics and clinical applications.

### 5.3.3 Robust and Individualized Depression Detection
In the realm of depression detection, traditional Graph Neural Network (GNN) methods often fall short due to their lack of specificity in addressing the unique characteristics of depression [22]. These methods typically treat depression as a generic classification problem, failing to account for the nuanced and individualized nature of depressive symptoms. Consequently, they may not adequately capture the complex and dynamic neural patterns associated with depression, leading to suboptimal detection performance. To address this issue, recent advancements have focused on developing more robust and individualized models that can better handle the variability and complexity inherent in depression.

One approach to enhancing the robustness and individualization of depression detection involves the integration of a Graph Pooling and Unpooling Module (GPUM) into an Improved GNN (IGNN) [22]. This module facilitates the extraction of hierarchical information from individualized graph connections, which is crucial for accurately modeling the neural dynamics underlying depression [22]. Specifically, the graph pooling operation automatically aggregates EEG channels into region-level graphs, thereby capturing the spatial relationships between different brain regions [7]. This hierarchical representation allows the model to focus on relevant features at multiple scales, improving its ability to detect subtle changes in brain activity that may indicate the presence of depression.

Furthermore, the incorporation of attention mechanisms and domain adversarial training into the GNN framework enhances the model's robustness against noisy data and individual variations. Attention mechanisms enable the model to weigh the importance of different brain regions and their interactions, while domain adversarial training helps in reducing the impact of domain-specific biases, making the model more generalizable across different populations. These enhancements collectively contribute to a more accurate and reliable detection of depression, paving the way for more personalized and effective mental health interventions.

# 6 Future Directions


The current landscape of Graph Neural Networks (GNNs) in EEG analysis, while promising, still faces several limitations and gaps. One of the primary challenges is the variability in EEG data across different individuals, which can lead to overfitting and poor generalization. Traditional GNN models often struggle to capture the intricate spatiotemporal patterns inherent in EEG signals, particularly in tasks such as motor imagery classification, emotion recognition, and disease diagnosis. Additionally, the interpretability of GNN models remains a significant concern, as the complex and black-box nature of these models can hinder their adoption in clinical settings. There is also a need for more robust methods to handle the dynamic and non-stationary nature of EEG data, which can vary significantly over time and across different cognitive states.

To address these limitations, several directions for future research are proposed. First, there is a need to develop more adaptive and personalized GNN models that can account for individual variability in EEG data. This could involve the integration of subject-specific features, such as genetic markers or demographic information, into the model architecture. Additionally, the use of federated learning frameworks could enable the training of GNN models on distributed datasets while preserving data privacy, thereby improving the robustness and generalizability of the models.

Second, the development of more interpretable GNN models is crucial for their acceptance in clinical and research settings. Techniques such as attention mechanisms, explainability methods like GNNExplainer, and the integration of domain knowledge into the model architecture can enhance the interpretability of GNNs. These methods can provide insights into the critical brain regions and connections that influence the model's predictions, making the models more transparent and trustworthy.

Third, the dynamic and non-stationary nature of EEG data requires the development of GNN models that can adapt to changes over time. Reinforcement learning (RL) and online learning techniques can be integrated with GNNs to create models that can dynamically adjust their parameters based on real-time data. This would be particularly useful in applications such as brain-computer interfaces (BCIs) and real-time monitoring of neurological conditions.

The potential impact of these proposed future directions is significant. More adaptive and personalized GNN models could lead to more accurate and reliable diagnoses of neurological conditions, such as Alzheimer's disease and stroke, and improve the effectiveness of treatments. Enhanced interpretability would foster greater trust in these models, making them more acceptable in clinical practice and research. Finally, the ability to handle dynamic and non-stationary data would expand the applicability of GNNs to a wider range of real-world scenarios, including personalized medicine and real-time cognitive monitoring. Collectively, these advancements have the potential to revolutionize the field of EEG analysis and contribute to major breakthroughs in neuroscience and clinical practice.

# 7 Conclusion



This survey has comprehensively explored the application of Graph Neural Networks (GNNs) in the analysis of Electroencephalography (EEG) data, covering a wide range of methodologies and applications. The main findings highlight the effectiveness of GNNs in capturing the intricate spatiotemporal patterns of brain activity, which are crucial for tasks such as motor imagery classification, emotion recognition, and disease diagnosis. Key techniques discussed include iterative pruning for efficient classification, mutual information for spatial-temporal filtering, and lightweight adapters for parameter-efficient fine-tuning. Advanced graph construction methods, such as multi-scale temporal convolution and reinforcement learning for dynamic classification, have shown significant improvements in performance. Additionally, the survey has addressed the challenges of robustness and interpretability in GNN models, emphasizing strategies like over-smoothing mitigation, attention mechanisms, and the use of GNNExplainer for model validation. These techniques not only enhance the performance of GNNs but also provide deeper insights into the underlying neural mechanisms.

The significance of this survey lies in its comprehensive overview of the current state of GNNs in EEG analysis, which serves as a valuable resource for researchers and practitioners in the field. By synthesizing and analyzing a wide range of research, this survey identifies key trends and discusses the strengths and limitations of current approaches. The insights and methodologies presented are expected to advance the field of EEG analysis, leading to more accurate and interpretable models. These advancements have the potential to make significant impacts in clinical and cognitive neuroscience, from improving the diagnosis and treatment of neurological disorders to enhancing our understanding of brain function and cognitive processes.

In conclusion, the rapid development and application of GNNs in EEG analysis present exciting opportunities for future research. We call for continued exploration of novel GNN architectures and techniques that can further enhance the performance and interpretability of EEG data analysis. Additionally, there is a need for more interdisciplinary collaboration between neuroscientists, machine learning experts, and clinicians to develop practical and clinically relevant applications. By addressing these challenges and building on the foundations laid by this survey, the field can move closer to realizing the full potential of GNNs in advancing our understanding of the brain and improving human health.

# References
[1] Graph Adapter of EEG Foundation Models for Parameter Efficient Fine  Tuning  
[2] A Survey of Spatio-Temporal EEG data Analysis  from Models to  Applications  
[3] EEG RL-Net  Enhancing EEG MI Classification through Reinforcement  Learning-Optimised Graph Neural N  
[4] GEFM  Graph-Enhanced EEG Foundation Model  
[5] Graph Neural Network-based EEG Classification  A Survey  
[6] Markov Chain-Guided Graph Construction and Sampling Depth Optimization  for EEG-Based Mental Disorde  
[7] LGGNet  Learning from Local-Global-Graph Representations for  Brain-Computer Interface  
[8] GNN4EEG  A Benchmark and Toolkit for Electroencephalography  Classification with Graph Neural Networ  
[9] Graph Neural Networks Uncover Geometric Neural Representations in  Reinforcement-Based Motor Learnin  
[10] EEG GLT-Net  Optimising EEG Graphs for Real-time Motor Imagery Signals  Classification  
[11] Parkinson's Disease Detection from Resting State EEG using Multi-Head  Graph Structure Learning with  
[12] Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of  Alzheimer's Disease using E  
[13] EEG Decoding for Datasets with Heterogenous Electrode Configurations  using Transfer Learning Graph  
[14] Self-Supervised Graph Neural Networks for Improved  Electroencephalographic Seizure Analysis  
[15] MEEG and AT-DGNN  Improving EEG Emotion Recognition with Music  Introducing and Graph-based Learning  
[16] MutualGraphNet  A novel model for motor imagery classification  
[17] Investigating Brain Connectivity with Graph Neural Networks and  GNNExplainer  
[18] EEG-GNN  Graph Neural Networks for Classification of  Electroencephalogram (EEG) Signals  
[19] Federated GNNs for EEG-Based Stroke Assessment  
[20] Balancing Spectral, Temporal and Spatial Information for EEG-based  Alzheimer's Disease Classificati  
[21] Towards Explainable Graph Neural Networks for Neurological Evaluation on  EEG Signals  
[22] A Hybrid Graph Neural Network for Enhanced EEG-Based Depression  Detection  
[23] Graph Neural Networks in EEG-based Emotion Recognition  A Survey  
[24] Dynamic GNNs for Precise Seizure Detection and Classification from EEG  Data  
[25] EEG-Based Emotion Recognition Using Regularized Graph Neural Networks  
[26] DB-GNN  Dual-Branch Graph Neural Network with Multi-Level Contrastive  Learning for Jointly Identify  