# 5/1/2025, 6:11:37 PM_Deep Learning for Small Molecule Discovery  

# 0. Deep Learning for Small Molecule Discovery  

# 1. Introduction  

The conventional small molecule drug discovery pipeline represents a complex, protracted, and costly endeavor [3,4,6,9,10,11,12,15,19,21,23,24,25,35,39,41,45]. This process is fraught with significant challenges, including high financial investment, often exceeding $\$ 2.5$ billion and timelines frequently extending beyond a decade or 10-15 years [3,9,11,39]. Compounding these difficulties is a notably low success rate, with failure rates for drug candidates surpassing $9 0 \%$ [9,29,41,45], particularly due to unfavorable pharmacokinetics and toxicity profiles discovered late in development [29]. Traditional methods, such as high-throughput screening (HTS) and combinatorial chemistry, are inherently limited in their ability to efficiently navigate the vast chemical space and handle the increasingly large and complex datasets generated in modern biological and chemical experiments [25,38,45]. Furthermore, the iterative nature of experimental design and the reliance on medicinal chemists' experience contribute to the lengthy and resource-intensive nature of phases like lead identification and optimization [6,27,38].​  

The advent of artificial intelligence (AI), machine learning (ML), and particularly deep learning (DL) technologies marks a transformative shift in addressing these long-standing challenges [5,7,9,10,18,21,26,32,39]. AI-driven approaches offer compelling solutions by accelerating various processes, improving overall efficiency, and enabling novel design methodologies throughout the drug discovery pipeline [1,4,6,9,10,17,19,24,39]. The increasing digitization of data within pharmaceutical research, coupled with significant advancements in computing power, has paved the way for the application of sophisticated AI models [7,12].  

Deep learning, as a sophisticated branch of AI, is particularly well-suited for handling the large, high-dimensional, and complex biological and chemical datasets that are now commonplace in modern drug discovery [7,17,19,21,32]. Unlike traditional computational methods that often require extensive feature engineering or are limited to smaller datasets, deep learning models can automatically learn intricate patterns and representations directly from raw data, such as molecular structures, biological assay results, and omics data [26,27,34]. This capability is crucial for navigating the immense chemical space, estimated to contain up to $1 0 ^ { 8 0 }$ potential drug-like molecules, far beyond the reach of traditional screening methods [36,45].​  

The integration of computational methods into drug discovery is not entirely new; computer-aided drug design (CADD) techniques like molecular docking and QSAR have been utilized since the 1990s [12,26,27]. However, the recent "reevolution" of QSAR studies highlights the significant impact of integrating machine learning, particularly deep learning, enabling the modeling of non-linear relationships on large datasets [24,26,27]. The history of AI's application in this field can be seen as an evolution from classical machine learning algorithms to the more complex and powerful deep learning architectures capable of handling the scale and complexity of current drug discovery data [7]. Key breakthroughs in areas like neural networks, fueled by advancements in computing, have extended their revolutionary impact from fields like natural language processing and computer vision to various areas of chemistry and biomedicine, fundamentally changing approaches to molecular design and property prediction [27,42].  

This survey provides a comprehensive overview of the landscape of deep learning applications in small molecule discovery. Specifically, it will review recent advancements in areas such as _de novo_ molecular design for generating novel drug candidates with desired properties [1,2,4,14,22,30,33,36,41,42,44], deep quantitative structure-activity relationship (QSAR) modeling for predicting molecular activities and properties [10,24,25,26,27,37], virtual screening for rapidly identifying potential hits from vast chemical libraries [8,14,18,26,35,39], and the prediction of absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties essential for assessing drug-likeness and reducing late-stage failures [10,25,29,33,45]. The survey will appraise case studies, analyze current challenges, and outline future directions for leveraging deep learning to revolutionize the discovery of small molecule drugs [6,15,24,41,44].  

# 2. Deep Learning Fundamentals and Molecular Representation  

Deep learning, a subset of machine learning, has emerged as a transformative technology in various scientific disciplines, including small molecule discovery. At its core, deep learning leverages artificial neural networks with multiple hidden layers to automatically learn hierarchical representations and complex patterns directly from data [7,12,25]. This capability is particularly valuable in drug discovery for tasks such as pattern recognition, data analysis, and modeling intricate relationships between chemical structures and their properties or biological activities [6,25]. Unlike traditional machine learning methods which often rely on manually engineered features, deep learning models can extract relevant features directly from raw or minimally processed molecular data, integrating feature extraction and model building [25].  

The application of deep learning in small molecule discovery is mediated by various neural network architectures, each designed to process different data structures and capture distinct types of relationships. Key architectures relevant to the field include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Graph Neural Networks (GNNs), Transformers, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Deep Neural Networks (DNNs), Autoencoders (AEs), Deep Belief Networks (DBNs), and Differentiable Neural Computers (DNCs) [6,7,10,17,25]. These architectures provide the computational frameworks necessary to model complex chemical space and predict or generate molecules with desired characteristics [2,41].​  

A fundamental challenge in applying deep learning to chemistry is effectively representing molecules in a format that computational models can interpret and learn from [3,11,24,41,44]. Various molecular representation methods have been developed, each possessing unique strengths and weaknesses in capturing the rich chemical information necessary for tasks like property prediction, virtual screening, and denovodesign [3,11,24,44].​  

Molecular descriptors and fingerprints represent historically prominent approaches [9,11,12,26,32]. Descriptors quantify specific physicochemical properties, while fingerprints encode structural fragments or topological features as bit vectors [9,11,32]. These fixed, expert-engineered representations are computationally efficient for storage and comparison, enabling rapid tasks like virtual screening [35]. However, their primary limitation lies in their potential to oversimplify complex molecular information and their inability to adapt during model training, potentially missing nuanced relationships [9,24,27].​  

String-based representations, such as SMILES, encode molecules as linear character sequences, drawing inspiration from natural language processing [14,36,41,42]. Their simplicity makes them popular inputs for sequential deep learning models like RNNs and Transformers, particularly in denovogenerative tasks [3,33,36]. Variants like SELFIES offer improved performance in validity and diversity generation [23]. A significant challenge for string representations is their inherent difficulty in explicitly capturing 3D structural information or complex graph topology, which is vital for understanding spatial interactions [3].​  

Graph-based representations model molecules as graphs where atoms are nodes and bonds are edges, naturally reflecting the molecular structure [3,36,40]. These are the native input format for Graph Neural Networks (GNNs), which can directly learn from the molecular topology without reliance on pre-calculated descriptors [3,11,36]. GNNs excel at capturing complex structural and connectivity information, potentially identifying features missed by traditional methods [11]. While powerful for prediction and generation, GNNs can face challenges in interpretability and computational cost for large molecules [11].​  

Emerging trends highlight the shift towards learned molecular embeddings, often referred to as "neural fingerprints," derived from string or graph inputs [5,9,24,27,28]. These high-dimensional vector representations are learned and optimized during model training, providing a task- and context-aware characterization of molecules that can improve predictive accuracy and tailor representations to specific drug discovery challenges [24,26,28].  

Furthermore, effectively incorporating target protein information and 3D molecular structure is crucial for structure-based drug design (SBDD) applications [3,26,30]. Representing 3D structures, binding sites, or protein-ligand interactions within deep learning models enhances the understanding of molecular recognition and guides the design process towards molecules complementary to the target pocket [4]. This can involve encoding 3D coordinates, utilizing atomistic neural networks, incorporating binding site descriptors, or integrating with docking simulations [18,35,37]. While promising, incorporating 3D information effectively and efficiently into current deep learning architectures remains an active area of research [3].  

In summary, the choice of molecular representation significantly impacts the capabilities and performance of deep learning models in small molecule discovery. The field is evolving from fixed, traditional methods towards more flexible, learned representations and increasingly incorporating explicit 3D and target-specific information to better capture the nuances of chemical space and biological interactions.​  

# 2.1 Molecular Representation Learning  

<html><body><table><tr><td>Representati on Type</td><td>Description</td><td>Input For DL Models</td><td>Captures (2D/3D/Topol ogy)</td><td>Key Pros</td><td>Key Cons</td></tr><tr><td>Descriptors / Fingerprints</td><td>Numerical/Bi t vectors encoding properties/fr agments</td><td>DNN,ML (RF, SVM, NB, KNN)</td><td>Limited 2D/3D, Topology</td><td>Computation ally efficient, Interpretable</td><td>Fixed, Oversimplify, Miss nuances</td></tr><tr><td>String (SMILES, SELFIES)</td><td>Linear character sequences</td><td>RNN, Transformer</td><td>Limited Topology/3D</td><td>Simple, Popular for generative tasks</td><td>Hard to capture 3D/Complex Topology, Invalidity</td></tr><tr><td>Graph</td><td>Atoms as nodes, bonds as edges</td><td>GNN (GCN, MPNN)</td><td>Topology, Some 2D/3D</td><td>Naturally reflects structure, Captures complex info</td><td>Interpretabili ty, Computation al cost (large)</td></tr><tr><td>Learned Embeddings</td><td>High- dimensional vectors learned training during</td><td>Various DL Architectures</td><td>Task- dependent</td><td>Task-aware, Adaptive, Improve accuracy</td><td>Less interpretable than descriptors, Dependent on model/data</td></tr></table></body></html>  

Effective molecular representation is foundational to applying deep learning methods in small molecule discovery, serving as the interface between chemical structures and computational models [11,41]. Various approaches exist, each with distinct strengths and weaknesses concerning their ability to capture structural, electronic, and physicochemical properties relevant to biological activity and synthetic accessibility. These representations are utilized as inputs for diverse deep learning architectures, influencing model performance in tasks such as property prediction, virtual screening, and denovo molecular design.​  

Historically, molecular descriptors (MD) and molecular fingerprints (FP) have been widely used in cheminformatics, particularly within Quantitative Structure-Activity Relationship (QSAR) and Quantitative Structure-Property Relationship (QSPR) modeling [11,12,26,32]. Molecular descriptors are numerical values quantifying physicochemical properties, ranging from 0D (e.g., molecular weight) to 3D (e.g., spatial arrangements) [11,32]. Fingerprints, a specialized form of descriptor, represent molecules as bit vectors encoding the presence or absence of predefined substructures or topological paths [9,11,32]. Common fingerprint types include ExtFP, MACCS, PubChem, and Morgan fingerprints [11,32,35]. While efficient for storage, comparison, and enabling tasks like virtual screening [35], traditional descriptors and fingerprints are typically calculated using fixed formulas and do not adapt during model training [9,24,27]. A significant limitation is their potential to oversimplify complex molecular information, potentially overlooking critical biological context or failing to capture the nuanced impact of subtle structural changes on activity [9,11,32]. Nevertheless, aggregated descriptors can be employed to condition the output of generative models, guiding denovodesign towards molecules with desired properties such as logP, TPSA, and molecular weight [1].​  

String-based representations, inspired by natural language processing, encode molecules as character sequences. The Simplified Molecular Input Line Entry System (SMILES) is the most prevalent string format, widely adopted as input for generative machine learning models due to its simplicity [14,36,41,42,43]. Alternative string representations like DeepSMILES and InChI also exist [36]. SELFIES has emerged as an improvement over SMILES, demonstrating enhanced performance in generating novel molecules with higher scaffold diversity when used with models like DRAGONFLY [23,36]. Despite their utility in sequence-based generative models, string representations inherently struggle to explicitly capture three-dimensional (3D) structural information, which is crucial for understanding ligand-protein interactions [3].​  

Graph-based representations model molecules as undirected graphs where atoms are nodes and bonds are edges [3,36,40]. Node features typically describe atom properties (e.g., atomic number, hybridization), while edge features describe bond properties (e.g., bond type, conjugation) [40]. These representations are particularly well-suited for Graph Neural Networks (GNNs), including Graph Convolutional Networks (GCNs), Weave Networks, and Message Passing Networks (MPNNs), which can learn directly from the graph structure without relying on pre-calculated descriptors [11,36]. This allows GNNs to extract structural information that traditional methods might miss [11]. Directed Message Passing Neural Networks (D-MPNNs) represent a specific advancement, initializing and iteratively updating atom and bond vectors based on their surrounding chemical environment, thereby capturing contextual molecular connectivity effectively and overcoming limitations of fixed representations [9,19]. GNNs have been successfully applied in various deep learning tasks, such as denovodrug design, sometimes integrated with reinforcement learning [11], and predicting complex structures like PROTAC linkers while incorporating spatial constraints derived from the fragment anchors [38]. However, a drawback of GNNs can be their "black box" nature and lower interpretability compared to descriptor-based methods, though research is ongoing to develop more interpretable architectures, such as those incorporating attention mechanisms [11].  

A key emerging trend in deep learning for small molecule discovery is the shift from fixed, expert-engineered representations to learned molecular embeddings [5,9,24,27,28]. In deep QSAR, molecular embeddings replace traditional descriptors, creating high-dimensional vector representations that are learned and optimized during model training on specific tasks, leading to improved property prediction accuracy [24,28]. These embeddings, sometimes referred to as "neural fingerprints" when generated by graph convolution models, provide a task- and context-aware characterization of molecules [5,9,19]. This allows deep learning models to automatically identify and weigh the most relevant structural and chemical features for a given task, such as predicting target binding or ADMET properties [26]. The success of D-MPNNs in discovering novel antibacterial drugs like halomicin and avilamycin exemplifies the power of these learned, context-aware representations in practical applications [19].​  

In summary, molecular representation learning for deep learning has evolved from relying on fixed, expert-defined descriptors and fingerprints towards data-driven, learned embeddings derived from string-based and, increasingly, graphbased inputs [5,9,24,27,28]. While string representations like SMILES and SELFIES remain popular for generative tasks, graph-based methods processed by GNNs are gaining prominence due to their ability to capture complex topology and context [3,11,36]. Learned embeddings, often derived from these inputs, represent a powerful paradigm shift, offering representations that are tailored to specific prediction tasks and can adapt during learning, promising enhanced performance in various drug discovery applications [5,9,24,28]. Emerging trends include the development of more robust string formats, advanced GNN architectures for better interpretability, and methods to effectively incorporate 3D structural information [3,11,23].  

# 2.2 Deep Learning Architectures  

![](images/f9eb31bd224727c23eeb31f6db81db93c91dc892f82944038358dffb6b64891b.jpg)  

This section categorizes and describes the fundamental principles and typical structures of the main deep learning architectures employed in small molecule discovery [6,7,10,17,25]. Different deep learning architectures, such as Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Convolutional Neural Networks (CNNs), Transformers, Deep Neural Networks (DNNs), Autoencoders (AEs), Deep Belief Networks (DBNs), and Differentiable Neural Computers (DNCs), are discussed [6,10,17,25]. These models leverage multiple hidden layers to automatically extract complex, non-linear features from diverse data representations [10,12,25].  

For each architecture, the discussion details how it is adapted to process molecular data, considering various representations like SMILES strings, molecular graphs, or image-based formats, and highlights its specific strengths and weaknesses for tasks such as molecular property prediction, denovomolecule generation, and molecule-target interaction modeling [9,24,30]. The importance of data quality, rigorous curation, and appropriate data representation for effectively training these deep learning models is also addressed [24]. Examples of successful applications across different architectures for solving various drug discovery challenges are provided. The overarching goal of utilizing these diverse deep learning architectures is to effectively explore the vast chemical design space to identify high-performance molecules with desirable properties, including potential synthesizability [2,41].  

# 2.2.1 Recurrent Neural Networks (RNNs)  

Recurrent Neural Networks (RNNs) represent a foundational class of deep learning models well-suited for processing sequential data, including the SMILES strings used to represent chemical molecules [12,25,27,36]. Functioning as chemical language models, RNNs learn the statistical distribution and syntax of molecules from training datasets by processing SMILES strings character by character or token by token [3,24]. During molecule generation, the RNN predicts the probability of the next character based on the current input and its internal state, which summarizes the sequence processed so far. The highest probability character is typically sampled and fed back as the next input, continuing until an end-of-sequence token is generated [3]. This sequential prediction process allows RNNs to generate novel molecules [1,4,24].​  

Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) are common variants that address the limitations of vanilla RNNs in handling long sequences by incorporating gating mechanisms. These gates enable the network to selectively remember or forget information, improving their ability to learn dependencies over longer SMILES strings compared to traditional RNNs [3,7,22,25]. LSTMs, for example, utilize complex gated structures to learn characteristic molecular properties even from extended SMILES representations [22].  

RNNs, particularly LSTMs and GRUs, have been widely applied as chemical language models in early SMILES-based de novo drug design efforts [24,30]. By learning the statistical relationships within large molecular databases like ChEMBL or ZINC, these models can form a latent space from which new molecular structures can be sampled [1,14,24,27]. To guide the generation process towards molecules with desired properties, conditional RNNs can be employed, where the initial memory state or input is modified based on target molecular descriptors [1]. Transfer learning is another strategy, involving pre-training a GRU-RNN model on a large dataset and fine-tuning it on smaller, specific datasets relevant to the target property [20]. Specific successful applications include the use of LSTM-based models trained on ChEMBL data and finetuned on target-specific sets to generate p300/CBP inhibitors and RIPK1 kinase inhibitors, leading to the discovery of preclinical candidates [6].  

Despite their successes, a common challenge for RNNs in molecular generation is the tendency to produce syntactically or chemically invalid SMILES strings [10,33]. A primary limitation of RNNs, including LSTMs and GRUs, is their difficulty in effectively capturing long-range dependencies within SMILES sequences. This limitation contributes significantly to a higher rate of invalid molecule generation compared to architectures better suited for such tasks [33]. Studies have indicated that simply increasing the depth or using bidirectional configurations of GRUs may not adequately address this issue in de novo drug design contexts [33].​  

Strategies to mitigate the invalidity problem and enhance the generation of molecules with desired properties include integrating the RNN into frameworks that provide external guidance or constraints. Reinforcement learning has been applied, where the RNN generator is trained via rewards based on the properties of generated molecules, effectively optimizing for specific objectives [8]. For instance, RNNs have been used within Monte Carlo tree search frameworks like ChemTS to guide the search towards valid and promising structures [14]. Furthermore, exploring enhanced architectures such as memory-augmented neural networks, like the Differentiable Neural Computer (DNC) which incorporates an external memory accessed by an LSTM controller, represents an avenue for improving the model's capacity to handle complex  

sequential dependencies [42]. While powerful for sequential data, the inherent limitations of RNNs in precisely modeling complex, non-sequential dependencies present in molecular graphs, as represented linearly by SMILES, highlight the need for alternative or augmented architectural approaches in de novo design.  

# 2.2.2 Generative Adversarial Networks (GANs)  

Generative Adversarial Networks (GANs), initially introduced in 2014, represent a powerful class of deep learning algorithms that are applied extensively in various generative tasks, including image synthesis [7,12]. In the context of small molecule discovery, GANs are recognized as significant generative models for denovomolecular design and lead optimization [5,25,30,44].​  

The core architecture of a GAN comprises two competing neural networks: a generator and a discriminator [6,27]. The generator $( G )$ aims to capture the data distribution of real molecules and to produce synthetic samples that are indistinguishable from authentic ones. The discriminator $( D )$ , on the other hand, functions as a binary classifier tasked with distinguishing between real molecules from the training set and fake molecules generated by $G$ [6,24,27].​  

The adversarial training process involves a minimax game where the generator and discriminator are trained simultaneously [42]. The generator attempts to maximize the probability of the discriminator making a mistake (i.e., classifying generated molecules as real), while the discriminator attempts to minimize this probability (i.e., correctly distinguishing real from fake) [27]. This competitive dynamic drives the generator to produce progressively more realistic and diverse molecular structures [27,42]. For instance, in the Reinforced Adversarial Neural Computer (RANC) model, the discriminator—a Convolutional Neural Network (CNN)—evaluates generated molecular structures and attempts to classify them as fake or real, thereby incentivizing the generator to create realistic samples [42]. The discriminator’s objective is often framed as minimizing the cross-entropy between the ground truth labels and the generated sequences [42]. This adversarial mechanism enables GANs to explore the chemical space efficiently and generate novel molecular entities.  

A key strength of GANs in molecular design is their ability to be conditioned on specific properties or attributes, guiding the generation process towards molecules with desired characteristics [1,24,30]. This conditioning can be implemented by incorporating molecular descriptors or target properties into the generator’s input [1]. For example, models can be conditioned on attributes such as activity against a specific protein, solubility, or synthetic feasibility [6]. Polykovskiy et al. (2018) proposed the ECAAE model, based on GAN principles, which generates molecular structures conditioned on various attributes [6]. The ECAAE was successfully applied to generate a novel JAK3 inhibitor, demonstrating promising invitro activity and selectivity [6]. This exemplifies how generative models like GANs can be utilized to design molecules with improved potency, selectivity, or other drug-like properties.  

While powerful, training GANs presents challenges, notably mode collapse—where the generator produces a limited variety of samples, failing to capture the full diversity of the training data distribution—and instability in the training process. Strategies to mitigate these issues are an active area of research. Compared to other generative models like Variational Autoencoders (VAEs), GANs are often noted for their capacity to generate high-fidelity samples, although VAEs typically offer more stable training and a disentangled latent space. Furthermore, a significant limitation of current generative models, including GANs, remains the challenge of ensuring that generated molecules are synthetically feasible and possess acceptable pharmacokinetic and safety profiles, thereby requiring downstream validation and optimization.  

# 2.2.3 Variational Autoencoders (VAEs)  

Variational Autoencoders (VAEs) are a class of generative models widely applied in denovomolecular design and lead optimization [25,44]. VAEs operate on an encoder–decoder architecture [9]. The encoder is tasked with compressing molecules, often represented as SMILES strings or structural graphs, into a continuous, low-dimensional latent space [5,9]. Conversely, the decoder learns to reconstruct the original molecular structures from this latent representation [9,24].  

A key characteristic of VAEs is their ability to learn a continuous latent representation of chemical space [24,30]. This continuity allows for interpolation between existing molecules in the latent space, enabling the exploration of novel chemical entities [5,25]. By sampling from the learned latent distribution, the decoder can generate novel molecules [9]. This process is particularly valuable for denovodesign due to its potential to explore the vast chemical space efficiently [25].  

Ensuring the quality and diversity of generated molecules is crucial, and regularization plays a vital role in achieving this [24]. VAE training involves optimizing parameters to minimize a loss function that typically includes a reconstruction term and a regularization term, often the Kullback–Leibler (KL) divergence [36]. This KL divergence term, which measures the difference between the learned latent distribution and a prior distribution (e.g., a standard normal distribution), encourages the latent space to be smooth and chemically meaningful [24]. For example, the loss function  

# 无效公式  

for encoder parameters $\theta$ and decoder parameters $\varphi$ incorporates both reconstruction loss and a KL regularization loss.   
Regularization helps prevent overfitting and promotes the generation of diverse and valid molecules.  

A powerful extension is the use of Conditional VAEs (CVAEs), which enable the generation of molecules with specific desired properties [30]. By conditioning the model on target properties (e.g., biological activity, physical properties, or structural features like pharmacophores [30]), the sampling process from the latent space can be directed towards generating molecules possessing these attributes [6,9]. For instance, CogMol, introduced in 2020, is a VAE model that employs a multiattribute controlled sampling scheme to design novel molecules for specific protein targets, incorporating constraints on factors such as target affinity, selectivity, drug-likeness, synthetic feasibility, and toxicity [6]. This controlled generation resulted in molecules with favorable predicted properties for SARS-CoV-2 targets [6]. VAEs have also been successfully applied in tasks like generating linkers for PROTAC molecules [38] and optimizing molecular structures, such as in the DeLinker model which encodes fragment structure and the entire molecule to generate linkers [8]. While powerful, limitations can include challenges in generating perfectly valid or novel structures consistently, and controlling specific complex properties can still be difficult [30].​  

# 2.2.4 Convolutional Neural Networks (CNNs)  

Convolutional Neural Networks (CNNs) represent a highly impactful architecture within deep learning, widely recognized for their success across diverse domains—including image and speech recognition, as well as natural language processing [7,12,25]. Originally developed to enable computers to discern visual patterns [7], typical CNN models are structured with text, convolutional, and pooling layers designed to systematically extract increasingly complex features from input data [25].  

The application of CNNs to small molecule discovery often involves representing molecular structures in formats amenable to convolutional operations, such as two-dimensional images or grids. This approach allows the convolutional filters to scan these representations and automatically identify relevant local features, analogous to detecting visual patterns. These features can correspond to chemical motifs, structural patterns, or other localized characteristics within the molecule. A notable example includes converting SMILES strings into two-dimensional matrix data, which then serves as input for masked neural network layers in models like PixelCNN [6]. Similarly, although applied to proteins, the use of CNNs in AlphaFold to convert amino acid sequences into distance and torsion angle matrices further illustrates this principle of transforming biological sequences into grid-like data for feature extraction [7]. A significant advantage of employing CNNs in this context is their capability to learn features directly from raw or minimally processed data, thereby integrating the feature extraction and classification steps—and potentially discarding the need for manual feature engineering [25,34]. They effectively exploit spatially stable local correlations within the input representation [34].  

CNNs have been leveraged for various tasks in drug discovery and related fields. While the specific applications of binding affinity prediction and virtual screening using CNNs for small molecules are broadly discussed in the field [25], the provided digests highlight other innovative uses. For instance, a PixelCNN model was proposed for molecular generation based on SMILES strings, demonstrating its effectiveness in fragment growth optimization of molecular structures—making it wellsuited for fragment-based drug discovery [6]. In contrast to RNNs, which showed superior performance in directly predicting molecular structures for target properties, the PixelCNN framework excelled in the fragment growth task [6]. Another approach, DEVELOP, utilizes a CNN as a feature extractor for ligand-based drug design, incorporating 3D pharmacophore constraints [8]. DeepFrag, which employs a CNN prediction strategy combined with a classification module, achieved a successful reconstruction rate on a test set for molecular structure optimization, though its generality is limited by simplifying the generation problem to classification [8]. Beyond small molecule–specific tasks, CNNs have also been applied to identify short antimicrobial peptides from DNA sequences in the Deep-AmPEP30 platform [7] and to analyze mass spectrometry data for small molecule classification [34], indicating their utility in analyzing various biological and chemical data representations. Furthermore, CNNs are employed as discriminators in generative models like RANC due to their proficiency in sequence classification tasks [42].  

Compared to conventional machine learning algorithms, CNNs offer potential advantages such as higher accuracy and effectiveness on raw data [34]. They inherently learn hierarchical representations of data features—from simple patterns in early layers to more complex arrangements in deeper layers. However, when compared to other deep learning methods, particularly Graph Neural Networks (GNNs) which naturally operate on molecular graphs, CNNs face certain limitations. Representing molecules as images or grids necessitates a specific featurization process (e.g., converting SMILES to a matrix), which might not fully capture the intrinsic graph structure and connectivity crucial for many molecular properties. This need for appropriate featurization can be a challenge. Additionally, while CNNs excel at learning complex patterns, interpreting the exact chemical meaning of the learned features within the convolutional layers can be difficult, posing a challenge for understanding the model's predictions and gaining chemical insights. The simplification of complex generation problems into classification, as seen in DeepFrag, can also limit the model's applicability to novel scenarios [8]. Despite these challenges, the ability of CNNs to learn complex local patterns directly from featurized data makes them a valuable tool in the deep learning toolkit for small molecule discovery.​  

# 2.2.5 Graph Neural Networks (GNNs)  

Graph Neural Networks (GNNs) represent a class of deep learning models particularly well-suited for molecular data, which is inherently structured as graphs where atoms are nodes and bonds are edges [32]. This graphical representation allows GNNs to directly encode molecular structures, capturing the connectivity and features of atoms and bonds [15]. This approach offers advantages over traditional molecular representations like SMILES strings, which are sequential and may struggle to capture complex 2D or 3D structural information efficiently. The historical development of GNNs for molecular data includes early models proposed around 2009, with significant progress made in 2016 through the application of convolutional algorithms to graphs, enhancing computational drug discovery efforts [32].​  

GNNs operate by iteratively aggregating information from a node's neighbors, allowing them to learn rich node and edge embeddings that encapsulate local and potentially long-range chemical information [15]. Different GNN architectures employ varying message-passing mechanisms. Examples include Graph Convolutional Networks (GCNs), Weave Networks, and Message Passing Neural Networks (MPNNs) [32,36,37]. MPNNs, for instance, update node hidden states by summing messages passed from neighboring nodes, often using gated recurrent units (GRU) for iterative updates [40]. A typical message passing step can be represented as:  

$$
m _ { v } ^ { t + 1 } = \sum _ { w \in N ( v ) } E ( e _ { v w } ) h _ { w } ^ { t }
$$  

$$
h _ { v } ^ { t + 1 } = G R U ( m _ { v } ^ { t + 1 } , h _ { v } ^ { t } )
$$  

where $h _ { v } ^ { t }$ is the hidden state of node $v$ at iteration $t$ , $N ( v )$ is the set of neighbors of $v$ , $E$ is a function (e.g., MLP) processing edge features $e _ { v w }$ , and GRU updates the state [40]. Some architectures, like Knowledge-Embedded Message Passing Neural Networks (KEMPNNs), integrate attention mechanisms to weigh the contribution of different neighbors or knowledge features, such as using an attention branch to modulate the final atom embedding $h _ { v } ^ { f } = h _ { v } ^ { T } \cdot a _ { v }$ ​ where $a _ { v }$ ​ is an attention score [40]. Graph embeddings for property prediction can then be obtained by aggregating node embeddings, for example, via summation or set2set aggregation, followed by a prediction head $\tilde { y } = \phi ( r )$ [40].​  

GNNs have been extensively applied in various tasks within small molecule discovery. A primary application is molecular property prediction, including pharmacokinetics, pharmacodynamics, and toxicity. Specifically, GNNs, such as Directed Message Passing Neural Networks (D-MPNNs), have been used to derive molecular characterizations that improve accuracy in predicting ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) indicators and bioactivity, including the discovery of antibacterial drugs [19,29]. Combining GNNs with other techniques has also proven effective; for example, Graph CNNs have been combined with DNA-encoded library (DEL) data for virtual screening to find new drug-like molecules, and ACP-GCN, using graph convolutional networks, has been applied to discover anti-cancer peptides [7]. Furthermore, GNNs are utilized in models that generate molecular description vectors which are then learned by neural networks [5]. GNNs can also encode complex representations like the 2D structure of a ligand and the 3D structure of a protein binding site, transforming them into a compressed feature vector, as seen in Graph Transformer Neural Networks (GTNNs) used for ligand-protein interactions [4].  

Beyond prediction, GNNs play a crucial role in denovomolecule design and generation [24]. Models utilize graph-tosequence architectures that process molecular graphs using GNNs [23]. Other generative approaches employ GNNs, such as GraphScaffold, which gradually adds atoms and bonds based on a scaffold [8]. Models like GGNN have demonstrated effectiveness in molecular graph generation compared to other deep generative models [38]. 3D generative models, like 3DScaffold, also leverage GNNs (e.g., G-SphereNet) to generate 3D coordinates of new candidates while adhering to physical constraints [8].​  

GNNs possess several advantages over traditional methods, particularly in their ability to inherently capture the complex graph structure of molecules and learn representations that incorporate both local and potentially long-range atomic interactions [19,29]. They excel at modeling complex structural features that may not be easily represented by fixed-size molecular descriptors. However, GNNs also face limitations. These include potential computational complexity, especially for large or dense graphs, and sensitivity to the quality and structure of the input graph data. While GNNs can directly encode molecular structures, their impact on universally improving the accuracy and generality of small molecule methods has been noted as somewhat limited [15]. Challenges also remain in ensuring the generation of chemically valid and readily synthesizable molecules in denovodesign applications [15].​  

# 2.2.6 Transformers  

Transformer architectures, renowned for their success in natural language processing, have been increasingly applied to molecular design tasks, primarily leveraging their attention mechanism to process molecular representations such as Simplified Molecular Input Line Entry System (SMILES) strings [33]. The attention mechanism enables the model to weigh the importance of different tokens (atoms or substructures) within a sequence, thereby capturing complex, long-range dependencies and relationships between distinct parts of a molecule, which is challenging for models limited by sequential processing [33].​  

Transformers offer distinct advantages over traditional Recurrent Neural Networks (RNNs), particularly when handling long SMILES sequences [15]. Unlike RNNs, which process tokens sequentially and can suffer from vanishing or exploding gradients over long distances, the self-attention mechanism allows Transformers to access information from any position in the input sequence directly, making them more effective at modeling long-range dependencies [33]. This capability is crucial for generating valid and complex molecular structures represented by long SMILES strings [33]. Furthermore, Transformer-based models can be designed to generate molecules conditioned on specific properties of interest, such as calculated log(P) or desired scaffolds, facilitating the design of molecules with targeted characteristics [9]. Models like MolGPT, based on the Generative Pre-trained Transformer (GPT) architecture, demonstrate this ability to generate new SMILES codes by conditioning on property vectors [9].​  

Transformers are effectively employed as generative models for denovosmall molecule design. A Transformer-encoderbased generative model, for instance, utilizes its multihead self-attention to manage long-range dependencies in SMILES sequences for this purpose [33]. These models are typically trained on extensive datasets of SMILES strings to internalize the “grammatical rules” governing valid chemical structures [33]. Specific applications include linker design, where Transformer language models like SyntaLinker generate SMILES strings to bridge molecular fragments [8]. Beyond generative tasks, Transformers have also been applied to predictive modeling, such as predicting drug-target interactions and developing algorithms for drug-likeness and toxicity prediction (ADMET indicators) [10,29].​  

Comparative studies highlight the performance benefits of Transformer-based models over RNNs in molecular generation. Compared to Gated Recurrent Units (GRUs), a type of RNN, Transformer models have demonstrated superior performance in terms of training stability and accuracy in generating valid molecules [33]. They also show better performance in reinforcement learning settings for optimizing generated molecules [33]. While the paper [15] indicates that the application of Transformers in small molecule drug discovery has shown limited improvement compared to their impact in other fields, the specific performance metrics available from studies like [33] suggest tangible advantages over RNNs regarding fundamental generation quality aspects like validity and training dynamics, which are prerequisites for achieving diversity and potential synthetic feasibility. Further research continues to explore how to fully leverage Transformer capabilities for complex tasks like controlling diversity and ensuring synthetic accessibility.​  

# 2.2.7 Other Architectures (DNNs, AEs, DBNs, DNCs)  

Beyond widely discussed architectures like CNNs and RNNs, other deep learning models—including Deep Neural Networks (DNNs), Autoencoders (AEs), Deep Belief Networks (DBNs), and Differentiable Neural Computers (DNCs)—have demonstrated utility in various aspects of small molecule discovery.  

Deep Neural Networks (DNNs), characterized by having more than two hidden layers, are capable of learning complex, nonlinear relationships within data [12,25]. This ability makes them suitable for tasks requiring intricate pattern recognition, which is prevalent in drug discovery data [25]. DNNs have become a preferred method in Quantitative Structure-Activity Relationship (QSAR) applications, particularly following benchmark challenges like the Kaggle Merck Molecular Activity Challenge in 2013 and the Tox21 Data Challenge in 2015 [26]. They are extensively applied in property prediction, including biological activity, toxicity, and pharmacokinetic properties such as ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) [5,7,10,29]. For instance, DNNs can process molecular representations like Simplified Molecular Input Line Entry System (SMILES) strings to predict compound cellular activity [7]. In virtual screening, DNNs are employed in methods like Deep Docking to rapidly predict docking scores for large chemical libraries after being trained on a subset, enhancing the efficiency of identifying potential hits [9]. Moon et al. utilized feedforward neural networks, a type of DNN, in multi-task learning models to investigate potential mechanisms of action between targets and drugs [10]. While powerful in capturing non-linear relationships and processing complex data like images [25], a critical analysis of DNNs reveals both advantages and disadvantages. Their primary advantage lies in their capacity to model highly non-linear mappings. However, they are susceptible to overfitting, especially with limited data, and typically require large datasets for optimal performance [25].​  

Autoencoders (AEs) represent another class of neural network widely used in unsupervised learning for tasks such as dimensionality reduction, data compression, and learning low-dimensional representations or latent embeddings [6,9,12,17]. AEs consist of an encoder that maps input data to a latent space and a decoder that reconstructs the input from this latent representation [27]. Deep Autoencoder Neural Networks (DAENs) are a form of AE used in drug discovery [5]. AEs are fundamental components within more complex generative models like Variational Autoencoders (VAEs) and Adversarial Autoencoders (AAEs), used for denovomolecular design [12,30]. In molecular generation, the encoder can map molecules to a continuous latent space, facilitating property prediction and optimization, while the decoder maps optimized latent representations back to generate new molecules with improved properties [27]. AEs can also extract embeddings by transforming molecular representations like non-canonical SMILES into canonical forms, capturing crucial molecular characteristics [9]. AEs have also been used in conjunction with other models like Support Vector Machines (SVM) and DNNs for tasks such as predicting genes related to diseases [10].​  

Deep Belief Networks (DBNs), composed of multiple layers of non-linear hidden units, offer an alternative architecture that can help mitigate issues such as redundancy and overfitting, contributing to robust model performance [25].  

More advanced architectures, such as Differentiable Neural Computers (DNCs), incorporate mechanisms for memory retention, addressing limitations in handling long-term dependencies present in sequence data. In the context of generative models, like the Reinforced Adversarial Neural Computer (RANC) architecture, a DNC is utilized as the generator [42]. The DNC component includes an explicit memory bank that allows it to account for both short and long-term patterns within sequences, enhancing its capability to generate complex data structures [42]. The DNC consists of a controller, which can be any type of DNN, coupled with external memory, and is trained end-to-end [42]. This integration of external memory in DNCs provides a mechanism to potentially improve the stability and quality of generated outputs compared to models lacking such memory capabilities, contributing to enhanced generative performance in denovomolecular design.​  

# 2.3 Incorporating Target Information and 3D Structure  

![](images/7022cf422b59e1824f9d00e861f1c90590884979ea88383d22f0eb84c2ad7b4f.jpg)  

Structure-based drug design (SBDD) fundamentally relies on the three-dimensional (3D) structure of the target protein, particularly its binding site, to guide the discovery and optimization of small molecules. In contrast to ligand-based approaches that primarily utilize information about known active molecules, incorporating target structure information allows for a more direct understanding of the molecular interactions governing binding affinity and selectivity [30]. This spatial context is crucial for designing molecules that are complementary in shape, size, and physicochemical properties to the target pocket, adhering to principles like the "lock and key" mechanism [4]. While many early generative models focused on 2D molecular representations, potentially missing critical information about protein-ligand binding sites, the field is increasingly moving towards methods that incorporate 3D structural data [27,41]. The inclusion of 3D descriptors in models enhances the accuracy of protein-ligand recognition by providing essential spatial context [26].  

Various approaches have been developed to represent protein structures and protein-ligand interactions within deep learning models. Predicting the target protein's 3D structure itself is a key prerequisite, with methods like AlphaFold (AF2) utilizing multiple sequence alignment (MSA) and pairwise representations processed by dual-track networks such as Evoformer to predict structures from amino acid sequences [3,7,9]. Once the protein structure is available, different strategies are employed to represent the binding site or the protein-ligand complex. Some models encode the 3D structure of the protein-binding site directly [4], while others operate on 3D molecular conformers using atomistic neural networks [37]. Approaches can represent interactions through 3D binding conformations of protein-ligand complexes including atomic physicochemical properties [3,30]. Knowledge-based methods, like PharmAI DiscoveryEngine, mine the Protein Data Bank (PDB) to identify and describe binding sites using unique fingerprints capturing geometric, physicochemical, and noncovalent interaction characteristics [18]. Other models incorporate structural 3D information by considering the relative spatial positions (angle and distance) of molecular fragments [38]. Implicit incorporation of target information is also possible through integration with molecular docking programs, where docking scores derived from tools like FRED are used to train QSAR models or act as reward functions in generative processes [14,35]. Deep learning-based docking techniques, such as DiffDock, transform the docking process into a generative modeling task, improving efficiency [19]. Models like DRAGONFLY explicitly learn drug target-ligand interaction networks by integrating structural and ligand information [23]. MolAICal exemplifies designing 3D drugs directly within 3D protein pockets, combining DL/GA for design with docking/DL for evaluation [7].​  

3D-aware deep learning models offer significant advantages over purely ligand-based approaches. By explicitly considering the protein binding site and 3D interactions, these models are better positioned to capture intricate details relevant to binding affinity and selectivity [26]. Methods that design molecules directly within the target binding pocket, such as MolAICal and DeepLigBuilder, can generate molecules with favorable binding poses and appropriate 3D structures from the outset [3,6,7]. DeepLigBuilder, which uses 3D structural representations of ligand-receptor interactions, aims to design chemically and conformationally valid 3D molecules end-to-end [3]. Furthermore, structure-aware generation can facilitate the discovery of novel chemotypes by exploring chemical space constrained by the specific requirements of the target site. For instance, DeepLigBuilder discovered compounds with new interactions with a target protein [6], and PharmAI DiscoveryEngine uses binding site characteristics to generate novel molecular scaffolds [18]. Models like RELATION, by utilizing 3D binding conformations, are shown to enhance efficiency in generating molecules with good pharmacophore features and binding modes tailored for specific targets [30].  

Pharmacophore features, which represent key molecular properties and their spatial arrangement necessary for molecular recognition, can be effectively used to constrain the vast chemical space explored by deep learning models. By incorporating pharmacophore constraints derived from the target binding site, models can be guided towards generating molecules with a higher likelihood of possessing desired binding properties [30]. Approaches like DEVELOP and STRIFE leverage pharmacophore information extracted from CNNs or binding pocket descriptors (such as FHM) as conditions to guide fragment replacement and achieve structure-aware conditional generation [3,8]. This ensures that generated molecules exhibit the correct spatial distribution of features required for productive interactions with the target.  

Despite their utility, defining appropriate pharmacophore models and integrating them effectively into deep learning architectures presents challenges. Accurately capturing the essential features of complex binding sites into a pharmacophore model requires careful consideration of diverse interaction types and their relative importance. Furthermore, seamlessly incorporating these discrete or continuous feature representations into various deep learning frameworks, such as generative adversarial networks (GANs), variational autoencoders (VAEs), or graph-based models, requires innovative architectural designs to ensure the constraints are respected during the generation process while maintaining molecular diversity and chemical validity.​  

# 3. Applications of Deep Learning in Small Molecule Discovery  

The application of deep learning (DL) across the small molecule drug discovery pipeline represents a transformative shift, offering significant potential to enhance efficiency, accuracy, speed, and cost-effectiveness compared to traditional methods [10,12,32,39]. This section systematically reviews the key applications of DL in each stage, from initial target identification to synthesis planning and specialized molecule design.  

In the crucial initial stage of target identification and validation, artificial intelligence (AI), encompassing machine learning (ML) and DL, serves as a powerful tool for analyzing vast biological and multi-omics datasets [5,7,10,21,32,39]. DL models, in particular, excel at processing complex, high-dimensional data, automatically learning intricate patterns and representations that enable the prediction of potential causal relationships and the identification of disease-related genes or proteins as therapeutic targets [5,10,21]. A central application involves predicting drug–target interactions (DTIs), leveraging architectures such as Deep Neural Networks (DNNs), Transformer-based networks, and Feedforward Neural Networks, sometimes offering advantages like bypassing the need for explicit 3D structural data required by traditional methods [10,17,32]. While computational predictions provide strong hypotheses, rigorous experimental validation of identified targets remains a critical challenge to confirm biological relevance and suitability, along with assessing potential off-target effects [19].​  

Following target identification, virtual screening (VS) is a fundamental computational technique for identifying potential lead compounds from extensive chemical libraries, aiming to predict molecular binding to a target and drastically reduce the need for costly and time-consuming experimental high-throughput screening (HTS) [6,9,19,39,45]. AI and DL methods are enhancing traditional structure-based (SBVS) and ligand-based (LBVS) approaches by improving scoring functions and enabling efficient exploration of significantly larger chemical spaces [6,9,10,19]. AI-based scoring functions, employing models like XGBoost or multilayer perceptrons, demonstrate improved flexibility and accuracy in capturing complex interactions compared to classical methods [6,10]. DL-based VS methods, such as Deep Docking, leveraging deep QSAR models, dramatically accelerate screening, showing remarkable data reduction and enrichment rates when applied to ultralarge chemical databases [9,24,28,35]. Case studies highlight substantial improvements in speed and increased hit rates over traditional benchmarks [6,18]. A persistent challenge lies in acquiring diverse and representative training data necessary for model generalizability across varied targets and chemical spaces [19].  

AI-assisted denovomolecular design represents a significant paradigm shift, enabling the generation of novel chemical entities with desired properties without strict adherence to existing structures, thereby facilitating the exploration of vast, uncharted chemical space and accelerating the discovery cycle [2,9,10,12,39]. This is primarily driven by generative deep learning models that learn the intrinsic patterns and syntax of molecular representations [2,24,28]. A variety of architectures, including Recurrent Neural Networks (RNNs) like LSTM and GRU, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Normalizing Flows, Diffusion Models, and Graph Neural Networks (GNNs), have been successfully applied [5,6,9,10,19,20,24,25,28,38]. To guide the generative process towards molecules with specific characteristics, reinforcement learning (RL) and other optimization techniques are widely integrated, often utilizing predictive models as reward functions [1,7,16,25,33]. Evaluating the generated molecules for novelty, diversity, chemical validity, synthetic feasibility, and desired biological/ADMET properties remains a key challenge requiring multi-dimensional scoring and validation [2,6,9,33]. Successful case studies highlight the capability of these AI-driven methods to generate potent and novel drug candidates, often significantly accelerating their progression [6,7,16,41,43].​  

Following hit identification, lead optimization is the critical stage focused on improving the potency, selectivity, and pharmacokinetic (ADMET) properties of promising compounds [10]. ML, particularly DL, is increasingly powerful in aiding the design and optimization process [21]. AI approaches include objective-oriented methods aimed at optimizing specific property functions and structure-oriented methods that focus on local molecular modifications, often leveraging generative AI for tasks like fragment replacement, linker design, or scaffold hopping [8]. DL plays a crucial role in the hit-to-lead phase by enabling the prediction and multi-objective optimization of various drug-like properties and potential toxicities early on, providing comprehensive guidance for candidate selection and refinement towards favorable clinical profiles [19]. Examples demonstrate AI-assisted identification of optimized scaffolds and compounds with improved efficacy and selectivity [18,22].  

Accurate molecular property prediction, especially for ADMET (absorption, distribution, metabolism, excretion, and toxicity) parameters, is indispensable throughout the drug discovery pipeline, as poor ADMET profiles are a major cause of clinical failure [4,9,12,25,29,45]. DL models significantly advance beyond traditional QSAR and ML by automatically learning complex features directly from molecular representations, bypassing the need for extensive manual feature engineering [6,27]. Architectures like Graph Convolutional Neural Networks (GCNNs), Deep Neural Networks (DNNs), Recurrent Neural  

Networks (RNNs), and Convolutional Neural Networks (CNNs) are widely employed for predicting a broad range of properties, including bioavailability, solubility, permeability, lipophilicity, hepatotoxicity, hERG toxicity, and various other ADMET endpoints [5,6,9,10,19,25,26,27,29,33,37]. DL models have shown competitive or superior performance compared to traditional methods in predicting key ADMET parameters [5,29]. However, challenges persist due to the scarcity of highquality experimental data, particularly for novel compounds, and the inherent complexity of biological systems [10,29,40]. Advanced strategies like multi-task learning, transfer learning, and multi-modal learning are being explored to address these limitations and improve model robustness and generalizability [25,29].​  

Deep learning is also proving invaluable in synthesis planning, particularly for retrosynthesis prediction, which involves working backward from a target molecule to identify suitable starting materials and reaction pathways [5,39]. DL models predict the necessary reactants, reagents, and reaction conditions, learning reaction transformations directly from data through template-based or template-free approaches [24]. Combining single-step prediction models with graph search algorithms allows for the design of comprehensive multi-step synthetic routes [7,39]. Advanced frameworks have demonstrated state-of-the-art performance and robustness, often preferred by expert chemists and capable of discovering novel routes not explicitly captured by traditional rule-based systems [31]. Challenges include handling the inherent complexity and ambiguity of chemical reactions, where multiple pathways can lead to the same product. Integrating these predictive tools into automated molecular design and synthesis systems holds significant potential to further accelerate the drug discovery process [24].​  

Furthermore, deep learning is being applied to specialized design tasks, such as the intricate design of proteolysis-targeting chimeras (PROTACs), particularly focusing on optimizing the crucial linker region that connects the target binder to the E3 ligase binder [38]. Encoder-decoder networks are being developed to generate novel linker structures, aiming to facilitate the required ternary complex formation for effective target degradation [38].​  

Overall, deep learning has become an indispensable tool across the small molecule drug discovery pipeline. It offers substantial advantages in processing complex biological and chemical data, accelerating computational workflows, and improving the accuracy of predictions and the quality of generated molecules. While significant challenges related to data availability, model interpretability, and robust validation remain, the continued advancements in deep learning architectures and methodologies, coupled with increasing data availability, promise to further revolutionize the drug discovery process, making it faster, cheaper, and more successful.​  

# 3.1 Target Identification and Validation  

The identification and validation of promising molecular targets represent a critical initial phase in the drug discovery pipeline [32]. Artificial intelligence (AI), encompassing both traditional machine learning (ML) and deep learning (DL), has emerged as a powerful tool to accelerate and enhance this process by analyzing vast biological datasets [5,10,21]. AI techniques facilitate the prediction of potential causal relationships and the identification of disease-related genes or proteins that could serve as therapeutic targets [5,21]. For instance, ML methods have been applied to predict potential causal relationships and identify disease-related genes using techniques such as decision trees [5]. Similarly, Support Vector Machines (SVMs) have been employed to categorize proteins as suitable drug targets for various cancers [5]. Natural Language Processing (NLP) aids in extracting and identifying associations between drugs, diseases, genes, and targets from the scientific literature, providing valuable evidence for target selection [5]. ML is also instrumental in guiding virtual screening campaigns, offering a more efficient alternative to traditional high-throughput screening for identifying molecules with potential bioactivity against hypothesized targets [19].  

Deep learning models have further advanced target identification by their capacity to process complex, high-dimensional biological data [10]. Compared to traditional ML classifiers—which may require extensive feature engineering and can sometimes struggle with highly complex or large datasets—DL models can automatically learn intricate patterns and representations directly from raw or minimally processed data. For target prediction, DL models have been utilized to predict genes associated with specific diseases, such as the application of Deep Neural Networks (DNNs) combined with autoencoders and SVMs for identifying Parkinson’s disease-related genes [10]. DL is also applied to predict complex biological patterns like splicing and to prioritize potential drug targets based on various data sources [10]. A central application of DL in this domain is the prediction of drug–target interactions (DTIs), a prerequisite for new drug discovery [17,32]. Transformer-based neural networks and feedforward neural networks are architectures used for this purpose [10]. DL approaches for DTI prediction can be broadly categorized into drug-based, structure (graph)-based, and drug–protein (disease)-based models [17]. A notable advantage of certain DL techniques for DTI prediction is their ability to overcome limitations associated with high-dimensional structures by employing non–structure-based methods, thereby potentially circumventing the need for 3D structural data or docking simulations [17]. The accumulation of structural data in databases like the Protein Data Bank (PDB) provides essential resources for computational researchers studying interactions, including compound–protein interactions [32].  

A significant strength of modern AI approaches, particularly deep learning, lies in their ability to integrate diverse types of biological information, known as multi-omics data [21,39]. Multi-omics data, including genomics, transcriptomics, proteomics, epigenomics, and methylation data, provide systematic measurements of biological systems [7,21]. By integrating and mining these disparate data sources, ML and DL algorithms can provide stronger evidence for target–disease associations and identify new targets that might not be apparent from single-omics analysis [21,39]. Examples include the use of ML with gene expression data to discover biomarkers and targets for sarcomas and the integration of GWAS catalog, gene expression, epigenomics, and methylation data through ML to identify target genes associated with juvenile idiopathic arthritis loci [7]. Supervised ML tools have also been developed to specifically identify cancer driver genes [7]. While the digests mention various AI/ML methods applied to data like gene expression or multi-omics, they do not explicitly provide a detailed comparison of different deep learning architectures (e.g., CNNs, RNNs, GCNs applied to gene expression, PPIs, or pathways) in terms of their specific performance or limitations for target identification tasks. However, the successful application of various DL models (DNNs, Transformers, Feedforward networks) to related tasks like gene prediction, prioritization, and DTI prediction underscores their versatility in handling complex biological data relevant to target identification [10,17].​  

Despite the advancements in computational target identification, the validation of predicted targets remains a significant challenge [19]. Computational predictions serve as hypotheses that require rigorous experimental validation to confirm their biological relevance and suitability as drug targets [19]. Furthermore, a critical consideration during validation is the potential for off-target effects, where a compound interacts with unintended targets, leading to adverse effects [19]. Therefore, the AI-driven identification process must be closely integrated with experimental validation workflows to ensure the reliability and safety of potential targets before proceeding to downstream drug discovery stages.  

# 3.2 Virtual Screening and Hit Identification  

![](images/55ab28328615ee3cba1992558c085af925637736055807f8a58f71f398c92eb7.jpg)  

Virtual screening (VS) represents a fundamental computational technique in the early stages of drug discovery, aimed at identifying potential lead compounds from vast chemical libraries. Its primary objective is to computationally pre-screen molecular candidates based on predefined criteria to predict their likelihood of binding to a specific biological target, thereby substantially reducing the number of compounds requiring costly and time-consuming experimental screening [6,39,45].​  

Traditional virtual screening approaches are broadly categorized into structure-based virtual screening (SBVS) and ligandbased virtual screening (LBVS) [10,39]. SBVS relies on the known three-dimensional structure of the target protein or receptor. It involves computationally docking large libraries of drug-like compounds into the target's binding site and scoring their potential binding affinity [39]. This approach directly models the protein–ligand interaction. Conversely, LBVS does not require the target structure but instead utilizes data from known active or inactive compounds to build models that differentiate molecules based on their biological activity or similarity to known ligands [39]. While SBVS provides insights into binding modes, its effectiveness is limited by the accuracy of docking algorithms and scoring functions, as well as the availability of high-resolution target structures. LBVS, while useful when structural information is scarce, is dependent on the quality and diversity of the training data and may fail to identify structurally novel compounds.  

Deep learning (DL) and artificial intelligence (AI) are increasingly being applied to enhance virtual screening by improving the ability to expand and diversify virtual compound libraries and refining screening algorithms [6]. AI algorithms can effectively reduce large compound libraries to more manageable sets of molecules with desired properties [10,18]. MLguided virtual screening offers a compelling alternative to traditional high-throughput screening (HTS), which is often characterized by high costs, extensive time requirements, and relatively low hit rates [9,19]. By employing ML models to rapidly evaluate molecules on a computer, AI-based VS enables the efficient exploration of significantly larger chemical spaces than are feasible with HTS [9,19].​  

A significant advancement in AI-driven VS is the development of AI-based scoring functions, which exhibit notable advantages over classical scoring functions [10]. These functions learn from large datasets to capture complex molecular interactions with improved flexibility and accuracy [10]. For example, the EAT-Score, which combines classical docking energy terms and protein–ligand interaction fingerprints with an XGBoost model, demonstrated a significant performance improvement in virtual screening compared to traditional scoring functions on the DUD-E dataset, increasing the ROC curve AUC value by approximately 0.3 [6]. Similarly, MILCDock, utilizing a multilayer perceptron, integrated predictions from five traditional docking tools to achieve superior performance over traditional and consensus methods on the DUD-E dataset [6]. More recently, TB-IECS combined energy terms from Smina and NNScore2 with XGBoost to outperform classical functions like Glide SP and Dock across multiple datasets and real-world virtual screening scenarios, balancing efficiency and accuracy [6].​  

DL-based virtual screening methods, such as Deep QSAR and Deep Docking, further accelerate the process and enhance screening effectiveness [9,28]. Deep QSAR models can predict docking scores efficiently for screening large libraries [24]. Deep Docking, which leverages deep learning to predict docking scores, is specifically designed to augment SBVS by speeding up the initial filtering of large chemical libraries [9,35]. This approach, based on deep QSAR modeling and often employing an FFNN to predict scores, allows for rapid identification and enrichment of high-scoring molecules, drastically increasing the speed of actual docking simulations [9,28,35]. Deep docking, utilizing 2D fingerprint descriptors and active deep learning, is suitable for screening ultra-large, giga-sized chemical libraries using standard computational resources, as demonstrated by its application to evaluate 1.4 billion ZINC15 compounds against SARS-CoV-2 Mpro [28].  

Several studies highlight the significant speedup and enrichment achieved by AI/DL-based methods. Deep Docking achieved up to 100-fold data reduction and a remarkable 6000-fold enrichment of high-scoring molecules when screening the ZINC15 library against 12 target proteins [35]. HASTEN, which iteratively trains ML models to predict docking scores, was able to find true high-scoring hits with $9 0 \%$ recall while only docking $1 \%$ of a 1.56 billion molecule library [6]. The V-SYNTHES method, designed for screening virtual libraries exceeding 11 billion compounds, reduced computational resources needed to $1 / 1 0 0$ of standard methods and demonstrated a hit rate of 6 out of 21 hits $\mathbf { \partial } \cdot < 1 0 \mu \mathbf { m o l } \cdot \mathbf { L } ^ { - 1 }$ ) against ROCK1 [6]. A recent AI-based VS study targeting Sirtuin-1 protein successfully screened 2.6 million compounds, identifying 434 potential inhibitors $( 0 . 0 2 \%$ of the library), with multi-stage validation confirming 7–9 novel inhibitors [6,18]. This specific study reported a 12-fold improvement in hit rate compared to competitive benchmarks for Sirtuin-1 [18]. Other applications include predicting $\beta$ - hydroxysteroid dehydrogenase type 1 inhibitors and discovering COVID-19 3CL protease inhibitors by combining ML and docking [7]. Molecular docking is also frequently used as an evaluation step for molecules generated by AI models [14,33,38].  

While specific deep learning architectures such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs) are widely used in cheminformatics, the provided digests specifically mention the use of XGBoost, multilayer perceptrons, and feedforward neural networks (FFNNs) within the context of developing AI-based scoring functions and predictive models for virtual screening [6,9]. Despite the significant progress, challenges remain in applying machine learning to preclinical drug discovery, including the critical need for diverse and representative training data to ensure the generalizability and robustness of the developed models across various target classes and chemical spaces [19]. Integrating uncertainty quantification into VS pipelines using methods like evidential deep learning can also help prioritize candidates based on prediction confidence [37].​  

# 3.3 De Novo Molecular Design  

![](images/0dfbbdd6118a6da9530a59644561099252481c2c45e87caac410ee0b70b9dc07.jpg)  

AI-assisted denovodrug design represents a paradigm shift in small molecule discovery, aiming to generate novel chemical entities with desired properties without strictly adhering to existing structural rules or libraries [2,10,39]. This approach facilitates the exploration of vast chemical space beyond known open-source libraries, significantly accelerating the identification of high-quality molecular candidates [9,12].​  

Central to AI-assisted denovodesign are generative deep learning models, which learn the statistical distribution and intrinsic syntax of molecular representations, enabling the creation of new, realistic molecules [2,24,28]. Various model architectures have been successfully employed, each with distinct characteristics and capabilities. Recurrent Neural Networks (RNNs), particularly those utilizing Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU), have been widely used, often learning from sequence-based representations like Simplified Molecular Input Line Entry System (SMILES) strings [10,20,25,28]. These models excel at learning chemical language syntax and generating molecules with properties similar to training or template compounds [10]. For instance, LSTM models have been applied to design histone acetyltransferase (HAT) inhibitors, leading to the identification of potent compounds with favorable invivoefficacy [41]. However, the SMILES representation itself presents certain challenges in effectively capturing the full structural and chemical information of molecules [10]. Transformer-encoder-based models are emerging as alternatives, aiming to improve the validity and drug-likeness of generated molecules compared to traditional RNN methods [33].  

Variational Autoencoders (VAEs) are another prominent class of generative models. VAEs compress molecules into a continuous, low-dimensional latent space using an encoder and then reconstruct them using a decoder [5,9,24]. This latent space allows for sampling and interpolation to generate novel structures, which can then be decoded back into molecular representations like SMILES [5]. Fragment-based VAEs have also been developed, demonstrating efficacy in generating tailored molecular libraries for specific targets like CDK2 [6].  

Generative Adversarial Networks (GANs) have also been applied in denovomolecular design, typically involving a generator network creating molecules and a discriminator network evaluating their validity and quality [6,24]. While effective for distribution learning, training GANs can be challenging. Normalizing flow models offer an alternative, defining a reversible transformation between a complex molecular distribution and a simple base distribution, which facilitates precise likelihood calculation for evaluating generated samples [6,9]. Diffusion models, which generate data by progressively denoising random noise signals based on a learned neural network, represent a more recent advancement with applications in molecular and even protein design [6,9,19]. Graph Neural Networks (GNNs) are also utilized for molecular generation, operating directly on graph representations of molecules [24], such as iteratively generating edges and adding atoms [38].​  

A significant challenge in denovomolecular design is the evaluation of generated molecules to ensure novelty, diversity, validity (chemical feasibility), and the desired properties [2,9,33]. Metrics such as the number of unique structures, the ability to pass medicinal chemistry filters (MCFs), adherence to Muegge criteria, and QED scores are commonly used [42]. Sampling frequency can also serve as a prioritization criterion [43]. Beyond structural validity, ensuring synthetic feasibility and bioavailability remains a critical bottleneck. Advanced platforms address this through multi-dimensional scoring and optimization incorporating criteria like synthesis difficulty and pharmacokinetic profiles [6].  

To steer the generative process towards molecules with specific desired characteristics, reinforcement learning (RL) and other optimization techniques are widely employed [1,7,16,25,33]. RL typically involves combining a generative model with predictive models that act as a "reward" function, guiding the generator to produce molecules that score highly on target properties [16]. Examples include optimizing for target specificity or ligand electrostatic similarity using algorithms like hill climbing [41]. Conditional RNNs can be conditioned directly on molecular descriptors to guide generation towards desired property profiles [1]. Notable RL-based frameworks like ReLeaSE integrate generative and predictive networks [7], while GENTRL combines RL with variational inference, tensor decomposition, and self-organizing maps [7,41]. Structure-based approaches integrate generative models with docking simulations or leverage 3D grid representations and techniques like pharmacophore constraints and Bayesian optimization to optimize binding affinity and conformation [14,30]. DRAGONFLY, a method combining a Chemical Language Model with interactome-based deep learning, demonstrates the ability to generate blueprints for potential drug molecules modulating protein activity based on 3D structure [4,23].  

Compared to traditional denovodesign methods that often rely on expert knowledge, rule-based systems, or fragmentation approaches, AI-driven methods offer the potential for unprecedented exploration of chemical space and accelerated discovery cycles [39]. Successful applications include the discovery of potent DDR1 inhibitors using GENTRL [7,16,41], ATM kinase inhibitors via RL-tuned RNNs [41], CDK8 inhibitors using generative chemistry and structure-based methods [41], and novel Nurr1 agonists using fine-tuned CLMs augmented with fragments [43]. Furthermore, integrated platforms like Insilico Medicine's Chemistry42, leveraging various generative models and optimization mechanisms, have demonstrated the capability to rapidly advance novel candidates, such as ISM001-055 for idiopathic pulmonary fibrosis, into clinical trials significantly faster and at a lower cost than traditional pipelines [6]. These results highlight the transformative potential of deep learning in generating novel, diverse, and high-quality small molecules for therapeutic applications.​  

# 3.4 Lead Optimization  

Following the identification of initial hit compounds, the drug discovery process transitions to the crucial stage of lead optimization, where the aim is to improve the potency, selectivity, and pharmacokinetic properties—including ADMET (absorption, distribution, metabolism, excretion, and toxicity) characteristics—of promising molecules [10]. Machine learning, particularly deep learning, has emerged as a powerful tool to enhance the design and optimization of small molecule compounds throughout this phase [21].  

AI approaches to lead optimization can be broadly categorized into objective-oriented and structure-oriented methods [8]. Objective-oriented methods, such as those based on reinforcement learning or metaheuristic algorithms, typically involve defining a specific objective function that quantifies desired molecular properties (e.g., binding affinity, low toxicity), and the AI model is trained to generate or modify molecules to maximize this function. In contrast, structure-oriented optimization does not explicitly require a predefined objective function in the same manner. Instead, it focuses on local structural modifications, aiming to solve the problem of predicting or generating the remaining part of a molecule based on a given partial structure, represented conceptually as  

$$
G \mu = f ( G \backslash \mu )
$$  

[8]. The provided digests highlight the classification and analysis of generative AI methods within this structure-oriented paradigm [8]. While the distinction between these approaches is clear, the provided materials do not detail the specific advantages and disadvantages of objective-oriented versus structure-oriented methods in a comparative manner.​  

Structure-oriented lead compound optimization methods are often categorized based on traditional medicinal chemistry strategies, such as fragment replacement, linker design, scaffold hopping, and side-chain modification [8]. Generative AI has been applied to these specific tasks [8]. While the analysis provided in the digests points to breakthroughs and difficulties related to model training data construction and generative AI tool development for each task, the available information does not include specific details on the deep learning methods used, the datasets employed, or the quantitative results achieved for each individual structure-based optimization task, nor does it detail the specific challenges and opportunities inherent to each area as requested.​  

Deep learning plays a significant role in the hit-to-lead optimization process by enabling the prediction and optimization of various properties of hit compounds to generate lead candidates [19]. Machine learning algorithms are employed to predict crucial parameters such as solubility, oral bioavailability, toxicity, and hERG safety [19]. By predicting these diverse drug-like properties and potential toxicities early, ML models offer comprehensive guidance for selecting and optimizing drug candidates towards favorable clinical profiles [19]. A key advantage of using deep learning for hit-to-lead optimization is its capacity to handle multiple objectives simultaneously by integrating predictions across various desired properties [19]. For instance, starting from a compound like B003, AI has facilitated the rapid identification of highly effective and selective p300/CBP inhibitors [22]. Similarly, virtual screening aided by AI can identify initial scaffolds, such as indoles and ureas in the case of Sirtuin 1 inhibitors, which then serve as starting points for subsequent optimization efforts [18].  

# 3.5 Molecular Property Prediction (including ADMET)  

<html><body><table><tr><td>Property Type</td><td>Examples (incl. ADMET)</td><td>Key DL Architectures Used</td><td>Key Challenges</td><td>Strategies to Mitigate</td></tr><tr><td>General Physicochemcia 一</td><td>Solubility, LogP/D, Melting point, Energy</td><td>DNN, GNN, RNN, CNN</td><td>Data scarcity, Complexity of properties</td><td>MTL,TL, Data Augmentation, Learned Embeddings</td></tr><tr><td>Biological Activity</td><td>Target Binding Affinity (IC50, Kd), Cellular Activity</td><td>GNN, DNN, Transformer, RNN</td><td>Data scarcity (specific targets), Activity cliffs</td><td>MTL (across targets), TL, Uncertainty Quantification</td></tr><tr><td>ADMET (Absorption, Dist., Met., Exc., Toxicity)</td><td>Oral Bioavailability, Permeability (Caco-2), P-gp inhibition, Hepatotoxicity, hERG toxicity, Plasma protein binding, LD50</td><td>GNN (GCN, D- MPNN), DNN, RNN, CNN</td><td>Data scarcity/quality, High complexity of biological systems,Lack of negative data</td><td>MTL (across ADMET), TL, Multi-modal learning, Data Curation, Uncertainty Quantification</td></tr><tr><td>Synthetic Accessibility</td><td>Ease of synthesis</td><td>DL integrated with Retrosynthesis</td><td>Defining reliable scores, Integrating constraints filters</td><td>RL, Conditional Generation, Post-processing</td></tr></table></body></html>  

Accurate prediction of molecular properties is fundamental to small molecule discovery, guiding candidate selection and optimization throughout the drug development pipeline [4,9,12]. Among these, the prediction of ADMET (absorption, distribution, metabolism, excretion, and toxicity) properties is particularly critical, as poor ADMET profiles are a major cause of drug candidate failure in clinical trials [25,29,45]. Traditional methods for ADMET prediction, often relying on empirical rules or computationally expensive simulations, face significant challenges in terms of accuracy, reliability, and the ability to predict properties for diverse and novel molecular structures [29].  

Machine learning (ML) and deep learning (DL) approaches have emerged as powerful tools to address these limitations, offering efficient and cost-effective means to predict molecular properties and ADMET characteristics [6,9,19]. Classical Quantitative Structure-Activity Relationship (QSAR) methods, which establish mathematical relationships between molecular descriptors and biological activities or properties, have long been integrated with ML algorithms such as Random Forests (RF), Support Vector Machines (SVM), Naive Bayes (NB), and K-Nearest Neighbors (KNN) [6]. These combined approaches have been applied to predict various properties, including activity against specific targets like protein kinases or HIV-1 inhibitors, skin permeation coefficients, and ADMET properties such as oral bioavailability, plasma protein binding, and potential toxicities [6,45]. Specific examples include KNN models for identifying active protein kinase inhibitors and NB models for classifying HIV-1 inhibitors, which demonstrated performance comparable to or better than other QSAR methods [6]. Similarly, RF models have shown strong performance in predicting blood-brain barrier peptides [6].  

Deep learning models represent a significant advancement over traditional QSAR methods, primarily because they do not rely on predefined, expert-crafted molecular features. Instead, they automatically learn complex features directly from molecular representations that are optimized for specific prediction tasks [27]. Various deep learning architectures have been explored for molecular property and ADMET prediction. Graph-based Convolutional Neural Networks (GCNNs) are frequently used to capture complex structural features of molecules, proving effective for predicting properties like water solubility and various ADME characteristics [10,19,25,26]. Other architectures include standard Deep Neural Networks (DNNs) for predicting Caco-2 cell permeability, ADME parameters, and biochemical indicators [5,10], and Recurrent Neural Networks (RNNs) utilized as QSAR models to predict activity values like pIC50 from SMILES strings [33]. Convolutional Neural Networks (CNNs) trained on 2D molecular images have also demonstrated predictive capabilities for pharmacokinetic and toxic properties, including CYP1A2 and P-glycoprotein inhibition, blood-brain barrier penetration, and Ames mutagenicity, showing comparable performance to ML models based on manual feature engineering [6].  

Specific applications of deep learning for ADMET prediction cover a wide range of endpoints, including oral bioavailability, water solubility, cell permeability, logD7.4, P-glycoprotein inhibition, hepatotoxicity [29], lipophilicity [10,37], hERG toxicity [9,19,27], solvation energy, atomization energy, hepatocyte clearance, median lethal dose (LD50), and plasma protein binding rates (PPBR) [37]. Some research groups have reported significant improvements in prediction accuracy for key ADMET parameters using newly developed AI models, with gains of approximately $1 0 \%$ over existing methods for properties like oral bioavailability, water solubility, cell permeability, logD7.4, P-glycoprotein inhibition, and hepatotoxicity [29]. The performance of deep learning models has been shown to be competitive or slightly better than traditional ML methods like Random Forests in benchmark studies like Kaggle competitions focusing on ADME parameters [5].  

Despite the significant progress, developing accurate and reliable predictive models for ADMET and other molecular properties remains challenging [10]. A major obstacle is the scarcity of high-quality experimental data for many endpoints, particularly for novel molecules outside the chemical space of the training data [29,40]. The inherent complexity of biological systems further complicates accurate prediction [29]. To address these challenges, researchers are exploring advanced modeling strategies. Multi-task learning, where a single model is trained to predict multiple related properties simultaneously, can leverage shared underlying patterns across different tasks and improve performance, especially for tasks with limited data [25,29]. Transfer learning, utilizing knowledge gained from large public datasets or related prediction tasks, is another promising approach to enhance model generalization to new data and targets [29]. Multi-modal learning strategies, integrating data from different sources or representations, are also being investigated to develop more robust and efficient drug-likeness prediction models [29]. While deep learning reduces reliance on manual feature engineering, the choice of molecular representation (e.g., SMILES, graphs, images) still influences model performance [6,10,27,33]. Combining learned features with expert-defined descriptors, such as quantum mechanics descriptors with graph convolutional neural networks, can sometimes further improve predictive performance for ADME properties [26].  

Datasets like MoleculeNet serve as valuable benchmarks for evaluating and comparing the generalization performance of different models, particularly when dealing with limited training data [40]. Ultimately, integrating these predictive models early in the discovery process, potentially guided by multi-attribute optimization schemes, is essential for identifying candidates with improved drug-like and clinical properties and increasing their chances of success [9]. Basic molecular properties like those considered in the "Rule of Three" (MW, cLogP, HBDs, HBAs, PSA) are also incorporated in the assessment of generated molecules [38].  

![](images/dbe8bd35f4bc4e7849b154ec404feb8a3579d678903f89cc3fc9983c794bf9f9.jpg)  

Deep learning has emerged as a powerful tool for aiding synthetic route design, addressing both forward synthesis prediction and retrosynthesis prediction [5]. Forward synthesis involves predicting the product of a given set of reactants, reagents, and reaction conditions [12], while retrosynthesis works backward from a target molecule to identify suitable starting materials through a series of plausible reactions [39]. Deep learning models predict the necessary reactants, reagents, and reaction conditions required to synthesize a target molecule via retrosynthetic steps [24]. This approach directly infers relationships between products and reactants, predicting viable retrosynthetic routes [24].  

Various deep learning architectures and strategies are employed in retrosynthesis prediction. These include both templatebased and template-free methods [24]. Template-based methods rely on predefined reaction rules or patterns extracted from databases, while template-free approaches learn reaction transformations directly from data without explicit rules [24]. Neural sequence-to-sequence models have been utilized for retrosynthetic prediction [5], treating molecules as sequences of tokens. Beyond single-step prediction, combining deep learning models for single-step prediction with graph search algorithms allows for the design of comprehensive computer-assisted organic synthesis (CAOS) programs capable of predicting multi-step pathways [39]. Examples of such systems include AiZynthFinder, an open-source tool based on Monte Carlo tree search regulated by neural networks, and systems employing multiple neural networks combined with Monte Carlo tree search to discover new routes [7]. Traditional approaches, such as Chematica, were developed based on decision trees for designing synthetic routes [7].​  

Deep learning models have demonstrated performance improvements over traditional methods. For instance, Microsoft researchers developed Chimera, a new framework incorporating advanced reaction models that achieve state-of-the-art performance in retrosynthesis prediction [31]. Evaluations by PhD-level organic chemists indicated a preference for Chimera's predictions due to their higher perceived quality compared to baseline models [31]. The robustness and generalization capabilities of Chimera have been validated on external datasets from pharmaceutical companies, showing effective performance under distribution shifts [31]. While traditional rule-based systems rely on curated reaction knowledge, deep learning models can learn complex, non-linear relationships directly from vast datasets, potentially  

discovering novel transformations and routes that might not be explicitly encoded in rule bases. Comparing the performance of these models typically involves assessing metrics such as the top-k accuracy of predicted reactants and the overall success rate in finding viable synthetic pathways.​  

Despite advancements, challenges remain in dealing with the inherent complexity and ambiguity of chemical reactions. A single product can often be formed through multiple distinct reaction pathways, and predicting the most chemically sound and experimentally feasible route is challenging. The potential for using attention mechanisms in deep learning models is being explored to improve prediction accuracy by focusing the model's attention on relevant parts of the molecular structure or reaction context, although specific examples from the provided digests are limited.  

The integration of machine learning and deep learning models into molecular construction and synthesis planning holds significant potential for automating molecular design and synthesis [28]. Such automated systems, potentially combined with robotic platforms, could make decisions on molecule structure and synthesis plans autonomously [28]. When coupled with deep quantitative structure-activity relationship (QSAR) models, these retrosynthesis prediction tools can guide the synthesis of compounds with desired biological and physicochemical properties, thereby accelerating the drug discovery process [28]. Examples like the accurate predictions facilitated by the Chimera framework highlight the progress towards this goal [31].​  

# 3.7 PROTAC Design  

The design of proteolysis-targeting chimeras (PROTACs) presents unique challenges, notably the critical requirement to optimize the linker connecting the target protein binder and the E3 ligase binder. This optimization involves determining the appropriate linker length and ensuring sufficient binding affinity for both the target protein and the E3 ligase simultaneously [38]. These factors are paramount for inducing effective ternary complex formation and subsequent target protein degradation.  

Deep learning approaches are being explored to address these complexities and facilitate the generation of novel and effective PROTAC molecules [38]. One such method is AIMLinker, specifically designed for generating novel structural linkers for PROTACs [38]. This system operates using an encoder-decoder deep learning network. It takes two unlinked fragments, corresponding to the target binder and the E3 ligase binder, as input [38]. The network then generates substructures that form a new, complete PROTAC molecule by connecting these fragments [38]. Following generation, the molecules undergo post-processing to extract potential drug-like compounds [38]. While the integration of computational validation methods such as molecular docking, molecular dynamics simulations, and free energy perturbation calculations is crucial for assessing the generated PROTACs, the specific application of these methods and detailed performance metrics regarding improved binding affinity or drug-like properties compared to existing molecules are areas actively being investigated within this field. Deep learning models like AIMLinker represent advancements in the computational generation of PROTAC linkers, aiming to navigate the intricate design space and identify molecules with desirable characteristics.​  

# 4. Advanced Techniques and Data Strategies  

Advancements in deep learning have significantly impacted small molecule discovery, necessitating the integration of sophisticated techniques and robust data management strategies to address inherent challenges and enhance model performance. This section provides an overview of advanced deep learning methodologies, including Reinforcement Learning (RL), Transfer Learning (TL), Multi-Task Learning (MTL), and Explainable AI (XAI), and critically analyzes the pervasive challenges related to data scarcity, quality, and representation, along with proposed strategies for mitigation.  

Advanced techniques such as RL, TL, and MTL are being actively explored to overcome limitations of traditional machine learning methods and expand the applicability of deep learning in drug discovery [16]. Reinforcement Learning, a distinct paradigm from supervised and unsupervised learning, is primarily applied to denovomolecular design and optimization by training agents to generate molecules with desired properties based on maximizing cumulative rewards [2,16,27]. Transfer Learning and Multi-Task Learning represent key strategies to leverage knowledge from large datasets or related tasks, significantly addressing the challenge of limited experimental data for specific targets or properties [16,24,30,33,43]. TL allows pre-trained models to be fine-tuned on smaller, target-specific datasets, improving efficiency and reducing data requirements [20,33], while MTL enables simultaneous prediction or optimization of multiple related objectives by leveraging shared information across tasks [16,24,30]. Furthermore, as deep learning models become more complex, the need for transparency and interpretability becomes paramount. Explainable AI (XAI) techniques are crucial for providing  

insights into model predictions, fostering trust, and enabling researchers to understand the rationale behind model outputs, which is particularly vital for generative models and applications like QSAR [2,26,39].  

The efficacy of deep learning models in small molecule discovery is intrinsically linked to the availability of high-quality, large datasets [5,10,21,39]. However, the field faces significant challenges related to data scarcity, quality, and representation compared to other data-rich domains [15,27]. Public databases provide foundational data [11,25], but the total volume is often insufficient for training large models effectively [15,27]. Data quality issues abound, including noise, bias, inconsistencies from varied sources, and the presence of promiscuous compounds [10,15]. Additionally, the scarcity of publicly available negative data due to publication bias presents a critical limitation for training classification models [15]. Effective model training requires careful data preparation, involving collection, descriptor generation, feature selection, and proper splitting and curation [11,21].​  

To mitigate these data-related challenges, several strategies are being employed. Data augmentation techniques, such as generating synthetic data or alternative molecular representations, are used to increase the size and diversity of training datasets [1,15,27,39,43]. Strategies like federated learning enable collaborative model training without direct data sharing, addressing privacy concerns [15]. Tackling the lack of negative data may involve stricter publication guidelines or crowdsourcing efforts [15]. As mentioned previously, Transfer Learning and Multi-Task Learning also serve as crucial strategies for improving data efficiency and model performance in data-limited settings [16,24]. Cumulative learning offers another approach for accumulating knowledge across small datasets [34]. Beyond technical methods, community-wide efforts are essential to promote data sharing, standardization, and systematic curation to build a more robust data foundation for deep learning in small molecule discovery [15,19].  

# 4.1 Reinforcement Learning (RL)  

![](images/d240840f24e9f723a9ad7c6c3ebb8ac6b3fb9ffcd97e45a2bbd3a49ea0287827.jpg)  

Reinforcement Learning (RL) constitutes a distinct paradigm within machine learning, differing from supervised and unsupervised learning by enabling an agent to learn through interaction with an environment to maximize cumulative rewards rather than generating mere predictions or identifying patterns [12,16]. The core components of an RL system include an agent, the environment, states representing the current situation, actions taken by the agent, goals, and reward functions that provide feedback on the desirability of actions [2,16]. The agent interacts with the environment, observes the state, selects an action, and receives a reward or penalty, with the objective of learning a policy that maximizes the total reward over time [16].​  

In the context of small molecule discovery, RL is primarily applied to denovomolecular design and molecule optimization [16]. The molecule generation process is framed as a sequence of actions where the agent makes "moves" to grow or modify a molecule by adding and changing atoms and chemical bonds [27]. The goal is typically defined by desired molecular property values, often derived from predictive models [27]. RL algorithms are employed to train agents to generate molecules exhibiting these desired properties, such as high potency or favorable drug-likeness [16]. This approach can enhance the efficiency of denovodesign, offering advantages in tasks like carbon and nitrogen atom position exchange, optimizing various molecular properties, and forming chiral compounds, thereby improving the likelihood of discovering hit compounds [25].​  

A significant application of RL in molecular generation involves its integration with predictive models to guide the synthesis of novel chemical entities possessing specific characteristics [33]. In this setup, the output of a predictive model, such as predicted biological activity or binding affinity, serves as the reward signal for the RL agent [14,33]. The RL process then optimizes the parameters of the generative model to maximize this expected reward [33]. For instance, the SBMolGen model utilizes docking scores as a reward within an RL framework to direct molecule generation towards compounds with high predicted binding affinity to a target protein [14]. Similarly, a transformer-based generative model can be optimized via RL where the reward is based on predicted activity, with the process anchored to a prior policy learned during pretraining to maintain molecular syntax and structural distribution [33]. Models like GENTRL combine RL with techniques such as variational inference, tensor decomposition, and self-organizing maps (SOMs) to define sophisticated reward functions [41]. In adversarial settings, such as the RANC model, RL trains a generator to simultaneously deceive a discriminator and maximize an objective reward function, which can include specific property filters [42]. Other examples include GraphAF, which employs RL in its autoregressive generation process, rewarding steps based on properties like log(P) and QED [9], DRLinker which uses RL for linker design based on physicochemical properties and activity [8], and 3D-Scaffold-RL incorporating a graph-based binding probability predictor into the reward for 3D structure evaluation [8].​  

A critical challenge in applying RL to molecular discovery is the design of appropriate reward functions and ensuring efficient exploration of the vast chemical space [30]. The specific form of the reward function is paramount for balancing exploration and exploitation capabilities within the generative model [2]. Effective reward design allows the network to pursue multiple desired goals simultaneously, leading to the generation of molecules with both high similarity to target molecules and favorable drug-likeness properties [27]. Systematically varying multi-objective reward functions and evaluating their impact on generative model performance is considered valuable for advancing the field [2]. Despite the challenges in defining and tuning reward functions, RL offers distinct advantages in optimizing the search space for druglike molecules. By training agents to iteratively refine molecular structures based on objective functions tied to desired properties, RL facilitates targeted exploration, significantly increasing the probability of discovering molecules that meet specific criteria, including high potency and overall drug-likeness [16,27]. This guided search is more efficient than random or exhaustive exploration of chemical space.​  

4.2 Transfer Learning (TL) and Multi-Task Learning (MTL)  

![](images/9c1227b9944ec81936ca4b04d9ca250f13d41d89501b3d62642afbc650093908.jpg)  

Transfer learning (TL) and multi-task learning (MTL) represent prominent strategies within deep learning to address common challenges in small molecule discovery, particularly the scarcity of large, high-quality experimental datasets for specific tasks. Transfer learning enables the leveraging of knowledge acquired from training on extensive datasets or related tasks to enhance model performance on smaller, target-specific datasets or tasks with limited available data [16,30,33,43]. This approach allows for the adaptation of pre-trained models to specific drug discovery problems, thereby improving computational efficiency and mitigating the substantial data requirements often associated with training complex deep learning architectures from scratch [33].  

A common application of TL involves pre-training a model on a large general dataset, such as the ChEMBL database, and subsequently fine-tuning it on a smaller, target-specific dataset or a focused library [20,22,43]. This fine-tuning process adapts the pre-trained model's learned features to the nuances of the target task, which can include denovodesign of novel lead compounds [9]. Studies have investigated how the characteristics of the target dataset influence the effectiveness of this fine-tuning process [20]. Specific TL implementations include adjusting only the parameters of the final layers during fine-tuning to preserve broadly learned molecular features [33], or employing bidirectional TL using architectures like the Domain Separation Network (DSN) to facilitate knowledge exchange between different data domains, such as ligands and ligand-protein complexes [30]. Models utilizing bidirectional TL have demonstrated improved generative performance metrics, including validity, uniqueness, and internal diversity [30]. Furthermore, fine-tuning pre-trained models has been recognized as an effective method for improving accuracy across various downstream prediction tasks [10]. While TL is widely adopted, alternative approaches like direct knowledge embedding into models [40] or methods that consolidate  

information from targets and ligands without explicit TL steps [4] are also explored. Federated learning represents another strategy that allows models to expand applicability by sharing private data implicitly rather than through explicit model transfer [15].​  

Related to TL in addressing data limitations, cumulative learning has been shown to enhance classification accuracy, particularly for datasets with small numbers of samples, such as mass spectrometry (MS) data [34]. Cumulative learning notably improves classification accuracies compared to training convolutional neural network (CNN) models from scratch and offers slight advantages over standard transfer learning in these small sample scenarios [34].​  

Multi-task learning (MTL) provides a paradigm for simultaneously optimizing or predicting multiple related objectives within a single model. In small molecule discovery, these objectives can include crucial properties such as activity against different targets, selectivity, and Absorption, Distribution, Metabolism, Excretion, and Toxicity (ADMET) characteristics [16,30]. MTL is particularly advantageous for building predictive models when large single-task datasets are unavailable [16]. By training a model to predict multiple properties concurrently, MTL leverages shared information and correlations between tasks, potentially leading to improved prediction accuracy compared to training separate single-task models [24]. This is beneficial in denovodrug design, where MTL can be employed to screen synthetic compounds for multiple desired chemical properties simultaneously [25]. MTL has also been specifically applied to address the small data challenge in predicting drug-likeness and to improve the accuracy of multiple key ADMET indicators [29]. Examples of MTL applications include the construction of multi-task models for predicting activity across a large number of kinases using extensive compound-kinase interaction data [24] and the development of platforms like ADMET-AI, which utilizes multi-task Chemprop-RDKit models trained on numerous ADMET datasets [9]. MTL also facilitates multi-objective optimization in QSAR by transferring knowledge across tasks with varying data availability [28]. Partial multi-task model sharing has also been explored in QSAR to expand model applicability without necessitating the sharing of sensitive underlying data [26]. However, it is important to note that the extent of improvement offered by deep QSAR methods using knowledge transfer over single-task models is not universally guaranteed and often depends on the degree of correlation among the activities against the individual targets [28].​  

# 4.3 Explainable AI (XAI)  

<html><body><table><tr><td>XAI Technique</td><td>Description</td><td>How it Helps Interpret</td><td>Example Application</td></tr><tr><td>Saliency Maps</td><td>Highlight influential parts of input</td><td>Identify key atoms/fragments for prediction</td><td>Interpreting SMILES- based model predictions (MolGPT)</td></tr><tr><td>Attention Mechanisms</td><td>Weigh importance of different input features</td><td>Show which molecular parts are weighted most</td><td>GNNs (KEMPNN), Transformers</td></tr><tr><td>Feature Attribution</td><td>Quantify contribution of each input feature</td><td>Provide local explanation for predictions</td><td>QSAR models, Property Prediction (using SHAP)</td></tr><tr><td>Visualization Methods</td><td>Map high-dim data/features to lower-dim space</td><td>Explore chemical space, identify clusters</td><td>Visualizing latent space (VAE), molecule distributions</td></tr></table></body></html>  

The application of deep learning models in drug discovery necessitates a strong emphasis on transparency and interpretability [39]. Explainable AI (XAI) techniques are instrumental in providing a clear understanding of the insights generated by these AI models, which is crucial for their adoption and trustworthiness within the scientific community [9,39]. Understanding the mechanisms by which models achieve their predictions is paramount, particularly for generative models where interpretability can optimize chemical understanding [2]. While data-driven techniques are effective in highdimensional spaces, analyzing their generation mechanisms remains a significant challenge [2].  

XAI methods enable researchers to delve into the rationale behind model predictions, facilitating the identification of potential biases and offering insights into the underlying mechanisms of drug action or molecular properties. Building trust in the model's ability to learn chemically and biologically relevant features is critical for its practical utility [9]. XAI techniques contribute directly to this trust by making the decision-making process transparent.​  

Various approaches have been employed to enhance model interpretability in cheminformatics applications, such as quantitative structure-activity relationship (QSAR) modeling. Polishchuk's review, for instance, critically assesses different interpretation methods for QSAR models, encompassing both model-related and model-independent techniques, and posits that contemporary QSAR models can indeed be considered interpretable [26,39]. Specific examples of XAI techniques implemented in deep learning models for molecular discovery include saliency maps and attention mechanisms. MolGPT, a Transformer-based model, demonstrates interpretability through saliency maps that highlight which SMILES markers significantly influence the model's decisions, revealing the learned importance of specific molecular fragments [9]. Similarly, the Knowledge-Embedded Message Passing Neural Network (KEMPNN) aims to improve the interpretability of Graph Neural Networks (GNNs) by incorporating knowledge and representing it in a per-atom attention-like format. This allows for a direct reflection of domain knowledge and indicates the importance of each part of the molecule in relation to the target property, contrasting with the "black-box" nature of some standard GNNs [40]. The benefits of employing these XAI methods are substantial, fostering greater confidence in model predictions, aiding in hypothesis generation, and accelerating the iterative process of molecular design and validation.  

# 4.4 Data and Training Strategies  

The effectiveness of deep learning models in small molecule discovery is profoundly dependent on the quality and quantity of available training data [5,10,21,39]. High-quality, accurate, precise, and complete datasets are crucial for maximizing the predictive power and generalizability of machine learning methods [21]. While deep learning models are adept at automatically extracting features from large, unprocessed datasets [12], the current landscape of small molecule data presents significant limitations compared to data-rich domains like image or text processing [15,27]. Public databases such as DrugBank, PubChem, ChEMBL, ZINC, and TargetMol serve as foundational resources, providing chemical data for computational techniques [11,25]. Specific datasets like the ZINC Clean Leads collection have been utilized for benchmarking generative models, featuring extensive filtering based on molecular properties and medicinal chemistry rules [30,36]. Other examples include custom ADMET databases built by aggregating literature data [29], specialized datasets for tasks like PROTAC linker prediction [38], or diverse collections for specific applications like mass spectrometry imaging analysis [34].​  

Despite the existence of these resources, the total amount of publicly available small molecule data, often in the range of tens to hundreds of thousands or a few million molecules, remains considerably smaller than the billions or trillions required for training very large deep learning models effectively [15,27]. Beyond sheer volume, data quality poses substantial challenges. Issues include data bias and noise stemming from varied sources, sampling biases, inconsistencies introduced when merging data from different experimental conditions, and the presence of promiscuous compounds such as Pan-Assay Interference Compounds (PAINs) [15]. The quality of existing benchmark datasets has also been questioned, potentially failing to represent real-world drug discovery scenarios accurately [10]. These datasets may suffer from redundancy in sequences or protein families, incomplete coverage of key drug target families like GPCRs and ion channels, and significant data imbalance [10]. A critical limitation highlighted is the scarcity of publicly available negative data (e.g., inactive compounds or failed synthesis attempts), primarily due to publication bias. This lack of negative examples adversely affects the classification accuracy of machine learning models trained for tasks like activity prediction [15].  

Effective model training necessitates careful data preparation. This involves acquiring datasets with relevant features, such as physicochemical properties influencing ADMET characteristics [11]. Molecules are commonly represented using formats like SMILES or FASTA, which are then converted into formats suitable for machine learning algorithms, such as matrices or graphs [11]. The choice of molecular descriptors, computational fingerprints, or graph-based representations is crucial [32]. For supervised learning tasks common in drug discovery, accurate compound labels are essential [11]. Data curation steps often include filtering molecules based on specific property ranges, undesirable elements or groups, or medicinal chemistry filters [1,36]. The prepared dataset is typically split into training, validation, and test subsets, with techniques like crossvalidation employed to ensure model robustness and mitigate overfitting [11,38]. Feature selection techniques, including PCA, t-SNE, feature selection algorithms, and autoencoders, may be applied to reduce dimensionality and focus on relevant variables [11]. Automated workflows are increasingly used to handle data curation and processing for both training and  

external datasets, aiming to ensure model reliability and applicability domain considerations [26,28]. Integrating expert knowledge, such as atom-level annotations regarding positive, negative, or neutral effects, can also enhance model training [40].  

To address the limitations of data quantity and quality, several strategies are being explored. Data augmentation is a key technique used to increase the diversity and size of the training data, thereby improving the robustness and generalization of deep learning models [15]. This can involve generating multiple valid representations for the same molecule, such as through SMILES enumeration [1], or augmenting training sets with fragments from weak agonists in low-data regimes [43]. While the advantages are clear in enhancing training set variability, specific impacts and disadvantages of different augmentation techniques require careful consideration [15]. Federated learning is proposed as a strategy to enable collaborative model training across different institutions or companies without the need for direct data sharing, mitigating privacy and intellectual property concerns [15]. The challenge of insufficient negative data can be tackled through stricter publication guidelines, generation of synthetic negative data, or leveraging crowdsourced initiatives which may provide more comprehensive experimental outcomes, including failures [15]. Crowdsourced projects, like COVID Moonshot, have demonstrated the potential of pooling data from distributed efforts [15]. Another approach to expand training data volume involves iterative methods where predicted hit molecules from initial model iterations are incorporated into subsequent training sets [35]. Transfer learning also helps leverage data, by pre-training models on large general chemical databases like ChEMBL before fine-tuning on smaller, target-specific datasets [20,33].​  

Beyond technical strategies, addressing data challenges requires community-wide efforts. Data sharing and standardization face significant hurdles, including privacy concerns and intellectual property issues [19]. There is a pressing need for the community to collectively work towards creating high-quality, well-annotated, and standardized datasets for drug discovery [19]. This includes adopting standard data reporting practices, such as providing complete code, scripts, parameters, trained models, and documentation in public repositories to enhance reproducibility and facilitate future research [2]. Expanding and curating publicly available data systems, such as the efforts to increase ADMET data endpoints through literature mining and manual correction [29], are vital steps in building a more robust data foundation for deep learning in small molecule discovery.​  

# 5. Evaluation, Validation, and Case Studies  

Evaluating the performance of deep learning models in small molecule discovery is critical for establishing their utility and reliability [17]. A diverse set of metrics is employed, tailored to the specific task at hand. For classification problems, fundamental measures such as accuracy, Area Under the Curve (AUC), log loss, and confusion matrices are commonly used [21,34]. When assessing the predictive power of models for quantitative properties or activities, regression metrics like Root Mean Squared Error (RMSE) and Pearson's correlation coefficient $( R )$ are frequently applied [33,37,40]. Correlation coefficients, such as $R ^ { 2 }$ , also serve to assess the agreement between predicted and experimental values [14,33].​  

In virtual screening and binding prediction tasks, metrics gain chemical and biological relevance. Precision, recall, and enrichment factors (EFs) at various thresholds are standard for assessing the retrieval of active compounds [35]. Full Predicted Database Enrichment (FPDE) provides another measure of enrichment effectiveness [35]. Binding affinity predictions are often evaluated using metrics derived from computational methods, such as docking scores, Root Mean Square Deviation (RMSD) for structural alignment, and calculated binding free energy ( $\Delta G _ { b i n d i n g } )$ using techniques like MMPBSA [14,33,38]. RMSD is computed as:​  

$$
R M S D = \sqrt { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } d _ { i } ^ { 2 } }
$$  

where $N$ is the number of atoms in the ligand, and $d _ { i }$ ​ is the Euclidean distance between the $i ^ { \mathrm { t h } }$ pair of corresponding atoms [38]. Binding free energy is typically given by:  

$$
\Delta G _ { b i n d i n g } = G _ { c o m p l e x } - ( G _ { p r o t e i n } + G _ { l i g a n d } )
$$  

where $G _ { c o m p l e x }$ ​ is the total free energy of the protein-ligand complex, and $G _ { p r o t e i n }$ and $G _ { l i g a n d }$ ​ are the total free energies of the separated protein and ligand in the solvent [38]. Experimental validation metrics, such as changes in melting temperature (TSA), IC50 values, and inhibition efficiencies measured by techniques like LC/MS, provide crucial biological endpoints for validating model predictions [18]. Successful candidates should ideally demonstrate potent activity (e.g., nanomolar IC50) and superior efficacy in relevant biological systems [22].  

For denovomolecular generation, evaluation focuses on the quality and diversity of the generated chemical space [1]. Key metrics include validity (percentage of chemically sound structures), uniqueness (percentage of non-redundant valid structures), and novelty (percentage of structures not found in the training data) [1,20,30,33,41]. Chemical diversity is assessed using molecular fingerprint similarity (e.g., Tanimoto on ECFPs) or structural scaffolds [1,30,42,43]. Similarity to reference datasets can be quantified using metrics like Tanimoto similarity, Frechet score, KL divergence, or the Fréchet ChemNet Distance (FCD) [20,30,36]. Furthermore, generated molecules are evaluated based on medicinal chemistry principles, including drug-likeness scores (e.g., Quantitative Estimation of Drug-likeness, QED) [41], synthetic accessibility scores (SAS) [33,36,41,42], and adherence to rules like Lipinski's rules or REOS [20]. Online platforms like ADMETLab/ADMETLab2.0 provide systematic evaluation of physicochemical properties, drug-likeness, and toxicity endpoints [29].  

Despite this array of metrics, the quantitative evaluation of deep learning models, particularly generative ones, remains challenging due to the nascent stage of the field and the lack of standardized metrics for diverse models and applications [2]. Current metrics may not fully capture the complexities of drug discovery requirements [2,41].​  

Rigorous validation and benchmarking are paramount to ensure the reliability and generalizability of deep learning models [15]. Methods like multi-fold cross-validation and evaluation on diverse test sets or challenging target families are essential [33,35]. Benchmarking against established processes or multi-team efforts provides crucial context [18]. Incorporating uncertainty quantification in predictions can further enhance validation and prioritization [37]. Addressing overfitting through resampling, validation sets, or regularization is necessary to ensure models generalize well to unseen data [21]. The quality and limitations of existing datasets necessitate the development of more realistic and challenging benchmarks [15]. Benchmark platforms like MOSES and GuacaMol provide standardized datasets and metrics to facilitate comparisons between generative models [27,36,41]. Public "blind" prediction competitions are suggested for more rigorous assessment [15].​  

The application of deep learning in the pharmaceutical industry has yielded significant success stories, moving beyond theoretical studies to practical, experimentally validated discoveries [24,44]. This progress is driven by factors including increased data availability, sophisticated deep learning architectures, and collaborative research approaches [2,17]. Notable examples include the discovery of halicin, a novel antibiotic identified using graph convolutions and validated experimentally [32], and the identification of novel compounds active against E.coliand A.bauman i using D-MPNN [9]. Deep learning models have also been successful in identifying novel inhibitors for various targets such as Sirtuin-1 [18], CDK20 [9], p300 HAT, ATM kinase, CDK8, and DDR1 [27,41], AKT1 and CDK2 [30]. The DRAGONFLY model successfully designed PPARγ modulators with favorable properties [4,23], while SBMolGen generated molecules for targets like CDK2, EGFR, AA2AR, and ADRB2 with improved predicted binding affinities and chemical space exploration [14]. A fragmentaugmented model designed a potent Nurr1 agonist [43]. Beyond lead discovery, deep learning has contributed to optimizing drug regimens [7] and predicting complex properties like ADMET [26,40]. Structure-based methods like Deep Docking have also shown efficacy across numerous targets [35].​  

A key demonstrated capability is the generation of novel small molecules with potentially higher activity than existing ligands, while maintaining similar binding interactions [33]. For instance, a Transformer-based generative model designed BRAF inhibitors that showed higher insilicoactivity and comparable interactions to known ligands [33]. Similarly, the AIguided design of the p300/CBP inhibitor B026 resulted in clear activity advantages over an existing compound [22]. This highlights the potential for deep learning to move beyond known chemical space to discover genuinely improved structures.  

Furthermore, deep learning models can provide valuable new ideas and suggestions to pharmaceutical chemists, accelerating and guiding the design process [2]. The ability of generative models to propose molecules with characterized properties and interactions, such as in the BRAF inhibitor case, offers direct insights for synthesis and optimization [33]. AI models played a guiding role in the successful design and optimization of new p300 inhibitors [22]. In retrosynthesis, AI frameworks have produced predictions preferred by expert chemists, underscoring their practical utility in complex tasks [31].​  

# 5.1 Evaluation Metrics and Benchmarking  

Evaluating the performance of deep learning models in small molecule discovery necessitates a comprehensive set of metrics tailored to the specific task, ranging from fundamental classification and regression measures to complex molecular properties and biological activity readouts. Basic evaluation metrics commonly employed include classification accuracy,  

Area Under the Curve (AUC), log loss, and confusion matrices for discriminative tasks [21,34]. For regression problems, Root Mean Squared Error (RMSE) and Pearson's correlation coefficient $( R )$ are frequently used to quantify prediction accuracy [33,37,40].​  

Beyond these general statistical measures, drug discovery applications utilize metrics that reflect the biological and chemical relevance of the model's output. In virtual screening, performance is often assessed using precision, recall, and enrichment values at various thresholds (e.g., top 10, 100, or 1000 molecules), alongside Full Predicted Database Enrichment (FPDE) [35]. Experimental validation provides critical biological endpoints such as changes in melting temperature (TSA), IC50 values for enzyme inhibition, and inhibition efficiency measured by techniques like LC/MS [18]. Successful candidates are further validated through activity assays, such as reporter gene assays, and binding affinity measurements like isothermal titration calorimetry (ITC) [43]. Promising compounds demonstrate potent activity (e.g., IC50 of $1 0 \mathsf { n M }$ ) and superior efficacy compared to existing drugs, ideally confirmed in relevant cell lines and invivoanimal models [22]. For predicting molecular properties like binding affinity, metrics such as docking scores, RMSD for structural similarity, and calculated binding free energy ( $\Delta G _ { b i n d i n g } \backslash$ ) using methods like MM-PBSA are employed [14,33,38]. The Root Mean Square Deviation (RMSD) between generated and reference structures is calculated as:​  

$$
R M S D = \sqrt { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } d _ { i } ^ { 2 } }
$$  

where $N$ is the number of atoms in the ligand, and $d _ { i }$ ​ is the Euclidean distance between the $i ^ { \mathrm { t h } }$ pair of corresponding atoms [38]. Binding free energy $\Delta G _ { b i n d i n g } )$ is typically computed as:  

$$
\Delta G _ { b i n d i n g } = G _ { c o m p l e x } - ( G _ { p r o t e i n } + G _ { l i g a n d } )
$$  

where $G _ { c o m p l e x }$ is the total free energy of the protein-ligand complex, and $G _ { p r o t e i n }$ ​ and $G _ { l i g a n d }$ are the total free energies of the separated protein and ligand in the solvent [38]. Correlation coefficients, such as $R ^ { 2 }$ , are used to assess the relationship between predicted and experimentally determined values [14,33].  

For denovomolecular generation, evaluation shifts towards metrics assessing the quality and diversity of generated molecules. Key metrics include validity (percentage of chemically valid structures), uniqueness (percentage of unique generated molecules), and novelty (percentage of generated molecules not present in the training set) [1,20,30,33,41]. Chemical diversity is measured using metrics like internal diversity based on molecular fingerprints (e.g., Tanimoto similarity of ECFPs) or structural scaffolds (Murcko scaffolds) [1,30,42,43]. Similarity to reference datasets can be assessed using Tanimoto similarity, Frechet score, KL divergence, or the Fréchet ChemNet Distance (FCD), which captures the similarity of chemical and biological property distributions [20,30,36]. Additionally, generated molecules are evaluated based on medicinal chemistry filters (e.g., Lipinski's rules, REOS, MCFs, Muegge criteria), synthetic accessibility scores (SAS), and targeted properties assessed via docking scores or predicted property distributions [20,33,36,42]. Visualizations like TSNE plots can also illustrate the chemical distribution of generated molecules [30].  

Despite the array of metrics, quantitatively evaluating the performance of deep learning models, particularly generative models, presents significant challenges [2]. The field of reverse design via generative models is still emerging, and there is often an absence of established precedents for optimal metrics tailored to different model types and specific scientific applications [2]. Current metrics may not fully capture the complexity of drug discovery, such as the balance between extrapolation and exploitation in chemical space or the ability to generate diverse structures while focusing on specific target properties [2,41]. Issues like overfitting and mode collapse in generative models necessitate metrics designed to specifically detect these problems [36]. The need for new metrics that better reflect the multifaceted requirements of drug discovery, including not only potency but also selectivity, pharmacokinetic properties, and toxicity, remains a critical area for development.​  

Rigorous validation is paramount to ensure the reliability and generalizability of deep learning models in this domain. This includes employing methods like multi-fold cross-validation [33], benchmarking against established processes or multiteam efforts [18], and evaluating performance across diverse target families [35]. Furthermore, incorporating uncertainty quantification in predictions can guide prioritization and improve screening effectiveness, providing an additional layer of validation beyond simple performance metrics [37].  

The quality and limitations of current benchmarking datasets also significantly impact model evaluation. Existing datasets are often criticized for being too small, potentially leading to overestimation of model accuracy when evaluated on data similar to the training set [15]. The development of more realistic, diverse, and challenging benchmarks is essential for truly assessing the capabilities and generalizability of deep learning approaches [15]. Initiatives like the MOSES benchmark platform provide standardized datasets and metrics to enable more insightful comparisons between generative models, addressing factors like distribution learning, chemical diversity, validity, and novelty [27,36,41]. However, continuously improving and expanding these benchmarks is crucial to keep pace with the rapid advancements in deep learning methodologies.​  

# 5.2 Case Studies and Success Stories  

The application of artificial intelligence (AI), particularly deep learning, in the pharmaceutical industry has moved beyond theoretical exploration to tangible successes, driving significant investment from pharmaceutical companies, technology giants, biotechnology startups, and academic centers [21]. These successes highlight the potential of AI to accelerate timelines and improve outcomes across various stages of drug development, from virtual screening and denovodesign to clinical trials [17,24]. Key factors contributing to these advancements include the availability of large biological and chemical datasets, the development of sophisticated deep learning architectures capable of handling complex molecular data, and the adoption of collaborative, end-to-end research designs [2]. Companies like AstraZeneca are successfully integrating AI throughout their drug development pipeline [17]. South Korean startup Deargen developed MT-DTI, a deep learning model predicting drug–protein interaction strength using simplified chemical sequences [17], while Benevolent AI leverages AI and machine learning for accelerated health research [17]. Notable accelerations have been reported by Exscientia, achieving a Phase I clinical trial in 12 months for an AI-designed molecule, and Insilico Medicine, reaching Phase from target discovery in 30 months using its Pharma.AI platform [24]. Collaborative efforts, such as the COVID Moonshot project, have also provided valuable data resources for machine learning applications [15].​  

Deep learning has demonstrated considerable success in the discovery of novel small molecules with therapeutic potential. A prominent example is the discovery of halicin, a novel antibacterial molecule identified using molecular graph convolutions, which was subsequently validated experimentally [32]. Other applications in antibiotic discovery include the use of D-MPNN to find novel compounds effective against E.coliand A.bauman i with minimum inhibitory concentrations (MIC) around $2 \mu \mathrm { g } / \mathsf { m l }$ [9], and the use of evidential uncertainties to prioritize antibiotic candidates from repurposing libraries, significantly improving the experimental hit rate [37].  

Generative models based on deep learning have been particularly impactful in denovomolecular design. JAEGER, a model based on JT-VAE, was successfully applied to design novel antimalarial compounds exhibiting nanomolar activity against the Plasmodiumfalciparum3D7 strain and low hepatocyte toxicity [9]. An LSTM model, pre-trained on a large SMILES dataset and fine-tuned on known inhibitors, yielded six synthesized compounds with nanomolar PI3Kγ activity invitro[9]. The AF2 predicted structure, when used for screening liver cancer therapeutic candidates, led to the discovery of a small molecule active against cyclin-dependent kinase 20 (CDK20) with a $K _ { d }$ of $8 . 9 \backslash { \mathsf { p m } } 1 . 6 \mu \mathsf { M }$ [9]. Deep generative models have also been instrumental in discovering inhibitors for various targets, including p300 HAT, ATM kinase, CDK8, and DDR1 [27,41]. The RELATION model was used to design potential inhibitors for AKT1 and CDK2, evaluating binding affinity using docking scores [30]. DRAGONFLY, a deep learning-based molecular design model, successfully designed PPARγ modulators, including effective partial agonists, demonstrating desired bioactivity, selectivity, and ADME properties [4,23]. The crystal structure of a ligand–receptor complex validated the anticipated binding mode [4]. The SBMolGen model generated molecules for targets like CDK2, EGFR, AA2AR, and ADRB2 that showed better binding affinity scores than known active compounds and explored a broader chemical space [14]. A case study using a fragment-augmented CLM successfully designed a novel Nurr1 agonist with remarkable agonism ( $E C _ { 5 0 } = 0 . 0 7 ~ \mu \mathrm { M } ,$ ) and confirmed binding ( $K _ { d }$ ​ values of $0 . 1 4 \mu \mathrm { M }$ and $2 . 4 \mu \mathrm { M }$ ) [43]. AI-driven virtual screening also successfully identified seven novel Sirtuin-1 inhibitors [18]. Furthermore, deep learning methods have been applied to the challenging area of SARS-CoV-2 drug discovery, identifying several novel drug compounds targeting COVID-19 targets [25]. Deep learning has also contributed to optimizing drug properties and interactions; for instance, a generated molecule (6boy_1268) showed improved chemical properties and binding affinity in the CRBN–BRD4 pocket compared to a redocked known compound, verified through MD and FEP simulations [38]. The Chimera framework, evaluated for retrosynthesis prediction, produced predictions preferred by expert organic chemists due to their higher quality and demonstrated generalizability on internal datasets [31].  

A significant advantage demonstrated by deep learning models is their capability to generate small molecules with potentially higher activity than existing ligands while maintaining similar interaction sites. For example, a Transformerbased generative model was employed to design BRAF inhibitors. The molecules generated by this model exhibited higher insilicoactivity compared to existing ligands identified in crystal structures and maintained comparable binding  

interactions [33]. Similarly, a novel p300/CBP inhibitor, B026, designed with the guidance of AI models, showed clear activity advantages at both molecular and cellular levels compared to the existing compound A-485, and achieved significant dosedependent tumor growth inhibition in animal models [22]. This capability is crucial as it suggests that deep learning is not merely replicating known chemical space but can explore novel structures with enhanced properties.  

Beyond discovering lead candidates, deep learning models can provide valuable new ideas and suggestions for pharmaceutical chemists. In the context of BRAF inhibitor development, the Transformer-based generative model's ability to propose molecules with superior activity and characterized interaction sites can serve as direct guidance for synthesis and further optimization efforts by medicinal chemists [33]. The success in designing and optimizing new p300 inhibitors further underscores the guiding role of AI models in this process [22]. This interaction between AI models and human expertise represents a powerful paradigm shift, enabling chemists to explore previously unconsidered molecular scaffolds and design strategies suggested by the data-driven insights from deep learning. The reported preference of expert chemists for predictions from the Chimera retrosynthesis framework highlights the practical utility and trustworthiness of AIgenerated suggestions in complex synthetic planning tasks [31]. These instances support the broader view that generative AI models can directly address fundamental challenges in chemical science by providing novel and actionable chemical insights [2]. Additionally, AI approaches have been successfully applied to optimize drug regimens, such as using CURATE.AI to determine an effective, lower dose for the cancer drug ZEN-3694 [7], demonstrating AI's utility beyond lead identification. Predicting properties like ADMET remains critical, though challenges like activity cliffs highlight the need for sophisticated modeling approaches, which advanced machine learning methods are beginning to address [26]. Furthermore, deep learning has shown improved performance in property prediction tasks, for example, KEMPNN outperformed baseline methods like MPNN and descriptor-based approaches in predicting polymer glass-transition temperature $( T _ { g } )$ , especially with limited data [40]. Structure-based approaches are also benefiting, with methods like Deep Docking $( D D )$ being tested across numerous targets and demonstrating efficacy in identifying known active ligands [35].  

# 6. Challenges and Future Directions  

The application of deep learning in small molecule discovery, while demonstrating significant promise and achieving notable successes, is currently confronted by several fundamental challenges that necessitate dedicated research and innovation [5,10,39]. Addressing these limitations is crucial for the widespread and effective adoption of AI technologies throughout the drug discovery pipeline.  

A pervasive and critical challenge is the inherent limitation regarding the availability, quality, and consistency of chemical and biological data required for training robust deep learning models [5,6,10,15,19,27,39]. Data is often scarce, particularly for specific targets or rare diseases, and data aggregated from diverse sources can suffer from variability, noise, bias, and lack of standardization [15,21,26]. Furthermore, existing benchmark datasets may not fully reflect the complexity and nuances of real-world drug discovery scenarios [10]. Future efforts must focus on the continuous accumulation and improvement of high-quality, large-scale, annotated datasets [5,19], alongside the development of advanced techniques for data augmentation, federated learning, and knowledge learning to leverage limited data resources effectively [15,27,40]. Initiatives promoting data sharing and standardization are paramount, despite challenges related to privacy and intellectual property [19,26,27].​  

Another significant hurdle lies in the interpretability and reproducibility of deep learning models [5,6,10,16,21,25,32,39]. The "black box" nature of many deep learning architectures hinders understanding the rationale behind predictions, which is critical for validating potential drug candidates and fostering trust among scientists, regulators, and clinicians [5,32]. Ensuring the reproducibility of AI-driven experiments also requires establishing uniform standards and verifying data applicability and reliability [10,21]. Developing more interpretable algorithms and applying Explainable AI (XAI) methods like attention mechanisms, visualization techniques, and feature attribution methods are crucial future directions to provide transparency and quantify prediction reliability [2,6,10,11,40].​  

For generative models aimed at denovomolecular design, ensuring the chemical validity and, crucially, the synthetic accessibility of generated molecules remains a considerable challenge [2,23,30]. While models can explore vast chemical space, generating structures that are practically synthesizable is essential for experimental validation and progression [2,12]. This involves overcoming issues with invalid molecular representations like SMILES [10,41] and integrating synthetic constraints directly into the generation process, potentially through reinforcement learning or conditional models [2]. A key challenge is the development of more accurate and reliable scoring functions for synthesizability and managing the  

inherent trade-off between generating novel structures and ensuring their synthetic feasibility while maintaining desired properties [1,22,23,30].  

Robust validation and benchmarking are indispensable for assessing the reliability and generalizability of deep learning models [15]. Challenges exist in defining appropriate datasets, metrics, and methods for model evaluation, including the crucial aspect of quantifying prediction uncertainty [10,27,37,41]. Developing more realistic and challenging benchmarking datasets and standardized procedures, particularly for generative models, is vital for fair comparison [2,8,36,41]. Ultimately, experimental validation remains the most reliable standard for confirming model predictions [14,15,41].​  

Ethical considerations are increasingly prominent with the expanded use of AI in drug discovery. Potential biases in training data can lead to unfair outcomes and exacerbate health disparities [21,39]. Data privacy and intellectual property protection are also significant concerns [39]. Responsible development and deployment of these technologies require careful consideration of fairness, transparency, and privacy safeguards [10,39].  

Looking ahead, the future of deep learning in small molecule discovery lies in its effective integration with other experimental and computational methods [10,16,19,39]. Synergistic approaches combining deep learning with techniques like high-throughput screening, structural biology, docking, molecular dynamics, and quantum mechanics calculations hold immense potential to accelerate and improve the discovery process [2,9,10,19,24,26,35,37]. The vision of autonomous design-make-test-analyze cycles, integrating generative models with automated laboratory systems, represents a significant future direction [28,41]. The integration of multi-modal data, leveraging diverse sources such as omics data, is essential for a more comprehensive understanding of complex biological systems [3,5,32].  

Further research is needed to develop more accurate and efficient models, including AI-driven foundation models, advanced multi-task optimization strategies, knowledge-enhanced representation learning, and sophisticated multiobjective generative architectures [4,8,28,30,40]. The potential impact of quantum computing on accelerating complex calculations also represents a promising future avenue [24]. Extending the application of deep learning to specialized tasks and new areas, including predicting ADMET properties, classifying spectroscopy data, and leveraging electronic health records, will continue to broaden its impact [5,29,34]. The successful transition of deep learning prototypes from research environments to production requires robust infrastructure and integration into existing workflows [28]. Promoting opensource resources and democratizing access to these technologies is crucial for fostering innovation and wider adoption [28,35]. Overcoming these challenges through continued research, interdisciplinary collaboration, and responsible innovation will be key to unlocking the full potential of deep learning in revolutionizing small molecule discovery.  

# 6.1 Data Limitations and Quality  

A fundamental challenge in applying deep learning to small molecule discovery is the pervasive limitation of available data [39]. The effectiveness of deep learning models is heavily reliant on large, high-quality datasets for training [5,10], yet the accessible chemical and biological data are frequently characterized by scarcity, suboptimal quality, inconsistency, and potential biases [15,27,39].  

The quality of data poses significant hurdles. Data is often aggregated from diverse sources with variable reliability and experimental conditions, leading to inconsistencies and noise [21,26]. Furthermore, existing benchmark datasets may not accurately represent real-world drug discovery scenarios [10], and data can be sparse, highly imbalanced, or lacking proper validation [10,26,28]. These limitations severely impact model performance and generalization, particularly when applied to data outside the training domain [27,28]. While large datasets are generally beneficial, they do not automatically ensure the successful exploration of novel chemical space, and model prediction confidence diminishes significantly for molecules divergent from the training set distribution [27].  

To mitigate data scarcity and improve model generalization, various strategies are being explored. Data augmentation techniques can artificially expand the training set size and variability, which is particularly beneficial for models trained on small datasets [15,27,34]. Federated learning allows models to be trained across multiple decentralized data sources without centralizing sensitive data, offering a potential solution for privacy concerns [15]. Generating synthetic data can also supplement real experimental data [15]. These approaches present opportunities to leverage distributed or limited data resources but also introduce challenges related to ensuring the relevance and quality of augmented or synthetic data, as well as the complexities of distributed training. Specific methods like fragment-augmented approaches can address the scarcity of known ligands for particular targets [43], while knowledge learning techniques can enhance molecular  

representation learning even on limited datasets [40]. Pre-training models using abundant unlabeled data through selfsupervised learning is another promising direction to improve feature extraction from limited labeled data [41].  

Addressing data limitations fundamentally requires the continuous accumulation and improvement of biological and chemical data resources [19]. Building high-quality, large-scale, annotated datasets is crucial for training high-performance machine learning models [5,19]. While significant data reserves exist in public databases such as DrugBank, PubChem, ChEMBL, and ZINC [32], integrating and standardizing data from diverse sources remains a challenge due to the lack of consistent experimental protocols and formats [21,26]. Initiatives to develop comprehensive, high-quality databases through systematic literature mining and manual correction are essential [29].  

Ultimately, overcoming data limitations necessitates collaborative data-sharing initiatives [27] and community-wide efforts to establish large, consistent, and high-quality experimental datasets [19,26]. However, these efforts face significant challenges, including privacy concerns and intellectual property issues associated with proprietary data [19]. Facilitating greater accessibility of data to the public domain is also advocated [41]. Therefore, fostering trust and developing robust frameworks for data sharing and standardization are critical future directions for the field.  

# 6.2 Model Interpretability and Explainability  

The application of deep learning in small molecule discovery necessitates a strong emphasis on model interpretability and explainability. While deep learning models excel at identifying complex patterns and making predictions, their often opaque "black box" nature can impede their adoption and trustworthiness, particularly within regulated fields like drug discovery [5]. Developing highly interpretable methods is crucial for rendering model prediction processes transparent and enabling the quantification of prediction reliability [10].  

In the context of drug discovery, interpretability is paramount because understanding the underlying mechanisms of action is critical for validating potential drug candidates and guiding further research [32]. Insights into which molecular features or spectral regions drive a model's prediction can provide valuable mechanistic information [34]. Without such interpretability, researchers may be hesitant to fully trust or act upon AI-driven predictions, especially when dealing with high-stakes decisions in preclinical drug discovery [5]. The inherent challenges in interpreting the complex predictions of deep learning models represent a significant hurdle in their broader clinical and regulatory acceptance [5,19].​  

Addressing this challenge involves the development and application of Explainable Artificial Intelligence (XAI) methods. Various techniques offer different approaches to shedding light on model decision-making. Attention mechanisms, for instance, can highlight the most influential parts of the input data (e.g., specific atoms or bonds in a molecule) driving a prediction. An example is the Knowledge-Embedded Message Passing Neural Network (KEMPNN), which trains its attention mechanism using per-atom knowledge annotations to enhance graph neural network (GNN) interpretability [40]. Visualization methods provide alternative insights; while dimensionality reduction techniques like t-SNE and UMAP are standard for visualizing high-dimensional data representations, there is a need for innovative, model-specific analyses tailored to the chemically relevant internal workings of models, particularly generative ones [2]. The level of interpretability offered by such visualizations can vary depending on the specific model architecture [2]. Feature attribution methods, such as SHAP values, are also gaining traction, providing quantitative measures of how much each input feature contributes to a model's output [41].​  

Incorporating XAI methods into models, including those used for candidate compound optimization, can elucidate the 'logic' or rationale behind the structures proposed by the models [8]. For generative models, advocating for greater interpretability is seen as a way to enhance understanding and build trust in AI-driven drug discovery processes [3].  

Ultimately, the future development of machine learning in preclinical drug discovery should prioritize creating more interpretable algorithms and techniques. This focus will not only enhance the understanding of predictive processes and ensure result reliability [10,19] but also significantly improve the trust and acceptance of deep learning technologies among scientists, regulators, clinicians, and patients [3,5].  

# 6.3 Validity and Synthesizability of Generated Molecules  

A critical requirement for denovomolecular design methods based on deep learning is the generation of molecules that are not only chemically valid but also synthetically feasible. While deep learning models can explore vast chemical spaces, constraining this exploration to increase the likelihood of generating synthesizable molecules is paramount for practical  

applications in drug discovery [2,30]. Generating synthetically feasible molecules is essential because lead compounds ultimately need to be synthesized and tested in the laboratory [2,12].  

Ensuring the chemical validity of generated molecules presents an initial challenge. Early models, such as the RELATION model, initially struggled with producing a low proportion of valid molecular structures [30]. Efforts to address molecule validity issues have explored alternative molecular representations beyond traditional SMILES, such as SELF-referencIng Embedded Strings (SELFIES) or DeepSMILES, although their effectiveness can depend on specific model configurations [10,41]. Comparing different deep learning architectures, studies have shown variations in their ability to generate valid molecules. For instance, a Transformer-based model demonstrated a lower percentage of invalid molecules compared to a GRU-based model [33]. Post-processing steps are also commonly employed to remove invalid structures, duplicates, or substructures known to be unfeasible for synthesis, including molecules that violate specific chemical rules like Bredt’s Rule [38].​  

Beyond mere validity, the synthetic accessibility of generated molecules is a key determinant of their utility. The long-term practicality of generative models is strongly linked to their ability to incorporate the constraints of synthetic chemistry [2]. Approaches for integrating synthetic constraints into deep learning models aim to directly influence the generation process towards more accessible structures. Recent advancements include the use of reinforcement learning to construct synthetic trees based on reaction templates and the development of conditional variational autoencoders that jointly encode molecules and their corresponding synthetic reaction sequences [2]. While some generated molecules have been demonstrated to be reasonably synthesizable or accessible by standard chemistry [14,43], a recognized limitation in the field is the need for more accurate and reliable scoring functions for synthesizability to improve the ease of synthesis of generated molecules [23]. The development of more accurate retrosynthesis prediction models, such as the Chimera framework, is also crucial as it directly impacts the assessment of synthetic routes for generated molecules [31].  

A fundamental trade-off exists between generating novel molecular structures and ensuring their synthesizability or maintaining desirable existing characteristics [22,30]. While generative models can propose molecules distinct from known compounds, pushing too far into novel chemical space can yield structures that are exceedingly difficult or impossible to synthesize using current methods. Balancing this novelty with synthetic accessibility is a significant challenge. Furthermore, researchers must concurrently balance synthetic feasibility with other crucial properties such as target potency, selectivity, and favorable pharmacokinetic profiles [22]. The ability of a generative model to maintain existing characteristics of known active compounds while exploring novel variations is key to finding potent yet synthesizable candidates [22]. Achieving this balance requires sophisticated models capable of multi-objective optimization that effectively incorporate synthetic accessibility alongside pharmacological criteria.  

# 6.4 Validation and Benchmarking  

Rigorous validation is paramount to establishing the reliability and generalizability of deep learning models in small molecule discovery [15]. Ensuring that model predictions hold true across diverse chemical spaces and biological contexts is critical for their utility in real-world applications [15]. Rigorous validation workflows are considered necessary, particularly for deep Quantitative Structure–Activity Relationship (QSAR) models, although executing these can demand significant computational resources [28]. A typical validation process involves verification after model training, requiring statistically significant results to confirm the utility of a new prediction model [32]. Validation strategies encompass the use of diverse test sets, blind prediction challenges, and thorough comparisons against traditional computational methods as well as experimental outcomes [15]. Comparing novel models against established baselines and descriptor-based methods on standard datasets serves as a common practice to gauge performance [40]. Metrics like consistent recall values between validation and test sets can provide assurance of model generalizability [35].​  

Despite advancements, significant challenges persist in validating deep learning models for drug discovery. A fundamental requirement for model evaluation is the availability of appropriate datasets, suitable data balancing techniques, and relevant measurement metrics [10]. Furthermore, quantifying the uncertainty associated with molecular property predictions is crucial but remains challenging, with no widespread consensus on the optimal methods for evaluating uncertainty quantification (UQ) [27,37]. Effective UQ methods need to be evaluated on realistic applications, providing guarantees regarding both model performance and prediction confidence for broader adoption [37]. Beyond technical hurdles, the substantial computational cost associated with implementing rigorous validation workflows poses a practical challenge [28].  

Current benchmarking practices and datasets also exhibit limitations. Existing metrics are often inherently limited, providing only an incomplete assessment of a model's true effectiveness in real-world scenarios [41]. Given the increasing complexity of modern deep learning architectures, there is a pressing need for suitable methods and best practices to fairly evaluate model performance [41]. Consequently, there is a strong emphasis on developing more realistic and challenging benchmarking datasets [15]. Standardized comparative studies and test sets are necessary, particularly for generative machine learning models in de novo drug design [36]. Developing benchmarking tasks that facilitate accurate comparison of generative models across multiple tasks and consider computational scalability is advocated [2]. However, caution is advised against over-interpreting superior performance on specific benchmarking tasks, as it may not directly translate to greater scientific value [2]. Establishing open benchmark tests, for instance, for candidate compound optimization, is considered valuable for the field [8].​  

Ultimately, the most compelling validation involves confirming deep learning predictions through experimental data. Validation strategies explicitly include rigorous comparisons with experimental results [15]. For instance, the predicted binding affinities from docking simulations can be validated by comparing them with experimental half-maximal inhibitory concentration (pIC50) values [30]. Similarly, generated molecules can be validated by comparing them to known active compounds [14]. While computationally intensive, simulations like Molecular Dynamics (MD) and Free Energy Perturbation (FEP) can be employed to estimate properties such as relative binding energy, serving as a bridge towards experimental confirmation [38]. It is crucial to recognize that contemporary AI-based methodologies function as powerful tools to accelerate research, but they are not a substitute for conventional experimental approaches or the invaluable expertise of human researchers [39].  

# 6.5 Ethical Considerations  

The application of deep learning in small molecule discovery introduces a range of critical ethical considerations that necessitate careful attention. A primary concern revolves around potential data biases inherent in the training datasets, which can inadvertently lead to inequitable outcomes in the drug discovery process [21]. Such biases can skew the development towards certain demographic groups or disease profiles while neglecting others, thereby exacerbating existing health disparities. Ethical considerations, particularly concerning fairness and biases, represent significant challenges inherent in utilizing AI-based techniques within this domain [39]. Beyond issues of bias and fairness, the deployment of deep learning models also raises complex challenges related to protecting data privacy and intellectual property. The vast amounts of sensitive biological and chemical data required for training these models necessitate robust privacy safeguards. Simultaneously, the innovative nature of deep learning outputs in generating novel molecular structures or predicting properties creates new challenges for establishing and protecting intellectual property rights. Therefore, ensuring the responsible and ethical development and deployment of deep learning methodologies is paramount to realizing their full potential while mitigating potential harms and upholding ethical standards in drug discovery.​  

# 6.6 Integration with Other Methods and Future Opportunities  

![](images/9694a80ff079073137aac93dff30eeda53e5772daa0414d6f9890f05a35b917c.jpg)  

Accelerating the drug discovery process necessitates the holistic integration of deep learning methodologies with both traditional experimental techniques and other computational approaches [39]. This synergy leverages the strengths of diverse methods to enhance reliability and efficiency. Deep learning, for instance, can directly guide experimental workflows, such as selecting optimal compounds for synthesis or designing novel experimental protocols, thereby streamlining preclinical development [19]. A notable long-term vision involves the creation of autonomous design-maketest-analyze cycles, integrating generative models with automated laboratory systems capable of synthesizing and experimentally evaluating generated compounds [41]. This aligns with the stimulation of efficient cheminformatics tools by rapid advancements in robotic platforms for planning and guiding organic synthesis [28]. Future research also aims to develop molecular generation models specifically based on available reagents to ensure synthetic feasibility [27].​  

Integration with other computational methods is equally crucial. Combining deep learning with molecular dynamics simulations and docking techniques significantly improves the accuracy and efficiency of structure-based drug discovery [19,24]. For example, Deep Docking (_DD_) exemplifies such integration, designed to work in conjunction with any docking program and readily incorporated into broader virtual screening pipelines [35]. Computational simulations, such as those using DiffDock, can also provide crucial support or challenges to protein targets identified through experimental methods like resistance screening and affinity chromatography [9]. Structure-based de novo design methods, often involving deep learning and docking, are applicable to target proteins with known 3D structures [14,33].  

Furthermore, combining deep learning with quantum mechanics (QM) calculations holds significant promise for creating more powerful and accurate predictive tools [24]. While QM calculations are highly accurate for characterizing interaction energies, their computational cost limits their direct application in large-scale generative model workflows, often relegating them to testing roles [2]. However, integrating machine learning interatomic potentials (MLIPs) with QM calculations offers a pathway to achieve near-DFT accuracy at reduced computational expense, making MLIPs valuable components for generative models [2]. The integration of QM descriptors into machine learning methods for QSAR is also identified as a key area for further development [24,26]. Beyond molecular interactions, integrating evidential methods into atomistic machine learning architectures could advance the prediction of potential energy surfaces and quantum mechanical properties [37]. The increasing generation of large volumes of diverse biomedical data, particularly omics data (genomics, proteomics, metabolomics, pharmacogenomics), presents an opportunity for machine learning to classify patients, aid diagnoses, and develop treatments, contributing to the rise of precision medicine [5,32]. Machine learning is also being applied to predicting ADMET properties and can be integrated into comprehensive software platforms for drug-likeness and safety prediction [29]. In silico experiments, computational simulations of biological processes, are becoming indispensable for handling the complexity of modern biology and processing large information volumes, thereby accelerating development [11].​  

Looking ahead, quantum computing represents a revolutionary breakthrough with the potential to significantly accelerate drug discovery applications, including deep QSAR and solving the Schrodinger equation for complex molecular systems [24,28].  

Future directions in the field include the critical integration of multi-modal data, which is seen as essential for accelerating discovery [3]. There is a continuous need for the development of more accurate and efficient models. This encompasses various aspects, such as developing AI-driven foundation models for customized molecular design strategies [4], enhancing deep QSAR models for multi-task optimization within automated design systems [28], utilizing knowledge embedding to improve molecule representation learning [40], optimizing model architectures for multi-objective controlled generation tasks [30], and employing reinforcement learning to derive target-oriented strategies for lead compound optimization, allowing interaction with chemists to integrate prior knowledge [8]. Progress is also anticipated in steering sequential data generation with recurrent neural networks [1].​  

The application of deep learning is expanding to new areas of drug discovery. This includes specialized tasks like PROTAC linker prediction, where end-to-end pipelines combine generation and post-processing [38]. De novo drug design methods are becoming applicable to any target protein with structural information [33], and AI-driven virtual screening processes show great potential in the early stages of discovering innovative drugs for specific targets like Sirtuin-1 [18]. Beyond drug molecules, related techniques may be applicable to other analyses, such as classifying spectroscopy data [34], or modifying existing structures to generate novel chemical entities [30]. The potential application of machine learning to electronic health records and real-world evidence could also improve clinical trial outcomes [5].​  

However, transitioning deep learning prototypes developed in research environments to full-scale production within pharmaceutical and biotechnology industries presents significant challenges [28]. Addressing these challenges requires robust infrastructure, validation protocols, and integration into existing complex pipelines. Furthermore, the advancement of computer-aided drug design necessitates the availability of open-source and democratized resources to foster wider adoption and innovation [28]. The public availability of tools, such as the _DD_ pipeline on GitHub facilitating automation on HPC clusters, exemplifies this crucial need [35].  

# 7. Conclusion  

This survey has highlighted the significant progress and transformative potential of deep learning in revolutionizing small molecule discovery, positioning it as a powerful tool to address the complexities inherent in traditional drug development [19,25]. Deep learning and machine learning algorithms have emerged as promising solutions to overcome these challenges [7], driving a paradigm shift in pharmaceutical research and development towards more data-driven, efficient, and costeffective processes [10,39]. The integration of machine learning into drug discovery workflows is increasingly enhancing traditional processes and is expected to define future projects [9].​  

Deep learning techniques have demonstrated capabilities across various stages of drug discovery. In generative molecular design, diverse models such as descriptor conditional recurrent neural networks (cRNNs) [1], Transformer-encoder based models [33], reinforced adversarial neural computers (RANC) [42], and deep generative models [3] have shown potential for generating novel compounds with desired properties. Specific applications include the design of novel agonists for underexplored targets using chemical language models (CLMs) [43], structure-based generation with models like RELATION [30], and the development of sophisticated platforms like DRAGONFLY, which integrates CLMs with interactome data for zeroshot de novo design [4,23]. Benchmarking tools like MOSES are crucial for evaluating these generative models [36]. Beyond generation, deep learning enhances predictive modeling, notably in quantitative structure-activity relationship (QSAR) studies, transforming them from rule-driven to data-driven approaches [24,26]. The integration of deep learning with QSAR has advanced virtual screening and ADMET property prediction [25,28,29], with methods like evidential deep learning offering robust uncertainty estimation [37]. Furthermore, AI-driven virtual screening has successfully identified novel inhibitors while significantly reducing experimental scope [18], and methods like Deep Docking enable efficient large-scale screening campaigns involving billions of molecules [35]. AI models also play a critical role in guiding the rapid design and optimization of compounds [6,14,22], identifying new targets, strengthening target-disease associations, and facilitating advancements even in clinical trials through biomarker identification and patient monitoring [21]. Techniques like cumulative learning and transfer learning are proving valuable for handling challenges like limited data availability [16,20,34], while incorporating human knowledge into models, such as with KEMPNN, can enhance prediction performance and generalizability [40].  

Despite these significant advancements, realizing the full potential of deep learning in drug discovery necessitates continued research and development to overcome existing challenges [4,18,19,25]. Key challenges include the need for more high-quality, diverse, and accessible data from both simulations and experiments [15,41], as the quality and quantity of training data are paramount [15]. Issues related to data quality, model validation, and applicability domain still need rigorous attention [28]. Furthermore, machine learning models need to be better aligned with realistic drug discovery endpoints [44]. Continued optimization of models and workflows is essential [23,39].​  

Looking ahead, the integration of deep learning and AI into drug discovery workflows promises a transformative impact on the pharmaceutical industry and holds immense potential for improving human health [19,25]. The vision includes an era where target molecules are generated, synthesized, characterized, and improved with minimal human intervention [2]. AIassisted methods are expected to reduce costs and accelerate the drug research and development process significantly [6]. Achieving this future requires continued technological breakthroughs, recognizing that initial doubts often precede significant advancements [32]. It is crucial to view AI as a powerful tool rather than a panacea [27], emphasizing the need for collaboration between computational experts and synthetic chemists [2]. By focusing on robust data, rigorous validation, tailored methods, seamless integration, and ethical deployment, the research community can unlock the full promise of deep learning, ushering in a more intelligent and efficient era of drug design and contributing substantially to tackling new and unforeseen disease risks [24,39].  

# References  

[1] Descriptor-Conditional RNNs for Targeted De Novo M https://www.nature.com/articles/s42256-020-0174-5 [2] 生成式人工智能：分子设计的新兴范式 https://cloud.tencent.com/developer/article/2326623 [3] 深度生成模型重塑药物发现：原理、应用与未来 https://blog.csdn.net/qq_44783177/article/details/130951358 [4] Nature子刊：深度学习 $^ +$ 化学语言模型助力药物从头设计 https://baijiahao.baidu.com/s? id=1798359787682842582&wfr=spider&for=pc​ [5] 机器学习助力医药研发：应用与展望 https://www.thepaper.cn/newsDetail_forward_7086146 [6] 人工智能赋能先导化合物发现与优化：技术进展与应用案例 https://roll.sohu.com/a/792809940_121123705 [7] AI赋能药物研发：从机器学习到深度学习的机器智能方法 https://blog.csdn.net/chbchen007/article/details/133319809 [8] 生成式AI助力药物分子结构优化：JACS综述 https://cloud.tencent.com/developer/article/2479335  

[9] 临床前药物发现中的机器学习应用 https://hub.baai.ac.cn/view/39636  

[10] AI辅助药物研发：技术、挑战与未来展望 https://mp.weixin.qq.com/s? _biz=MzUxNTM5MDcwMA $\scriptstyle 1 = =$ &mid=2247581961&idx $\ v =$ 4&sn=746e4509de8ee1d70241c626ce7bddee&chksm=f862877e015d30   
744d6eaf53a69c3f46a145c0b3504d50c1adee2de2c78e778b3539bf9f763d&scene=27  

[11] 机器学习在药物发现中的应用与发展趋势 https://baijiahao.baidu.com/s?id $=$ 1721157080537949425&wfr=spider&for=pc [12] AI Technology Accelerates Drug Development https://www.solutions.bocsci.com/resources/new-technology-topromote-drug-development-ai-technology.html  

$=$  

[13] REINVENT4：开源AI分子设计框架新纪元 https://baijiahao.baidu.com/s?id 1783813657125312833&wfr=spider&for=pc   
[14] AI结合Docking的SBMolGen模型：基于结构的分子从头设计 http://baijiahao.baidu.com/s?   
id=1712232054065165739&wfr $\mathbf { \bar { \rho } } = \mathbf { \rho }$ spider&for=pc​   
[15] 牛津大学团队：数据驱动小分子药物发现的未来 https://hub.baai.ac.cn/view/41309   
[16] 药物发现中机器学习技术的进步：强化学习、迁移学习和多任务学习   
https://blog.csdn.net/ShenggengLin/article/details/112231206   
[17] 深度学习在药物发现中的应用综述 https://baijiahao.baidu.com/s?id=1775863206582805728&wfr=spider&for=pc   
[18] AI虚拟筛选发现新型Sirtuin-1抑制剂 https://www.medsci.cn/article/show_article.do?id=4602e9324149​   
[19] 机器学习助力临床前药物发现：现状与展望 https://cloud.tencent.com/developer/article/2440612   
[20] 基于RNN迁移学习的特定库分子生成指南 http://baijiahao.baidu.com/s?id $\ c =$ 1686434032031682907&wfr=spider&for=pc   
[21] 机器学习在药物研发中的应用：机遇与挑战 https://cloud.tencent.com/developer/article/1781452   
[22] AI助力！发现高效选择性p300/CBP抑制剂 https://baijiahao.baidu.com/s?id $\ c =$ 1665673233785436543&wfr=spider&for=pc​   
[23] DRAGONFLY模型：基于深度学习的分子设计应用与优势 https://baijiahao.baidu.com/s?   
id=1809779713048525861&wfr=spider&for=pc​   
[24] 深度学习赋能QSAR：药物发现新趋势 https://cloud.tencent.com/developer/article/2394048​   
[25] 药物发现中的深度学习：人工智能赋能新药研发 https://mp.weixin.qq.com/s?   
__biz=MzA4ODY4MDE0NA $\scriptstyle = =$ &mid=2247554448&idx=4&sn $\mid = \mid$ 53a428173a5b26baabe56ce57f31eb0d&chksm $\mid =$ 9024262fa753af39   
7f380bf648bb3e66839fafceff0af9ea84c10701d5c723dfc5f1285f10d1&scene=27   
[26] 机器学习驱动QSAR研究再发展：JCIM精选论文回顾 https://cloud.tencent.com/developer/article/2217455   
[27] 深度学习在分子生成与性质预测中的应用进展 https://mp.weixin.qq.com/s?   
__biz $: =$ Mzg3NTA2NTg5NA $\scriptstyle = =$ &mid $\ c =$ 2247489675&idx $\underline { { \underline { { \mathbf { \Pi } } } } } =$ 1&sn=f49f1eea4da997d405fa3c55db3d43f3&chksm $\mid =$ cec67fc2f9b1f6d471   
f7f53aba023d07a021fb8cd4fa42d49c72f6dc377f33097e12e5f544ae&scene $^ { - 2 7 }$ ​   
[28] 深度学习赋能药物发现：QSAR建模的变革与应用 https://hub.baai.ac.cn/view/33833   
[29] 基于机器学习的药物成药性与安全性预测研究 https://faculty.csu.edu.cn/caodongsheng/en/yjfx/3403/content/1874.htm​   
[30] RELATION：基于结构的 De Novo 药物设计深度生成模型阅读笔记   
https://blog.csdn.net/Pikaqiu_life/article/details/127705529   
[31] Microsoft Research Highlights: NeoMem, Chimera, Re https://www.microsoft.com/en-us/research/blog/research-focus  
week-of-december-16-2024/?lang=fr_ca&locale=zh-cn​   
[32] 机器学习赋能新药研发：历史、应用与未来 https://baijiahao.baidu.com/s?id $\ c =$ 1711054548265733255&wfr=spider&for=pc​   
[33] Transformer-Based Generative Model for Accelerated https://pubs.acs.org/doi/full/10.1021/acsomega.1c05145   
[34] Cumulative Learning Enables CNNs for Small Mass Sp https://www.nature.com/articles/s41467-020-19354-z​   
[35] Deep Docking: Accelerating Structure-Based Drug Di https://pubs.acs.org/doi/10.1021/acscentsci.0c00229   
[36] MOSES: 分子生成模型基准测试平台   
https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2020.565644/full​   
[37] Evidential Deep Learning for Molecular Property Pr https://pubs.acs.org/doi/10.1021/acscentsci.1c00546​   
[38] Deep Learning for PROTAC Linker Prediction: An Enc https://pubs.acs.org/doi/full/10.1021/acs.jcim.2c01287  