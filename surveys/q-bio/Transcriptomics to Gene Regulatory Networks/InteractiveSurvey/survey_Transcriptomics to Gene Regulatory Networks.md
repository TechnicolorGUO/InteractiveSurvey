# A Survey of Transcriptomics to Gene Regulatory Networks

# 1 Abstract


Transcriptomics has revolutionized our understanding of gene expression and regulation, providing insights into the dynamic processes that govern cellular function. This survey paper focuses on the methodologies and advancements in the inference of gene regulatory networks (GRNs) from transcriptomics data, highlighting the integration of multi-omics data, advanced computational techniques, and policy and systemic aspects. The paper reviews high-performing algorithms, such as machine learning and Bayesian networks, and discusses their application in multi-species GRN inference, integrative and computational approaches, and the integration of TWAS and GWAS for disease-specific networks. Key findings include the enhanced accuracy and robustness of GRN models through the integration of network metrics, Evo-Devo principles, and advanced statistical methods. The paper also explores the potential of quantum computing and GPU acceleration to handle large-scale transcriptomic datasets, facilitating more efficient and scalable network inference. In conclusion, this survey provides a comprehensive overview of the current state of the field, offering insights into the challenges and future directions in gene regulatory network inference.

# 2 Introduction
Transcriptomics, the study of the complete set of RNA transcripts produced by the genome under specific conditions, has emerged as a cornerstone in modern molecular biology. The advent of high-throughput sequencing technologies has revolutionized our ability to profile gene expression at an unprecedented scale, providing insights into the dynamic regulation of genes in various biological processes [1]. Transcriptomics data serve as a critical intermediate layer between the genome and the proteome, offering a unique perspective on how genetic information is translated into cellular function. The integration of transcriptomics with systems biology approaches has enabled the construction of gene regulatory networks (GRNs), which are essential for understanding the complex interactions that govern gene expression [2]. These networks not only reveal the regulatory mechanisms underlying cellular responses but also provide a framework for identifying key regulatory nodes and potential therapeutic targets [1].

This survey paper focuses on the methodologies and advancements in the inference of gene regulatory networks (GRNs) from transcriptomics data [3]. The paper aims to provide a comprehensive overview of the current state of the field, highlighting the key challenges and recent developments in multi-species GRN inference, integrative and computational approaches, and advanced statistical and machine learning models [4]. The integration of transcriptomics with other omics data, such as genomics and epigenomics, has significantly enhanced our ability to construct accurate and biologically relevant GRNs [5]. Moreover, the application of quantum computing and GPU acceleration has opened new avenues for handling the computational demands of large-scale transcriptomic datasets, enabling more efficient and scalable network inference.

The paper delves into the integration of network metrics for disease gene prioritization, a critical aspect of GRN analysis [6]. Network metrics, such as degree centrality, betweenness centrality, and eigenvector centrality, provide quantitative measures of a gene's importance within the network, facilitating the identification of key regulatory hubs and potential disease drivers [4]. The integration of multiple network metrics enhances the robustness and accuracy of disease gene prioritization, mitigating the biases inherent in any single metric and providing a more nuanced view of the network's structure [6]. Additionally, the paper explores the combination of evolutionary developmental biology (Evo-Devo) with mathematical modeling, which allows for the construction of more accurate and robust models of GRNs by incorporating both temporal and spatial aspects of gene regulation.

The survey also discusses high-performing algorithms for GRN inference, which leverage advanced computational techniques such as machine learning and Bayesian networks [4]. These algorithms can integrate multiple data sources, enhance the reliability of predicted interactions, and incorporate evolutionary conservation across multiple species. The integration of data mining and bioinformatics tools has further advanced the elucidation of regulatory pathways, particularly in the context of gene regulatory networks. The paper highlights the importance of policy and systemic aspects in regulatory frameworks, which ensure the ethical, safe, and effective use of GRN data and models in biotechnology and personalized medicine.

Furthermore, the paper examines the integration of Transcriptome-Wide Association Studies (TWAS) and Genome-Wide Association Studies (GWAS) for constructing disease-specific gene regulatory networks, with a focus on cardiovascular genomics [7]. This integration leverages the strengths of both methodologies to provide a more comprehensive view of the genetic architecture of diseases like Coronary Artery Disease (CAD) [7]. Advanced statistical and machine learning models, such as Block-Diagonal Covariance for Gaussian Locally-Linear Mapping (BLLiM) and the Causal Dropout Model for scRNA-seq data, are discussed in detail, highlighting their contributions to handling high-dimensional and sparse transcriptomic data.

In conclusion, this survey paper provides a comprehensive overview of the methodologies and advancements in the inference of gene regulatory networks from transcriptomics data [8]. It highlights the integration of multi-omics data, the application of advanced computational techniques, and the importance of policy and systemic aspects in the field. The paper aims to serve as a valuable resource for researchers and practitioners, offering insights into the current challenges and future directions in this rapidly evolving area of research.

# 3 Multi-Species Gene Regulatory Network Inference

## 3.1 Network Topology and Gene Targeting Scores

### 3.1.1 Integrating Network Metrics for Disease Gene Prioritization
Integrating network metrics for disease gene prioritization represents a significant advancement in the field of systems biology, leveraging the rich topological information embedded within gene regulatory networks (GRNs) [6]. Network metrics, such as degree centrality, betweenness centrality, and eigenvector centrality, provide quantitative measures of a gene's importance within the network [4]. These metrics capture various aspects of a gene's connectivity and influence, allowing researchers to identify key regulatory hubs and potential disease drivers [6]. For instance, genes with high betweenness centrality are often located at critical junctures within the network, facilitating the flow of information between different modules. This positioning can indicate their potential role in mediating disease processes, making them prime candidates for further investigation.

The integration of multiple network metrics enhances the robustness and accuracy of disease gene prioritization. By combining different metrics, such as degree centrality, which measures the number of direct interactions a gene has, and closeness centrality, which assesses how close a gene is to all other nodes in the network, researchers can gain a more comprehensive understanding of a gene's functional significance [4]. This multi-faceted approach helps to mitigate the biases inherent in any single metric and provides a more nuanced view of the network's structure. Moreover, the integration of network metrics with other types of biological data, such as functional annotations and pathway information, further enriches the prioritization process, enabling the identification of genes that are not only topologically significant but also biologically relevant.

In the context of disease gene prioritization, the integration of network metrics is particularly valuable for uncovering novel disease-associated genes that may not be apparent through traditional approaches [6]. For example, genes with high eigenvector centrality, which measures a gene's influence based on the influence of its neighbors, can highlight genes that are part of highly interconnected and influential subnetworks [4]. These genes, even if they have a moderate number of direct interactions, can play crucial roles in disease pathways by modulating the activity of key regulatory nodes. By leveraging the synergistic power of integrated network metrics, researchers can effectively prioritize candidate genes for experimental validation, accelerating the discovery of new therapeutic targets and biomarkers.

### 3.1.2 Combining Evo-Devo with Mathematical Modeling
The integration of evolutionary developmental biology (Evo-Devo) with mathematical modeling represents a powerful approach to understanding the complex dynamics of gene regulatory networks (GRNs) across multiple species. By leveraging the principles of Evo-Devo, researchers can trace the evolutionary history of regulatory interactions, identifying conserved and divergent elements that contribute to species-specific traits. This integration allows for the construction of more accurate and robust models of GRNs, as it incorporates both the temporal and spatial aspects of gene regulation, which are crucial for predicting the behavior of biological systems under various conditions [4].

Mathematical models, such as ordinary differential equations (ODEs) and Boolean networks, are commonly used to simulate the dynamic behavior of GRNs [4]. When combined with Evo-Devo insights, these models can be enriched with cross-species data, providing a more comprehensive view of regulatory mechanisms. For instance, comparative analyses of gene expression data from closely related species can reveal conserved regulatory modules and highlight lineage-specific adaptations [8]. This multi-species approach not only enhances the predictive power of the models but also aids in the identification of key regulatory nodes that may have been overlooked in single-species studies [8]. The use of graph theory and network topology measures, such as degree centrality and betweenness centrality, further refines the analysis by quantifying the importance of individual genes and their interactions within the network [4].

Moreover, the integration of Evo-Devo with mathematical modeling facilitates the exploration of evolutionary constraints and innovations in regulatory networks. By analyzing the evolutionary trajectories of regulatory interactions, researchers can uncover the underlying principles that govern the emergence and maintenance of complex traits. This approach is particularly useful in studying the evolution of developmental pathways, where the interplay between genetic and environmental factors plays a critical role. Ultimately, the combination of Evo-Devo and mathematical modeling provides a framework for dissecting the intricate regulatory landscapes that shape the diversity of life, offering new insights into the fundamental mechanisms of development and evolution.

### 3.1.3 High-Performing Algorithms for GRN Inference
High-performing algorithms for gene regulatory network (GRN) inference have evolved significantly, driven by the need for more accurate and robust methods to unravel the complex regulatory mechanisms governing gene expression [4]. These algorithms leverage advanced computational techniques, including machine learning and Bayesian networks, to infer interactions from high-throughput data such as RNA-seq and ChIP-seq. Machine learning approaches, particularly those utilizing deep neural networks, have shown promise in capturing non-linear relationships and integrating diverse data types, thereby improving the precision of inferred networks. Bayesian networks, on the other hand, offer a probabilistic framework that can handle uncertainty and incorporate prior biological knowledge, making them particularly suitable for scenarios where data are sparse or noisy.

One of the key challenges in GRN inference is the integration of multiple data sources to enhance the reliability of predicted interactions. High-performing algorithms often employ ensemble methods that combine predictions from different inference techniques, each capturing distinct aspects of the regulatory landscape. For instance, integrating correlation-based methods, which are effective at identifying co-expression patterns, with motif analysis, which can pinpoint direct regulatory interactions, can lead to more comprehensive and accurate network reconstructions. Furthermore, the inclusion of epigenetic data, such as DNA methylation and histone modifications, provides additional layers of regulation that can refine the inferred networks and improve their biological relevance.

Another significant advancement in high-performing GRN inference algorithms is the incorporation of evolutionary conservation across multiple species. Comparative genomics approaches leverage the principle that conserved regulatory interactions are more likely to be functionally important. By integrating orthologous gene expression data from closely related species, these algorithms can identify conserved regulatory modules and reduce the noise inherent in single-species datasets. This multi-species approach not only enhances the robustness of the inferred networks but also provides insights into the evolutionary dynamics of gene regulation, contributing to a more holistic understanding of the underlying biological processes [8].

## 3.2 Integrative and Computational Approaches

### 3.2.1 Data Mining and Bioinformatics for Regulatory Pathways
Data mining and bioinformatics have emerged as indispensable tools in the elucidation of regulatory pathways, particularly in the context of gene regulatory networks (GRNs) [2]. The integration of diverse data types, including transcriptomic, proteomic, and epigenomic data, has enabled the construction of comprehensive models that capture the dynamic and complex nature of gene regulation. Advanced data mining techniques, such as machine learning algorithms and network inference methods, have been pivotal in identifying regulatory modules and key transcription factors (TFs) that orchestrate cellular responses. These approaches often involve the use of probabilistic graphical models, such as Bayesian networks and Markov models, which can infer causal relationships and predict the behavior of regulatory networks under various conditions [3].

The application of bioinformatics in regulatory pathway analysis has been further enhanced by the advent of high-throughput sequencing technologies, which generate vast amounts of data at unprecedented resolution. These technologies, including RNA-seq and ChIP-seq, have facilitated the identification of cis-regulatory elements and the mapping of TF binding sites, thereby providing a detailed view of the regulatory landscape. Additionally, the integration of multi-omics data, such as combining transcriptomic and epigenomic profiles, has allowed researchers to uncover the intricate interplay between gene expression and chromatin state. This multi-layered approach has been crucial in understanding how regulatory elements are coordinated to achieve precise control over gene expression.

Despite these advancements, several challenges remain in the field of data mining and bioinformatics for regulatory pathways. One of the primary challenges is the integration of heterogeneous data sources, which often require sophisticated normalization and harmonization techniques to ensure consistency and reliability. Moreover, the computational complexity of analyzing large-scale datasets necessitates the development of efficient algorithms and scalable computational frameworks. Another challenge is the interpretation of the biological significance of inferred regulatory networks, which often requires experimental validation to confirm the predicted interactions [8]. Nevertheless, the continued refinement of data mining and bioinformatics tools holds great promise for advancing our understanding of regulatory pathways and their role in health and disease.

### 3.2.2 Policy and Systemic Aspects in Regulatory Frameworks
In the context of regulatory frameworks, policy and systemic aspects play a crucial role in shaping the governance and operational dynamics of gene regulatory networks (GRNs). These frameworks are designed to ensure the ethical, safe, and effective use of GRN data and models, which are increasingly being applied in biotechnology, pharmaceuticals, and personalized medicine. Policy aspects encompass the legal and regulatory guidelines that govern data privacy, intellectual property, and the ethical use of genetic information. For instance, the handling and sharing of gene expression data, which are often sensitive and personal, must comply with stringent data protection regulations such as the General Data Protection Regulation (GDPR) in the European Union and the Health Insurance Portability and Accountability Act (HIPAA) in the United States. These policies not only protect individual privacy but also facilitate the responsible sharing of data among researchers, thereby accelerating scientific discovery and innovation.

Systemic aspects, on the other hand, focus on the broader organizational and infrastructural elements that support the development and application of GRN models. This includes the establishment of standardized protocols for data collection, processing, and analysis, as well as the development of robust computational tools and databases. Standardization is essential for ensuring the interoperability of data across different platforms and studies, which is critical for the integration and synthesis of large-scale gene expression datasets. Moreover, the systemic infrastructure must be capable of handling the computational demands of complex GRN modeling, which often involves high-dimensional data and sophisticated algorithms. The availability of high-performance computing resources and cloud-based services is therefore a key systemic requirement for advancing the field of GRN research.

Finally, the integration of policy and systemic aspects is essential for the sustainable development and application of GRN models. Effective collaboration between policymakers, researchers, and industry stakeholders is necessary to address the emerging challenges and opportunities in the field. For example, the development of regulatory sandboxes and pilot programs can provide a controlled environment for testing new GRN technologies and applications, while also allowing for the refinement of regulatory guidelines. Additionally, public engagement and education are crucial for building trust and support for the use of GRN models in healthcare and other applications. By balancing the need for innovation with the imperative of ethical and responsible governance, the policy and systemic aspects of regulatory frameworks can significantly enhance the impact and utility of GRN research.

### 3.2.3 TWAS and GWAS Integration for CAD-Specific Networks
In the realm of cardiovascular genomics, the integration of Transcriptome-Wide Association Studies (TWAS) and Genome-Wide Association Studies (GWAS) has emerged as a powerful approach to elucidate the genetic architecture of Coronary Artery Disease (CAD) [7]. This integration leverages the strengths of both methodologies to construct CAD-specific gene regulatory networks, thereby enhancing our understanding of the molecular mechanisms underlying CAD [7]. TWAS, which predicts gene expression levels from genetic variation, complements GWAS by identifying genes whose expression is associated with CAD risk [9]. By combining these two types of data, researchers can pinpoint specific genes and pathways that are dysregulated in CAD, providing a more comprehensive view of the disease's etiology.

The integration of TWAS and GWAS data for CAD-specific networks involves several key steps. Initially, TWAS is used to predict gene expression levels in relevant tissues, such as the arterial wall, using genetic variants identified in GWAS [9]. These predicted expression levels are then correlated with CAD case-control status to identify genes whose expression is significantly associated with the disease. Subsequently, these genes are integrated into a network framework, where they are connected based on known or inferred regulatory relationships, such as those derived from co-expression analysis or transcription factor binding sites [1]. This network construction allows for the identification of key regulatory hubs and pathways that may play critical roles in CAD pathogenesis.

Moreover, the integration of TWAS and GWAS data facilitates the identification of novel therapeutic targets and biomarkers for CAD. By focusing on genes and pathways that are both genetically associated with CAD and exhibit altered expression patterns, researchers can prioritize candidates for further functional validation [7]. This approach not only enhances the biological interpretability of GWAS findings but also provides a systematic framework for translating genetic discoveries into actionable insights. The resulting CAD-specific networks serve as a valuable resource for understanding the complex interplay between genetic variation and gene expression in the context of cardiovascular disease, ultimately contributing to the development of more effective prevention and treatment strategies.

# 4 Quantum and Advanced Computational Methods for GRN Inference

## 4.1 Advanced Statistical and Machine Learning Models

### 4.1.1 Block-Diagonal Covariance for Gaussian Locally-Linear Mapping
Block-Diagonal Covariance for Gaussian Locally-Linear Mapping (BLLiM) represents a significant advancement in the modeling of complex biological systems, particularly in the context of single-cell RNA sequencing (scRNA-seq) data. By leveraging the Gaussian Locally-Linear Mapping (GLLiM) framework, BLLiM introduces a block-diagonal structure to the covariance matrix, which effectively captures the local linearity of the data while reducing the dimensionality and computational complexity associated with high-dimensional biological datasets. This approach is particularly advantageous in scenarios where the data exhibit non-linear relationships that cannot be adequately modeled by traditional linear methods.

The core of the BLLiM model lies in its ability to partition the data into clusters, each of which is characterized by a locally-linear relationship between the input and output variables. This clustering is achieved through a model selection process that identifies specific modules of genes within each cluster, thereby allowing for a more nuanced and accurate representation of the underlying biological processes. The block-diagonal structure of the covariance matrix ensures that the relationships within each cluster are captured independently, while still maintaining the overall coherence of the model. This modular approach not only enhances the interpretability of the results but also facilitates the identification of key regulatory pathways and interactions within the data.

In practice, the BLLiM model is implemented through an iterative estimation procedure that alternates between updating the cluster assignments and refining the parameters of the locally-linear mappings. This procedure is computationally efficient and scalable, making it suitable for large-scale scRNA-seq datasets. The effectiveness of BLLiM has been demonstrated through both simulated and real-world applications, where it has shown superior performance in predicting gene expression patterns and inferring gene regulatory networks compared to other state-of-the-art methods. The ability to handle high-dimensional data and capture non-linear dependencies makes BLLiM a valuable tool for advancing our understanding of complex biological systems.

### 4.1.2 Causal Dropout Model for scRNA-seq Data
The Causal Dropout Model for scRNA-seq data represents a significant advancement in addressing the inherent challenges of dropout events, which are pervasive in single-cell RNA sequencing (scRNA-seq) datasets [10]. These dropouts, characterized by the failure to detect expressed genes, can lead to a substantial loss of biological information and introduce biases in downstream analyses such as gene regulatory network (GRN) inference [10]. The causal dropout model aims to mitigate these issues by incorporating a causal framework that explicitly models the dropout mechanism, thereby allowing for a more accurate representation of the underlying biological processes.

In contrast to traditional methods that often treat dropouts as missing at random or use zero-inflated models to account for excess zeros, the causal dropout model leverages a probabilistic approach to distinguish between true biological zeros and technical dropouts. This is achieved by integrating a causal inference component that models the relationship between the observed data and the unobserved true expression levels. By doing so, the model can effectively handle the non-ignorable nature of dropouts, which is a critical aspect of scRNA-seq data [10]. The causal framework also allows for the estimation of the causal effects of gene expression levels, providing a more robust basis for inferring gene-gene interactions and constructing GRNs [4].

The implementation of the causal dropout model involves a combination of statistical and computational techniques, including Bayesian inference and continuous-valued optimization, which are designed to handle the high dimensionality and sparsity of scRNA-seq data. This approach not only improves the accuracy of gene expression quantification but also facilitates the integration of scRNA-seq data with other omics data types, such as genetic variants and epigenetic marks, to provide a more comprehensive understanding of cellular heterogeneity and regulatory mechanisms. The model's ability to handle non-linear relationships and complex dependencies in the data makes it a powerful tool for advancing our understanding of gene regulatory networks in single-cell contexts.

### 4.1.3 Linear Gaussian Bayesian Networks for Gene Expression Prediction
Linear Gaussian Bayesian Networks (LGBNs) are a subset of Bayesian networks specifically designed to model continuous variables, such as gene expression levels, using a linear Gaussian distribution. In the context of gene expression prediction, LGBNs provide a powerful framework for inferring the regulatory relationships among genes by leveraging the probabilistic nature of gene interactions. These networks are represented as directed acyclic graphs (DAGs), where nodes correspond to gene expression levels, and directed edges indicate potential regulatory influences. The linear Gaussian assumption simplifies the computational complexity of the model while maintaining a reasonable approximation of the underlying biological processes.

The key advantage of LGBNs lies in their ability to integrate multiple sources of data, including both genotypic and transcriptomic information, to enhance the accuracy of gene expression predictions. By incorporating cis-eQTLs (expression quantitative trait loci) and trans-eQTLs, LGBNs can capture both local and distal regulatory effects, thereby providing a more comprehensive view of the gene regulatory network. This integration is crucial for understanding the genetic basis of complex traits and diseases, as it allows for the identification of key regulatory genes and pathways that might not be apparent from gene expression data alone. The recursive nature of LGBNs, where the expression of a gene is predicted based on its own cis-eQTLs and the cis-eQTLs and/or predicted expression levels of its parent genes, further enhances the predictive power of these models.

Despite their strengths, LGBNs face several challenges, particularly in handling the high dimensionality and sparsity of gene expression data. To address these issues, recent advancements have focused on developing efficient algorithms that can scale to large datasets while maintaining computational feasibility. Techniques such as regularization and sparse learning have been employed to reduce overfitting and improve the interpretability of the inferred networks. Additionally, the integration of prior biological knowledge, such as known transcription factor binding sites and pathway information, can further refine the network structure and enhance the biological relevance of the predictions. Overall, LGBNs represent a robust and flexible approach to gene expression prediction, offering valuable insights into the complex regulatory mechanisms underlying cellular function.

## 4.2 Quantum Computing and Bayesian Networks

### 4.2.1 NO-BEARS Algorithm for Efficient BN Construction
The NO-BEARS algorithm represents a significant advancement in the construction of Bayesian Networks (BNs) from transcriptomic data, addressing the computational challenges inherent in the NO-TEARS framework [5]. Unlike traditional score-based methods that suffer from exponential computational complexity with an increase in the number of attributes, NO-BEARS leverages continuous optimization techniques to efficiently explore the space of possible network structures. By reformulating the problem, NO-BEARS introduces a new constraint that is theoretically equivalent to the acyclicity constraint used in NO-TEARS but can be evaluated in O(n^2) steps, significantly reducing the computational burden.

A key innovation of the NO-BEARS algorithm is the introduction of a novel regression loss function designed to handle the non-linearities often observed in gene expression data [5]. This loss function is tailored to capture the complex relationships between genes, which are not adequately represented by linear models. The combination of the efficient acyclicity constraint and the non-linear regression loss enables NO-BEARS to construct more accurate and biologically plausible BNs. This approach not only improves the computational efficiency but also enhances the predictive power of the inferred networks, making it particularly suitable for large-scale transcriptomic datasets.

To validate the effectiveness of the NO-BEARS algorithm, extensive experiments were conducted using both simulated and real datasets. The algorithm was tested on synthetic data from the DREAM5 Systems Genetics Challenge, as well as real data from eQTL mapping studies in yeast and human populations [9]. The results demonstrated that NO-BEARS outperforms existing methods in terms of both accuracy and computational efficiency. By integrating the principles of omnigenic inheritance, NO-BEARS is able to leverage both cis- and trans-eQTLs, leading to improved models for downstream applications such as transcriptome-wide association studies (TWAS). The robust performance of NO-BEARS across different datasets underscores its potential as a powerful tool for gene regulatory network inference.

### 4.2.2 Quantum Single-Cell Gene Regulatory Network Modeling
Quantum single-cell gene regulatory network (qscGRN) modeling leverages the principles of quantum computing to infer gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data [2]. In this approach, each gene is represented by a qubit, and the entire network is modeled using a parameterized quantum circuit [2]. The qscGRN model comprises two primary layers: the encoder layer and the regulation layer. The encoder layer translates the scRNA-seq data into a superposition state, effectively encoding the expression levels of genes into the quantum state of qubits. This transformation is crucial as it maps the binary expression values into a high-dimensional Hilbert space, allowing for a more nuanced representation of cellular states.

The regulation layers in the qscGRN model are responsible for entangling the qubits, thereby modeling the interactions between genes. By entangling qubits, the model captures the complex and non-linear relationships that exist within GRNs, which are often missed by traditional correlation-based inference methods. This quantum entanglement provides a powerful framework for identifying gene-gene interactions, as it can efficiently explore the vast combinatorial space of potential regulatory relationships. The parameterized quantum circuit is optimized to minimize a loss function that reflects the discrepancy between the predicted and observed gene expression patterns, ensuring that the model accurately recovers the structure of the GRN [2].

To enhance the performance of the qscGRN model, a new constraint is introduced that is theoretically equivalent to the one used in the NO-TEARS algorithm but can be evaluated more efficiently. This constraint, combined with a novel regression loss function designed to handle the non-linearities in gene expression data, forms the basis of the NO-BEARS algorithm [5]. The NO-BEARS algorithm is particularly well-suited for constructing Bayesian networks (BNs) from large-scale transcriptomic data, offering a principled optimization method that can efficiently infer the directed acyclic graph (DAG) structure of GRNs. The qscGRN model, therefore, represents a significant advancement in the field of gene regulatory network inference, providing a robust and scalable solution for understanding the complex regulatory mechanisms that govern gene expression in single cells.

### 4.2.3 GPU Acceleration for Large-Scale Transcriptomic Data
GPU acceleration has emerged as a critical technology for handling the computational demands of large-scale transcriptomic data, particularly in single-cell RNA sequencing (scRNA-seq) studies. The vast amount of data generated by scRNA-seq experiments necessitates powerful computational resources to process, analyze, and infer gene regulatory networks (GRNs) efficiently. GPUs, with their parallel processing capabilities, offer a significant advantage over traditional CPU-based systems by enabling the rapid execution of complex algorithms. This is particularly important for tasks such as matrix operations, which are common in the construction of GRNs and other network models. By leveraging the parallel architecture of GPUs, researchers can significantly reduce the computational time required for these tasks, thereby facilitating more extensive and detailed analyses of transcriptomic data.

One of the key applications of GPU acceleration in transcriptomics is the inference of GRNs from scRNA-seq data. GRN inference involves identifying regulatory relationships between genes based on their expression patterns across different cells [2]. This process is computationally intensive, especially when dealing with large datasets containing thousands or even millions of cells. GPU-accelerated algorithms can handle these large datasets more efficiently, allowing for the construction of more accurate and comprehensive GRNs. For instance, the NO-TEARS algorithm, which is used for the construction of Bayesian networks (BNs) from transcriptomic data, can be significantly accelerated using GPUs. This acceleration not only reduces the time required for network construction but also enhances the accuracy of the inferred networks by enabling the use of more sophisticated optimization methods.

Furthermore, GPU acceleration has also been applied to other aspects of transcriptomic data analysis, such as the identification of differentially expressed genes, clustering of cells into distinct populations, and the prediction of gene expression from genetic data. These tasks often involve large-scale matrix operations and machine learning models, which can be computationally expensive. By offloading these computations to GPUs, researchers can achieve substantial speedups, making it feasible to analyze large datasets in a reasonable amount of time. This has important implications for the broader field of systems biology, where the integration of multiple types of omic data is becoming increasingly common. GPU acceleration thus plays a crucial role in advancing our understanding of complex biological systems by enabling more efficient and accurate analysis of large-scale transcriptomic data.

# 5 Causal Inference from Observational and Interventional Data

## 5.1 Efficient Optimization Methods for GMRFs

### 5.1.1 Scalable and Computationally Efficient GMRF Inference
Scalable and computationally efficient inference of Gaussian Markov Random Fields (GMRFs) is crucial for handling large-scale datasets in various applications, including genomics and spatial statistics. Traditional maximum likelihood estimation (MLE) methods for GMRFs often suffer from computational inefficiency due to the need to invert large matrices, which scales cubically with the number of variables [11]. This complexity becomes prohibitive when dealing with high-dimensional data, such as those encountered in gene regulatory network analysis, where the number of genes can easily exceed tens of thousands. To overcome these limitations, recent research has focused on developing alternative optimization methods that maintain statistical accuracy while significantly reducing computational costs.

One such approach involves leveraging sparse approximation techniques to estimate the precision matrix, which encodes the conditional dependencies among variables in a GMRF. These methods, such as the graphical lasso, impose sparsity constraints on the precision matrix to reduce the number of parameters that need to be estimated. By doing so, they transform the problem into a convex optimization task that can be solved efficiently using algorithms like coordinate descent or proximal gradient methods. These algorithms not only scale better with the dimensionality of the data but also provide theoretical guarantees on the consistency and convergence of the estimates. Moreover, they can handle structured sparsity patterns, which are common in biological networks, thereby improving the interpretability of the inferred models.

Another key aspect of scalable GMRF inference is the development of parallel and distributed computing strategies. These strategies leverage the inherent parallelism in the optimization algorithms to distribute the computational load across multiple processors or machines. For instance, block-coordinate descent methods can be adapted to update blocks of variables in parallel, leading to substantial speedups. Additionally, distributed memory architectures can be utilized to store and process large datasets, further enhancing the scalability of the inference process. By combining these computational advancements with robust statistical methods, researchers can now infer complex GMRF models from massive datasets, enabling deeper insights into the underlying biological processes and facilitating the discovery of novel regulatory mechanisms.

### 5.1.2 Statistical Guarantees and High-Dimensional Settings
In the realm of high-dimensional settings, the statistical guarantees of methods designed to infer Directed Acyclic Graphs (DAGs) from observational data are of paramount importance. These guarantees ensure that the inferred models are not only computationally feasible but also statistically reliable, even when the number of variables \( p \) far exceeds the number of samples \( n \). Specifically, the non-asymptotic consistency of such methods is crucial, as it provides a theoretical foundation for the accurate recovery of the underlying graphical structure [11]. This consistency is particularly challenging in high-dimensional scenarios due to the curse of dimensionality, where traditional methods often fail to provide reliable estimates without strong assumptions.

To address these challenges, recent advances have focused on developing methods that can handle high-dimensional data while maintaining statistical consistency. One such approach involves the use of continuous optimization techniques, such as those employed in the NO TEARS framework, which introduces a continuous acyclicity constraint to avoid the combinatorial complexity associated with traditional DAG learning. Despite its computational advantages, NO TEARS and similar methods face limitations in terms of the sensitivity to data scaling, as the inferred topological order can be influenced by the marginal variances of the variables. This sensitivity highlights the need for robust statistical guarantees that can account for such variations and ensure that the inferred structures remain valid under different data transformations.

Moreover, in high-dimensional settings, the estimation of causal effects from observational data becomes increasingly complex. Methods that can provide bounds on the total causal effects within an equivalence class of DAGs are particularly valuable, as they offer a way to quantify uncertainty and make informed decisions about potential interventions [3]. The derivation of such bounds, however, requires careful consideration of the underlying statistical properties and the ability to handle the increased computational demands of high-dimensional problems. By providing rigorous statistical guarantees, these methods not only enhance the reliability of causal inference but also pave the way for more effective intervention designs in various applications, including genomics and econometrics.

### 5.1.3 Application to Gene Regulatory Networks
The application of causal inference methods to gene regulatory networks (GRNs) has emerged as a powerful approach to elucidate the complex interactions governing gene expression [3]. In this context, the primary challenge lies in the identifiability and scalability of learning Directed Acyclic Graphs (DAGs) from high-dimensional transcriptomic data. Traditional methods, such as graphical Gaussian models, often provide undirected graphs that fail to capture the causal relationships between genes [3]. However, the advent of high-throughput technologies, particularly Perturb-seq, has revolutionized this field by enabling the direct interrogation of causal regulatory relationships through highly parallel CRISPR interventions [12].

Perturb-seq leverages the precision of CRISPR gene editing to systematically perturb gene expression and measure the resulting changes in the transcriptome [12]. This approach has been instrumental in mapping gene regulatory networks at a genome-wide scale [4]. By linking high-dimensional transcriptomic readouts to specific gene perturbations, Perturb-seq facilitates the identification of direct regulatory interactions, which are essential for understanding the underlying biological processes. The effects of gene interventions, such as knock-outs or knock-downs, are particularly informative, as they can reveal the causal influence of one gene on another. For instance, if gene Y is regulated by gene X, the expression of gene Y will be altered when gene X is perturbed.

Incorporating both observational and intervention data further enhances the accuracy of causal network inference [3]. This hybrid approach allows for the construction of more robust and reliable GRNs by leveraging the strengths of both data types. For example, comparing gene expression values under wild-type conditions with those under intervention can provide strong evidence for regulatory relationships. This method has been successfully applied in various studies, including the DREAM4 challenge, where it demonstrated superior performance in network estimation. The ability to infer dynamic regulatory networks, especially in the context of spatially heterogeneous diseases like glioblastoma multiforme (GBM), opens new avenues for understanding the molecular mechanisms underlying complex biological processes and can lead to the identification of novel therapeutic targets [11].

## 5.2 Joint Estimation of Causal Effects

### 5.2.1 Integrating Observational and Interventional Data
Integrating observational and interventional data is crucial for enhancing the identifiability and accuracy of causal models, particularly in scenarios where observational data alone may lead to ambiguous or incorrect structural inferences. In the context of linear Structural Equation Models (SEMs), hard interventions play a pivotal role by effectively removing the influence of upstream variables on the intervened node, thereby allowing for the marginal estimation of error variances. This process is illustrated in Figure 1, where the removal of incoming edges to the target node under a hard intervention results in a reduction of the marginal variance, reflecting the elimination of upstream variance and the introduction of intervention-specific effects [12].

However, the naive incorporation of interventional data into existing frameworks, such as NO TEARS, which primarily rely on observational data, is insufficient for achieving consistent structural recovery [12]. This is because NO TEARS and similar methods often struggle with variance sensitivity, leading to potential misidentification of the true causal structure within a class of Markov equivalent DAGs [12]. To address this issue, a more sophisticated approach is required, one that leverages the marginal estimates of error variances obtained from interventional data. By correcting for variance sensitivity through these estimates, the integrated model can achieve a higher degree of identifiability and robustness.

In this section, we explore the theoretical foundations and practical implications of integrating observational and interventional data. We demonstrate how the marginal estimation of error variances, derived from hard interventions, can be used to refine the likelihood function and improve the maximum likelihood estimators. This integration not only enhances the accuracy of the estimated causal structure but also provides a more reliable basis for inferring causal effects. Furthermore, we discuss the analytical first-order derivatives and Fisher information, which are essential for the direct estimation of the graph structure and causal effects, even in complex intervention designs involving multiple simultaneous interventions [3].

### 5.2.2 Propensity Score Matching and Inverse Probability Weighting
Propensity Score Matching (PSM) and Inverse Probability Weighting (IPW) are two widely employed techniques in causal inference to address selection bias and confounding in observational studies. PSM involves matching units based on their propensity scores, which are the probabilities of being assigned to a treatment group given a set of observed covariates. This method aims to create a balanced sample where the distribution of observed covariates is similar across treatment and control groups, thereby reducing bias in the estimation of causal effects. IPW, on the other hand, assigns weights to each unit based on the inverse of the probability of receiving the treatment actually received, effectively creating a pseudo-population where the treatment assignment is independent of the observed covariates.

In the context of causal inference, PSM and IPW serve to mimic the conditions of a randomized controlled trial (RCT) by balancing the covariates across treatment groups. PSM is particularly useful when the number of covariates is manageable and the sample size is large enough to find suitable matches. However, it can be sensitive to the choice of matching algorithm and may lead to a loss of sample size if many units are unmatched. IPW, while more flexible and capable of handling a larger number of covariates, can suffer from high variance in the estimated weights, especially when the propensity scores are close to zero or one. This can lead to unstable estimates and increased sensitivity to model misspecification.

Both PSM and IPW have been extended and adapted to handle complex data structures and settings, such as time-varying treatments and high-dimensional covariates. For instance, in the presence of time-varying treatments, methods like marginal structural models (MSMs) can be used in conjunction with IPW to account for time-dependent confounding. Additionally, machine learning techniques have been integrated into PSM and IPW to improve the estimation of propensity scores and reduce model dependence. Despite these advancements, the choice between PSM and IPW often depends on the specific research question, the nature of the data, and the assumptions one is willing to make about the underlying causal relationships.

### 5.2.3 Structural Equation Modeling for Confounding Variables
Structural Equation Modeling (SEM) has been widely utilized in various fields to address confounding variables, which can distort the observed relationships between variables. In the context of causal inference, confounding variables are extraneous variables that correlate with both the independent and dependent variables, potentially leading to biased estimates of causal effects. To mitigate this issue, SEM incorporates latent variables that account for unobserved confounders, thereby providing a more accurate representation of the underlying causal structure. This approach is particularly valuable in scenarios where direct measurement of all potential confounders is impractical or impossible.

In the specific application of SEM to confounding variables, the model is extended to include both observed and latent variables, allowing for the estimation of direct and indirect effects while controlling for unmeasured confounders. The inclusion of latent variables in SEM enables the model to capture the complex interplay between observed and unobserved factors, thus improving the robustness of causal inferences. Moreover, the use of SEM in this context facilitates the identification of structural paths that might otherwise be obscured by the presence of confounding variables, enhancing the interpretability of the causal relationships within the system.

To effectively apply SEM for confounding variables, it is crucial to specify the model structure accurately, including the correct identification of latent variables and their relationships with observed variables. Techniques such as Bayesian methods and maximum likelihood estimation are commonly employed to estimate the parameters of the SEM, ensuring that the model fits the data well and that the estimated causal effects are reliable. Additionally, recent advancements in SEM have integrated machine learning algorithms to automate the process of model specification and parameter estimation, further enhancing the applicability of SEM in complex systems with numerous potential confounders.

# 6 Future Directions


The current state of research in the inference of gene regulatory networks (GRNs) from transcriptomics data has made significant strides, but several limitations and gaps remain. One of the primary challenges is the integration of multi-omics data, including genomics, epigenomics, and proteomics, into a cohesive framework. While methods for integrating these data types have advanced, the complexity and heterogeneity of the data often lead to issues in consistency and reliability. Additionally, the computational demands of handling large-scale, high-dimensional datasets continue to pose a significant barrier, particularly in the context of real-time or near-real-time analysis. Another limitation is the need for more robust and interpretable models that can handle the non-linear and dynamic nature of gene regulation, especially in the context of single-cell data. Current methods often struggle to accurately capture the temporal and spatial aspects of gene expression, which are crucial for understanding the regulatory mechanisms underlying cellular processes.

To address these limitations, several directions for future research are proposed. First, the development of advanced integrative and computational approaches is essential. This includes the creation of more sophisticated machine learning algorithms that can effectively integrate multi-omics data and handle the high dimensionality and sparsity of transcriptomic datasets. Techniques such as deep learning, particularly those that can model temporal and spatial dynamics, hold promise in this area. Additionally, the integration of quantum computing and GPU acceleration can significantly enhance the computational efficiency and scalability of GRN inference, enabling the analysis of larger and more complex datasets. The development of hybrid models that combine the strengths of different computational approaches, such as the integration of quantum computing with classical machine learning, could provide a more comprehensive solution.

Second, there is a need for more robust statistical and causal inference methods. While existing methods have made significant progress, they often rely on strong assumptions that may not hold in real-world scenarios. Developing methods that can handle non-linear relationships, account for confounding variables, and provide reliable causal inferences is crucial. This includes the use of advanced causal discovery algorithms that can leverage both observational and interventional data to infer causal relationships. Additionally, the development of methods that can handle the uncertainty and variability inherent in biological systems, such as Bayesian methods and probabilistic graphical models, can enhance the reliability and interpretability of the inferred networks.

The potential impact of the proposed future work is substantial. By addressing the current limitations and gaps, the proposed directions can lead to more accurate and biologically relevant GRNs, which are essential for understanding the complex regulatory mechanisms underlying cellular processes. These advancements can have far-reaching implications in various fields, including personalized medicine, drug discovery, and synthetic biology. For instance, more accurate GRNs can facilitate the identification of novel therapeutic targets and biomarkers for diseases such as cancer and cardiovascular disease. In personalized medicine, the ability to construct patient-specific GRNs can enable more precise and effective treatment strategies. Moreover, the integration of multi-omics data and the use of advanced computational techniques can accelerate the discovery of new biological insights, ultimately contributing to the advancement of systems biology and the broader field of molecular biology.

# 7 Conclusion



This survey paper has provided a comprehensive overview of the methodologies and advancements in the inference of gene regulatory networks (GRNs) from transcriptomics data. Key findings include the integration of multi-omics data, the application of advanced computational techniques such as machine learning and Bayesian networks, and the importance of policy and systemic aspects in regulatory frameworks. The paper highlights the use of network metrics for disease gene prioritization, the combination of evolutionary developmental biology (Evo-Devo) with mathematical modeling, and high-performing algorithms for GRN inference. Additionally, the integration of Transcriptome-Wide Association Studies (TWAS) and Genome-Wide Association Studies (GWAS) for constructing disease-specific gene regulatory networks, particularly in the context of cardiovascular genomics, has been discussed. The paper also explores the use of quantum computing and GPU acceleration to handle the computational demands of large-scale transcriptomic datasets, as well as the application of causal inference methods to observational and interventional data.

The significance of this survey lies in its comprehensive coverage of the current state of the field, providing a valuable resource for researchers and practitioners. By synthesizing recent advancements and highlighting key challenges, the paper offers insights into the complex and dynamic nature of gene regulatory networks. The integration of multi-omics data and advanced computational techniques has significantly enhanced our ability to construct accurate and biologically relevant GRNs, which are essential for understanding the regulatory mechanisms underlying cellular responses and disease processes. The paper also emphasizes the importance of ethical and systemic considerations in the use of GRN data and models, ensuring that the research is conducted responsibly and effectively.

In conclusion, the field of gene regulatory network inference from transcriptomics data is rapidly evolving, driven by technological advancements and interdisciplinary collaborations. This survey paper serves as a foundational resource, guiding future research and applications. Researchers are encouraged to continue exploring innovative methodologies and integrative approaches to further refine our understanding of gene regulatory networks. The ongoing development of computational tools and the integration of diverse data types will be crucial for advancing the field and translating these insights into practical applications in biotechnology and personalized medicine. We call upon the scientific community to collaborate and share data, fostering a collaborative environment that accelerates the pace of discovery and innovation.

# References
[1] Inferring interaction networks from transcriptomic data  methods and  applications  
[2] Quantum gene regulatory networks  
[3] Joint likelihood calculation for intervention and observational data  from a Gaussian Bayesian netwo  
[4] From time-series transcriptomics to gene regulatory networks  a review  on inference methods  
[5] Scaling structural learning with NO-BEARS to infer causal transcriptome  networks  
[6] Gene targeting in disease networks  
[7] A network-driven framework for enhancing gene-disease association  studies in coronary artery diseas  
[8] Multi-species network inference improves gene regulatory network  reconstruction for early embryonic  
[9] Predicting the genetic component of gene expression using gene  regulatory networks  
[10] Gene Regulatory Network Inference in the Presence of Dropouts  a Causal  View  
[11] Efficient Inference of Spatially-varying Gaussian Markov Random Fields  with Applications in Gene Re  
[12] dotears  Scalable, consistent DAG estimation using observational and  interventional data  