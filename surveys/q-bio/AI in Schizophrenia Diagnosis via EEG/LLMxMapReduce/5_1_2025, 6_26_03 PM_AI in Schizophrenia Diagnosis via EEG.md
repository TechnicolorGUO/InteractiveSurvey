# 5/1/2025, 6:26:03 PM_AI in Schizophrenia Diagnosis via EEG​  

# 0. AI in Schizophrenia Diagnosis via EEG  

# 1. Introduction  

Schizophrenia (SCZ) constitutes a chronic and severe mental illness that imposes a substantial burden on individuals, families, and society globally [8,10]. Affecting approximately 1 in 300 individuals, SCZ impacts cognitive, emotional, and behavioral aspects, necessitating accurate and timely diagnosis for effective management [10]. Current clinical diagnosis and treatment evaluation predominantly rely on subjective measures, such as clinicians' experience, patient self-reports, and questionnaire surveys [3,4,8,16,23].  

![](images/06ed6d531a1b847f549c7a8b1ef1ef55e1830df394d6e77e958e72c05f6f5e2b.jpg)  

These traditional methods are often prone to subjectivity, potential patient concealment or avoidance of questions, and lack objective biological indicators [3,4,8]. This underscores a critical need for objective and quantitative diagnostic approaches [7,8,23].  

Electroencephalography (EEG) is a non-invasive neuroimaging technique that measures the electrical activity of the brain. Its portability, relatively low cost, and high temporal resolution make it a valuable tool for investigating brain function and identifying neurophysiological markers associated with various neurological and psychiatric disorders [13,24]. Research has explored the potential of using EEG/ERP (Event-Related Potential) technology to identify abnormalities in ERP components, functional connectivity, or frequency band oscillations in individuals with SCZ [3,4,5,8,23]. These electrophysiological features represent potential objective biomarkers for the disorder [8,10,23].  

Despite the potential of EEG, traditional analysis methods can be challenging due to the complexity and volume of the data. Manual analysis is often time-consuming, labor-intensive, and susceptible to human error, hindering efficiency and potentially impacting diagnostic reliability [6]. Artificial Intelligence (AI), particularly Machine Learning (ML) and Deep Learning (DL), offers promising solutions to overcome these limitations [2,6]. AI algorithms can automate the analysis of complex EEG data, extract meaningful features, and build predictive models to differentiate between patients with SCZ and healthy controls [2,3,4,8]. By providing objective, data-driven insights, AI-enhanced EEG analysis can improve diagnostic accuracy, provide more reliable measures, and potentially serve as computer-aided diagnostic (CAD) systems to assist clinicians, reducing workload and diagnosis time [6,19]. Some studies have demonstrated the feasibility of combining EEG biomarkers with machine learning for SCZ diagnosis, achieving accuracies up to $7 9 \%$ using features like ERP component amplitudes [3,4]. The application of AI in medical diagnosis is also evident in other fields, such as the detection of dementia using multimodal data or the classification of cognitive states from EEG [13,19,35].  

The clinical significance of early and accurate diagnosis in schizophrenia cannot be overstated. Timely identification and intervention are crucial for optimizing treatment outcomes, mitigating the long-term effects of the illness, and improving the patient's quality of life [10,13]. Developing objective diagnostic tools, potentially leveraging AI and EEG, holds the promise of enabling earlier detection and more personalized treatment strategies [8,30].​  

This survey aims to provide a comprehensive overview of the application of AI techniques in the diagnosis of schizophrenia using EEG. We will delve into the methodologies, findings, challenges, and future directions in this rapidly evolving field. Specifically, the survey will cover essential aspects including EEG data acquisition and preprocessing techniques, various approaches for feature extraction from EEG signals, the application of different machine learning and deep learning models for classification and diagnosis, current clinical applications and their limitations, significant challenges hindering widespread adoption, and promising future directions for research and development.  

# 2. Background on Schizophrenia and EEG  

Schizophrenia (SZ) is a chronic and severe mental illness affecting millions globally, characterized by significant heterogeneity in its clinical presentation [2,8]. The symptoms associated with schizophrenia are typically categorized into positive domains, such as auditory delusions and hallucinations [16,30], negative domains, involving deficits in normal emotional and thought processes, and cognitive impairments [16,21]. Cognitive deficits, highly prevalent among patients, impact neurocognition (e.g., memory, attention, executive function) and social cognition (e.g., emotion recognition, theory of mind), profoundly affecting social functioning [21].  

Achieving accurate and timely diagnoses of schizophrenia presents significant challenges due to the disorder's heterogeneity and the overlap in symptoms with other mental health conditions [10,16]. The current standard diagnostic process relies predominantly on subjective clinical interviews, behavioral observations, and patient self-reports guided by criteria like DSM-5 or ICD [3,4,8,23,30]. This reliance on subjective assessments [3,4,23,30], coupled with the inherent variability in symptom presentation, critically lacks objective indicators [7,8,16,23] and can be susceptible to biases [3,4]. While structural and functional brain deviations are recognized [10], objective biological markers for schizophrenia diagnosis are needed to overcome the limitations of behavioral symptom-based assessments [7,8,16,23].  

Electroencephalography (EEG) is a foundational non-invasive neurophysiological technique that measures the electrical activity of the brain, offering high temporal resolution [23]. The electrical potentials recorded by EEG primarily arise from the synchronized firing of large neuronal populations in the cerebral cortex [23]. EEG signal acquisition involves placing electrodes on the scalp, with proper techniques such as maintaining low electrode impedance and using suitable sampling rates being crucial for signal quality [5]. EEG boasts several advantages, including its non-invasive nature, high temporal resolution, and relatively low cost, making it a promising tool in personalized medicine for mental disorders [13,23].  

Traditional methods for analyzing EEG data involve visual inspection and quantitative approaches like quantitative EEG (qEEG) and Event-Related Potentials (ERP) analysis [23]. These methods employ signal processing techniques such as Fourier transform to quantify features like frequency band power, amplitude, and ERP components [23]. In the context of schizophrenia, traditional EEG analysis has revealed abnormalities in oscillatory activity across various frequency bands and alterations in ERP components like Mismatch Negativity (MMN) and P300 [3,4,23]. However, traditional EEG interpretation in schizophrenia diagnosis faces limitations. These methods often rely on predefined features and linear relationships, struggling to capture the full complexity and high dimensionality of EEG signals in conditions like schizophrenia. The subjective variability inherent in visual inspection and the difficulty in detecting subtle, non-linear patterns within the complex EEG signals of heterogeneous disorders like schizophrenia highlight the need for more sophisticated analytical approaches [23].​  

# 2.1 Schizophrenia: Symptoms, Diagnosis, and Challenges  

Schizophrenia (SZ) is a chronic and severe mental illness posing a significant burden on individuals, families, and society [8]. Globally, it affects approximately $0 . 7 \%$ of the population [21]. The clinical presentation of schizophrenia is heterogeneous and typically characterized by a spectrum of symptoms categorized into positive, negative, and cognitive domains [16,21]. Positive symptoms include experiences such as auditory delusions and hallucinations [16,30]. Negative symptoms involve deficits in normal emotional responses or other thought processes. Cognitive impairments are highly prevalent, affecting a substantial percentage of patients, ranging from $7 3 \%$ to $9 8 \%$ [21]. These disturbances span both neurocognition, encompassing aspects like memory, attention, and executive function, and social cognition, involving abilities such as emotion recognition, theory of mind, social perception, and attributional bias [21]. These social cognitive deficits notably impair social functioning, impacting interpersonal relationships, education, and employment [21].  

The standard diagnostic process for schizophrenia relies predominantly on clinical interviews, behavioral observations, and patient self-reports, typically guided by standardized diagnostic criteria such as those outlined in the DSM-5 or ICD [3,4,8,23,30].  

<html><body><table><tr><td>Aspect</td><td>Description</td></tr><tr><td>Key Symptom Domains</td><td>Positive (Delusions, Hallucinations), Negative (Emotional/Thought Deficits), Cognitive (Memory, Attention, Social</td></tr><tr><td></td><td>Cognition Impairments)</td></tr></table></body></html>  

<html><body><table><tr><td>Current Diagnosis</td><td>Primarily based on subjective clinical interviews,behavioral observations,patient reports (DSM-5/ICD criteria).</td></tr><tr><td>Challenges</td><td>Subjectivity, Overlap with other disorders, Symptom Variability, Lack of Objective Biomarkers, Potential for Stigma.</td></tr></table></body></html>  

However, this approach is inherently subjective [3,4,23,30]. Current clinical diagnosis and treatment evaluation are largely based on the clinician's experience and the patient's subjective statements, critically lacking objective indicators [8]. Traditional diagnostic methods, often relying on questionnaires, are susceptible to potential biases in patient responses [3,4].​  

Diagnosing schizophrenia presents significant challenges. One major difficulty stems from the considerable overlap in symptoms with other mental disorders, complicating the precise and timely identification of SZ [10,16]. Furthermore, the variability in symptom presentation across individuals and over time adds to diagnostic complexity. A critical limitation of current diagnostic practices is the absence of objective biomarkers [7,8,16,23]. While structural and functional deviations within the brain, such as diminishing brain tissue and abnormal neural connections observed in MRI scans, are considered significant factors [10], these changes are often subtle and non-specific to schizophrenia, making them easily overlooked by human experts [10]. The reliance solely on behavioral symptoms and subjective assessments is limited [23] and can also contribute to the negative "污名化" or "标签化" effect on patients [23]. There is a clear need for objective measures to improve diagnostic accuracy and overcome the inherent difficulties in studying the complex and diverse pathological mechanisms underlying the disease [7,23].  

The etiology of schizophrenia is understood to involve a complex interplay of genetic predisposition and environmental influences [16]. Structural and functional deviations within the brain are considered significant contributing factors [10], reflecting the underlying biological underpinnings of the disorder.  

# 2.2 EEG: Principles, Acquisition, and Traditional Analysis  

Electroencephalography (EEG) is a foundational non-invasive neurophysiological technique employed to measure the electrical activity of the brain with high temporal resolution [23]. The electrical potentials detected by EEG originate primarily from the synchronized firing of large populations of neurons in the cerebral cortex [23]. The analysis of these electrical signals, which constitute EEG signal processing, aims to extract information useful for various decision-making processes, including clinical diagnosis [24,36].​  

![](images/10afb2f23b735cf87081be075910fb4d196d0b756d588d1ede2ed0cd4c921a25.jpg)  

The process of EEG signal acquisition involves placing electrodes on the scalp according to standardized systems, although specific systems like the 10-20 system are not detailed in the provided digests. However, one study explicitly utilized a 64- channel EEG system for data collection, highlighting the potential for dense electrode arrays [5]. Signal acquisition typically involves recording brain activity under specific conditions, such as the resting state with eyes closed, over a duration of several minutes (e.g., 5-10 minutes) [5,8]. Maintaining low electrode impedance (e.g., below 5KΩ) and employing a suitable sampling rate (e.g., 1000Hz) are critical steps to ensure signal quality [5].  

Traditional methods for analyzing EEG data encompass both visual inspection of raw waveforms and quantitative approaches [23]. Quantitative EEG (qEEG) and Event-Related Potential (ERP) analysis are prominent techniques in this domain, involving signal processing methods such as averaging and Fourier transform [23]. These methods allow for the quantification of various EEG features, including frequency band power, amplitude, and latency/amplitude of ERP components. For instance, traditional analysis correlates specific EEG features like frontal asymmetry, resting-state peak frequency, and sensory ERP response time with cognitive assessments [13]. Theta power, in particular, has been explored as a potential indicator for early cognitive decline detection [13]. While traditional EEG analysis provides valuable insights and serves as a non-invasive, relatively inexpensive screening tool [13], it often relies on predefined features and linear relationships. The inherent complexity and high dimensionality of EEG signals, particularly in neurological and psychiatric conditions like schizophrenia, can pose challenges for traditional analysis methods, necessitating more sophisticated approaches for accurate diagnosis and understanding.  

# 2.3 EEG Characteristics and Biomarkers in Schizophrenia  

<html><body><table><tr><td>EEG Domain</td><td>Characteristics/Featu res in SCZ</td><td>Examples</td><td>Potential Role</td></tr><tr><td>Frequency Band</td><td>Abnormal oscillatory power/activity, altered power spectra</td><td>Reduced Delta/Theta, Increased Betal/Beta2/Gamma</td><td>Reflectsaltered neuronal synchrony/excitation /inhibition</td></tr><tr><td>Connectivity</td><td>Abnormal functional connectivity patterns,differences in brain network topology</td><td>Increased Gamma band connectivity in specific networks</td><td>Reflects disrupted neural communication pathways</td></tr><tr><td>Event-Related Potentials (ERPs)</td><td>Characteristic abnormalities in component amplitude/latency</td><td>Reduced MMN amplitude, Abnormal P300</td><td>Reflects deficits in sensory processing/cognitive function</td></tr><tr><td>Other</td><td>Features from Time- Frequency (Wavelets), Statistical, Non- linear analysis</td><td>DE,Lempel-Ziv complexity</td><td>Captures transient dynamics, complexity changes</td></tr></table></body></html>  

Schizophrenia is characterized by profound neurophysiological alterations that can be probed using electroencephalography (EEG). Traditional EEG methods, such as quantitative EEG (qEEG) and event-related potentials (ERPs), have long been employed to identify potential biomarkers of the disorder [23]. EEG studies reveal abnormalities in oscillatory activity across various frequency bands, functional connectivity patterns, and ERP components in patients with schizophrenia compared to healthy controls [3,4].​  

Specific frequency band abnormalities observed in schizophrenia patients include reduced power or weaker activity in the lower frequency bands, such as delta and theta [23]. Conversely, studies utilizing power spectral analysis have demonstrated significantly increased power spectra in the high-frequency bands, specifically beta1 $( 1 3 - 2 0 H z )$ , beta2 (20–30 Hz), and gamma $( 3 0 - 5 0 H z )$ , predominantly in the frontal, temporal, and parietal lobes [8]. Abnormalities in oscillatory activity across certain frequency bands are also noted during task performance [3,4].  

Beyond power spectral analysis, investigations into functional connectivity and brain network topology have unveiled crucial differences in schizophrenia. Abnormalities in functional connectivity are observed in patients [3,4]. For instance, a study focusing on first-episode, treatment-naïve patients found increased gamma band $( 3 0 - 5 0 H z )$ functional connectivity during the resting state within a specific subnetwork involving regions such as the left inferior frontal/orbital frontal, lateral and medial temporal lobes, and inferior parietal regions [5]. This study employed techniques like power envelope correlation between orthogonalized signals to mitigate the impact of volume conduction [5]. Furthermore, brain network topology, particularly in the gamma band, exhibits significant differences in frontal, fronto-parietal, and other regions in schizophrenia patients compared to healthy controls [8].  

Event-related potentials (ERPs), which reflect the brain's response to specific stimuli, are critical for diagnosing various neurological disorders, including schizophrenia [6]. Schizophrenia patients exhibit characteristic abnormalities in ERP components [3]. Meta-analyses have consistently reported a reduced amplitude of the Mismatch Negativity (MMN), an automatic response to auditory deviations, in individuals with schizophrenia [23]. The P300 component, related to cognitive processing and attention, is also frequently reported as abnormal or relevant for diagnosis, highlighted as a potentially significant biomarker across mental disorders [3,4,23].  

Comparing findings across different experimental paradigms reveals distinct patterns. Resting-state studies, such as the analysis of gamma band functional connectivity, identify inherent network alterations [5]. In contrast, task-related EEG and ERP studies are particularly useful for probing cognitive deficits, revealing abnormalities in oscillatory activity, functional connectivity, and ERP components specifically elicited by external stimuli or cognitive demands [3,4]. These task-related deficits, such as MMN reduction or P300 abnormalities, are considered neurophysiological correlates of cognitive and perceptual symptoms often observed in the disorder [23].  

The utility of different EEG measures for differentiating schizophrenia patients from healthy controls varies. Power spectral analysis effectively highlights group differences in frequency band power, particularly in the high-frequency range [8]. Measures of brain network topology and functional connectivity capture alterations in neural communication patterns, with gamma band connectivity and network structure showing significant differences [5,8]. ERPs, especially MMN and P300, serve as robust markers of sensory processing and cognitive function deficits [6,23]. Various feature extraction techniques, such as the Discrete Wavelet Transform (DWT), can be applied to these EEG signals to derive quantitative features for differentiation [6].​  

The observed EEG abnormalities are believed to stem from underlying brain dysfunctions affecting neural circuits and their synchrony. For instance, altered gamma band activity and connectivity may reflect deficits in inhibitory neurotransmission or impaired coordination of neuronal ensembles, potentially linked to cognitive fragmentation and perceptual disturbances. The differential findings across frequency bands and brain regions (e.g., frontal, temporal, parietal lobes, specific subcortical/cortical networks) point towards distributed network pathology rather than localized deficits [5,8].  

However, relying on a single EEG biomarker for schizophrenia diagnosis presents significant limitations due to the heterogeneity and complexity of the disorder. Each measure captures only a specific aspect of the neurophysiological dysfunction. For example, while P300 is a relevant feature [3,4], it alone may not fully characterize the multifaceted nature of schizophrenia. Therefore, there is a strong motivation and necessity for adopting multi-feature approaches that integrate information from power spectra, connectivity, network topology, and various ERP components to achieve a more comprehensive and accurate neurophysiological profile of schizophrenia, ultimately enhancing diagnostic accuracy [6].  

# 3. EEG Data Acquisition and Preprocessing  

The application of artificial intelligence to schizophrenia diagnosis using electroencephalogram (EEG) signals critically depends on the quality and reliability of the acquired data, necessitating rigorous acquisition procedures and subsequent preprocessing [36]. Standard EEG data acquisition in schizophrenia research employs multi-channel systems, such as a 62- lead system utilized in studies focusing on event-related potentials during auditory tasks like the Oddball paradigm [3,4]. Alternatively, simplified setups, such as single-channel acquisition with three electrodes placed at prefrontal regions (Fp1, Fp2) with a reference at Fpz, have also been explored, particularly for assessing specific cognitive correlates [13]. Acquisition parameters vary across studies, including sampling frequencies ranging from $5 0 0 \mathsf { H z }$ [13] to $1 0 0 0 { \mathsf { H z } }$ [3,4], and initial bandpass filtering applied during recording, such as $0 . 1 \mathrm { - } 1 0 0 \mathsf { H z }$ , often accompanied by the removal of power line interference (e.g., 60 Hz) [3,4]. Studies may also employ different paradigms, such as resting-state EEG acquisition [5], compared to task-based approaches. The choice of acquisition parameters and experimental conditions directly influences the characteristics of the recorded signal and can impact the effectiveness of subsequent AI analyses.​  

EEG data is highly susceptible to various forms of contamination, commonly referred to as artifacts. These artifacts originate from physiological sources other than brain activity, such as eye movements and blinks (EOG artifacts), muscle activity (EMG artifacts), and cardiac activity, as well as non-physiological sources like power line interference, electrode impedance fluctuations, cable movements, and environmental noise [9]. Artifacts caused by initial electrode cap adjustments may also be present at the beginning of recordings [1]. These contaminants can obscure the underlying neural signals relevant to schizophrenia pathology, leading to inaccurate feature extraction and potentially compromising the diagnostic accuracy of AI models [9]. Technical issues, such as delays between stimuli presentation and ERP recording, can also alter data interpretation [6].  

![](images/1c968c18e72fc5d9da102d8b541c62c9223db4b160633f9c51354acb601671c1.jpg)  

Effective preprocessing is therefore essential to mitigate the impact of artifacts and prepare EEG data for reliable analysis. Various techniques are employed for artifact removal and signal enhancement. Filtering is a fundamental step, involving the application of bandpass filters (e.g., $1 { - } 3 0 \mathsf { H z }$ [4], $0 . 1 \mathrm { - } 7 0 \mathsf { H z }$ [5], $0 . 5 { - } 4 9 . 5 \mathsf { H z }$ [9]), high-pass filters (e.g., 0.3 Hz or $1 0 H z$ for EMG), low-pass filters (e.g., $3 0 H z$ or $7 5 \mathsf { H z }$ for EMG), and notch filters to remove specific frequency components like power line noise [4,5,9]. Artifact removal techniques include visual inspection, which is commonly used to identify and reject data segments or epochs contaminated by large artifacts [4,5]. Automated methods like Independent Component Analysis (ICA) are effective for separating and removing components related to eye blinks, eye movements, and muscle activity [5]. Software tools such as Scan 4.3 have been used specifically for blink artifact removal [3,4]. Amplitude thresholding (e.g., rejecting epochs exceeding $\pm 7 5 \mu \nu$ or $\pm 8 0 0 \mu \nu ,$ ) is another method for rejecting corrupted segments [1,4]. Other steps include re-referencing the data, such as to an average reference [5], segmenting continuous data into epochs aligned to stimuli for ERP analysis [3,4], or into shorter epochs for resting-state analysis [5]. Downsampling the data (e.g., to $2 5 6 \mathsf { H z }$ or ${ 1 0 0 } \ H z )$ ) is often performed to reduce computational load [1,5]. Time–frequency analysis techniques like Fast Fourier Transform (FFT) or wavelet analysis are also applied to process the cleaned signals and extract spectral features [13]. The importance of these preprocessing steps lies in their ability to enhance the signal-to-noise ratio and isolate neural activity, thereby improving the quality and reliability of data features used for training and testing AI models. Beyond signal processing, general data preparation steps may also involve handling missing values or reformatting data [28].  

A significant challenge in EEG analysis is inter-subject variability, where differences in skull conductivity, electrode placement, and individual brain anatomy and activity patterns can lead to substantial signal variations across participants. While the provided digests do not detail specific normalization or standardization techniques applied to address this challenge in the context of schizophrenia diagnosis, such methods are generally crucial in multi-subject studies to ensure that group-level analyses and machine learning models are not biased by these differences and can generalize effectively.  

# 4. Feature Extraction and Selection  

Effective feature extraction and selection are paramount for developing high-performing and interpretable AI/ML models for the analysis of complex EEG signals in the context of schizophrenia diagnosis [2,9,16,30,34]. These processes transform raw EEG data into quantitative representations that highlight relevant neural patterns while mitigating the challenges posed by high dimensionality, noise, and inter-subject variability inherent in neurophysiological signals [9,23].​  

Feature extraction involves deriving quantitative measures from EEG signals, which can be broadly categorized by the domain of analysis: time, frequency, time-frequency, connectivity, statistical, and non-linear properties. Frequency-domain features are widely used, often extracted via power spectrum analysis using techniques like Fast Fourier Transform (FFT) or Discrete Fourier Transform (DFT) [1,23]. These methods quantify signal power distribution across standard frequency bands: delta $( \delta , 0 . 5  – 4 mathsf { H z } )$ , theta $( \theta , 4 \mathrm { - } 8 \mathsf { H z } )$ , alpha $( \alpha , 8 – 1 3 \mathsf { H z } )$ , beta $( \beta , 1 3 - 3 0 \mathsf { H z } )$ , and gamma (​γ , ${ \tt > } 3 0 \mathsf { H z } )$ [8,13,28]. Derived features include absolute power, relative power, and band ratios [9]. Specific frequency bands, such as beta1, beta2, and gamma, have shown particular relevance in schizophrenia research [5,8].​  

Time-domain features capture signal characteristics directly from the time series. A significant type is Event-Related Potentials (ERPs), such as the P300 component, characterized by amplitude and latency, obtained by averaging EEG segments time-locked to stimuli [3,4,23]. Basic statistical measures like mean, standard deviation, and variance are also simple time-domain features [9]. More complex temporal patterns can be captured by advanced techniques or pre-trained models [37].​  

Time-frequency analysis, using techniques like Continuous Wavelet Transform (CWT) and Discrete Wavelet Transform (DWT) [1], allows for the extraction of features that describe how spectral content evolves over time, capturing transient dynamics [6,13]. Differential entropy (DE), derived from power spectrum features, is another example used in this context [17].  

Connectivity features quantify the interactions between brain regions or channels, reflecting functional or effective networks. These include measures of brain network topology and attributes [8]. Functional connectivity can be estimated using methods like correlation or coherence, or through techniques like power envelope correlation for specific bands such as gamma [5], or Hilbert transform-based inter-electrode connectivity [1]. Statistical features like skewness, kurtosis, and entropy, alongside non-linear features such as Lempel-Ziv complexity and fractal dimension, quantify signal distribution and complexity, potentially revealing subtle differences not captured by linear analyses [9].​  

Furthermore, features can be extracted from different spatial representations of EEG data. Sensor-level features are derived directly from electrode recordings [3,4], while source-level features are estimated from activity in underlying brain regions using techniques like eLORETA to obtain time series or connectivity measures from dipole or ROI locations [3,4,5]. Sourcelevel features may offer a more direct insight into regional brain activity and connectivity [3,4]. The relevance of these diverse features stems from the complex pathophysiology of schizophrenia, which manifests as alterations in cognitive processing (linked to ERPs like P300) and abnormal neuronal oscillations and network coordination (reflected in frequency band power and connectivity changes) [3,4,5,8]. The variety of features allows capturing different facets of the disorder.  

Following feature extraction, feature selection and dimensionality reduction are crucial steps to manage the high dimensionality of EEG data, which can improve model performance, reduce computational burden, and enhance model interpretability [16,30,34]. Dimensionality reduction techniques transform the feature space into a lower dimension, for instance, using Principle Component Analysis (PCA) or Linear Discriminant Analysis (LDA) [13,16,30]. While PCA can reduce dimensions, it may sometimes decrease decoding accuracy, as observed in one study [1].  

Feature selection methods, conversely, select a subset of the most relevant original features. Filter methods, independent of the classifier, include F-score/ANOVA [3,4,30], Correlation Matrix analysis [35], ReliefF, Fisher Score, and Chi-Squared tests [9,16]. Network-Based Statistic (NBS) can also serve as a filter for connectivity features [5]. Wrapper methods use a classifier to evaluate feature subsets, such as Recursive Feature Elimination (RFE) [30] or algorithms like Genetic Algorithms (GA) and Ant Colony Optimization (ACO) [16]. Embedded methods perform feature selection during model training, with LASSO regression being a prominent example using L1 regularization to shrink less important feature coefficients to zero [12,22,38]. Some algorithms, like Area Under the Curve Random Forests (AUCRF), Boruta, Elastic Net regression, and Bayesian Additive Regression Trees (BART), incorporate variable selection intrinsically [12].  

The selection process significantly impacts model performance and interpretability. Dimensionality reduction, while reducing data size, can make features less interpretable. Feature selection, by retaining original features, often improves interpretability by highlighting specific neurophysiological markers relevant to the condition. Tools like Explainable AI (XAI) further aid in understanding model decisions and feature relevance [10]. The comparative effectiveness of different feature selection and dimensionality reduction methods is often context- and data-dependent, necessitating empirical evaluation and optimization [1,22,30]. Some studies have explored combining features extracted from both sensor-level and sourcelevel data to potentially leverage complementary information and achieve higher diagnostic accuracy [3,4]. For instance, P300 amplitudes/latencies from electrodes were combined with averaged dipole time series from source space in one study [3,4]. This combined approach, coupled with appropriate feature selection (e.g., F-score), represents an effort to build more robust diagnostic models.​  

# 4.1 Types of EEG Features  

Electroencephalography (EEG) signals are complex time series that require sophisticated feature extraction techniques to reveal underlying brain activity patterns relevant to neurological and psychiatric conditions like schizophrenia. Feature extraction transforms raw EEG data into a set of quantitative measures that capture salient aspects of the signals, making them suitable for subsequent analysis and classification by machine learning models.  

<html><body><table><tr><td>Feature Category</td><td>Description</td><td>Examples</td></tr><tr><td>Frequency Domain</td><td>Power distribution across frequency bands</td><td>Absolute/Relative Power (Delta,Theta,Alpha, Beta, Gamma), Band Ratios</td></tr><tr><td>Time Domain</td><td>Signal amplitude/waveform characteristics directly from time series</td><td>ERP components (Amplitude, Latency - e.g., P300), Mean, Std Dev, Variance</td></tr><tr><td>Time-Frequency</td><td>How spectral content evolves over time</td><td>Wavelet Coefficients (DWT, CWT), Differential Entropy (DE)</td></tr><tr><td>Connectivity</td><td>Interactions between brain regions/channels (functional/effective)</td><td>Correlation, Coherence, Power Envelope Corr., Brain Network Topology</td></tr></table></body></html>  

<html><body><table><tr><td>Statistical</td><td>Measures summarizing data distribution</td><td>Skewness, Kurtosis, Entropy</td></tr><tr><td>Non-linear</td><td>Captures complexity and non-stationarity</td><td>Lempel-Ziv Complexity, Fractal Dimension</td></tr><tr><td>Spatial Domain</td><td>Features derived from different spatial representations</td><td>Sensor-level, Source-level</td></tr></table></body></html>  

Various types of features can be derived from EEG signals, broadly categorized based on the domain of analysis: time, frequency, time‐frequency, connectivity, statistical, and non‐linear properties. Furthermore, features can be extracted from different spatial levels, including sensor space (electrode level) and source space [4].  

Frequency‐domain features are among the most commonly utilized in EEG analysis for psychiatric disorders [23,28]. Power spectrum analysis, often performed using methods like the Fast Fourier Transform (FFT) or Discrete Fourier Transform (DFT) [1,23], quantifies the distribution of signal power across different frequency bands. Standard frequency bands analyzed include delta ( $\delta$ , typically $0 . 5 { - } 4 \ H z )$ ), theta $( \theta , 4 \mathrm { - } 8 \mathsf { H z } )$ , alpha $( \alpha , 8 – 1 3 \mathsf { H z } )$ , beta $( \beta , 1 3 - 3 0 \mathsf { H z } )$ , and gamma $( \gamma , > 3 0 \ H z )$ [8,13,28]. Features derived from these bands include absolute power, relative power (power in a band relative to total power), and band ratios [9]. Studies investigating schizophrenia have specifically focused on power spectrum features in bands such as beta1, beta2, and gamma [8], and gamma band power in resting state [5].  

Time–frequency analysis techniques like Continuous Wavelet Transform (CWT) and Discrete Wavelet Transform (DWT) provide insights into how spectral content changes over time [1], allowing for the extraction of features that capture transient spectral dynamics [13]. Differential entropy (DE), derived from power spectrum features in specific frequency bands, has also been employed [17].​  

Time–domain features capture characteristics of the EEG signal’s amplitude and waveform shape directly from the raw or processed time series data. Event-Related Potentials (ERPs) are a significant category of time–domain features, obtained by averaging EEG segments time-locked to specific stimuli or events [23]. ERP components, such as the P300 (a positive deflection occurring around 300 ms after a stimulus), are characterized by their amplitude and latency [3,4]. These features, particularly P300 amplitude and latency extracted from sensor-level electrodes, have been utilized in studies aiming to diagnose schizophrenia [3,4]. Basic time–domain statistical features like mean, standard deviation, and variance can also be extracted [9]. More complex temporal patterns can be captured by pre-trained models designed to extract fine-grained temporal information from EEG signals [37].​  

Connectivity features quantify the functional or effective interactions between different brain regions or EEG channels. These are often derived from analyzing the relationships between signals recorded at different electrodes or source locations. Brain network features, including network topology and attributes, are examples of connectivity features used in schizophrenia classification studies [8]. Functional connectivity can be estimated using various methods, such as correlation or coherence between time series from different channels or sources. One study specifically focused on gamma band functional connectivity estimated via power envelope correlation in first-episode schizophrenia patients, highlighting potential connectivity abnormalities in this frequency range [5]. Hilbert transform–based inter-electrode connectivity features are another example of how signal processing techniques are applied to derive connectivity measures [1].​  

Statistical features provide measures summarizing the distribution of EEG data, while non-linear features capture the complexity and non-stationarity inherent in brain activity. Examples of statistical features include skewness and kurtosis, describing the shape of the data distribution, and entropy, measuring signal unpredictability [9]. Non-linear features like Lempel–Ziv complexity and fractal dimension quantify signal complexity and structure across scales [9]. These features can potentially capture subtle differences in brain dynamics associated with schizophrenia that are not evident from linear time or frequency analysis alone.​  

Beyond traditional signal processing approaches, some studies explore novel, data-driven features. Machine learning–based features, such as those derived from wavelet-packet analysis like VC9, ST4, and A0, have been shown to correlate with cognitive task performance [13], suggesting their potential relevance in capturing cognitive deficits often associated with schizophrenia. Features can also be extracted not only from the sensor level (electrode space) but also from the source level (source space), which involves estimating the neural activity in underlying brain regions [3,4]. Source-level features, such as averaged dipole time series, provide a potentially more direct measure of regional brain activity and connectivity [3,4].  

The relevance of these diverse EEG features in distinguishing schizophrenia patients from healthy controls stems from the fact that schizophrenia is associated with widespread alterations in brain function and structure, which are reflected in abnormal brain electrical activity. For example, deficits in cognitive processing, such as those measured by the P300 ERP component, are well-documented in schizophrenia and provide a link between this feature and the disorder’s pathophysiology related to attentional and working memory dysfunction [3,4]. Changes in power in specific frequency bands, particularly increased gamma band activity or altered connectivity, have been hypothesized to reflect disruptions in neuronal oscillations and network coordination involved in cognitive integration and sensory processing, potentially linking these frequency and connectivity features to the positive and negative symptoms of schizophrenia [5,8]. The variety of features extracted reflects the multifaceted nature of schizophrenia pathophysiology, suggesting that a combination of features across different domains and spatial levels is likely necessary for comprehensive characterization and accurate diagnosis.  

# 4.2 Feature Selection and Dimensionality Reduction Methods  

In the application of machine learning to EEG-based schizophrenia diagnosis, the selection of relevant features and reduction of dimensionality are critical steps to improve model performance, reduce computational complexity, and enhance interpretability [16,30]. A variety of methods are employed for this purpose, broadly categorized into dimensionality reduction techniques and feature selection methods.  

Dimensionality reduction techniques transform the original feature space into a lower-dimensional space. Principle Component Analysis (PCA) is a common unsupervised method that projects data onto principal components, which capture the largest variance in the data [13,16,30]. PCA has been used for feature dimensionality reduction before classification in some studies [13]. However, the application of PCA is not universally beneficial; one study found that PCA led to a decrease in decoding accuracy, consequently deciding not to use it in the final analysis [1]. Linear Discriminant Analysis (LDA), while often used for classification, can also function as a dimensionality reduction technique by finding linear combinations of features that characterize or separate two or more classes [13]. Some EEG features have been specifically calculated using the LDA technique, suggesting its use in feature extraction or transformation [13].​  

Feature selection methods, conversely, aim to identify and select a subset of the most relevant features from the original set. These methods can be broadly classified into filter, wrapper, and embedded methods. Filter methods select features based on statistical measures independent of the chosen learning algorithm. The F-score is a filter method used for feature selection, often employed to choose a specific number of features from combined sensor and source-level data [3,4]. Analysis of Variance (ANOVA), which selects features based on F-scores, has also been applied [30]. Other filter methods mentioned in the context of related biomedical applications include Correlation Matrix analysis, which selects features with a correlation above a certain threshold with the target variable [35], ReliefF (ReF), a supervised algorithm that weights features based on their ability to distinguish between instances from different classes [9], Fisher Score, and Chi-Squared tests [16]. Network-Based Statistic (NBS) can also be viewed as a feature selection technique in the context of functional connectivity analysis, identifying subnetworks with significant differences between groups [5]. Furthermore, meta-analysis has been proposed as a method to prioritize consistent findings across multiple studies as potential biomarkers, effectively acting as a form of feature selection at a higher level of evidence [23].​  

Wrapper methods use a specific machine learning algorithm to evaluate the quality of feature subsets. Recursive Feature Elimination (RFE) is an example that iteratively removes the least important features based on classifier weights [30]. Other wrapper methods like Genetic Algorithms (GA) and Ant Colony Optimization (ACO) are also relevant in the broader context of AI in medical diagnosis [16].​  

Embedded methods incorporate feature selection within the model training process. LASSO regression (Least Absolute Shrinkage and Selection Operator) is a prominent embedded method that performs L1 regularization, forcing the coefficients of less important features towards zero and effectively selecting a sparse set of features [12,22,38]. Studies using LASSO have selected features with statistical significance (e.g., $P \leq 0 . 0 5$ ) [38] and employed cross-validation to optimize parameters, often including checks for collinearity among selected variables using metrics like the Variance Inflation Factor (VIF) [22]. Some machine learning algorithms, such as Area Under the Curve Random Forests (AUCRF), Boruta, Elastic Net regression, and Bayesian Additive Regression Trees (BART), inherently perform variable selection during model building [12].  

The choice of feature selection or dimensionality reduction method significantly impacts model performance and interpretability. While dimensionality reduction can condense information, it may obscure the original meaning of features, potentially reducing interpretability and, as observed with PCA, even decreasing accuracy [1]. Feature selection, by identifying a subset of original features, can improve interpretability by highlighting the specific neurophysiological markers most relevant to the diagnosis. The interpretability is further enhanced by techniques like Explainable AI (XAI), which aids in the selection of relevant features and makes the model's decision-making process transparent, which is crucial in highstakes medical applications [10].​  

Comparing the effectiveness of different methods requires empirical evaluation as their performance can be datadependent. One study applied PCA, ANOVA, and RFE for dimensionality reduction [30], suggesting a comparative approach, although direct performance comparisons across these specific methods for schizophrenia diagnosis were not detailed in the digest. The contrasting finding regarding PCA's impact on accuracy [1] underscores that no single method is universally superior, and optimization is essential. While some studies proceed without explicit feature selection or dimensionality reduction, using all extracted features [28], this approach risks including noisy or irrelevant features that could degrade model performance. Therefore, optimizing the feature selection process, whether through careful method choice, parameter tuning (e.g., optimizing LASSO's lambda [22]), or evaluating feature relevance with tools like XAI [10], is paramount for developing accurate, robust, and interpretable AI models for EEG-based schizophrenia diagnosis.  

# 5. AI and Machine Learning Techniques for EEG Analysis in Schizophrenia  

The application of Artificial Intelligence (AI) and Machine Learning (ML) techniques to the analysis of Electroencephalography (EEG) data represents a significant area of research for the diagnosis and understanding of neurological and psychiatric disorders, including schizophrenia [6,15,19,23]. AI predominantly encompasses machine learning and deep learning, with the latter being a subset of ML [29]. ML utilizes computational algorithms to identify patterns in data and build predictive models, while Deep Learning (DL), through deep neural networks (DNNs), excels at processing raw data and learning complex relationships via multiple hidden processing layers [29]. These techniques are broadly categorized into traditional machine learning algorithms and deep learning models for EEG-based schizophrenia diagnosis.  

Traditional supervised machine learning methods applied to EEG data typically involve a two-stage process: feature extraction followed by classification [1,13]. Common algorithms explored include Support Vector Machines (SVM), K-Nearest Neighbors (KNN), Decision Trees, Random Forests (RF), Linear Discriminant Analysis (LDA), Logistic Regression (LR), Artificial Neural Networks (ANN) or Multi-Layer Perceptrons (MLP), and ensemble methods like AdaBoost and Gradient Boosting [1,3,4,9,12,16,17,22,28,30,34,35,38]. The core principles vary; SVMs find an optimal hyperplane for separation, KNN classifies based on nearest neighbors, Decision Trees use hierarchical attribute tests, RF combines multiple decision trees, LDA projects data for maximal class separability, LR models outcome probability, and ANNs process information through interconnected layers [29,30,38]. Feature engineering is crucial for traditional methods, involving techniques like PCA, LDA, and Differential Entropy extraction from EEG signals, sometimes combined with methods like Riemannian Geometry decoding [1,13,17]. Feature selection methods like ReliefF, Lasso, and Elastic Net are also employed to handle high dimensionality and potentially improve robustness [9,12,22,25].  

Deep learning models, conversely, can automatically learn relevant hierarchical features directly from raw or minimally processed EEG data, bypassing the need for manual feature engineering and potentially capturing more complex, non-linear patterns [1,14,16,29]. Architectures commonly explored for neurophysiological data include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) like LSTMs, Temporal Convolutional Networks (TCNs), Autoencoders, and Deep Belief Networks (DBNs) [14,16,20]. Specific CNN variants like Inception, ResNet, and DenseNet have been applied, often with transfer learning, particularly in multimodal contexts [10,16,35,37]. Architectures specifically designed for EEG, such as variants from the Braindecode framework including Shallow ConvNet, EEGNet, Deep4, and TCNs, have been implemented, often utilizing optimized training techniques like cropped training, AdamW optimizer, cosine annealing, batch normalization, early stopping, and dropout regularization [1,14,35]. Contrastive learning frameworks employing CNN base encoders have also been explored for learning inter-subject representations [17].  

Comparing traditional ML and deep learning approaches for EEG-based schizophrenia diagnosis reveals distinct advantages and disadvantages. Traditional ML methods require expert-driven feature engineering, which can be time-consuming but allows for greater interpretability of the derived features and models. They may perform well on smaller datasets or when domain knowledge strongly guides feature selection [29]. However, their performance is limited by the quality and relevance of the handcrafted features. Deep learning, by contrast, offers end-to-end learning capabilities, automatically discovering complex patterns and interactions within the data that might be missed by traditional feature extraction methods [1,16,29]. This potential for automated, high-level feature learning makes DL particularly promising for complex, high-dimensional data like raw EEG. Deep learning models, such as CNNs and RNNs, have shown strong performance in classifying various EEG-related states [14,20]. Nevertheless, deep learning models typically require significantly larger annotated datasets for effective training and substantial computational resources [16]. They often suffer from a lack of interpretability, presenting "black box" models, which can be a barrier to clinical adoption [20].​  

The suitability of a specific technique depends on factors such as dataset size, data modality (e.g., raw EEG vs. precomputed features), required model interpretability, and available computational resources. Traditional ML, when combined with effective feature engineering and selection techniques, can be suitable for smaller datasets and scenarios where feature interpretability is prioritized. Deep learning models are more suitable for large datasets and tasks where automatic feature learning is advantageous, particularly for raw or less processed data. Hybrid models, combining strengths from both traditional and deep learning approaches or different deep learning architectures, are also being explored to enhance accuracy and robustness [20].  

Challenges associated with implementing these techniques for EEG-based schizophrenia diagnosis include the scarcity of large, high-quality, well-annotated EEG datasets specifically for schizophrenia, the high dimensionality and noise susceptibility inherent in EEG signals, and the need for models that are both accurate and clinically interpretable and robust [16,20]. Promising techniques and potential solutions to these challenges include transfer learning to leverage knowledge from other domains or tasks, data augmentation techniques to artificially increase dataset size, integrating explainable AI (XAI) methods to improve model transparency, and developing multimodal frameworks that combine EEG with other data sources like MRI, clinical data, or genetic information [10,16,20,35,37]. Ensemble methods and optimized model architectures and training strategies are also promising for improving performance and robustness [1,30]. Addressing these challenges is crucial for the successful translation of AI/ML models from research to clinical practice for schizophrenia diagnosis.​  

# 5.1 Machine Learning Algorithms (Traditional)  

<html><body><table><tr><td>Algorithm</td><td>Core Principle</td><td>Typical Application in EEG- SCZ</td></tr><tr><td>Support Vector Machine (SVM)</td><td>Finds optimal hyperplane to separate classes</td><td>Classification of EEG/ERP features,often with RBF kernel</td></tr><tr><td>K-Nearest Neighbors (KNN)</td><td>Classifies based on majority label of nearest data points</td><td>Classification based on feature similarity</td></tr><tr><td>Decision Tree</td><td>Hierarchical attribute tests leading to classification results</td><td>Building rule-based classifiers from features</td></tr><tr><td>Random Forest (RF)</td><td>Ensemble of multiple decision trees trained on bootstrapped samples</td><td>Robust classification by combining multiple tree decisions</td></tr><tr><td>Linear Discriminant Analysis (LDA)</td><td>Projects data for maximal class separability</td><td>Classification, sometimes used for dimensionality reduction</td></tr><tr><td>Logistic Regression (LR)</td><td>Models the probability of a binary outcome using a linear function</td><td>Binary classification of SCZ vs Control</td></tr><tr><td>Artificial Neural Network (ANN)/MLP</td><td>Processes information through interconnected</td><td>Multi-layer perceptrons for classificationafter feature</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>layers of nodes</td><td>extraction</td></tr><tr><td>Ensemble Methods</td><td>Combines multiple weaker models</td><td>AdaBoost, Gradient Boosting for improved robustness</td></tr></table></body></html>  

Traditional supervised learning methods have been widely explored for classifying electroencephalography (EEG) data to aid in the diagnosis of schizophrenia. These approaches typically involve extracting relevant features from preprocessed EEG signals, followed by training a classifier to distinguish between patient and control groups. Several studies have employed algorithms such as Support Vector Machines (SVM), K-Nearest Neighbors (KNN), Decision Trees, Random Forests (RF), Linear Discriminant Analysis (LDA), Logistic Regression (LR), and Artificial Neural Networks (ANN) or Multi-Layer Perceptrons (MLP) [1,16,17,30,34,35,38].  

The underlying principles of these algorithms vary. SVM functions as a generalized linear classifier that seeks to identify the optimal hyperplane to separate data points in a high-dimensional feature space, primarily used for binary classification tasks [30,38]. KNN classifies instances by measuring the distance between feature values and assigning a class based on the majority label among its nearest neighbors [30,38]. Decision Trees are hierarchical structures where internal nodes represent tests on attributes, branches represent outcomes, and leaf nodes signify classification results [38]. RF extends this by constructing an ensemble of multiple decision trees trained on bootstrapped samples and making predictions based on a voting mechanism [12,30,38]. LDA is a linear method that aims to project data onto a lower-dimensional space while maximizing class separability [30]. LR generates a hyperplane through a linear transformation function to model the probability of a binary outcome [30]. ANNs, including MLPs, are inspired by biological neural networks, processing information by adjusting connections between layers of nodes [38]. A three-layer MLP, for instance, has been used as a classifier following feature extraction [17]. Ensemble methods like AdaBoost and Gradient Boosting, which combine multiple weaker models, have also been employed, sometimes integrated within automated machine learning frameworks [1,35].​  

Application to EEG data for schizophrenia diagnosis involves specific preprocessing and feature extraction steps. Studies have utilized SVM with features derived from EEG electrode signals and source space analysis [3,4]. SVM with a radial basis function (RBF) kernel has been investigated [1]. Feature extraction techniques such as Principal Component Analysis (PCA) and LDA have been applied for dimensionality reduction prior to classification [13]. Differential entropy features extracted from EEG have been used as input for MLP classifiers [17]. Riemannian Geometry (RG) decoding, which operates on covariance matrices representing functional connectivity, has been evaluated in conjunction with SVM classifiers [1]. Specific parameters, such as setting n_neighbor $\displaystyle { s = 5 }$ for the KNN algorithm, have been reported in studies applying these methods to brain signals [28]. Feature selection plays a role, with methods like ReliefF used alongside RF [9]. Regression techniques like Lasso and Elastic Net, which incorporate regularization for feature selection and coefficient shrinkage, have also been explored in related diagnostic contexts [12,22,25].​  

While various traditional ML algorithms have been applied, the provided digests do not offer detailed comparative analyses of their performance specifically for schizophrenia diagnosis using EEG, nor do they extensively discuss the specific advantages and disadvantages of each algorithm in handling the inherent challenges of EEG data, such as its high dimensionality, susceptibility to noise, and artifacts. The use of feature extraction (e.g., PCA, LDA, RG) and feature selection techniques (e.g., ReliefF) alongside these classifiers implicitly addresses the high dimensionality and potential noise by reducing the feature space or selecting more robust features [1,9,13]. However, explicit evaluation of classifier robustness to different types and levels of noise or artifacts within the EEG-based schizophrenia diagnosis context is not elaborated in the available digests. Further research is needed to systematically compare these traditional ML approaches under standardized conditions and evaluate their resilience to the complexities of real-world EEG data in clinical settings.  

# 5.2 Deep Learning Models  

<html><body><table><tr><td>Feature/Aspect</td><td>Traditional Machine Learning</td><td>Deep Learning</td></tr><tr><td>Feature Engineering</td><td>Requires manual, expert- driven feature extraction</td><td>Automatically learns hierarchical features from raw/processed data</td></tr><tr><td></td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td>Data Requirement</td><td>Can perform well on smaller datasets</td><td>Typically requires significantly larger annotated datasets</td></tr><tr><td>Interpretability</td><td>Generally higher (features often meaningful)</td><td>Often lower ("black box" nature)</td></tr><tr><td>Computational Cost</td><td>Generally lower</td><td>Typically higher (training complex networks)</td></tr><tr><td>Pattern Learning</td><td>Relies on handcrafted features; may miss complex non-linearities</td><td>Can capture complex, non- linear patterns automatically</td></tr><tr><td>Model Examples</td><td>SVM, RF, LDA, LR, KNN</td><td>CNNs, RNNs (LSTMs), TCNs, Autoencoders, DBNs</td></tr></table></body></html>  

Deep learning models represent a significant advancement in analyzing complex biological signals such as electroencephalogram (EEG) data for diagnostic purposes. Unlike traditional machine learning approaches that often rely on handcrafted features extracted through extensive domain expertise, deep learning models possess the inherent capability to automatically learn relevant hierarchical features directly from raw or minimally preprocessed data [1,14,16]. This automatic feature extraction eliminates the need for manual feature engineering, potentially capturing more intricate and non‐linear relationships within the data that might be missed by conventional methods [16]. The application of deep learning techniques to EEG analysis holds considerable promise for improving the accuracy and efficiency of schizophrenia diagnosis.​  

Various deep learning architectures have been explored for neurophysiological data analysis, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, Temporal Convolutional Networks (TCNs), Autoencoders, and Deep Belief Networks (DBNs) [14,16]. Specific CNN architectures applied include standard feed-forward ConvNets, 1D/2D/3D CNN variants, Inception, ResNet, DenseNet, Generative Adversarial Networks (GANs), and CapsNet [10,14,16,35]. RNNs often utilize unidirectional and bidirectional LSTMs to process sequential data like EEG signals [14].​  

Regarding specific architectural implementations in EEG analysis, studies have utilized architectures inspired by or directly based on models like EEGNet within frameworks such as contrastive learning. In one approach, a CNN with a base encoder comprising spatial and temporal convolutional layers was employed within a contrastive learning framework, using a projector to compute sample similarity for unsupervised representation learning [17]. Another study compared several Braindecode network variants, including Braindecode Shallow ConvNet (BD-Shallow), Braindecode Eegenet (BD-Eegenet), a four-layer ConvNet (BD-Deep4) with separable convolutions and exponential linear units, and a TCN (BD-TCN) with multiple levels of temporal convolutions and max pooling [1]. These networks were typically trained using optimized techniques such as cropped training and the AdamW optimizer with a cosine annealing schedule for learning rate and weight decay, minimizing classification cross-entropy loss [1]. For multimodal integration, models like EEG Q-Net have been utilized as pre-trained components for EEG data processing within larger deep learning frameworks [37].​  

While detailed architectures were not always provided, studies using deep neural networks (DNNs) or specific architectures like DenseNet121, ResNet50, Inceptionv3, and VGG16 applied transfer learning, especially for image‐based modalities like MRI, which conceptually extends to leveraging pre‐trained models for EEG when applicable [10,34,35,37]. Optimization techniques such as learning rate scheduling, batch normalization, early stopping, and dropout regularization are commonly applied to enhance training stability and generalization [35].  

The comparison of different deep learning architectures often reveals varying performance characteristics depending on the dataset and task. While specific performance metrics comparing all mentioned architectures on schizophrenia diagnosis using EEG are not extensively detailed across the provided studies, investigations into different ConvNet and TCN variants suggest that architecture choice and training methodology significantly impact results [1]. CNNs have shown particular promise in EEG signal classification, demonstrating capabilities in predicting clinically relevant states like fatigue levels in multiple sclerosis patients, which highlights their potential for similar applications in schizophrenia [20].  

Despite their advantages, applying deep learning models to schizophrenia diagnosis via EEG faces challenges. The primary challenges include the need for large, well-annotated EEG datasets, which are often scarce, and the significant computational resources required for training complex models [16]. However, several opportunities and potential solutions exist. Transfer learning, where models pre-trained on larger, related datasets (even from other modalities or tasks) are adapted for the specific task of EEG‐based schizophrenia diagnosis, can mitigate the data scarcity issue [10,35,37]. Data augmentation techniques can artificially expand the training dataset size. Furthermore, combining deep learning with explainable AI techniques can enhance model interpretability and robustness, which is crucial for clinical applications [20]. Hybrid models, combining different deep learning architectures or integrating deep learning with traditional methods, also offer a path to potentially improved accuracy and robustness [20].  

# 6. Datasets and Experimental Design  

The development of robust artificial intelligence models for schizophrenia diagnosis using electroencephalography (EEG) is fundamentally reliant on the characteristics and quality of the datasets employed and the rigor of the experimental designs used for evaluation.  

<html><body><table><tr><td>Aspect</td><td>Description/Challenge</td><td>Validation/Mitigation Strategy</td></tr><tr><td>Dataset Size</td><td>Often small cohorts in research studies</td><td>K-fold CV, LOOCV, Data Augmentation, Transfer Learning</td></tr><tr><td>Dataset Diversity</td><td>Limited representation of full SCZ heterogeneity, comorbidities</td><td>Curating large, diverse datasets; Cross-subject validation</td></tr><tr><td>Data Scarcity</td><td>Few large, publicly available, SCZ-specific EEG datasets</td><td>Collaborative data sharing, Creating benchmark datasets, Transfer learning</td></tr><tr><td>Data Heterogeneity</td><td>Variations in acquisition protocols,equipment, sites, patient characteristics (age, medication)</td><td>Standardization efforts, Harmonization techniques, Inter-site cross-validation</td></tr><tr><td>Clinical Data</td><td>Potential inaccuracies or incompleteness of associated clinical info</td><td>Using standardized instruments, Data cleaning/validation protocols</td></tr><tr><td>Validation Rigor</td><td>Single train/test split may overestimate performance, lack of external validation</td><td>K-fold CV (e.g.,5,10), LOOCv, Dedicated hold-out test sets, External validation studies</td></tr><tr><td>Generalizability</td><td>Models trained on specific cohorts/protocols may not generalize to unseen data/populations</td><td>Cross-subject validation, Inter-site cross-validation, Robust regularization techniques</td></tr></table></body></html>  

A critical challenge in this domain, as inferred from the provided literature, appears to be the limited availability and scale of publicly described EEG datasets specifically curated for schizophrenia research. While general pathological EEG datasets like the Temple University Hospital (TUH) Abnormal EEG Corpus are publicly available, containing recordings from patients with various neurological conditions including epilepsy and stroke, their primary annotation is binary (pathological/nonpathological) and not specific to psychiatric disorders like schizophrenia [1]. Other publicly available datasets mentioned in the digests, such as ISRUC-Sleep, Sleep-EDF, and Physiobank CAP sleep database, are focused on sleep studies and related disorders [9], or emotion recognition like THU-EP and SEED datasets [17], and thus are not directly applicable to  

schizophrenia diagnosis via EEG. Multimodal datasets relevant to schizophrenia, such as subsets of FBIRN, BSNIP, and COBRE, are noted to include sMRI, fMRI, SNP, and behavioral data, often alongside some form of neurophysiological data, but their explicit description as comprehensive public E Gdatasets for SCZ is not consistently detailed in the provided digests [10,16].  

Studies specifically investigating EEG for schizophrenia diagnosis, as represented in the digests, primarily rely on relatively small, likely private, datasets. For instance, several studies involve cohorts of around 34 schizophrenia patients and 34 healthy controls [3,4], while another focuses on 22 first-episode schizophrenia patients and 22 healthy controls [5]. These datasets typically involve specific experimental paradigms, such as the auditory Oddball task [3,4] or resting-state EEG recordings [5,8]. The modest sample sizes and specific nature of these datasets (e.g., focusing on first-episode patients or patients on medication [8]) raise questions regarding their representativeness of the broader schizophrenia population, which is highly heterogeneous in terms of symptoms, illness severity, duration, and treatment response. The lack of detailed demographic information in some digests, or the specification of narrow inclusion criteria like age range and nationality [30], further limits the generalizability of findings derived from these specific cohorts. Comprehensive clinical information, including standardized diagnostic criteria [12,21], symptom profiles, and medication status, is crucial for developing models that can account for the clinical diversity of schizophrenia and potentially subtype patients or predict treatment response. The inclusion of such detailed clinical data alongside high-quality EEG recordings is essential for training robust and clinically relevant AI models.​  

The experimental design, particularly the methods used for validating AI models, significantly impacts the reliability and validity of the reported results. Common strategies involve splitting the dataset into training and testing sets, with ratios such as 70/30 [22,25,28,38], 80/20 [35], or using a dedicated hold-out test set (e.g., $2 0 \%$ [30], $1 0 \%$ [14]). Cross-validation techniques are frequently employed on the training or development set to optimize hyperparameters and estimate model performance more robustly than a single train/test split. These include k-fold cross-validation (e.g., 5-fold [1,35] or 10-fold [12,30]) and leave-one-out cross-validation [30].  

However, the validity and generalizability of these validation methods can be influenced by potential sources of bias. Standard k-fold cross-validation on a single, small dataset might overestimate performance on unseen data, especially when the test set is drawn from the same limited distribution as the training data. A more rigorous approach involves separating a completely independent test set that is not used during any part of the training or cross-validation process [30]. For inter-subject variability inherent in EEG data, cross-subject validation, where the model is trained on a set of subjects and tested on entirely different subjects [17], provides a more realistic assessment of how the model would perform on new individuals. Furthermore, multi-site studies with inter-site cross-validation, training on data from certain sites and testing on data from other sites [7], are crucial for evaluating the generalizability of findings across different recording environments and protocols, mitigating site-specific confounds. Potential biases in experimental design include the method of data splitting, such as using a chronological split which can be appropriate for time-series data but introduces specific dependencies [1], or reliance solely on self-reported diagnostic information [22]. The use of standardized diagnostic instruments [12] helps ensure the quality of diagnostic labels, reducing classification bias.​  

The heterogeneity in datasets (size, patient characteristics, task vs. resting state, recording protocols) and experimental designs makes direct comparison between different AI approaches challenging. The absence of large-scale, standardized, publicly available EEG datasets specifically for schizophrenia, collected using consistent protocols across multiple sites and encompassing diverse patient populations, hinders the field's progress towards developing widely applicable diagnostic biomarkers. The adoption of standardized evaluation metrics is also crucial, but without common datasets and transparent experimental designs, comparing the reported performance metrics across studies remains problematic. Future research would benefit significantly from collaborative efforts to create such standardized resources and from researchers adopting more rigorous validation protocols, such as multi-site and cross-subject cross-validation, to enhance the reliability and generalizability of AI models for schizophrenia diagnosis via EEG.​  

# 7. Performance Evaluation and Validation  

<html><body><table><tr><td>Metric</td><td>Calculation</td><td>Description</td><td>Clinical Relevance</td></tr><tr><td>Accuracy (ACC)</td><td>(TP +TN)/(TP + TN + FP +FN)</td><td>Overall proportion of correctly</td><td>General performance; less</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>classified instances</td><td>informative for imbalanced data</td></tr><tr><td>Sensitivity (Recall)</td><td>TP/(TP +FN)</td><td>True Positive Rate: Ability to correctly identify SCZ cases</td><td>Minimizing missed diagnoses (False Negatives)</td></tr><tr><td>Specificity</td><td>TN/(TN + FP)</td><td>True Negative Rate: Ability to correctly identify Healthy Controls</td><td>Minimizing incorrect diagnoses (False Positives)</td></tr><tr><td>Precision</td><td>TP/(TP +FP)</td><td>Proportion of positive predictions that are true positives</td><td>Confidence in a positive diagnosis</td></tr><tr><td>F1-score</td><td>(2·TP)/(2·TP + FP +FN)</td><td>Harmonic mean of Precision and Recall</td><td>Balanced measure, useful for imbalanced datasets</td></tr><tr><td>AUC (ROC)</td><td>Area Under Receiver Operating Characteristic</td><td>Discriminatory capacity across all thresholds</td><td>Threshold- independent overall performance (0.5- 1.0)</td></tr><tr><td>Brier Score</td><td>Mean Squared Difference (Prob vs Actual)</td><td>Accuracy of probabilistic predictions</td><td>Measures calibration and accuracy of probabilities</td></tr></table></body></html>  

Evaluating the performance and assessing the generalizability of AI/ML models developed for schizophrenia diagnosis using EEG data are critical steps in determining their clinical utility and reliability. This section outlines the commonly employed performance evaluation metrics and validation techniques used in this domain. Performance metrics quantify how well a model discriminates between individuals with schizophrenia and healthy controls, while validation techniques ensure that these performance estimates are robust and reflect the model's likely performance on unseen data, thereby mitigating issues like overfitting.​  

Performance evaluation metrics provide quantitative measures of a model's success in correctly classifying instances. Key metrics widely reported in the literature include Accuracy, Precision, Recall (Sensitivity), Specificity, F1-score, and the Area Under the Receiver Operating Characteristic curve ([14,16,28,30,35,38]). Accuracy, defined as the proportion of correctly classified instances, offers a general sense of performance ([1,3,4,17,35]). However, its interpretability is limited in the presence of imbalanced datasets, where high accuracy might be achieved by simply predicting the majority class. For a more detailed understanding of classification errors, especially crucial in clinical settings, Sensitivity (or Recall) and Specificity are essential. Sensitivity measures the true positive rate—the ability to correctly identify individuals with ispdcrehindztiiocftpyihohrnesan $( S e n s i t i v i t y = \frac { T P } { T P + F N } )$ $( S p e c i f i c i t y = \frac { T N } { T N + F P } )$ p)c(eo[c3nif5i]cd)i.teyPnrcmeceiasnisoaunrp,eosdsteitfhievneterduiaeasgntnehogesairtsiav(t[ie3or5ao]tf).etrT—uhtehsepeoasmbiiteliitvtreyicstsot orceaovlrleraeplcotslhiytei ve $( P r e c i s i o n = \frac { T P } { T P + F P } ;$ trade-offs inherent in classification thresholds; for example, increasing sensitivity to minimize missed diagnoses might decrease specificity, leading to more false positives ([7]). The F1-score, the harmonic mean of precision and recall ( $F 1 S c o r e = \frac { 2 \cdot T P } { 2 \cdot T P + F P + F N } )$ 2 ⋅ TP + FTP + FN ​ ), provides a balanced measure, particularly valuable for imbalanced datasets ([28,35]). The AUC is a threshold-independent metric that assesses the overall discriminative capacity across all possible thresholds providing a robust measure of model performance less affected by class distribution ([12,22,30,37]). Other metrics like the Brier Score quantify the accuracy of probabilistic predictions ([12]), while for tasks beyond binary classification, such as predicting continuous outcomes, metrics like R-squared $[ R ^ { 2 }$ ) and Root Mean Squared Error (RMSE) might be used,  

although less standard for the primary diagnostic task ([25]). Researchers often report multiple metrics to offer a comprehensive view of model performance and its potential clinical implications ([14,16,28,30,35,38]).  

To ensure that reported performance metrics are reliable and indicative of how the model will perform on new, unseen data, various validation techniques are employed. The fundamental single train/test split involves dividing the dataset into separate sets for training and evaluation ([22,28]), but this can lead to high variability and inefficient data utilization. Crossvalidation (CV) methods provide more robust estimates by partitioning the dataset into $k$ folds and iteratively training on $k - 1$ folds and testing on the remaining fold ([25]). K-fold CV (e.g., 5-fold or 10-fold) reduces the impact of data partitioning bias ([1,10,12,22,25,30,35,38]), while Leave-one-out CV (LOOCV), an extreme case where $k$ equals the number of samples, offers a nearly unbiased estimate but is computationally expensive ([30]). To assess generalizability beyond the specific dataset, particularly crucial for clinical deployment, internal validation with separate training and validation cohorts allows for hyperparameter tuning and generalizability checks during development ([22]). More rigorously, inter-site crossvalidation and cross-subject cross-validation evaluate model performance on data entirely independent from the training set, directly assessing how well the model generalizes across different data collection sites or new individuals, respectively ([7,17]). These techniques are crucial for validating the potential of AI/ML models as diagnostic tools in varied clinical settings ([7]). Statistical methods, such as t-tests, permutation tests, and Network-Based Statistics (NBS), may also be used to validate findings or compare results between groups or models ([1,5,13,30]).  

The choice of performance metrics and validation techniques significantly impacts the interpretation of results. Relying solely on accuracy might obscure poor performance on minority classes in imbalanced datasets, while a combination of sensitivity, specificity, F1-score, and AUC provides a more complete picture. Similarly, validating a model only on a single train/test split from one dataset provides limited confidence in its real-world applicability compared to evaluation using kfold CV or, ideally, inter-site or cross-subject validation ([7,17]). The challenges of comparing performance across different studies are substantial due to variations in datasets (size, characteristics, imbalance), experimental designs, EEG acquisition protocols, and preprocessing techniques. These inconsistencies make direct comparisons of reported metric values difficult and highlight the need for standardized benchmarks and appropriate statistical methods when attempting cross-study comparisons ([1,4,30]). Addressing performance evaluation on imbalanced datasets remains a challenge, often requiring the use of metrics less sensitive to class distribution (like F1-score and AUC) or techniques like data resampling or using costsensitive learning algorithms, although specific strategies for addressing imbalance are not extensively detailed in the provided digests. Robust evaluation, incorporating a suite of appropriate metrics and rigorous validation strategies, particularly those assessing generalization to diverse, unseen data, is paramount for advancing the field towards clinically viable AI/ML tools for schizophrenia diagnosis.​  

# 7.1 Performance Metrics  

Evaluating the performance of AI/ML models in schizophrenia diagnosis via EEG necessitates the use of appropriate quantitative metrics that reflect a model's discriminative power and clinical utility. A variety of metrics are commonly employed in the literature to assess the accuracy, reliability, and clinical relevance of classification models [14,16,28,30,35,38].​  

Accuracy (ACC) is a fundamental metric defined as the ratio of correctly classified instances (both true positives and true negatives) to the total number of instances. It is calculated as  

$$
A c c u r a c y = \frac { T P + T N } { T P + T N + F P + F N } ,
$$  

where TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives, respectively [35]. Accuracy provides an intuitive measure of overall correctness and is frequently reported as a primary performance indicator [1,3,4,17]. Reported accuracies in EEG-based schizophrenia diagnosis range, for example, from $6 6 . 3 5 \%$ when using unimodal features to $8 8 . 2 4 \%$ with combined features [3,4,10]. While straightforward, accuracy can be misleading, particularly in datasets with imbalanced class distributions, where a model might achieve high accuracy by simply predicting the majority class.​  

To provide a more nuanced understanding, especially in clinical contexts where different types of errors carry different weights, metrics like Precision, Recall (Sensitivity), and Specificity are essential. Precision is defined as the ratio of true positives to the total predicted positives  

$$
P r e c i s i o n = \frac { T P } { T P + F P } ,
$$  

indicating the accuracy of positive predictions [35]. Recall, also known as Sensitivity, measures the proportion of actual positives that are correctly identified  

$$
S e n s i t i v i t y = \frac { T P } { T P + F N } ,
$$  

reflecting the model's ability to detect all true cases [35]. Specificity is the ratio of true negatives to the total actual negatives $S p e c i f i c i t y = \frac { T N } { T N + F P } ,$  

indicating the model's ability to correctly identify negative cases [35]. These metrics are crucial in diagnostic applications; high sensitivity minimizes false negatives (missed diagnoses), while high specificity minimizes false positives (incorrect diagnoses). Studies have reported sensitivity and specificity values, for instance, exceeding $7 9 \%$ and $8 1 . 5 \%$ respectively in some models [7]. A common trade-off exists between sensitivity and specificity, where increasing one may decrease the other depending on the chosen classification threshold.  

The F1-score provides a single metric that balances precision and recall, calculated as the harmonic mean of the two $F 1 ~ S c o r e = \frac { 2 \cdot T P } { 2 \cdot T P + F P + F N } ,$  

which is particularly useful when dealing with imbalanced datasets, as it penalizes models that perform poorly on either precision or recall—unlike accuracy, which might be high even if recall for the minority class is low [28,35].  

The Area Under the Receiver Operating Characteristic curve (AUC) is another widely used metric that assesses the model's ability to discriminate between positive and negative classes across all possible classification thresholds [22,30,37]. AUC values range from 0 to 1, with 0.5 indicating no discriminatory power and 1.0 indicating perfect discrimination. Interpretation guidelines suggest AUC values of 0.7–0.8 as acceptable, 0.8–0.9 as good, and ${ \geqslant } 0 . 9$ as outstanding [12]. AUC is often favored as it provides a measure of overall performance independent of the class distribution and the specific threshold chosen, making it suitable for comparing different models or evaluating performance in scenarios where the optimal threshold may vary based on clinical context. Reported AUC values in relevant studies span a range, such as 0.71 to 0.75 [37] and 0.804 to 0.814 [22], with some models achieving values close to or at 1.00 in training sets, although these tend to be lower in testing sets [38].  

Beyond classification metrics, other evaluation approaches exist. The Brier Score quantifies the accuracy of probabilistic predictions by measuring the mean squared difference between the predicted probability and the actual outcome. Lower Brier scores indicate better calibration and accuracy [12]. Some studies, not focused on building classification models, may use statistical tests like p-values from General Linear Mixed Models (GLMMs) or post-hoc tests to evaluate the significance of differences in specific EEG features between groups [5,13]. For regression tasks, metrics like R-squared $( R ^ { 2 } )$ and Root Mean Squared Error (RMSE) are used to evaluate how well a model predicts continuous outcomes [25]; however, these are not standard for the primary task of binary classification in diagnosis.  

In the clinical application of AI/ML models for schizophrenia diagnosis, the choice and interpretation of performance metrics are critical. While accuracy is intuitive, a comprehensive evaluation requires considering sensitivity, specificity, precision, and F1-score—especially when dealing with potential class imbalances or varying costs of false positives and negatives. AUC offers a valuable threshold-independent assessment of discriminatory power. The clinical relevance of a model is not solely determined by high metric values but also by how these values align with clinical objectives—whether the priority is high sensitivity for screening, high specificity for confirmation, or a balance that minimizes overall harm. Therefore, researchers typically report multiple metrics to provide a comprehensive view of model performance [14,16,28,30,35,38].​  

# 7.2 Validation Techniques  

<html><body><table><tr><td>Technique</td><td>Description</td><td>Purpose</td><td>Considerations</td></tr><tr><td>Single Train/Test Split</td><td>Dataset divided into training and separate evaluation sets</td><td>Basic performance estimate</td><td>High variability, inefficient data use</td></tr><tr><td>K-fold Cross- Validation (CV)</td><td>Dataset partitioned into k folds; train on</td><td>More robust performance</td><td>Less biased than single split,</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>k-1, test on 1; average results over k iterations</td><td>estimate, uses entire dataset for testing</td><td>computationally heavier</td></tr><tr><td>(LOOCV)</td><td>where k= number of samples; train on N-</td><td>estimate, max training data per fold expensive</td><td>computationally</td></tr><tr><td></td><td>model development</td><td></td><td></td></tr><tr><td></td><td>entirelydifferent</td><td>new individuals</td><td>subject variability</td></tr><tr><td></td><td>specific sites, test on data from other sites</td><td>generalization across different collection</td><td>studies, protocol variations</td></tr></table></body></html>  

Evaluating the performance and ensuring the reliability of machine learning (ML) and artificial intelligence (AI) models is paramount in clinical applications, particularly for complex tasks like schizophrenia diagnosis based on electroencephalography (EEG) data. Validation techniques assess a model's ability to generalize to unseen data and mitigate the risk of overfitting, where a model learns the training data too well and performs poorly on new instances. Various strategies are employed to achieve these validation goals.  

One fundamental approach is the single train/test split, where the dataset is divided into two distinct sets: one for training the model and another for evaluating its performance [22,28]. While straightforward and computationally inexpensive, this method suffers from high variability in performance estimates depending on how the data is split. It might not effectively utilize the entire dataset for both training and evaluation, potentially leading to less reliable performance metrics and an increased risk of overfitting to the training partition.  

To provide a more robust evaluation, cross-validation (CV) techniques are widely adopted. K-fold cross-validation is a prominent method that involves partitioning the dataset into $k$ equally sized folds [25]. The model is then trained $k$ times; in each iteration one fold is used as the test set while the remaining $k - 1$ folds serve as the training set [25]. The final performance metric is computed as the average of the results from all $k$ iterations [25]. Common values for $k$ include 5 [1,35,38] and 10 [12,22,25,30]. Ten-fold cross-validation, for instance, involves partitioning the sample into 10 subsets, using 9 for training and 1 for testing, and iterating this process ten times so that each subset serves as the test set exactly once [25]. This procedure is often repeated multiple times (e.g., 10 repeats) to further enhance the reliability of the performance estimate [25]. K-fold CV offers a more reliable estimate of a model's performance on unseen data compared to a single split because it utilizes the entire dataset for both training and testing across iterations, thereby reducing the impact of data partitioning bias and providing a better measure of generalization ability.​  

Leave-one-out cross-validation (LOOCV) is an extreme case of k-fold CV where $k$ equals the number of samples in the dataset [30]. In each iteration, a single sample is used as the test set and the model is trained on the remaining $N - 1$ samples. LOOCV provides a nearly unbiased estimate of model performance and maximizes the amount of data used for training in each fold, which can be beneficial for small datasets. However, its significant computational cost, requiring $N$ training iterations, makes it impractical for large datasets.  

Beyond basic train/test splits and $k$ -fold CV, more sophisticated strategies are necessary to assess generalizability, especially when data comes from heterogeneous sources or diverse populations. Internal validation using a separate validation  

cohort, distinct from both the training and final test sets, is employed during model development [22]. The data is randomly divided into training and validation groups (e.g., a 7:3 ratio) to tune hyperparameters and assess model generalizability during development [22]. This helps prevent overfitting to the final test set, which should ideally only be used once for unbiased performance evaluation.  

For assessing generalizability across different clinical sites or subject pools, techniques such as inter-site cross-validation [7] and cross-subject cross-validation [17] are crucial. Inter-site CV involves training a model on data from one or more sites and testing it on data from entirely different sites, directly evaluating how well the model generalizes across varied data collection protocols and patient populations [7]. Similarly, cross-subject cross-validation assesses whether a model trained on a set of subjects can generalize to new, unseen subjects [17]. These methods provide a more realistic assessment of a model's utility in real-world, heterogeneous clinical environments and are essential for validating potential biomarkers across multiple independent cohorts [7]. Some studies also employ a dedicated generalization test to further confirm performance on completely independent data [17].  

Other practices that contribute to robust model evaluation and overfitting prevention include pre-extracting features from different datasets before model training [13] and monitoring model performance using specified callbacks during the training process to ensure optimal convergence and prevent premature stopping or overfitting [14]. Although not traditional cross-validation, statistical methods—such as t-tests, permutation tests, and Network-Based Statistics (NBS)—are employed to identify significant differences between groups [5], thereby providing statistical validation of findings in group-level analyses. Furthermore, correlation analysis may be used to validate aspects of a method, for example by examining correlations between derived features or metrics and established clinical scores [13].​  

The choice of validation technique for AI/ML models in schizophrenia diagnosis is influenced by several factors: the size of the dataset (with smaller datasets potentially benefiting more from k-fold or LOOCV), available computational resources (since LOOCV is resource-intensive), the need to assess generalizability across different sites or individuals (necessitating inter-site or cross-subject CV), and the specific stage of model development (e.g., internal validation sets for tuning versus final test sets for unbiased evaluation). Ultimately, robust validation—ideally employing multiple techniques—is indispensable for developing reliable and generalizable AI/ML models capable of assisting in schizophrenia diagnosis using EEG data.​  

# 8. Applications and Clinical Implications  

AI-enhanced electroencephalography (EEG) analysis holds significant potential to revolutionize the clinical diagnosis and management of schizophrenia, promising improvements across the disease trajectory from early detection to personalized intervention strategies and long-term monitoring [24,27].  

A primary clinical impact lies in enabling earlier and more accurate diagnosis of schizophrenia. EEG provides objective insights into brain activity [23], and machine learning techniques can process complex EEG data to identify subtle abnormalities and potential biomarkers associated with the disorder [15]. Research indicates that combining different types of EEG features, such as those from sensor- and source-level analyses, significantly improves diagnostic accuracy [3,4], thereby facilitating earlier and more accurate identification. Specific EEG markers, such as increased gamma band connectivity, have been explored as potential indicators for first-episode schizophrenia [5]. The capability for improved early detection, including through more accessible methods like single-channel EEG combined with ML for cognitive assessment relevant to schizophrenia [13], aligns with efforts in other neurological conditions like dementia and multiple sclerosis, where AI/ML improves early detection and diagnostic accuracy [19,20,35]. Earlier and more accurate diagnosis is crucial for reducing the duration of untreated psychosis (DUP), a key factor linked to poorer long-term outcomes [19]. Furthermore, AIdriven analyses informing clinical tools like nomograms for risk stratification can aid in timely identification and intervention [22]. The identification of objective biomarkers through AI-EEG integration represents a significant step towards achieving this goal [8,23].​  

Beyond diagnosis, AI-based EEG analysis shows promise in predicting treatment response in schizophrenia, paving the way for personalized treatment strategies. Predicting how an individual patient will respond to antipsychotic medication is currently challenging but critical for optimizing outcomes and minimizing adverse effects. While direct EEG-based prediction studies in schizophrenia are evolving, research on other biomarkers, such as striatal dysfunction assessment (FSA) scores, has demonstrated a significant association with antipsychotic response [7], suggesting the feasibility of identifying biological predictors. Studies evaluating the impact of treatment on EEG characteristics [8] also indicate the  

sensitivity of EEG measures to pharmacological effects, implying that pre-treatment EEG features could hold predictive information. Analogous applications in other neurological disorders, such as using multimodal deep learning to predict antiseizure medication outcomes in epilepsy [37], underscore the potential for complex biological data, including EEG, to forecast therapeutic efficacy in psychiatry. Successfully leveraging AI-driven EEG analysis to identify patterns predictive of treatment response could enable clinicians to select the most appropriate medication for patients from the outset, enhancing the likelihood of favorable outcomes.  

AI-based EEG analysis is also valuable for monitoring disease progression in schizophrenia and objectively assessing the effectiveness of interventions over time. While direct applications are under development, the capability for longitudinal monitoring using AI/ML on neurophysiological data is supported by advancements in other conditions. For example, algorithms have demonstrated high performance in predicting the course of dementia [35], and machine learning is utilized for real-time monitoring in multiple sclerosis [20]. AI-driven tracking of physiological indicators provides insights into disease progression and treatment efficacy in various neurological contexts [27]. Furthermore, analysis of specific EEG features has shown potential for tracking changes associated with cognitive decline [13], a common feature in schizophrenia. Biomarkers like FSA may also be utilized for monitoring changes over time, pending further validation in schizophrenia populations [7]. These examples collectively highlight the potential for AI and EEG to provide objective, longitudinal data for tracking the trajectory of schizophrenia and evaluating the impact of therapeutic interventions.  

Finally, AI presents opportunities for the objective assessment and targeted intervention of social cognitive impairments, a core challenge in schizophrenia significantly impacting functional recovery [21]. While traditional methods often rely on subjective assessments, AI-driven approaches could offer more objective and granular measures. Methodologies used for assessing emotional responses during social interaction, potentially integrating EEG with behavioral data [34], offer a basis for developing AI-based tools for objective assessment of social cognitive abilities in this population. Research quantifies social cognition within diagnostic frameworks [12], providing a foundation for AI model development. AI could also explore complex relationships, such as those between perceptual processing and social deficits [36], potentially identifying mechanisms underlying impairments. Integrating AI into interventions could enable personalized and data-driven therapeutic strategies targeting specific social cognitive deficits, potentially complementing or offering alternatives to pharmacological interventions [21]. By providing more precise assessments and informing targeted interventions, AI has the potential to substantially improve social functioning and quality of life for individuals with schizophrenia [21]. The application of interpretable AI models in healthcare could further enhance the efficiency and accessibility of these advanced diagnostic and therapeutic tools [14].  

# 8.1 Early Diagnosis and Detection  

![](images/e9f1f715650fc63802a19ba01eac8d521e26fa009f7714825d1417e39b88a9ab.jpg)  

The timely and accurate diagnosis of schizophrenia is critical for improving long-term outcomes and minimizing the duration of untreated psychosis (DUP) [19]. AI-based analysis of electroencephalography (EEG) signals offers a promising avenue for achieving earlier and more precise detection of the disorder. EEG provides objective measures of brain activity, which can reveal subtle abnormalities indicative of the initial stages of schizophrenia [23].  

Machine learning techniques are instrumental in processing complex EEG data to identify these potential biomarkers. For instance, studies have identified increased gamma band connectivity in resting-state EEG as a potential marker for firstepisode schizophrenia patients, suggesting its utility in early diagnosis [5]. The application of machine learning to EEG signals allows for the detection of various brain abnormalities relevant to neurological and psychiatric conditions, including schizophrenia [15,24].  

Research indicates that combining different types of EEG features, such as those derived from both sensor and source-level analyses, can significantly improve diagnostic accuracy for schizophrenia [3,4]. This enhanced accuracy contributes directly to the potential for earlier detection. Furthermore, the development of simplified systems, such as those leveraging singlechannel EEG combined with machine learning features for detecting cognitive decline, demonstrates the feasibility of using more accessible EEG methods for early screening purposes [13]. Tools like nomograms, which can be informed by such analyses, can aid in screening individuals potentially at risk for diseases [22].  

The utility of machine learning for early detection has been demonstrated in other neurological conditions, such as improving early detection capabilities in Multiple Sclerosis (MS) using EEG [20] and detecting minor alterations symptomatic of initial stages of dementia through multimodal machine learning models [35]. While not always EEG-specific, AI-assisted techniques, including pupillometry and eye movement analysis, have also been shown to improve diagnostic accuracy and enable earlier intervention strategies potentially relevant to schizophrenia [27]. These examples underscore the broader potential of AI applied to neurophysiological or related data for early disease detection. Biomarkers, such as the striatal dysfunction biomarker (FSA), also hold potential for improving early diagnosis by providing objective measures, which could complement EEG findings [7]. By identifying key features that differentiate clinical groups, machine learning models can inform effective early diagnosis and detection strategies [12]. The integration of AI with EEG for objective biomarker identification and improved diagnostic accuracy represents a significant step towards reducing DUP and facilitating earlier, more appropriate interventions in schizophrenia care [19].​  

# 8.2 Prediction of Treatment Response  

Predicting individual treatment response to antipsychotic medication is a critical challenge in schizophrenia management. The ability to forecast how a patient will respond could significantly improve clinical outcomes by enabling personalized treatment strategies from the outset, minimizing trial-and-error periods, and reducing exposure to ineffective medications and their associated side effects. AI-based analysis of physiological data, such as that derived from electroencephalography (EEG), holds potential for developing such predictive models.  

Research exploring biomarkers for treatment response in schizophrenia provides foundational support for this approach. For instance, studies have investigated the association between specific physiological markers, such as striatal function assessment (FSA) scores, and antipsychotic treatment response in schizophrenia [7]. The significant association observed between FSA scores and response highlights the feasibility of identifying biological correlates predictive of treatment outcome, conceptually extending to the utility of neurophysiological signals captured by EEG.  

Furthermore, studies have utilized EEG data to evaluate the effects of treatment in schizophrenia patients. Research has aimed to assess the impact of specific medications, such as clozapine, by comparing EEG characteristics before and after the administration of the drug [8]. While primarily focused on evaluating treatment effects rather than predicting future response, this work demonstrates the sensitivity of EEG measures to changes induced by antipsychotic medication, suggesting that pre-treatment EEG features could potentially contain information predictive of the nature and magnitude of these changes.​  

The application of AI and machine learning for predicting treatment outcomes is also explored in other neurological conditions, such as predicting antiseizure medication outcomes in epilepsy using multimodal deep learning [37]. This indicates a broader trend towards employing advanced computational techniques on complex biological data to forecast therapeutic efficacy, a paradigm potentially transferable to predicting antipsychotic response in schizophrenia using EEG data. In contrast to studies focusing on treatment prediction, some research has focused solely on differential diagnosis and did not address the prediction of treatment response [12].  

Synthesizing these perspectives, the potential for AI-driven EEG analysis to predict treatment response in schizophrenia lies in identifying patterns or biomarkers within the pre-treatment EEG signal that are predictive of subsequent response to antipsychotic medication. Successfully achieving this could revolutionize personalized treatment by guiding clinicians in selecting the most appropriate medication for individual patients based on their predicted response profile, thereby improving the likelihood of favorable outcomes and enhancing patient well-being.  

# 8.3 Monitoring Disease Progression  

AI-based EEG analysis presents a promising avenue for tracking disease progression in schizophrenia and assessing the effectiveness of interventions over time. While direct applications in schizophrenia are an active area of research, studies in other neurological conditions demonstrate the feasibility of using AI/ML for longitudinal monitoring. For example, algorithms have shown outstanding performance in predicting the course of dementia, thereby enabling better monitoring of disease progression [35]. Similarly, machine learning facilitates real-time monitoring in multiple sclerosis [20]. AI-based tracking of physiological patterns, such as nystagmus, provides valuable insight into disease progression and treatment  

efficacy in other contexts, offering an analogy for potential applications in schizophrenia [27]. Beyond EEG, related neuroimaging techniques like resting-state fMRI have been utilized to monitor disease progression in Alzheimer's disease patients by assessing changes in intrinsic brain activity and functional connectivity [18]. Specific EEG feature analysis also shows potential for tracking changes associated with cognitive decline over time [13], which is highly relevant given the cognitive impairments often observed in schizophrenia. Furthermore, the identification of specific biomarkers, such as those indexing striatal dysfunction, suggests the potential for these markers to be used in monitoring disease progression, although this application may require further validation in schizophrenia [7]. Collectively, these findings from related fields and initial explorations of EEG biomarkers underscore the potential for leveraging AI and EEG data to create objective, longitudinal measures for tracking the trajectory of schizophrenia and evaluating therapeutic responses.​  

# 8.4 Assessing Social Cognitive Impairments  

Social cognitive impairments, encompassing deficits in areas such as emotion recognition and Theory of Mind (ToM), represent a significant challenge in schizophrenia, profoundly impacting social functioning and overall quality of life [21]. Addressing these impairments is crucial for improving patient outcomes [21].  

AI holds considerable potential for objectively assessing social cognitive abilities in schizophrenia. While direct comparisons between AI-driven assessments and traditional methods are not explicitly detailed in the provided digests, the application of methodologies used for assessing emotional responses during social interaction—even in non‐schizophrenia contexts— suggests a pathway for developing AI‐based tools tailored for this population [34]. These methods, potentially integrating EEG data with behavioral observations, could offer more objective and granular insights compared to subjective clinical evaluations or potentially limited neuropsychological tests. Furthermore, studies incorporating social cognition measures within assessment batteries, such as those used in differential diagnosis research [12], demonstrate the feasibility of quantifying these constructs, providing a foundation upon which AI models can be built for automated analysis. The potential of AI extends to exploring nuanced relationships, such as the observed link between the irregularity of visual motion perception and negative symptoms—factors closely related to social cognition in schizophrenia [36]. AI could be leveraged to further investigate this connection, potentially identifying specific perceptual deficits that underpin social cognitive challenges.  

The integration of AI into interventions aimed at improving social cognition is a promising avenue. By using AI to identify and analyze specific deficits, interventions could be personalized and dynamically adjusted based on objective performance metrics. For instance, AI could power digital therapeutic platforms that target specific aspects of social cognition, such as emotion recognition or understanding social cues—potentially informed by analyses linking perceptual processing with social deficits [36]. While pharmacological interventions are a current approach to addressing social cognitive deficits [21], AI-driven interventions could offer complementary or alternative strategies, potentially providing more targeted and engaging therapeutic experiences.  

The potential benefits of using AI to address social cognitive deficits in schizophrenia are substantial. These include the possibility of more precise and objective assessments, enabling earlier identification and more accurate tracking of impairment severity. AI could facilitate the development of novel, data-driven interventions that target the specific underlying mechanisms of social cognitive dysfunction, potentially offering more effective alternatives or complements to existing therapies [21]. Ultimately, by improving the assessment and treatment of social cognitive impairments, AI could play a crucial role in enhancing the social functioning and overall quality of life for individuals with schizophrenia [21].  

# 9. Multimodal Approaches  

![](images/842e1f1befa627fc20bd17d837c2b9d88659d0132f82d91484aecadac3ac3dfb.jpg)  

Multimodal approaches, which integrate data from diverse sources, offer significant potential for enhancing the diagnosis and understanding of complex neurological and psychiatric disorders such as schizophrenia [16,23]. The rationale behind multimodal integration is that different data modalities capture complementary aspects of brain structure, function, and the underlying biological mechanisms, providing a more comprehensive picture than any single modality alone [2]. This integrated view can potentially improve the accuracy and robustness of AI-based diagnostic models.  

Research in schizophrenia diagnosis has explored the combination of various neuroimaging modalities, including structural MRI (sMRI) and resting-state functional MRI (rs-fMRI) [30]. Studies have demonstrated the benefits of combining sMRI and fMRI features, noting that classification performance tends to improve as features from different modalities are merged [10]. Specifically, fusing features from two or three modalities has shown a significant boost in accuracy compared to using single-modality features for schizophrenia classification [10]. Beyond neuroimaging, multimodal approaches have expanded to include the integration of genetic markers such as single nucleotide polymorphisms (SNP), functional imaging data (e.g., PET), molecular imaging, transcriptomic data, clinical factors, and even multi-biological data [7,10,26,37]. The integration of these diverse data types facilitates the detection and analysis of intricate relationships within the brain and body [23].​  

While numerous studies explore multimodal integration primarily using MRI variants for schizophrenia [2,16,26,30], the specific integration of EEG data with other modalities for schizophrenia diagnosis, along with direct performance comparisons against EEG-only models, is less explicitly detailed in the provided digests. However, the general principle of performance enhancement through multimodal integration is supported. For instance, in the context of predicting antiseizure medication outcomes, a multimodal deep learning framework integrating EEG, MRI, clinical factors, and molecular drug features achieved an AUC of 0.75, surpassing the best unimodal model’s AUC of 0.71 [37]. This suggests that combining EEG with other data types holds promise for improving diagnostic or predictive accuracy in neurological/psychiatric conditions, including schizophrenia. Future research is encouraged to directly compare EEG-only models with models that integrate EEG with modalities such as MRI, clinical data, or genetics for schizophrenia diagnosis.  

Integrating data from different modalities presents several technical challenges. These include addressing data heterogeneity—which arises from differences in data types (e.g., time series EEG, volumetric MRI, categorical clinical data, genetic sequences), collection methods, and resolutions—as well as managing the high-dimensionality resulting from the combined feature space. Dimensionality reduction, effective feature selection, or extraction methods are often necessary to identify the most discriminative features across modalities and fuse them appropriately.  

Machine learning and deep learning techniques are extensively applied to address these challenges and perform multimodal data fusion. Various fusion strategies exist, including early fusion (concatenating features before input to the model), late fusion (combining predictions from modality-specific models), and hybrid or intermediate fusion (integrating features at different layers within a model). Examples from the literature include the application of integrated machine learning frameworks [26], graph convolutional networks (GCN) and multi-omics GCN (MO-GCN) to capture complex data relationships [26], and deep learning architectures that integrate 3D and 2D CNNs for multimodal image and connectomics analysis [26]. Deep learning frameworks are particularly well-suited for learning complex, non-linear relationships between features from different modalities [10,37].​  

The challenges associated with multimodal data integration include the need for large, well-curated datasets with consistent collection protocols across modalities, the complexity of developing sophisticated fusion models, and the interpretability of decisions made by these integrated models. Opportunities lie in leveraging the complementary information across modalities to uncover subtle biomarkers, improve diagnostic accuracy, enable subtype identification, and facilitate personalized treatment strategies. Recommendations for future research include developing standardized protocols for multimodal data collection, exploring novel deep learning architectures for efficient and interpretable multimodal fusion (with a specific focus on incorporating EEG), and conducting rigorous comparative studies to quantify the performance gains of specific multimodal combinations versus unimodal approaches for schizophrenia diagnosis [20]. Furthermore, advancements in integrating EEG with wearable technology and other clinical data streams hold promise for developing accessible and continuous monitoring and diagnostic tools [20].  

# 10. Challenges and Limitations  

Despite the promising advancements in applying AI/ML techniques to electroencephalography (EEG) for schizophrenia diagnosis, the field faces significant challenges and limitations that hinder the widespread clinical translation and reliable deployment of these models. A primary technical hurdle lies in the inherent complexity of EEG data analysis. Raw EEG signals are highly susceptible to contamination from various sources of noise and artifacts, including physiological signals like muscle activity, eye movements, and cardiac rhythms, as well as external interference [1,9,13]. Effectively dealing with these artifacts and the non-stationary nature of brain signals requires robust and often complex preprocessing techniques, which are crucial for ensuring the quality and integrity of the data used for model training.​  

Compounding the technical challenges are fundamental limitations related to the data itself. There is a significant scarcity and limited diversity in currently available EEG datasets for schizophrenia research [11,12,16,20,22,28,30]. Many studies rely on small, private cohorts that may not adequately represent the full clinical and demographic heterogeneity of the schizophrenia population, including different subtypes and comorbidities [12,13,28]. Data heterogeneity also arises from variations in acquisition protocols, recording equipment, and clinical data collection practices across different sites [7,11,20,21,31], as well as differences in patient characteristics like age distribution [1]. The lack of large-scale, diverse, and standardized datasets impedes the development and training of powerful AI models, particularly deep learning models, which typically require vast amounts of data to achieve optimal performance and generalizability [16]. Furthermore, issues related to the accuracy and completeness of associated clinical data can introduce additional complexities [22,38].  

Methodological inconsistencies and constraints are also prevalent. Studies frequently employ relatively small sample sizes, limiting the statistical power and the ability to perform robust validation procedures [12,13,20,30,38]. The use of homogeneous patient populations restricts the generalizability of findings to the broader spectrum of individuals with schizophrenia [12,13]. Critiques also highlight the absence of rigorous validation techniques like cross-validation in some studies, which can lead to biased performance estimates [28]. Variations in EEG recording protocols, including differences between resting-state and task-related paradigms, further contribute to methodological heterogeneity and impact the comparability of results across studies [5]. The absence of standardized protocols for data acquisition and preprocessing remains a significant barrier to developing models that can reliably perform across different clinical settings and devices [20,31].​  

These data and methodological limitations directly contribute to the significant problem of overfitting and limited model generalization. Models trained on small or specific datasets with inconsistent methodologies are prone to learning noise and idiosyncratic patterns, leading to poor performance when applied to unseen data or different populations [13,28]. While techniques like cross-validation, regularization, and contrastive learning are employed to mitigate overfitting and improve generalization [7,9,12,14,17,22,30,35], achieving robust generalization across the diverse landscape of schizophrenia patients, recording conditions, and technological variations remains a key challenge [7,11,17,20]. External validation on independent and diverse datasets is crucial but often lacking [12,22,29,38].  

Furthermore, the "black box" nature of many advanced AI models, particularly deep neural networks, poses a significant challenge for clinical acceptance and trust [10,14,28,29,38]. Clinicians need to understand the reasoning behind a model's prediction to integrate it effectively into patient care [29,38]. The lack of interpretability hinders the ability to gain neurophysiological insights and limits the clinical utility of these models [28]. Explainable AI (XAI) techniques are being explored to address this by identifying key features and patterns driving predictions, but effectively balancing high predictive performance with transparency remains an active research area [1,10,12,14,17,22,25,29,35].  

Finally, integrating AI models from research settings into clinical practice introduces critical ethical considerations. These include ensuring data privacy and security for sensitive patient information [20,27,29], addressing algorithmic bias that can arise from unrepresentative training data and lead to disparities in diagnosis or care [6,20,27,29], and navigating the responsible use and potential misuse of these technologies. Establishing clear frameworks for fairness, transparency, and accountability is essential [27,29]. The challenges related to regulatory approval and determining responsibility in cases of diagnostic errors also need to be addressed [27,29]. Obtaining informed consent and defining the appropriate role of AI as a clinical support tool, rather than a replacement for expert judgment, are paramount for ethical deployment [13,27,29]. Collectively, these technical, data, methodological, model-specific, and ethical challenges significantly impact the reliability, validity, and clinical applicability of current AI models for EEG-based schizophrenia diagnosis.​  

# 10.1 Data Quality and Variability  

<html><body><table><tr><td>Challenge Area</td><td>Description</td><td>Impact on Al Models</td></tr><tr><td>Noise & Artifacts</td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Physiological (EMG, EOG, ECG), External (Power line, electrode issues, movement)</td><td>Obscures neural signals, introduces spurious patterns, reduces accuracy</td></tr><tr><td>Signal Variability</td><td>Intra-subject (over time), Inter-subject (anatomy, conductivity, placement, state)</td><td>Complicates identifying generalizable patterns across individuals</td></tr><tr><td>Data Scarcity</td><td>Limited number of participants, small dataset sizes</td><td>Hinders training powerful models (especially DL), increases overfitting risk</td></tr><tr><td>Limited Diversity</td><td>Datasets may not represent full SCZ heterogeneity (subtypes,comorbidities, age, medication)</td><td>Limits model generalizability to broader population</td></tr><tr><td>Data Heterogeneity</td><td>Variations across sites (protocols,equipment), clinical data consistency</td><td>Reduces comparability, requires complex harmonization</td></tr><tr><td>Clinical Accuracy</td><td>Potential inaccuracies or incompleteness in associated clinical/self- report data</td><td>May introduce bias in diagnostic labels and outcomes</td></tr></table></body></html>  

A fundamental challenge in leveraging electroencephalography (EEG) for AI-driven schizophrenia diagnosis lies in the quality and variability of the data. Raw EEG signals are inherently susceptible to contamination from various sources of noise and artifacts, making direct processing difficult [9]. These artifacts can originate from physiological sources such as muscle movements (electromyography), eye blinks or movements (electrooculography), and cardiac activity (electrocardiography), as well as external factors like power line interference, electrode contact issues, or adjustments to the electrode cap [1]. The presence of such artifacts distorts the underlying neural signals, potentially masking relevant diagnostic features or introducing spurious patterns, thereby significantly impacting the accuracy and reliability of AI/machine learning models trained on this data. Efforts to address this include careful monitoring and the use of trained operators during recording to minimize artifacts, though challenges remain, particularly in scenarios like single-channel recordings where artifact removal is more complex than in multi-channel systems [13].  

Beyond transient artifacts, EEG signals exhibit considerable variability, both within individuals over time (intra-subject variability) and, more significantly, between different individuals (inter-subject variability). This inherent variability stems from differences in brain anatomy, scalp conductivity, electrode placement, and cognitive state, among other factors. For AI/ML models, this variability complicates the task of identifying robust, generalizable patterns indicative of schizophrenia across a diverse population. The development of models that can effectively handle inter-subject variability without losing sensitivity to subtle diagnostic markers is an active area of research. Approaches like contrastive learning are being explored to align neural representations across different subjects, aiming to extract common features despite individual differences [17]. Although discussed in the context of multi-site neuroimaging data, the challenge of variability across data acquisition environments [7] underscores the broader need for robust methods—potentially including normalization or standardization techniques and validation strategies like inter-site cross-validation—to ensure models generalize well across different settings and populations. While strong correlations between certain EEG features across different bands and locations have been observed [1], the underlying structure contributing to these correlations can still be influenced by inter-subject variability.​  

Furthermore, the advancement of AI in EEG-based diagnosis is constrained by the scarcity and limited diversity of currently available datasets. Many studies rely on private or relatively small cohorts [28], which may not adequately capture the full heterogeneity of schizophrenia presentations, including different subtypes or comorbidities. This limitation is analogous to challenges faced in other neuroimaging domains where the scarcity of diverse datasets can hinder the ability of AI systems to distinguish between closely related disorders [11]. The lack of large-scale, publicly accessible, and diverse EEG datasets representative of the global schizophrenia population impedes the training of powerful deep learning models that typically require vast amounts of data. Consequently, models developed on limited datasets may suffer from poor generalizability to unseen populations or clinical settings [12]. External validation on independent and diverse datasets is crucial for assessing the true clinical applicability of these models, highlighting the need for collaborative efforts to create larger and more representative data repositories [12]. Issues related to the accuracy of associated clinical data, such as potential inaccuracies arising from the use of self-reported information in some contexts [22], also add another layer of complexity to ensuring overall data quality for model development.​  

# 10.2 Methodological Inconsistency  

Current research on AI applications for schizophrenia diagnosis via EEG is often limited by significant methodological inconsistencies and constraints, which impede the generalizability and robustness of findings. A recurring limitation is the reliance on relatively small sample sizes [13]. Furthermore, studies frequently utilize specific or homogeneous patient populations, such as senior participants in a rehabilitation center [13], which restricts the applicability of trained models to the broader, more diverse spectrum of individuals affected by schizophrenia.  

Beyond sample size and population specificity, many studies exhibit a lack of methodological rigor. Critiques highlight the absence of essential validation techniques like cross-validation and limited exploration of feature engineering approaches [28]. This is in contrast to studies that incorporate rigorous methods such as 10-fold cross-validation to evaluate model performance and identify discriminating patterns [12]. While collecting data from multiple sites can introduce methodological inconsistencies, advanced validation techniques like cross-validation can be employed to mitigate these issues [7].​  

Data heterogeneity and a lack of standardized EEG protocols pose substantial challenges. Variations can arise from data collection across multiple sites [7], differences in recording equipment, preprocessing pipelines, and feature extraction methods. Moreover, crucial clinical factors such as age may be overlooked during analysis [9], and imbalances in data distribution or infrequent yet potentially significant biological events may not be adequately addressed [9]. Discrepancies observed between findings from different recording states, such as resting-state versus task-related EEG [5], further underscore the impact of protocol variations on results.  

These pervasive methodological inconsistencies and data variations create significant challenges for generalizing AI models trained on one dataset to different populations or clinical settings. Models developed on specific datasets using particular protocols may not perform effectively when applied to data collected under different conditions or from diverse patient groups. Therefore, there is a critical need for more robust validation strategies. Cross-sectional study designs, for instance, inherently limit the ability to establish causality and necessitate further validation through prospective studies [22]. Future research must prioritize validation across diverse populations, varying clinical settings, and utilizing standardized or harmonized EEG acquisition and processing protocols to ensure the reliability and clinical utility of AI models for schizophrenia diagnosis.​  

# 10.3 Model Generalization and Overfitting  

Overfitting represents a critical challenge in the development of machine learning models for EEG-based diagnosis, particularly in conditions like schizophrenia. This phenomenon occurs when a model learns the training data too well, capturing noise and specific patterns that do not generalize to unseen data. The primary implication of overfitting is poor performance on new, independent datasets, which is detrimental to the reliability and applicability of AI models in clinical settings. For instance, relying solely on a single train/test split without robust validation procedures can directly lead to overfitting and consequently impair generalization performance on novel data [28].  

To mitigate overfitting and enhance model generalizability, researchers employ a variety of techniques. Cross-validation is a widely adopted strategy for evaluating model performance and ensuring robustness. K-fold cross-validation, such as the 10- fold approach utilized in one study to evaluate classification performance and identify discriminating profiles, helps to provide a more reliable estimate of how the model will perform on unseen data [12]. Furthermore, using both crossvalidation and a separate, untouched test set is crucial for preventing overfitting and guaranteeing model performance for generalization [30].  

The generalizability of specific biomarkers has been demonstrated through inter-site cross-validation, suggesting that models developed with these biomarkers are less likely to be overfitted to data from a single recording environment [7].  

Beyond standard cross-validation, techniques like using 1000 bootstrap resamples have been applied to improve accuracy estimation and reduce overfitting bias in accuracy metrics [22].  

Regularization techniques and strategic model architecture choices also play a significant role in controlling model complexity and preventing overfitting. These include L2-regularization and dropout criteria, which were used in one study to address overfitting issues [14]. Similarly, the combination of learning rate scheduling, batch normalization, early stopping, and dropout regularization has been effectively employed to combat overfitting in neural network models [35]. The strategy of using pre-extracted EEG features that have been validated in previous studies performed on specific populations, such as healthy young subjects, can also serve as a form of regularization by constraining the feature space and focusing the model on established patterns [13]. More advanced algorithmic approaches, such as employing meta-learning methods like AdaBoost to produce base-learner classifiers from extracted features, have also been investigated for their potential in mitigating "overfiring issues," likely referring to overfitting, and improving accuracy [9]. Innovative methods like Contrastive Learning for Subject Alignment (CLISA) aim explicitly at improving generalization by learning common representations across different subjects, thereby enhancing the model's ability to generalize to new individuals [17].  

Despite these techniques, generalizing AI models for EEG-based schizophrenia diagnosis across diverse populations, varied recording settings, and different EEG devices remains a substantial challenge. Differences in demographic factors, genetic backgrounds, symptomology presentation, acquisition protocols, and hardware specifications can introduce significant variability in EEG data. While inter-site cross-validation offers a means to test generalization across different locations [7,17], the inherent heterogeneity of clinical populations and recording conditions necessitates robust validation strategies and models capable of learning invariant or adaptable representations to ensure reliable performance in real-world diagnostic applications.​  

# 10.4 Interpretability and Explainability  

The integration of artificial intelligence (AI) and machine learning (ML) models into clinical practice for schizophrenia diagnosis necessitates a strong emphasis on interpretability and explainability. Clinical acceptance and trust are paramount, as clinicians require an understanding of the reasoning behind a model's prediction before relying on it for patient care [38]. The inherent complexity of many sophisticated models, particularly deep neural networks (DNNs), presents a significant challenge, often leading to a "black box" issue where the internal decision-making process is opaque [10]. This lack of transparency is compounded by the automatic feature extraction capabilities of deep learning methods [11], which, while automating the pipeline from raw data to classification, can obscure the specific features driving a prediction. Simpler models, such as the K-Nearest Neighbors (KNN) algorithm, can also lack interpretability, hindering the understanding of underlying neurophysiological mechanisms relevant to the classification task [28].  

<html><body><table><tr><td>Technique/Approach</td><td>Description</td><td>Goal/Benefit</td><td>Examples in EEG/Neuroscience</td></tr><tr><td>Feature Attribution</td><td>Quantifying contribution of individual features to model output</td><td>Identify key features driving prediction, highlight relevant biomarkers</td><td>SHAP values, Layer- wise Relevance Propagation (LRP)</td></tr><tr><td>Model Analysis</td><td>Analyzing model responses to data perturbations or internal states</td><td>Understand how specific data changes affect output, visualize learned patterns</td><td>Perturbation analysis,Integral Gradient methods, Visualizing kernels</td></tr><tr><td>Feature Importance</td><td>Ranking features based on their significance in the model (often classifier-based)</td><td>Identify most discriminative features</td><td>Feature ranking with Random Forests, Model coefficient analysis (e.g. LASSO)</td></tr><tr><td>Visualization</td><td>Presenting model insights or data</td><td>Aid human understanding of</td><td>Visualizing covariance matrices,</td></tr></table></body></html>  

<html><body><table><tr><td></td><td>patterns visually</td><td>complex data/model behavior</td><td>Nomograms, Heatmaps</td></tr><tr><td>Model-based XAl</td><td>Using models that inherently perform variable selection</td><td>Select sparse set of features while building model</td><td>LASSO, Elastic Net</td></tr><tr><td>Integrating XAl & Domain Knowledge</td><td>Combining computational explanations with established clinical/biological understanding</td><td>Enhance biological plausibility, build clinician trust, refine explanations</td><td>Linking Al-identified patterns to known mechanisms/biomar kers</td></tr></table></body></html>  

To address the "black box" problem and facilitate clinical translation, explainable AI (XAI) techniques are increasingly crucial [10]. Various XAI methods are being explored and applied to understand how these complex models arrive at their predictions, enhancing transparency and providing verifiable insights.  

Several techniques focus on identifying and quantifying the importance of features or variables contributing to a model's output. Shapley Additive exPlanation (SHAP) values, for instance, are utilized to clarify the underlying features that influence a model's predictions, offering insight into the decision-making process by assigning a contribution value to each variable across various combinations [25,35]. Studies exploring different XAI methods have found SHAP to be effective in highlighting crucial data features [14]. Layer-wise Relevance Propagation (LRP) is another explainability technique employed to perform feature selection and provide clinically relevant insights by distributing the prediction relevance back through the network layers [10]. Furthermore, machine learning algorithms can be leveraged for variable selection, enabling the ascertainment of a variable's contribution to the model's performance [12].​  

Other XAI approaches involve analyzing model responses to data perturbations or visualizing learned patterns. Perturbation analysis can be used to identify specific data characteristics, such as frequency ranges or electrode locations in EEG signals, that are most informative for diagnostic classification [1]. Integral gradient methods provide insight into features learned by deep models by identifying important features and visualizing corresponding spatial and temporal patterns within convolutional kernels [17]. Feature importance analysis, often used with methods like Random Forests, can rank the significance of hand-crafted features [1]. Visualization techniques, such as visualizing the values of class covariance matrix differences in Riemannian geometry-based decoding pipelines [1] or utilizing visual tools like nomograms to represent predicted probabilities [22], also aid interpretability and clinical utility. While techniques like GradCAM and LIME are also explored for model interpretation, comparative studies suggest varying effectiveness in capturing nuanced data patterns [14].​  

A critical opportunity in developing more interpretable AI models lies in connecting model-derived insights to established clinical and biological understanding. For example, linking patterns identified by AI models to known biological mechanisms, such as correlating striatal hyperactivity with dopaminergic function and genetic risk profiles in schizophrenia, enhances the biological plausibility and clinical relevance of the AI's findings [7]. This integration of data-driven patterns with mechanistic knowledge is vital for building trust and enabling the effective translation of AI models into diagnostic tools for complex conditions like schizophrenia. Developing methods that inherently provide interpretable feature representations or that can effectively explain predictions post-hoc remains an active area of research, balancing the need for high predictive performance with the imperative for transparency and clinical utility.  

# 10.5 Ethical Considerations  

The application of artificial intelligence (AI) in the diagnosis of schizophrenia, particularly through the analysis of electroencephalography (EEG) data, introduces a spectrum of ethical considerations that necessitate careful examination. Foremost among these are concerns surrounding data privacy and security, which represent a significant challenge in the handling of sensitive patient data [20,27]. The use of large datasets for training complex machine learning (ML) and deep learning (DL) models raises critical questions about safeguarding patient confidentiality and preventing unauthorized access or breaches [27].​  

Another key ethical challenge is algorithmic bias [27]. Bias in AI models can arise from unrepresentative or skewed training data, potentially leading to differential performance across various demographic groups. In the context of schizophrenia diagnosis, biased algorithms could result in misdiagnosis, delayed treatment, or disparities in healthcare access for certain populations, thereby undermining the principles of fairness and equity. Rahul et al. specifically highlighted ethical concerns related to the use of ML and DL models for EEG-based schizophrenia detection, underscoring the importance of addressing such potential biases [6].​  

Furthermore, the potential for misuse of AI technology in diagnostic settings presents a considerable risk. This could involve the application of models outside their validated scope, over-reliance on AI outputs without clinical oversight, or the exploitation of diagnostic tools for non-therapeutic purposes. Such misuse could have detrimental impacts on patient wellbeing and trust in healthcare systems.​  

Addressing these challenges mandates a commitment to ensuring fairness, transparency, and accountability throughout the development and deployment lifecycle of AI-based diagnostic tools. Transparency involves making the decision-making processes of AI models understandable, at least to healthcare professionals, to build trust and facilitate appropriate clinical interpretation. Accountability requires establishing clear lines of responsibility for the performance and impact of AI tools. Ensuring fairness necessitates proactive steps to identify and mitigate algorithmic bias, promoting equitable outcomes for all patients.​  

The ethical considerations extend directly to the integration of AI into clinical decision-making for schizophrenia. Beyond data and algorithmic issues, this involves questions of informed consent and the appropriate role of AI as a support tool rather than a replacement for clinical expertise. Studies utilizing EEG data in clinical contexts have demonstrated ethical diligence by obtaining ethical approval from relevant committees and securing Informed Consent Forms (ICF) from participants in accordance with guidelines such as the Declaration of Helsinki [13]. This underscores the importance of respecting patient autonomy and ensuring they are fully informed about the use of their data and the role of AI in their assessment. Responsible development and deployment frameworks are essential to navigate these complex ethical landscapes, ensuring that AI technologies genuinely benefit patients while upholding fundamental ethical principles [27].  

# 11. Future Directions and Research Opportunities  

The application of artificial intelligence (AI) to electroencephalography (EEG) data holds significant promise for advancing the diagnosis and understanding of schizophrenia, yet substantial research opportunities and directions remain to address current limitations and gaps. A key area for future exploration involves the development of novel and more robust AI algorithms and architectures. This includes exploring advanced deep learning (DL) architectures to enhance accuracy and robustness [3,22,23], potentially combining machine learning (ML) methods with DL in hybrid frameworks [11]. Future work should focus on more powerful representation learning techniques that can automatically extract robust, generalizable features, perhaps guided by principles like functional alignment across individuals [17]. Further research is also needed to understand the specific contributions of different feature types, such as sensor-level versus source-level EEG features, and to investigate the functional significance of identified neural correlates [4].  

Another critical direction is the integration of EEG data with other modalities to improve diagnostic accuracy and provide a more comprehensive understanding of schizophrenia [5,11,20,23]. Combining EEG with neuroimaging modalities like fMRI or sMRI [11], or with other biological data such as genetic information, gut microbiota, and blood parameters, can enhance discriminative analysis and contribute to a holistic view [5,7,30]. However, realizing the full potential of multimodal integration necessitates addressing the significant challenges posed by data heterogeneity [11]. This requires the development and application of sophisticated data fusion techniques capable of effectively combining diverse data streams with different characteristics, scales, and resolutions [11].  

Leveraging AI/ML for personalized diagnostic and treatment strategies based on individual characteristics represents a significant opportunity [22,23,29]. AI/ML models can identify unique patterns and biomarkers in individual patient data, potentially predicting treatment responses [7] or pinpointing targets for intervention [12]. This aligns with the broader trend towards personalized medicine across neurological and psychiatric conditions, facilitating tailored clinical management and potentially real-time monitoring [20,37].  

For AI models to be effectively adopted in clinical practice, further research into Explainable AI (XAI) techniques is essential to improve interpretability and clinical utility [28]. Interpretability is crucial for clinician trust and provides valuable insights into the underlying neurobiological mechanisms highlighted by the AI model [14,28]. XAI methods can help identify which specific EEG characteristics are most influential in a model's prediction, potentially revealing novel biomarkers [35]. Collaboration between AI researchers and clinical domain experts is needed to refine these tools for practical application [14].​  

A foundational requirement for advancing this field is the development of standardized EEG protocols and the creation of large-scale, diverse, and well-annotated datasets [22,23]. Standardization across data collection sites is necessary to reduce bias and ensure comparability [10,20]. Large, diverse datasets are paramount for developing robust and generalizable AI models and for enabling rigorous validation in independent datasets and clinical settings [3,11,12,20,22,35,38]. Such resources are also vital for enabling longitudinal studies and exploring the utility of AI/ML tools for large-scale screening and early intervention [13,31].  

Finally, accelerating progress in this complex area necessitates robust interdisciplinary collaboration between neuroscientists, computer scientists, engineers, and clinicians [14]. Developing unified platforms capable of integrating and analyzing multi-modal data streams is also suggested to revolutionize detection and monitoring of neurological diseases, applicable to schizophrenia as well [27].​  

# 11.1 Advanced AI Model Development  

The development of advanced artificial intelligence (AI) algorithms is crucial for enhancing the accuracy, reliability, and generalizability of electroencephalography (EEG)-based diagnosis of schizophrenia. While studies have utilized established techniques such as Support Vector Machines (SVM) [3], there is a recognized need to explore more sophisticated and advanced AI models to improve diagnostic performance and facilitate individualized EEG analysis [3,22,23].​  

Future research directions emphasize the potential of exploring novel AI architectures and training methodologies to overcome limitations inherent in current models. Deep learning (DL) architectures, for instance, are suggested as a promising avenue to enhance the accuracy and robustness of EEG-based classification, although their application has also been explored in related domains such as mood disorder classification [28]. A key distinction in DL approaches is their capacity for intelligent feature extraction, which can obviate the need for manual feature engineering [11]. However, this implicit feature extraction may sometimes lead to performance differences compared to traditional machine learning (ML) methods that rely on supervised feature selection [11]. Consequently, combining ML and DL methodologies within hybrid frameworks represents a potential strategy to leverage the strengths of both approaches for improved outcomes in computer-aided diagnosis systems [11].  

Beyond specific architectures, exploring more powerful representation learning methods is also essential. Techniques aimed at automatically learning temporal and spatial filter patterns, potentially guided by objectives like "functional alignment" across different individuals, could yield more robust and generalizable representations of EEG data [17]. Such advancements in training methodologies are vital for creating models that generalize effectively across diverse patient populations and recording conditions. Ultimately, the exploration of new algorithms and architectures is foundational to improving the diagnostic accuracy and reliability of AI systems in this field [16]. Features identified in prior studies, such as gamma power and functional connectivity from resting-state EEG, can serve as valuable inputs for these advanced AI/ML models to further refine classification and potentially predict treatment response [5].​  

# 11.2 Enhanced Multimodal Integration  

Integrating electroencephalography (EEG) data with other biological and neuroimaging modalities holds significant potential for enhancing the diagnosis of schizophrenia and providing a more comprehensive understanding of its underlying neurobiological mechanisms [5,11,23]. While EEG offers valuable insights into neuronal activity with high temporal resolution, its limitations in spatial resolution can be complemented by modalities such as functional magnetic resonance imaging (fMRI) or structural MRI [11,20]. Beyond neuroimaging, the integration of EEG data with genetic information or other multi-biological data, including gut microbiota and blood parameters, is suggested to improve discriminative analysis and contribute to a more holistic view of the disorder [5,7,30]. Such multimodal fusion, encompassing various data types like neuroimaging, molecular imaging, and transcriptomic data, is anticipated to improve diagnostic accuracy [7].  

The realization of the benefits of multimodal integration is contingent upon the availability of appropriate datasets [11,16]. The development of large-scale, well-curated multi-modal datasets is considered crucial for advancing research in the diagnosis of brain disorders [11]. A significant challenge in multimodal data integration is the inherent heterogeneity of the data types involved, which can originate from disparate sources and possess different characteristics, scales, and resolutions. Effectively combining these diverse data streams necessitates the development and application of sophisticated data fusion techniques. These techniques must address issues such as data alignment, normalization, and the identification of complementary or synergistic information across modalities to build models capable of achieving improved diagnostic performance and deeper insights into the complexities of schizophrenia [11]. The application of AI models to routinely collected multimodal clinical data in other neurological contexts highlights the practical utility of such integrated approaches [19].​  

# 11.3 Personalized Approaches  

The application of artificial intelligence (AI) and machine learning (ML) holds significant potential for advancing personalized diagnostic and treatment strategies in clinical neuroscience, including the complex domain of schizophrenia. The inherent variability in patient presentation and treatment response underscores the need for tailored interventions. ML and deep learning algorithms are instrumental in promoting the continuous development of research towards individualization, particularly in the analysis of complex neurophysiological data such as electroencephalography (EEG) [23].  

By analyzing individual patient data, AI/ML models can identify unique patterns and biomarkers that distinguish patient subgroups or predict individual responses. For instance, studies have demonstrated the capability of specific markers, such as Functional Striatal Activation (FSA) scores derived from individual patient data, to predict antipsychotic treatment response [7]. This ability to forecast treatment outcomes based on individual characteristics is a crucial step towards optimizing therapeutic interventions for schizophrenia.  

Beyond treatment prediction, AI/ML can also aid in refining diagnostic processes and identifying specific targets for intervention based on individual profiles. Research in related neurological and psychiatric conditions highlights this potential, where unique discriminating features identified through ML analysis have pointed towards specific target areas for early intervention programs, such as attention and empathy in differential diagnosis contexts [12]. While this example pertains to other disorders, the underlying principle of using ML to pinpoint individual-level features relevant to pathology translates directly to the potential for identifying personalized diagnostic indicators or targets for intervention in schizophrenia.​  

The application of ML extends to enabling personalized treatment management and real-time monitoring in neurological disorders [20]. Similarly, predictive models aimed at enhancing treatment outcome prediction, such as in epilepsy management, exemplify a move towards personalized approaches based on individual patient data, even when leveraging multimodal data sources [37]. These examples from related fields underscore the broader capability of AI/ML to utilize individual data for tailoring clinical management strategies.  

Ultimately, leveraging AI/ML to analyze individual patient data—including neurophysiological markers from EEG—facilitates the development of individualized diagnostic and treatment plans [22]. This shift from generalized approaches to patientspecific strategies, driven by data-informed insights, promises to lead to more effective, targeted, and potentially less burdensome interventions for individuals with schizophrenia, reflecting a broader trend towards personalized medicine across neurological and psychiatric conditions [20,37].  

# 11.4 Developing Explainable AI (XAI)  

The successful integration of artificial intelligence (AI) models into clinical practice for schizophrenia diagnosis using electroencephalography (EEG) necessitates a focus on model interpretability and transparency [28]. While complex models often achieve high performance, their black-box nature can impede adoption by clinicians who require an understanding of the rationale behind a diagnostic prediction [28]. Consequently, a crucial area for future research lies in the development and application of Explainable AI (XAI) techniques within EEG analysis [14,28].​  

Improving the interpretability of AI models serves multiple critical functions. Firstly, it allows clinicians to comprehend the specific features or patterns in the EEG data that contribute to a model's diagnostic output, thereby enhancing trust in the technology and facilitating clinical acceptance [28]. An opaque model, regardless of accuracy, is unlikely to be readily adopted in high-stakes medical decision-making environments. Secondly, interpretability analysis in model development is essential not only for validating the model's decision process but also for potentially yielding novel neurobiological insights [14].​  

By revealing which EEG characteristics (e.g., specific frequency bands, electrode locations, connectivity patterns) are deemed most significant by the AI for distinguishing schizophrenia, XAI techniques can point towards potential biomarkers or underlying neural mechanisms. For example, techniques such as SHAP (SHapley Additive exPlanations) values can be employed to quantify the contribution of individual features to a model's prediction, offering insights into the critical regions or attributes influencing the output [35]. Although this approach has been demonstrated in studies involving image data, the same principle applies to EEG analysis, where understanding feature contributions might highlight specific brain regions or oscillatory activities relevant to schizophrenia pathology [35].​  

Thus, advancing XAI in this domain is vital for both increasing clinical utility and potentially advancing neurobiological understanding.  

# 11.5 Standardized Protocols and Datasets  

Advancing the application of artificial intelligence (AI) in schizophrenia diagnosis via electroencephalography (EEG) critically hinges upon the establishment of standardized protocols and the availability of large, diverse, and well-annotated datasets. The development of multi-center studies and the creation of extensive databases are recognized as essential steps to enhance the reliability and robustness of research findings in psychiatric diagnoses [23]. Similarly, the integration of diverse data sources, such as relevant laboratory and imaging data, is deemed necessary to improve diagnostic accuracy, highlighting the broader need for comprehensive datasets beyond just EEG [22].​  

Standardization of data collection protocols is fundamental to mitigating potential biases introduced by variations in equipment, procedures, and environmental factors across different research sites [20]. This is analogous to the recognized need for standardizing magnetic resonance imaging (MRI) data from various datasets to counteract biases arising from differences in image contrasts produced by disparate imaging equipment [10]. Implementing standardized protocols ensures consistency and comparability across studies, which is vital for aggregating data from multiple sources and for the replicability of results.​  

Furthermore, the creation of large, diverse, and representative datasets is paramount for developing AI models with high reliability and generalizability [20]. The capacity of machine learning models to generalize effectively to unseen data is directly dependent on the size, variety, and representativeness of the training data [20]. Replication studies using independent, large test sets of completely unseen data are considered necessary to properly assess the generalizability of ML models, underscoring the need for such extensive datasets [12]. Diverse datasets that capture the variability within the schizophrenic population, as well as appropriate control groups, help prevent models from overfitting to specific characteristics of a small or homogenous sample.  

Beyond facilitating model development and validation, standardized protocols and accessible datasets can also promote the translation of research findings into clinical practice. Cost-effective and easily deployable tools, enabled by widely accepted and standardized assessment methodologies, are crucial for enabling earlier detection and wider accessibility of diagnostic approaches in community settings [13]. Therefore, investing in the infrastructure for standardized data collection and repository development is not merely a technical requirement but a strategic imperative for accelerating progress and ensuring the clinical utility of AI-powered EEG diagnostics for schizophrenia.​  

# 12. Conclusion  

This survey has explored the significant potential of integrating artificial intelligence (AI) with electroencephalography (EEG) for enhancing the diagnosis of schizophrenia (SZ). The research reviewed highlights the capacity of AI-based EEG analysis to improve the accuracy, efficiency, and objectivity of diagnostic processes, offering a promising avenue for complementing traditional clinical assessments [6,24,36].​  

Key findings from the analyzed literature demonstrate the feasibility and effectiveness of various machine learning (ML) and deep learning (DL) approaches applied to EEG data for classifying psychiatric and neurological conditions, including SZ [1,19]. Studies have successfully identified electrophysiological markers in resting-state EEG that differentiate SZ patients from healthy controls, revealing significant differences in power spectra and brain network topology [5,8]. Furthermore, combining different types of EEG features, such as sensor and source-level information, has been shown to substantially improve diagnostic accuracy, reaching classification rates as high as $8 8 . 2 4 \%$ with methods like Support Vector Machines (SVM), suggesting that source-space data can function as valuable EEG biomarkers for SZ [4]. The application of advanced techniques like ensemble learning and multimodal integration has further boosted classification performance and holds  

potential for improving clinical diagnosis and treatment evaluation [10,30,37]. The development of objective EEG feature extraction methods, potentially from wearable systems, also suggests improved detection rates and opportunities for earlier intervention [13]. Moreover, AI's expanding role in teleneurology indicates its potential to streamline workflows and increase access to remote healthcare services, which is highly relevant for extending the reach of advanced diagnostics in mental health [27].  

Despite these advancements, the widespread adoption of AI-based EEG for SZ diagnosis faces notable challenges and limitations that require concerted future research efforts. A primary concern is the critical need for larger, more diverse, and high-quality datasets to train and validate models effectively, mitigating issues like training data biases and the risk of overfitting observed in smaller studies [13,20,28]. Differences in data quality across studies also pose challenges [20]. Furthermore, the interpretability of complex AI models remains a significant hurdle for clinical acceptance and trust among healthcare professionals [14,20,35]. While explainable AI (XAI) techniques are being explored and show promise in providing insights, their limitations necessitate refinement and integration with domain expert knowledge [14]. Robust validation and generalization of findings to diverse patient populations and clinical settings are essential to translate research outcomes into practical diagnostic tools [8,13]. Addressing these limitations requires improving data preprocessing techniques, expanding and optimizing training datasets, and focusing on the development of more interpretable and generalizable AI models [20,23,28].​  

In conclusion, the integration of AI and EEG holds transformative potential for the future of mental health care, particularly in enhancing the diagnosis of schizophrenia. By leveraging AI to analyze the complex patterns within EEG data, clinicians may gain access to more objective, efficient, and accurate diagnostic support. Continued research is paramount to overcome current limitations, emphasizing the need for larger datasets, advancements in model interpretability, and rigorous validation across diverse populations. The successful and responsible implementation of AI-based diagnostic tools necessitates close collaboration among AI researchers, neuroscientists, and clinical practitioners, alongside careful consideration of ethical implications. This ongoing effort to "decipher the EEG code" of mental illnesses represents a critical frontier in improving diagnostic capabilities and ultimately advancing patient outcomes [23].  

# References  

[1] 基于机器学习的脑电病理诊断：特征工程与深度学习的比较研究 https://cloud.tencent.com/developer/article/1802083   
[2] AI for Schizophrenia Detection: A Comprehensive Su https://link.springer.com/10.1007/s11042-022-13809-9   
[3] 联合EEG电极与溯源空间特征的机器学习方法诊断精神分裂症   
https://blog.csdn.net/weixin_41880581/article/details/112578062​   
[4] 联合EEG电极和溯源空间特征的机器学习诊断精神分裂症 https://baijiahao.baidu.com/s?   
id=1735117831877479508&wfr=spider&for=pc​   
[5] 首发精神分裂患者静息态EEG γ频段功能连接增强的溯源分析 https://cloud.tencent.com/developer/article/1765352   
[6] AI and ML for Neurological Disorder Diagnosis https://www.frontiersin.org/journals/human  
neuroscience/articles/10.3389/fnhum.2025.1558584/abstract​   
[7] Striatal Dysfunction Biomarker for Schizophrenia:  http://www.brainnetome.org/rh/202003/t20200324_548068.html   
[8] 基于脑电特征和机器学习的精神分裂症分类研究 http://cnki.nbsti.net/KCMS/detail/detail.aspx?   
filename=1021748364.nh&dbcode=CMFD&dbname=CMFD2022   
[9] Machine Learning for Multi-Modal Sleep Stage Class https://link.springer.com/article/10.1186/s12911-024-02522-2   
[10] Multi-modal Deep Learning for Schizophrenia Classi   
https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2024.1384842/full​   
[11] AI-Assisted ASD Detection via MRI: A Review https://www.frontiersin.org/journals/molecular  
neuroscience/articles/10.3389/fnmol.2022.999605/full​   
[12] Machine Learning for Differential Diagnosis of Soc http://dx.doi.org/10.3389/fpsyt.2020.00545   
[13] Single-Channel EEG Features Correlate with Cogniti https://www.frontiersin.org/articles/10.3389/fnagi.2022.773692/full   
[14] Interpretable AI for ECG Classification: Exploring https://dl.acm.org/doi/fullHtml/10.1145/3643488.3660294   
[15] 严颖：自动化学院讲师个人信息及研究简介 https://faculty.nuist.edu.cn/yanying/zh_CN/index/110297/list/   
[16] AI技术在基于MRI的精神分裂症诊断中的应用、挑战与未来 https://blog.csdn.net/Sherlily/article/details/137190932   
[17] 对比学习：一种提取跨个体共性脑电表征的新方法 https://mp.weixin.qq.com/s? _biz=MzU2OTQwOTgxNw $\scriptstyle 1 = =$ &mid=2247485853&idx $\vDash$ 1&sn=a3475a46de9ca4a87083bc1810702c4c&chksm=fcfe61ebcb89e8fd   
4425ea7444de4bf9f216f26f6ee46c5445500a7c1de7d4657c146b7ba28d&scene=27​   
[18] 睡眠障碍对轻度阿尔茨海默病患者静息态脑功能的影响研究 https://xuewei.bjmu.edu.cn/docinfo.action?   
id1=a94d3458fffc14df37b76c3dc5f28557&id2 $\vDash$ 9NZ1i54NObg%253D   
[19] AI-Powered Differential Diagnosis of Dementia https://www.nature.com/articles/s41591-024-03147-8​   
[20] 机器学习赋能脑电图：多发性硬化症诊疗的新机遇 https://www.ebiotrade.com/newsf/2025-4/20250430114446613.htm​   
[21] Pharmacological Interventions for Social Cognition   
https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.878829/full​   
[22] ML-Based Nomogram for Predicting Self-Reported OA https://link.springer.com/article/10.1038/s41598-024-83524-y​   
[23] 破译脑电密码：揭秘精神疾病的生物学标记物 http://baijiahao.baidu.com/s?id $\ c =$ 1724499511323101081&wfr=spider&for=pc   
[24] MLESP 2024: Machine Learning for EEG Signal Proces https://mlesp2024.sciencesconf.org/​   
[25] Insomnia Associations in College Students: A Machi https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-   
024-06074-7​   
[26] 华南理工大学医学信息及神经影像研究室科研论著 https://www.scholat.com/vpost.html?pid=133891​   
[27] AI-Powered Teleneurology: Advancing Diagnostics wi   
https://www.frontiersin.org/journals/neurology/articles/10.3389/fneur.2025.1582142/abstract​   
[28] 基于EEG脑电信号的抑郁症识别分类 https://blog.csdn.net/YINTENAXIONGNAIER/article/details/130042231​   
[29] AI in Cancer Precision Treatment: A Review of Clin https://link.springer.com/article/10.1186/s12967-025-06139-5​   
[30] Brain Atlases and Machine Learning Impact Schizoph   
https://www.frontiersin.org/articles/10.3389/fnins.2021.697168/full​   
[31] Eye-Tracking Paradigms for Assessing Mild Cognitiv   
https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1197567/full​   
[32] 第七届神经信息国际会议：脑器交互与心身医学 https://mp.weixin.qq.com/s?   
__biz=MzIzMjQyNzQ5MA $\scriptstyle 1 = =$ &mid=2247632660&idx $\mathrel { \mathop : }$ 3&sn $\mid =$ 188d272894144ce229b4805fe21c492d&chksm $\mid =$ e8999d99dfee148f7   
0ac3221f7ec680f117b3d33becea7f9dc7e43a47770df8a5bbeb5aae0a5&scene=27   
[33] BMC Neuroscience: Cellular, Functional, and Develo https://www.ncpssd.org/journal/details?   
gch=25489&nav=1&langType=2​   
[34] EEG-Based Emotion Recognition in HRI with Optimize https://dl.acm.org/doi/10.1007/978-3-031-35894-4_21​   
[35] Multimodal Dementia Prediction Using Machine Learn https://link.springer.com/article/10.1007/s41870-024-02326-7​   
[36] 2024年论文集锦 http://www.innovation2030.zju.edu.cn/2019/0910/c21970a1657892/page.htm​   
[37] Multimodal Deep Learning Predicts Antiseizure Medi https://www.medrxiv.org/content/10.1101/2025.03.12.25323644v1   
[38] Machine Learning for New-Onset ACS Risk Prediction https://www.frontiersin.org/journals/public  
health/articles/10.3389/fpubh.2022.947204/full​  