# 5/1/2025, 6:27:24 PM_Large Language Models in Chemistry  

# 0. Large Language Models in Chemistry  

# 1. Introduction  

The growing volume of chemical literature, particularly patents detailing synthesis processes, has spurred significant interest in leveraging artificial intelligence (AI) techniques for automated information extraction and accelerated discovery, especially within the realm of drug development [40]. The rapid expansion of scientific data underscores the necessity for advanced AI technologies to transcend human cognitive limitations in processing and synthesizing complex information, thereby accelerating the pace of scientific discovery [3]. The integration of AI is fundamentally altering the landscape of chemical research by enhancing the accuracy of computational simulations, optimizing synthesis pathways, automating laboratory procedures, and expediting the discovery and development of new materials [7,10,28].​  

However, the application of conventional natural language processing (NLP) tools to scientific texts in chemistry and materials science presents considerable challenges. These difficulties stem primarily from the highly specialized terminology and the intricate domain-specific reasoning inherent in chemical concepts and discourse [15,18]. Beyond the linguistic domain, traditional experimental and computational methodologies in various chemical fields exhibit inherent limitations. For instance, established drug discovery techniques, such as high-throughput screening and combinatorial chemistry, face significant hurdles in efficiently managing large datasets and comprehensively exploring the vast potential chemical space [2,36]. Similarly, conventional computer-aided drug design (CADD) methods have often been restricted by their reliance on potentially biased human expertise and a limited scope of chemical space exploration [16]. In the domain of chemical synthesis, existing machine learning tools are frequently designed for specific, narrow applications, preventing the realization of fully autonomous, end-to-end reaction design processes [44]. Traditional approaches to optimizing complex reactions, such as electrochemical C-H oxidation, typically necessitate extensive and time-consuming experimental screening [6]. Overcoming these deeply rooted limitations is critically important for substantially accelerating fundamental and applied research in chemistry, including the development of novel pharmaceuticals and materials, gaining deeper insights into molecular interactions and dynamic behaviors [25,40], and optimizing complex chemical processes.​  

Building upon the significant advancements in general AI and machine learning applications within chemistry [8,20,33], Large Language Models (LLMs) have emerged as a potentially transformative technology [7,28]. LLMs possess distinct capabilities in processing extensive textual data and facilitating interaction through natural language interfaces, thereby opening novel avenues for predictive modeling and knowledge retrieval in chemistry [29]. The potential of LLMs to fundamentally reshape routine chemical research has been widely recognized [26]. This potential is being actively explored through various initiatives, including the development of chemistry-specific LLMs like ChemLLM, specifically designed to integrate structured chemical knowledge and handle unique symbolic representations [4,23]. Furthermore, domain-specific platforms such as SynAsk are being developed for tasks like organic synthesis prediction [13,22]. Another critical area involves the integration of LLMs with laboratory automation and external chemical tools to create autonomous systems, such as Coscientist and ChemCrow, capable of complex experimental design and execution [5,12,27,38].  

This survey provides a comprehensive overview of the rapidly evolving landscape concerning the application of Large Language Models in Chemistry. It aims to synthesize the significant advancements made, analyze the specific technical and domain-related challenges encountered when applying LLMs to diverse chemical tasks, and outline promising future directions for this interdisciplinary field, offering a structured perspective on the state of the art [4].​  

# 2. Fundamentals, Architectures, and Methodologies  

This section provides an overview of the foundational aspects underpinning the application of large language models (LLMs) in chemistry. It focuses specifically on the core Transformer architecture, relevant molecular representations, and the strategies employed for model training and adaptation.  

The Transformer architecture, which is central to most modern LLMs, relies on mechanisms such as self-attention to capture long-range dependencies within sequential data [20,42]. This capability is crucial for processing various chemical data representations including sequential formats like SMILES strings, SELFIES, and biological sequences such as peptides and proteins [9,21,30,39]. The architecture’s ability to handle such sequences enables LLMs to perform tasks analogous to natural language processing in the chemical domain, such as generating novel molecular structures (text generation), converting between representations (translation), and engaging in the complex reasoning required for autonomous research systems [5,9,11,12,21,24,30,38].​  

Effective application of LLMs in chemistry necessitates a careful consideration of molecular representation. While historical methods relied on fixed descriptors and fingerprints, the rise of deep learning—particularly through Transformers and graph neural networks (GNNs)—has favored variable-length representations such as SMILES, SELFIES, and molecular graphs [16,21,37,42]. Sequence-based representations are naturally processed by Transformer models, whereas graph-based representations, which explicitly capture atomic connectivity and sometimes spatial information, are typically handled by GNNs [2,16,37]. The choice of representation involves trade-offs between data density, spatial information encoding, and compatibility with model architectures. Hybrid or multimodal approaches combining different representations, such as SMILES and molecular graphs, can enhance model performance for complex tasks like polymer property prediction [32]. Furthermore, adapted Transformer models like Graphormer demonstrate efforts to apply attention mechanisms directly to graph structures [20,25].​  

Adapting general-purpose LLMs or training new models for chemistry relies on a variety of training strategies. The predominant paradigm involves pre-training on large datasets followed by fine-tuning on smaller, task-specific chemical data [42]. Instruction fine-tuning is a key technique for aligning models such as ChemLLM with chemical tasks and enabling natural dialogue interaction [4,23]. Transfer learning is widely employed to leverage knowledge from related tasks or larger datasets, which proves particularly beneficial in data-scarce scenarios such as specific reaction predictions [24]. In addition, semi-supervised and unsupervised learning methods—including techniques like pseudo-labeling with calibration, label rebalancing, and latent space augmentation—are used to mitigate the reliance on extensive amounts of labeled chemical data, as demonstrated by models like ChemMatch [18]. Specialized models tailored for particular domains, such as SynAsk for organic chemistry, utilize fine-tuning and integration with external resources [13,22]. While foundational models like GPT4 can be applied directly using sophisticated prompting for autonomous systems [38,44], fine-tuning remains crucial for optimizing performance on specialized chemical tasks.​  

Comparing and contrasting different architectures in the chemical domain highlights the strengths of Transformers for sequence processing and capturing global context, whereas GNNs excel at capturing explicit graph structures and local connectivity. Although this section primarily focuses on Transformer-based LLMs, related deep learning architectures such as recurrent neural networks (RNNs) and GNNs are also applied in chemistry—often in scenarios where they are better suited or as components within hybrid models [2,42]. The selection and configuration of these architectures, as well as the employed training methodologies, are critical determinants of an LLM's overall performance and its suitability for diverse chemical applications.​  

# 2.1 Transformer Architecture and Core Concepts  

The Transformer architecture serves as the foundational framework for the majority of contemporary Large Language Models (LLMs) and has demonstrated significant efficacy in processing sequential data, including chemical representations [20,42]. Introduced in 2017, this architecture primarily relies on the attention mechanism to capture dependencies within sequences, offering a notable advantage in parallel computation compared to earlier recurrent models like RNNs [2,24].  

![](images/ed377522ead54af71831a2ce4f1eab44106f2a152d0d00f9e48c02c943971fd8.jpg)  

A fundamental component of the Transformer is the self-attention mechanism, particularly multi-head attention [5,20,42]. This mechanism enables each element in an input sequence to dynamically weigh and combine information from all other elements, allowing for the efficient encoding of long-range dependencies and contextual relationships [39,42]. The process involves projecting input element embeddings $X ^ { ( i ) }$ into Query $( Q ) , { \mathsf { K e y } } ( K )$ , and Value $( V )$ matrices using learned weight matrices $W _ { Q } , W _ { K } , W _ { V }$ ​ [42]:​  

$$
\begin{array} { c } { { Q = X ^ { \left( i \right) } W _ { Q } } } \\ { { K = X ^ { \left( i \right) } W _ { K } } } \\ { { V = X ^ { \left( i \right) } W _ { V } } } \end{array}
$$  

The single-head attention is then calculated by scaling the dot products of $Q$ and $K ^ { T }$ , applying a softmax function to obtain attention scores, and multiplying by $V$ [2,42]:  

$$
A t t e n t i o n ( Q , K , V ) = \mathrm { s o f t m a x } \Big ( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \Big ) V
$$  

Multi-head attention concatenates the results of multiple such heads operating in parallel [42]. This "global attention" allows elements to reference any other elements in the sequence, capturing complex interactions [42]. The Transformer also utilizes positional encoding to inject information about the order or position of elements in the sequence, as the attention mechanism itself is permutation-invariant [2,42]. This can be achieved using sinusoidal functions or relative positional encodings that generalize better to sequences of varying lengths [2,42]. Encoding blocks typically combine self-attention networks with feedforward neural networks and residual connections to process and update the element embeddings through multiple layers [30,42]. Many Transformer models, particularly those used in LLMs, follow an encoder-decoder structure or utilize only the decoder part, processing sequential inputs and generating outputs based on learned representations [30,39].  

These architectural components collectively enable LLMs based on the Transformer to learn general "language" representations, which are highly relevant to chemical tasks. In chemistry, sequences such as SMILES strings representing molecules, amino acid sequences for peptides and proteins, or reaction sequences can be treated as forms of language [9,21,30]. The self-attention mechanism allows the model to understand the complex relationships and dependencies within these sequences, akin to how it understands dependencies in natural language sentences [39]. This allows LLMs to encode chemical sequences into rich, contextualized hidden layer representations [30].​  

The ability of LLMs to perform various natural language processing (NLP) tasks maps directly to valuable applications in chemistry. Text generation, while primarily associated with natural language, underlies the generation of novel molecular structures (e.g., generating SMILES strings) or protein/peptide sequences [9,30]. Translation tasks are pertinent to converting between different chemical representations, such as translating SMILES strings to molecular structures or vice versa, or predicting reaction outcomes by viewing the transformation from reactants to products as a form of translation [11,21,24]. Question answering, planning, and reasoning capabilities inherent in advanced Transformer-based LLMs like GPT4 are leveraged in autonomous chemical research systems, enabling them to process user inputs, interpret experimental results, plan research workflows, and interact with external tools like databases or experimental robotics platforms [5,12,38]. Furthermore, the ability to learn representations from diverse data types, including multimodal inputs combining sequential data like SMILES with graph representations, extends the applicability of Transformers to complex chemical entities like polymers and facilitates insightful analysis of learned features through attention scores [32]. Models adapted from the Transformer, such as Graphormer, also find use in processing graphical chemical data [25]. This versatility underscores why the Transformer architecture is central to the burgeoning field of LLMs in chemistry.​  

# 2.2 Training Strategies (Pre-training, Fine-tuning, Transfer Learning, Semisupervised/Unsupervised)  

<html><body><table><tr><td>Strategy</td><td>Description</td><td>Key Technique/Example</td><td>Purpose/Benefit</td></tr><tr><td>Pre-training</td><td>Learn general patterns on large text corpora</td><td>Self-supervised learning</td><td>Acquire broad capabilities</td></tr><tr><td>Fine-tuning</td><td>Adapt pre-trained model to specific task/domain</td><td>Instruction fine- tuning(ChemLLM, SynAsk)</td><td>Specialize on chemical tasks, dialogue ability</td></tr><tr><td>Transfer Learning</td><td>Leverage knowledge from related tasks/datasets</td><td>Pre-train on general reactions, fine-tune Heck</td><td>Address data scarcity,improve specific tasks</td></tr><tr><td>Semi- supervised/Unsuper vised</td><td>Use labeled & unlabeled data,or only unlabeled</td><td>Pseudo-labeling, Label Rebalancing (ChemMatch)</td><td>Mitigate reliance on labeled data</td></tr></table></body></html>  

The adaptation of large language models (LLMs) for chemical applications heavily relies on sophisticated training strategies, primarily involving a two-stage process of pre-training and fine-tuning. This paradigm enables LLMs to first acquire broad general language capabilities and then specialize in domain-specific tasks. Pre-training typically involves training models on vast text corpora using self-supervised learning strategies, allowing them to learn general representations and linguistic patterns [42]. Subsequently, fine-tuning adapts these pre-trained models to specific chemical tasks by training them on smaller, labeled domain-specific datasets [42]. This two-stage approach, exemplified by models like ChemLLM, allows the model to first enhance its general language understanding and then integrate chemical knowledge through further tuning on mixed datasets [4,23]. Fine-tuned LLMs have demonstrated performance comparable to, and occasionally surpassing, recent bespoke machine learning models for tasks like inorganic synthesis prediction, often requiring significantly less expertise, cost, and development time [17]. Models such as SynAsk undergo fine-tuning with domain-specific data to become proficient in areas like organic chemistry tasks [22]. Similarly, models have been fine-tuned for predicting polymer properties [32] and for natural language processing tasks in materials chemistry, such as Named Entity Recognition and textto-text generation, using models like XLNet fine-tuned on specific datasets [15]. The effectiveness of fine-tuning has also been observed in adapting general LLMs like GPT-3 for chemical and materials science questions, demonstrating comparable or superior performance to conventional methods, particularly with limited data [29]. While models like GPT-4 are often used in their pre-trained form, leveraging their extensive foundational training [38,44], task-specific adaptation through prompting or fine-tuning remains critical for domain performance.​  

Transfer learning represents a crucial strategy for leveraging knowledge gained from one task or dataset to improve performance on a related but different task, particularly beneficial in scenarios with limited labeled data [24,41]. In chemistry, this can involve pre-training a model on a large dataset of general chemical reactions and then transferring this knowledge by fine-tuning it on a smaller dataset for a specific reaction type, such as the Heck reaction [24]. This "transformer-transfer learning model" effectively transfers foundational chemical understanding to enhance prediction accuracy where data is scarce [24]. Transfer learning has also been applied in molecular generation, for instance, using GRUbased networks pre-trained for generating molecules with specific biological activities [2]. In materials science, transfer learning addresses data limitations by pre-training models on readily available "proxy properties" and then transferring the learned representations or model components to predict properties with limited data, as demonstrated by libraries like XenonPy.MDL [41]. Language models applied to biological sequences, such as DNA and proteins, also utilize a form of transfer learning where representations learned from vast sequence data are transferred to downstream tasks like contextualizing genes, predicting protein function, or identifying regulatory elements [9,39].  

To mitigate the reliance on large quantities of labeled data, semi-supervised and unsupervised learning techniques are increasingly employed. Semi-supervised methods leverage both labeled and unlabeled data during training. A common technique in this context is pseudo-labeling, where a model trained on labeled data is used to generate predicted labels for unlabeled data, which are then used for further training [18].  

Innovative semi-supervised approaches, such as those implemented in the ChemMatch model, incorporate several advanced techniques [18]. ChemMatch employs pseudo-labeling on unlabeled data but with a critical calibration step to ensure the distribution of pseudo-labels aligns with that of the real labels, improving the reliability of the generated labels [18]. To address class imbalance inherent in many chemical datasets, label rebalancing is utilized [18]. Furthermore, data augmentation is performed not in the input space but in the latent space using SoftMix, enhancing the diversity and robustness of the training data by creating interpolated samples [18]. These techniques collectively enhance model performance and generalization, particularly when working with complex chemical data landscapes that may be incomplete or imbalanced.​  

Other training considerations include monitoring metrics like perplexity during training, particularly for generative tasks involving molecular representations like SMILES or SELFIES, to identify optimal stopping points and prevent overfitting [11,14,21]. Techniques like knowledge distillation can also be used to reduce model complexity before subsequent finetuning steps like reinforcement learning [2]. The creation of effective training sets, sometimes employing methods like Latin hypercube sampling, is also fundamental to the success of these training strategies [30].​  

# 2.3 Molecular Representation for LLMs  

The initial step in applying machine learning models, including large language models (LLMs), to chemical tasks involves selecting an appropriate molecular representation [42]. This choice is contingent upon factors such as the molecular structure type, the specific downstream task, and the chosen learning model [42]. Historically, early efforts in cheminformatics for organic molecules utilized fixed, hand-crafted representations like molecular descriptors and fingerprints [42]. For instance, Morgan fingerprints have been employed for reactivity modeling [6], and extendedconnectivity fingerprints (ECFP) have represented monomers and amino acids in polymer design [43]. These representations, often used with simpler models such as feed-forward neural networks (FFNNs), provide a concise, fixedlength input but may not fully capture complex structural nuances [45].​  

The field has seen increasing adoption of variable-length sequence-based and graph-based representations, particularly with the rise of deep learning architectures like recurrent neural networks (RNNs), Transformers, and Graph Neural Networks (GNNs) [42]. String-based representations, such as Simplified Molecular Input Line Entry System (SMILES), SELFreferencing Embedded Strings (SELFIES), and International Chemical Identifier (InChI), encode molecular structures as sequences of characters [16,21,37]. SMILES is notable for its conciseness and space-saving properties [2] and has been widely used as input for deep learning models, including RNNs in de novo molecular design [36] and Transformers [37]. It finds application in tasks like compound retrieval [38] and multimodal polymer property prediction when combined with other features [32]. A significant challenge with SMILES is that strings generated by models may not represent valid molecules due to grammar violations [16,37]. SELFIES addresses this by modifying the grammar rules to intrinsically ensure molecular validity [16,37], potentially making it easier for Transformer models to learn meaningful chemical transformations compared to SMILES in certain drug design contexts [11]. Molecular strings are typically encoded as sequence data for processing by sequence models like RNNs and Transformers [37]. Biological sequences, such as amino acid sequences for peptides and proteins or nucleotide sequences for DNA, are a natural fit for LLMs as they can be tokenized and treated directly as text [9,39]. Transformer encoder components are commonly used in this domain to convert sequences into high-dimensional representations [39], which can then be embedded into a continuous space for downstream tasks [25,30].  

Molecular graphs, representing atoms as nodes and bonds as edges, constitute another principal input format for molecular generation models [2]. These representations, including 2D topological graphs and 3D geometric graphs, inherently contain more explicit structural information than linear strings, albeit requiring more storage space [2,16,37]. GNNs are specifically designed to process this graph-structured data by iteratively updating node and edge features based on local neighborhood information [16,37]. Techniques such as directed message passing neural networks (d-MPNNs) update atomic representations through message passing, which are then aggregated to form a molecular representation [45]. Geometric GNNs are employed when 3D structural information is available to capture application-relevant symmetries in space [16,37].  

In comparing string-based and graph-based representations, a key trade-off exists between data density and spatial requirements [2]. SMILES is compact but can suffer from invalidity issues and may implicitly encode structural relationships in a way that is challenging for models to fully decipher compared to the explicit connectivity in graphs. SELFIES addresses validity but is still a linear sequence. Graph representations provide rich structural context but are less compact. The computational cost is tied to both the representation size and the complexity of the model architecture (e.g., processing long sequences with Transformers vs. graph convolutions with GNNs). LLMs are primarily adapted to sequence formats, requiring molecular graphs to be linearized (e.g., via graph-to-sequence translation) or for the models to be integrated with graph processing components. The adaptation often involves specialized tokenization and embedding layers to convert the molecular representation into a format amenable to the LLM's architecture, such as encoding molecular strings as sequence data for RNNs and Transformers or processing biological sequences directly as text [37,39]. The optimal representation ultimately depends on the specific chemical task, data availability (e.g., 3D structures), and the capability of the chosen model to effectively learn and utilize the encoded information [42].  

# 2.4 Graph-based Approaches  

Graph-based approaches have emerged as a prominent method for representing chemical structures, leveraging molecular graphs as input for various machine learning models [2,45]. This representation captures the topological nature of molecules, with nodes typically representing atoms and edges representing bonds. Beyond basic connectivity, these methods can also incorporate spatial information. For instance, Geometric Graph Neural Networks (GNNs) and Equivariant Neural Networks (EGNNs) are designed to capture application-related symmetries and geometric symmetries present in 3D molecular structures, making them particularly suitable for tasks like molecular modeling and protein prediction where spatial arrangements are critical [3,37].  

Graph Neural Networks (GNNs) are commonly employed within this framework, operating by iteratively updating node and edge features based on information from their neighboring nodes and edges [37]. This message passing mechanism allows the model to learn complex relationships and distributed representations across the molecular graph. An example of this is seen in Directed Message Passing Neural Networks (d-MPNNs), a class of 2D graph-convolutional networks that utilize learned representations and perform multiple message passing steps to update atom and bond features based on neighborhood information [45].​  

Graph-based models have been applied to a diverse range of chemical tasks. In molecular generation, models like connection tree VAEs and VAEs with graph-structured encoders and decoders directly operate on molecular graphs to generate novel structures [2]. For property prediction, GNNs have been successfully applied to tasks such as predicting ADME properties of compounds, often employing multi-task learning strategies [36]. Furthermore, graph representations have been combined with other modalities, such as SMILES strings, to enhance tasks like polymer property prediction, suggesting the benefits of a multimodal approach [32]. Beyond standard properties, graph-based models like the Distributional Graphormer (DiG) are utilized to model complex molecular systems and predict equilibrium distributions, demonstrating their capability to capture intricate interactions within a structure [25]. While primarily focused on chemical structures, GNNs have also shown utility in related domains such as text classification, performing comparably to methods like Random Forest, Linear SVM, and Logistic Regression on materials chemistry text data [15].​  

The increasing popularity of Transformer architectures, particularly in sequential and text data processing, has spurred research into integrating them with graph-based methods for handling chemical structures. Efforts are underway to adapt Transformers to learn effectively with chemical structures represented as graphs [42]. The combination of representations, such as using both SMILES (often processed by sequential models like Transformers) and molecular graphs (processed by GNNs), exemplifies a direction towards integrating capabilities from both domains to achieve improved performance on chemical tasks [32]. This integration harnesses the strengths of GNNs in capturing local graph topology and spatial information with the global context-awareness and powerful representation learning abilities of Transformers.  

# 3. Applications of LLMs in Chemistry (Task-Based)  

<html><body><table><tr><td>Application Area</td><td>Description</td><td>Key Tasks/Examples</td></tr><tr><td>Chemical Information Extraction</td><td>Structured data from unstructured text (patents, literature)</td><td>NER, Event Extraction, Database/Web Search (Coscientist, SynAsk), Semantic Filtering</td></tr><tr><td>Chemical Property Prediction</td><td>Predict properties from structure/composition</td><td>ADME/ADMET, Physicochemical, Materials (alloys,polymers,MOFs), Protein/Peptide, Reaction Outcome (Yield, Conditions)</td></tr><tr><td>Chemical Reaction Prediction & Synthesis Planning</td><td>Predict outcomes,propose synthesis routes</td><td>Retrosynthesis (SynAsk, ChemLLM), Product Prediction, Condition Recommendation, Autonomous Planning (ChemCrow, Coscientist)</td></tr><tr><td>Molecular Design & Generative Chemistry</td><td>Create novel molecules/materials with desired properties</td><td>De Novo Design, Distribution Learning, Goal-Oriented (Conditional, Optimization), Protein Sequence Generation</td></tr></table></body></html>  

The application of Large Language Models (LLMs) in chemistry is rapidly transforming various research domains, offering powerful capabilities to address long-standing challenges and accelerate scientific discovery. This section provides a structured overview of these applications, categorizing them into key areas: chemical information extraction, property prediction, reaction prediction and synthesis planning, and molecular design and generative chemistry [20]. Across these diverse tasks, LLMs and related AI/ML techniques are demonstrating significant potential, often rivaling or surpassing traditional methods and enabling new levels of automation and insight [17,29].​  

Chemical Information Extraction involves the challenging task of extracting structured chemical information from unstructured text sources such as patents and scientific literature [40]. Traditional Natural Language Processing (NLP) techniques like Named Entity Recognition (NER) and Event Extraction (EE) have been fundamental in identifying entities such as compounds, reaction conditions, and yields, and establishing relationships between them [40]. LLMs, leveraging their advanced language understanding, have proven increasingly effective in these tasks. LLM-powered agents and platforms like Coscientist, SynAsk, and LLM-RDF can process prompts to search databases and the web, extracting relevant details on synthesis parameters, stoichiometry, and experimental procedures [5,13,22,38,44]. LLMs can also perform targeted extraction and semantic filtering of literature [6,26]. Domain-specific datasets such as ScholarChemQA and the ChEMU chemical reaction corpus are crucial for training and evaluating models for these tasks [18,40]. The effectiveness of LLMs is influenced by factors including model architecture, data quality, and prompt engineering to guide output and reduce issues like hallucination [26].  

Chemical Property Prediction is a critical application area spanning drug discovery, materials science, and reaction engineering. LLMs and machine learning models like ChemLLM, often trained on extensive datasets such as ChemData, are employed to predict a wide range of properties from molecular structures [4,20,29]. In drug discovery, predictions include ADME/ADMET properties, physicochemical parameters (solubility, pKa, binding affinities), and virtual screening [8,16,25,34,36]. Materials science applications cover mechanical and functional properties of alloys, MOFs, polymers, and crystals, as well as predicting the impact of composition [7,28,32,41]. Prediction of protein and peptide properties, such as function, structure, and aggregation propensity, is also enabled by protein language models and deep learning [30,39,43]. Reaction outcome prediction, including yield and conditions, is another facet, with platforms like SynAsk and models for complex reactions like electrochemical C-H oxidation showing high accuracy [6,13,22]. Various molecular representations (SMILES, graphs, descriptors, sequences) are used, with multimodal inputs sometimes improving performance [21,30,32,35,39]. While explicit head-to-head comparisons to traditional QSAR are not always detailed in provided digests, the demonstrated capabilities suggest these advanced methods provide powerful alternatives [34,36].  

Chemical Reaction Prediction and Synthesis Planning are central to molecular synthesis. AI, particularly LLMs, is being used to learn reaction patterns and mechanisms from vast databases to propose retrosynthetic routes or predict outcomes [5,20,38]. Computer-assisted synthesis planning (CASP) is significantly enhanced by domain-specific LLMs like SynAsk, capable of predicting reaction types, recommending conditions, and planning retrosynthetic routes, even generating novel routes [13,22]. ChemLLM is also applied to retrosynthesis and product prediction [4]. Autonomous systems like ChemCrow and Coscientist integrate LLM reasoning with external tools, databases, and even laboratory hardware control to plan syntheses, search for information, perform calculations, and generate code, moving towards independent chemical research [5,12,27,38]. AI also aids in inorganic synthesis prediction and precursor selection [17]. Reaction outcome prediction includes predicting product formation, yields, and optimal conditions using fine-tuned LLMs and Transformer models [24,26,29,43]. Compared to traditional expert systems or pattern-based deep learning, LLMs uniquely integrate language understanding, symbolic reasoning, and tool interaction, enabling more complex problem-solving [27,38]. Challenges include accurately predicting factors like regioselectivity and overcoming the vastness and complexity of chemical space [22,24,44]. AI advancements are expected to significantly accelerate the synthesis phase, reducing failure rates [35].​  

Molecular Design and Generative Chemistry focuses on creating novel chemical entities with desired properties, a process often referred to as de novo design [2,19,25,33,35]. This field employs deep generative models across paradigms like distribution learning and goal-oriented generation (conditional generation and optimization) [1,16,37]. Architectures include RNNs, VAEs (including CVAEs), GANs (like MolGAN), normalized flows, diffusion models, and Transformer models [1,2,16,20,35,36,37]. Transformers, particularly in autoregressive sequence generation (e.g., SMILES), and graph-based models are increasingly used [11,14,16,25,39]. Applications extend to generating novel protein sequences using protein language models [9,39]. AI frameworks also enable generative design in materials science, including MOFs, crystals, and 2D materials [7,28,41], and LLMs are explored for inverse design [29]. Tools like ChemCrow can design complex molecules like organic catalysts [27]. Major challenges include reliably generating molecules that are simultaneously valid, novel, and synthesizable while possessing desired properties [9,10,14,35]. LLMs, by leveraging extensive chemical knowledge, may help guide generation towards synthesizable and functional matter.​  

The successful deployment and advancement of these applications rely heavily on rigorous Benchmarking and Evaluation. Standardized datasets like ScholarChemQA for question answering, ChemBench for core chemical capabilities, and specialized datasets for tasks such as molecular generation (e.g., ChEMBL subsets), inorganic synthesis, reaction prediction (e.g., ChEMU corpus, Heck reaction data), materials modeling (OMat24), text processing (arXiv-cond-mat), and toxicity prediction (Tox21) are crucial for assessing model performance [4,6,7,11,13,15,17,18,23,24,28,36,40]. Evaluation metrics vary by task, including accuracy, precision, recall, F1-score, RMSE, $R ^ { 2 }$ , perplexity, validity, uniqueness, novelty, QED, SA Score, Tanimoto similarity, and specific scores for synthesis planning quality [11,13,15,17,24,30,38,43]. While computational evaluation is prevalent, experimental validation and human expert assessment are indispensable, particularly for complex tasks and evaluating the practical utility of results [6,12,38]. Comparative analysis on these benchmarks, such as ChemMatch outperforming general models on chemical Q&A or ChemLLM performing comparably to GPT-4 on ChemBench, helps highlight the strengths of domain-specific models and tool-integrated approaches [4,12,18,38].​  

Overall, LLMs are proving to be versatile tools across a wide spectrum of chemical tasks, significantly accelerating research by automating information retrieval, enhancing prediction capabilities, streamlining synthesis planning, and enabling the generation of novel chemical structures [35].  

# 3.1 Chemical Information Extraction  

Extracting structured chemical information from unstructured text sources like patents and scientific literature presents significant challenges due to the complex and domain-specific linguistic properties inherent in these documents [40].  

Natural Language Processing (NLP) techniques, including Named Entity Recognition (NER) and Event Extraction (EE), are fundamental to addressing these challenges. NER involves identifying and classifying key entities within text, such as chemical compounds, reaction conditions (e.g., time, temperature), and yields [40]. The ChEMU 2020 tasks, for instance, defined ten specific entity labels across four categories, detailing roles like STARTING_MATERIAL, REAGENT_CATALYST, REACTION_PRODUCT, SOLVENT, TIME, TEMPERATURE, and different types of YIELD [40]. EE builds upon NER by identifying relationships and steps in chemical processes, such as connecting reactants, reagents, conditions, and products within a reaction event [40].  

Early work utilized Transformer models like XLNet trained on domain-specific datasets such as MatScholar for NER in materials science literature, successfully identifying entities relevant to materials design in abstracts and full texts [15]. Similarly, chemically-driven NER models have been applied to solid-state chemistry literature to identify precursors and targets, aiding knowledge discovery and improving the understanding of synthesis processes [41].​  

Large Language Models (LLMs) have demonstrated growing effectiveness in chemical information extraction tasks, leveraging their ability to process and understand complex natural language. LLM-powered agents, such as the "Literature Scouter" or "Web searcher" modules in autonomous systems like Coscientist, function by transforming prompts into search queries, browsing web pages or databases like Semantic Scholar, and extracting relevant information on chemical synthesis parameters, stoichiometries, or experimental details [5,38,44]. Platforms like SynAsk also integrate literature acquisition functions, facilitating the extraction of information from publications [13,22].  

LLMs can also perform more targeted extraction and semantic analysis. A study employing LLMs for the semantic analysis of scientific literature relevant to electrochemical C-H oxidation reactions achieved a high accuracy of $9 6 \%$ in filtering relevant papers based on predefined criteria in a benchmark evaluation [6]. This demonstrates the potential of LLMs to act as effective filters and extractors of information based on nuanced semantic understanding. Furthermore, the effectiveness of LLMs in extracting chemical information has been specifically explored in contexts like MOF synthesis research [26].  

By employing prompt engineering techniques, researchers guided models like ChatGPT to automatically perform text mining of MOF synthesis conditions from diverse literature formats and styles. This approach proved effective in mitigating common issues such as information hallucination, allowing the LLM-based chatbot to answer specific queries related to MOF synthesis with improved accuracy and reduced fabrications.  

The development of domain-specific datasets, such as ScholarChemQA for answering questions derived from chemical paper titles, further facilitates the fine-tuning and evaluation of models for complex information extraction and retrieval tasks [18]. The performance of LLMs on these tasks is significantly influenced by factors including the underlying model architecture, the size and relevance of the training data, and the application of techniques like prompt engineering to guide the model's output and reduce errors.  

# 3.2 Chemical Property Prediction  

The prediction of chemical properties is a fundamental task across various domains of chemistry, including drug discovery, materials science, and reaction engineering. Large Language Models (LLMs) and related machine learning (ML) approaches are increasingly employed for this purpose, leveraging their ability to process and learn from vast datasets of molecular structures and associated properties [20,29]. Models are typically trained on extensive data resources, such as the ChemData dataset used for training ChemLLM, to learn complex relationships between molecular structures and their properties [4].  

The scope of properties predicted using these techniques is broad and diverse. In drug discovery, applications include predicting ADME (Absorption, Distribution, Metabolism, Excretion) and ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) properties, which are critical for assessing drug candidates and account for a significant portion of drug failures [35,36]. Physicochemical parameters such as water solubility, lipophilicity, pKa, and drug-target binding affinities are also common prediction targets [8,25,36]. AI, including Generative AI, is used for virtual screening to identify molecules with desired properties [34].  

Beyond pharmaceuticals, ML is widely applied in materials science for predicting diverse material characteristics. This includes predicting mechanical properties like the strength and ductility of alloys, functional properties such as battery lifetimes, gas adsorption in metal-organic frameworks (MOFs), water adsorption isotherms, and optical spectra using models like GNNOpt [7,28,41]. Predicting the impact of elemental composition on properties like alloy corrosion resistance is also explored [28]. For polymers, multimodal transformers are used to predict properties like density, glass-transition temperature $( T _ { g } )$ , melting temperature ( $T _ { m } .$ ), volume resistivity, and conductivity [32].  

Protein and peptide properties are another significant area. Protein language models, trained on amino acid sequences, are employed to predict protein functions and structures [39]. Specifically, the TRN model predicts peptide aggregation propensity (AP), which, when corrected by hydrophobicity (log P) to yield APHC, aids in differentiating self-assembly from precipitation and screening for hydrogel-forming peptides [30]. Deep learning models also predict sequence-dependent aggregation during peptide synthesis [43].​  

Prediction of chemical reaction outcomes is facilitated by AI models. SynAsk, a domain-specific platform, incorporates capabilities for predicting reaction performance and yield, demonstrating a mean error of $1 1 . 7 \%$ for SNAr reaction yield prediction [13,22]. Machine learning models are also developed to predict the reactivity and site-selectivity of substrates in complex reactions like electrochemical C-H oxidation, achieving high accuracy exceeding $9 1 . 7 \%$ with AUC values of $9 7 . 2 \%$ [6].​  

Various molecular representations are employed as inputs for these prediction models, significantly impacting performance. Common representations include SMILES strings, molecular graphs, and generated molecular descriptors [21,32,35]. Protein and peptide sequences are used for biomolecule prediction [30,39]. Some models utilize multimodal inputs; for instance, a multimodal transformer predicting polymer properties showed improved performance when using both SMILES and dimer configurations compared to SMILES alone [32]. Preprocessing and feature selection are often applied to transform molecular data into formats suitable for specific models like KNN [10]. The study of uncertainty characterization highlights the influence of input data noise, molecular representation, and model architecture on prediction accuracy, particularly for properties like enthalpy and HOMO-LUMO gap [45]. The effectiveness of descriptors generated by Transformer models for downstream property prediction tasks has also been investigated, revealing that their performance may saturate before full training completion [21].​  

While the provided digests demonstrate the extensive application of AI/ML/LLMs, including Transformer-based models, in predicting a wide array of chemical properties and report performance metrics for specific models in particular tasks [6,13], detailed head-to-head comparisons concerning the accuracy and efficiency of current LLM-based methods versus traditional QSAR models or other non-LLM machine learning techniques are not comprehensively elaborated across these specific sources. The focus is primarily on showcasing the capabilities and achieved performance metrics of the applied AI/ML/LLM methodologies for various prediction tasks. Deep learning methods, including multi-task learning and Graph Neural Networks, have been applied for ADME prediction, representing advancements over simpler methods, but direct  

quantitative comparisons to traditional QSAR are not extensively detailed within these digests [36]. Despite the limited explicit comparative data in these digests, the demonstrated high accuracy for specific tasks and the ability to process complex representations suggest that these advanced ML and LLM-based methods offer powerful alternatives and complements to traditional approaches, enabling the prediction of more complex properties and facilitating virtual screening [34].  

# 3.3 Chemical Reaction Prediction and Synthesis Planning  

Chemical reaction prediction and synthesis planning are fundamental yet complex tasks in chemistry, critical for drug discovery, materials science, and chemical manufacturing. Artificial intelligence, particularly Large Language Models (LLMs), has emerged as a powerful tool to address these challenges. LLMs can be trained on vast databases of chemical reactions, enabling them to learn intricate patterns, rules, and reaction mechanisms [5,20,38]. This learned knowledge is then leveraged to propose plausible retrosynthetic routes or predict the outcomes of chemical reactions.​  

A key application is computer-assisted synthesis planning (CASP), which relies heavily on retrosynthetic analysis to identify suitable reaction pathways and starting materials [35]. LLMs are being developed into domain-specific platforms to automate and enhance this process. For instance, SynAsk, a domain-specific LLM, demonstrates capabilities in predicting reaction types, recommending reaction conditions, and performing retrosynthetic route planning [13,22]. Notably, SynAsk successfully planned retrosynthetic routes for $5 5 \%$ of 11,549 molecules in the ChEMBL database and provided novel insights for designing drug synthesis routes [13]. Similarly, ChemLLM has been applied to both retrosynthesis and product prediction tasks [4]. Systems like ChemCrow combine LLM reasoning with chemical expertise and external tools to plan syntheses for various molecules, solving complex chemical reasoning tasks independently [27]. Coscientist represents a further advancement towards autonomous chemical research, capable of planning chemical syntheses of known compounds, accessing public data and the internet for reaction information, stoichiometry, and conditions, and even providing reasoning for specific choices based on concepts like reactivity and selectivity [5,12,38]. Coscientist has been shown to plan catalytic cross-coupling experiments like Suzuki-Miyaura and Sonogashira reactions, including necessary calculations and code generation for automation [5,12]. Beyond organic chemistry, LLMs are also being used for inorganic synthesis prediction, assessing synthesizability and suggesting precursors [17].​  

Reaction outcome prediction also benefits significantly from LLMs and related AI techniques. This includes predicting product formation, reaction yields, and optimal conditions. Fine-tuned GPT-3 models have demonstrated capability in predicting chemical reaction yields [29], and LLMs have been used to develop code for adjusting synthesis conditions and optimizing yields [6]. Transformer models, widely utilized in chemistry, are applied in chemical reaction simulation [20] and specific prediction tasks, such as predicting products of Heck reactions, including addressing challenges like regioselectivity and site selectivity through transfer learning [24]. Machine learning models have also successfully predicted specific synthesis outcomes, such as the crystallization results of MOF experiments with over $8 7 \%$ accuracy [26]. Furthermore, deep learning models have been applied to predict and optimize specific steps in synthesis, like peptide coupling [43]. AI generally excels at mining vast datasets to identify optimal synthesis plans and reaction conditions and enables the rapid generation of new chemical reactions [10].​  

Despite these advancements, reaction prediction and retrosynthesis face inherent challenges stemming from the vastness and complexity of the chemical space and the intricate nature of chemical mechanisms [22,44]. Predicting factors like regioselectivity and site selectivity accurately, as seen in Heck reactions, remains difficult [24]. The successful synthesis depends on numerous subtle factors that are not always explicitly captured in structured databases or easily inferred by models.​  

Comparing LLM-based tools with traditional expert systems and other AI approaches highlights the unique contributions of LLMs. While deep learning models like RetroExplainer have proven effective for retrosynthesis prediction based on pattern recognition [7,28], and other AI methods like ML and KNN are used for optimization and pattern mining [10,26,43], LLMs offer the distinct advantage of integrating language understanding, symbolic reasoning, and the ability to interact with external tools and knowledge sources. Autonomous systems like Coscientist and ChemCrow exemplify this by combining LLM reasoning with capabilities like accessing the internet, performing calculations, writing code, and controlling laboratory equipment, enabling more complex and independent problem-solving compared to models focused solely on data-driven pattern prediction [5,6,27,38]. AI advancements, including those driven by LLMs, are expected to significantly accelerate the synthesis phase of chemical research and development, reducing failure rates and accelerating the overall design-make-testanalyze (DMTA) cycle [35].​     ​  

# 3.4 Molecular Design and Generative Chemistry  

![](images/07d268522c922a4e4b75f084bbb58f4ff30a33cd70a9f137e6fce2ce8c94a207.jpg)  

Large Language Models (LLMs) hold significant potential to accelerate the discovery and design cycles in both drug discovery and materials science by enabling the generation of novel chemical entities with desired properties [2,25]. This generative process is a cornerstone of modern computational chemistry, often referred to as de novo design [19,33,35].  

Generative molecular design broadly encompasses two main paradigms: distribution learning and goal-oriented generation [16,37]. Distribution learning focuses on learning the probability distribution of existing molecular datasets to sample new molecules that statistically resemble the training data [16,37]. Goal-oriented generation, in contrast, explicitly aims to produce molecules with specific attributes and is typically further subdivided into conditional generation and molecular optimization [16,37]. Conditional generation involves generating structures based on predefined criteria such as desired properties, structural motifs, biological targets, or even phenotypic conditions [16,37]. Molecular optimization, conversely, focuses on iteratively modifying candidate structures to enhance specific properties, such as solubility, bioavailability, or target binding affinity, improving their suitability as drug candidates or materials [16,37,43]. Some frameworks even integrate strategies from lead optimization into de novo design to improve the synthesizability of generated compounds [19].​  

The field utilizes a diverse array of deep generative model architectures [1,2,37]. Prominent architectures include Recurrent Neural Networks (RNNs), often combined with reinforcement learning to generate molecules with high target affinity or desired properties [2,36]. Autoencoders, particularly Variational Autoencoders (VAEs) and Conditional VAEs (CVAE), are widely used, sometimes integrated with techniques like reinforcement learning and tensor decomposition (e.g., GENTRL) for de novo design [1,2,35,36]. Generative Adversarial Networks (GANs) and specific variants like MolGAN are also applied for de novo synthesis [1,2,16,35,36]. Other architectures include normalized flows like MoFlow [1,16] and diffusion models.​  

Transformer models, which form the basis of LLMs, are increasingly applied in chemical structure generation [20]. They can be used in an autoregressive fashion, particularly for sequence-based representations like SMILES strings, predicting the next token given preceding ones [16,39]. This autoregressive approach is mirrored in protein language models like ProGen and ProtGPT2, used for generating novel protein sequences [9,39]. Transformer models have demonstrated the capability to generate novel molecules with desired bioactivity, learning to modify known active compounds while maintaining or improving activity against specific targets [11,14]. Beyond sequence generation, graph-based generative models like Distributional Graphormer (DiG) are used for tasks such as generating molecular conformations [25]. Optimization methods like REINVENT, DeepFMPO, and MCMG are employed for optimizing objective functions, while models like DeLinker and Delete focus on modifying molecular graph structures [1].​  

AI frameworks extend these generative capabilities to materials science. Examples include GHP-MOFsassemble for generating and assembling MOF structures for applications like carbon capture and FlowLLM for increasing the efficiency of generating stable materials [7,28]. DeepMind's GNoME tool discovers new crystals with stable structures [7], while deep learning generative models have been used for proposing new compositions for 2D materials [41]. LLMs are also explored for inverse design, potentially generating molecules or materials by inverting queries related to desired properties [29]. Specific LLM-based tools like ChemCrow have demonstrated the ability to design complex molecules such as insect repellents and organic catalysts [27].  

Despite significant advancements, the generative chemistry field faces several challenges. A primary challenge is reliably generating molecules that are simultaneously valid, novel, and synthesizable while possessing desired properties. Deep generative models often require large training datasets, which can be a limitation [10]. Ensuring chemical validity requires robust representations and generation processes. Novelty is crucial for exploring the vast chemical space beyond known compounds [9]. Synthesizability remains a critical bottleneck, as generating theoretically valid and novel structures is insufficient if they cannot be practically synthesized. Assessing synthesizability, for instance, by comparing generated molecular transformations with matched molecular pair transformations, is an important step [14,35]. Developing methods that inherently favor synthesizable structures or effectively integrate synthesis planning into the generation process is an active area of research. LLMs, with their ability to process and generate complex sequences and leverage extensive chemical knowledge encoded in training data, may offer new avenues for addressing these challenges, particularly in guiding the generation towards synthesizable and functional chemical matter.​  

# 3.5 Benchmarking and Evaluation  

The evaluation of Large Language Models (LLMs) in chemistry is crucial for assessing their utility and capabilities across diverse tasks. Benchmarking serves as a standardized method to compare the performance of different models. Specific datasets have been developed to address the unique requirements of chemical research. For instance, the ScholarChemQA dataset is designed as a benchmark for evaluating LLMs in answering chemistry-related questions, encompassing topics such as chemical reaction mechanisms and material properties that necessitate specific chemical background knowledge [18]. Another significant benchmark is ChemBench, specifically created to evaluate the core chemical capabilities of LLMs while minimizing the confounding impact of variations in language model output style [4]. ChemBench comprises 4,100 multiple-choice questions and assesses performance across nine distinct chemical tasks related to molecules and reactions [4,23]. This benchmark is made publicly available through the OpenCompass open-source project [4].  

Beyond general chemical question answering, evaluation is conducted on domain-specific tasks. For molecular generation, benchmarks often involve subsets of databases like ChEMBL, with models evaluated retrospectively to simulate real-world drug design scenarios [11]. Datasets for inorganic synthesis prediction and precursor selection are used to compare finetuned LLMs against existing machine learning models [17]. Reaction prediction is evaluated on datasets such as the Heck reaction dataset derived from commercial databases [24] and specific reaction types like SNAr for yield prediction [13]. In text mining and information extraction from chemical literature, datasets like the ChEMU chemical reaction corpus, typically split into training, development, and test sets, are employed [40]. Benchmarks for materials science include the OMat24 dataset, containing over 110 million DFT calculation results, used for training and evaluating models in materials modeling [7,28], as well as datasets like arXiv-cond-mat for evaluating text generation models on tasks like title/abstract generation [15]. Furthermore, benchmarks have been established for evaluating LLMs' ability to generate code and interact with laboratory hardware in electrochemical research, utilizing experimental datasets of reaction conditions and yields. Toxicity prediction benchmarks, such as the Tox21 challenge, also serve to evaluate model performance in predicting compound properties [36].​  

Comparative analysis of different LLMs, including general-purpose models like GPT-4 and chemistry-specific models such as ChemMatch and ChemLLM, on these benchmarks reveals varied performance characteristics. On the ScholarChemQA benchmark, the chemistry-specific ChemMatch model has demonstrated superior performance compared to general models like GPT-3.5 and GPT-4, indicating its enhanced effectiveness in answering complex chemical questions [18]. Similarly, ChemLLM has achieved results on the ChemBench comparable to GPT-4 across nine major chemical tasks, while also surpassing other models of similar scale in general benchmark tests [23]. For tasks requiring complex reasoning and action, such as chemical synthesis planning, LLM-driven agents like Coscientist (which can leverage GPT-4) are evaluated against models like GPT-3.5, GPT-4, Claude 1.3, and Falcon-40B-Instruct using test sets of compounds [12,38]. Performance in synthesis planning is often rated based on the detail and chemical accuracy of the proposed procedures, typically on a score scale (e.g., 1 to 5) [38]. When evaluating the efficacy of LLMs integrating external tools (e.g., ChemCrow), comparison against GPT-4 through expert assessments has shown that tool-integrated models can achieve higher scores in chemical correctness, reasoning quality, and task completion, highlighting the limitations of solely relying on LLMs for evaluation in complex synthetic chemistry tasks and the necessity of human expert judgment. Benchmarking of various LLMs (including LLaMA, Claude, OpenAI models, and GPT-4) on tasks involving code generation and function calls for machine learning in chemistry also reveals performance differences across different models and tasks [6].​  

A wide array of evaluation metrics is employed across different chemical tasks. For molecular generation, standard metrics include validity, uniqueness, and novelty to assess the quality and diversity of generated compounds. More specific metrics like Quantitative Estimate of Drug-likeness (QED) and Synthetic Accessibility Score (SA Score) are also commonly used, alongside evaluation of physicochemical properties, 3D structure design, diversity, and synthetic accessibility. During model training, perplexity is a frequent evaluation metric. Chemical scores, such as the number of generated molecules and scaffold transformations, and similarity metrics like Tanimoto similarity to known active molecules are also applied. For prediction tasks, performance is assessed using metrics such as Root Mean Square Error (RMSE) for continuous properties like UV-Vis trace characteristics [43], and accuracy, precision, recall, and F1-score for classification or prediction tasks like  

synthesizability and precursor selection [17] or text mining output validation. Top-1 accuracy is used for reaction prediction, such as Heck reactions, while the coefficient of determination $R ^ { 2 }$ is applied in regression tasks like predicting peptide selfassembly rules. Mean error quantifies the accuracy of yield predictions. In text processing tasks like text-to-text generation, ROUGE scores are utilized. Evaluation procedures commonly involve using validation and test sets to ensure accuracy and generalization ability. While computational evaluation is prevalent, experimental validation remains the gold standard for evaluating the practical efficacy of generated molecules and reaction predictions. Furthermore, human expert assessment is often indispensable, particularly for evaluating the chemical correctness and reasoning quality in complex tasks like synthesis planning and verifying the accuracy of extracted information. Evaluation approaches sometimes simulate realworld constraints, such as training models without access to data on the target protein in drug design [11]. The complexity of chemical tasks necessitates a comprehensive evaluation approach considering multiple computational metrics and expert validation.​  

# 4. Domain-Specific Impact  

Large Language Models (LLMs), often integrated within broader Artificial Intelligence (AI) and Machine Learning (ML) frameworks, are profoundly impacting various domains within chemistry. They accelerate research and discovery by automating tasks, predicting properties, and generating novel structures and syntheses. This section summarizes their application across key areas such as drug discovery, materials science, inorganic chemistry, and aspects of synthesis, highlighting the unique challenges and opportunities inherent in each field.​  

In drug discovery, AI and ML techniques, including LLMs, are applied across the entire pipeline [8,34,35]. Early-stage activities like target identification are enhanced through the analysis of complex biological data [35]. Virtual screening leverages ML to efficiently identify hit compounds from vast libraries [8,10]. Generative AI and deep learning models are crucial for designing novel molecules and protein binders, predicting drug-target interactions, and enabling the screening of millions of potential candidates [2,14,25,34,35,36]. Specific examples include the use of DiG for predicting protein-ligand interactions and binding free energy [25], GENTRL for discovering DDR1 inhibitors [2], and Transformer models for generating active molecules for targets like COX2 [14]. LLMs are also explored for designing therapeutic proteins like antibodies [9]. Further down the pipeline, AI and deep learning predict critical ADME and toxicity properties, simulating interactions and assessing safety and effectiveness [35,36]. Lead optimization extensively employs generative AI and deep learning to refine drug candidates by modifying properties like solubility and target affinity through techniques such as fragment replacement and scaffold hopping [1,19,37]. AI also assists in optimizing reaction conditions for synthesis [6], and LLMs integrated with tools handle complex synthesis planning [27]. AI is also applied to drug repositioning, notably for COVID-19 treatments [36]. Despite this progress, challenges include the critical need for high-quality data, the difficulty in interpreting complex AI models for chemical insights, and dynamically integrating medicinal chemists' specific design preferences [37,40].​  

In materials science, AI and LLMs accelerate discovery and development primarily through property prediction, material design, and process optimization [10,28,41]. AI systems analyze chemical data to predict properties and recommend new materials [10]. LLMs have shown utility in predicting polymer properties, and fine-tuned LLMs like GPT-3 can perform well even with limited data, addressing a common challenge in the field [29,32]. For material design and generation, AI models facilitate the creation of new compositions [41], while generative AI like DiG predicts properties such as density in material systems [25]. LLMs also hold potential for designing novel materials, including proteins with materials relevance [9]. NLP libraries like ChemNLP aid this by analyzing materials chemistry literature. Successful applications include discovering new crystal structures, designing high-entropy alloys, identifying compositions for 2D materials and perovskites, designing MOFs (with LLMs like ChatGPT assisting in literature analysis and predicting synthesis pathways) [7,26,28,41], and advancing research in battery materials and superconductors [28]. Challenges in materials science mirror those in drug discovery, including the complexity of materials and significant data availability issues [9,37]. However, the performance of LLMs on small datasets presents an opportunity [29].  

In inorganic chemistry, LLMs are emerging tools, particularly for predicting the synthesizability of inorganic compounds and identifying precursors [17]. Fine-tuning LLMs on relevant data allows them to learn synthesis patterns, automating and accelerating planning [17]. Studies show fine-tuned LLMs can achieve performance comparable to bespoke ML models designed specifically for these tasks, leveraging their broad pre-training [17]. LLMs also serve as practical baselines for evaluating new computational methods and act as readily deployable assistants for experimentalists, providing initial synthesis insights and guiding experimental design [17]. Computational techniques, including named-entity recognition  

models, are also applied to analyze solid-state chemistry literature to extract synthesis information [41]. The potential of LLMs extends into other synthesis domains, such as organic chemistry, with domain-specific platforms being developed [22].  

Beyond general synthesis planning, advanced AI techniques, specifically deep learning, are applied to optimize specific synthetic processes like peptide synthesis. Challenges such as aggregation and incomplete coupling can be addressed by deep learning models that analyze experimental data to predict outcomes and identify optimal conditions [43]. By predicting issues like aggregation during Fmoc deprotection, these models enable proactive protocol modification, enhancing efficiency and reliability compared to traditional empirical methods [43]. Deep learning also contributes to predicting other crucial properties like peptide self-assembly behavior, often using complex models such as Transformers [30]. While this area highlights the impact of specific deep learning architectures rather than LLMs directly, it exemplifies how data-driven AI is transforming critical chemical synthesis procedures, aligning with the broader trend of accelerating research across chemistry domains.​  

Overall, while AI/ML encompasses a wide array of techniques, LLMs are increasingly relevant due to their versatility, ability to process textual data (critical for literature analysis and synthesis planning), and potential to perform well even with limited domain-specific data when fine-tuned [29]. Common challenges across these domains include data availability, the need for high-quality, domain-specific datasets, and the crucial issue of model interpretability to gain actionable chemical insights [37]. However, the ability of LLMs to leverage vast pre-training knowledge and their adaptability offer significant opportunities for overcoming these hurdles and further accelerating discovery and development in diverse chemical fields [29].​  

# 4.1 Drug Discovery  

Large Language Models (LLMs), alongside broader Artificial Intelligence (AI) and Machine Learning (ML) techniques, are increasingly applied across numerous stages of the drug discovery pipeline, aiming to accelerate the process and reduce associated costs [35]. These methods are employed from early-stage activities such as target identification and molecule generation to later stages like lead optimization and synthesis planning [1,27,33].​  

In the initial phases, AI assists in identifying potential drug targets by analyzing complex biological systems and identifying disease biomarkers [35]. Subsequently, ML techniques are utilized for virtual screening to efficiently identify hit compounds and potential leads from vast chemical libraries [8,10]. This includes tasks like classifying chemical substances crucial for drug screening [10]. Generative AI, including LLMs, plays a significant role in designing protein binders and enabling the virtual screening of millions of molecules [34]. Deep learning models are also applied to identify potential drug candidates and predict drug-target interactions [35,36]. The Distributional Graphormer (DiG) model, for instance, has demonstrated capabilities in revealing protein-ligand interactions and predicting binding free energy, aiding modern drug discovery efforts [25]. DiG can also accurately sample protein states, such as the active and inactive conformations of B-Raf kinase, a protein linked to cancer [25].  

The generation of novel drug candidates is a key area benefiting from these technologies. Molecular generation techniques are applied to generate and screen candidate compounds, potentially reducing the time and cost of new drug development [2]. Specific generative models like GENTRL have been used to discover DDR1 inhibitors, showcasing their potential for rapid molecular design [2]. In hit expansion, which focuses on designing active compounds against a protein target given known hits, models like the Transformer have proven effective [14]. When trained and tested on targets like COX2, DRD2, and HERG, the Transformer model successfully generated highly active molecules, providing valuable insights for designing potent compounds for these targets [14]. Furthermore, LLMs are being explored for designing therapeutic proteins, including the development of antibodies for treating diseases like cancer and autoimmune disorders [9]. Companies like Nabla Bio are specifically applying LLMs to antibody research for drug discovery [9].  

Beyond initial discovery, AI and deep learning are crucial for predicting ADME (Absorption, Distribution, Metabolism, Excretion) properties and toxicity profiles of drug candidates [35,36]. This acceleration in ADMET prediction helps reduce development costs [35]. AI can simulate drug-target interactions and predict the safety and effectiveness of candidates [35]. Lead optimization is another critical stage where generative AI and deep learning are extensively applied [1,19,37]. Molecular optimization involves refining the properties of drug candidates by making small structural modifications to improve safety, efficacy, and pharmacokinetic profiles like solubility, bioavailability, and target affinity [37]. Techniques include fragment replacement, linker design, scaffold hopping, and side chain modification [1]. An example cited is the design of tyrosine kinase 2 inhibitors, where optimized compounds achieved a 1000-fold increase in activity through linker connection [1]. This optimization is vital for transforming virtual-screened small ligands with low affinity into potentially effective medicines [19]. AI is also being applied to optimize reaction conditions for synthesizing drug-like substances or intermediates, with studies successfully identifying high-yielding conditions using AI-collaboration [6]. Furthermore, LLMs integrated with tools, such as ChemCrow, can handle complex tasks including synthesis planning [27]. Deep learning models have also facilitated the repositioning of existing drugs, notably demonstrated in efforts to find treatments for COVID-19 [36].​  

The practical application of these AI/ML methods in drug discovery is being validated experimentally in both academic and industrial settings [33]. ML-assisted commercial drug discovery activities are becoming prevalent, with some candidates successfully advancing to clinical trials [16]. Companies like Deloitte, Amgen, and Terray Therapeutics leverage platforms and AI models (e.g., NVIDIA's BioNeMo) to enhance their drug discovery processes [34].​  

Despite the significant progress, challenges remain. A critical requirement is the availability of high-quality data for training and validating models [37]. Data sources like chemical patents contain valuable information about new compounds, which is essential but often requires sophisticated processing [40]. Furthermore, interpretability of complex AI models is crucial for gaining chemical insights and building trust in model predictions [37]. Achieving the specific design preferences of medicinal chemists efficiently and dynamically also presents a challenge [33]. Continued research is needed to address these limitations and further harness the potential of LLMs and AI in accelerating the discovery and development of new therapeutic agents.​  

# 4.2 Materials Science  

Large Language Models (LLMs) and broader Artificial Intelligence (AI)/Machine Learning (ML) approaches are increasingly applied in materials science to accelerate discovery and development. Key areas of application include property prediction, material design and discovery, and process optimization [28,41]. AI systems analyze chemical data, predict properties, and recommend potential new materials, significantly enhancing the pace and precision of research [10].​  

In the domain of property prediction, various AI and ML models are employed [7,10,41]. LLMs, specifically, have been utilized for predicting the properties of polymers [32]. The performance of fine-tuned LLMs, such as GPT-3, can be notable, particularly in projects characterized by limited data, suggesting their value in data-scarce materials science problems [29].  

AI and LLMs also play a crucial role in material design and the generation of novel material structures [7,29]. AI models facilitate the generation of new compositions [41], while generative AI approaches like the Distributional Graphormer (DiG) can predict properties relevant to material systems, such as density in catalyst-adsorbate complexes [25]. LLMs also hold potential for designing entirely novel materials, including applications in designing proteins with materials science relevance [9]. Specialized natural language processing libraries, such as ChemNLP, are being developed to support these efforts by analyzing materials chemistry literature and data from sources like arXiv and PubChem.​  

Successful applications of AI/ML in materials science are numerous [7,28,41]. These include the discovery of new crystal structures [7,28], the design of high-entropy alloys [7], and the identification of compositions for 2D materials and double perovskites for photovoltaic applications [41]. Furthermore, AI is being applied to the design of Metal-Organic Frameworks (MOFs) [28], with specific studies demonstrating the use of LLMs like ChatGPT for text mining and predicting MOF synthesis pathways. AI is also contributing to advancements in areas such as battery materials, superconductors, and other functional materials [28]. Beyond property prediction and design, AI is utilized for optimizing material performance [28].  

Despite significant progress, the application of LLMs and AI in materials science faces challenges. The inherent complexity and intricate structures of materials present difficulties for computational modeling [9]. Furthermore, materials science, like many scientific fields, often contends with challenges related to data availability and the need for large, high-quality datasets for effective model training [9,37]. Nevertheless, the demonstrated effectiveness of some fine-tuned LLMs on limited data sets suggests potential strategies for addressing data scarcity in certain applications [29].​  

# 4.3 Inorganic Chemistry  

The application of Large Language Models (LLMs) within inorganic chemistry represents a growing area of research, particularly concerning the prediction of inorganic compound synthesizability and the identification of suitable precursors [17]. These tasks are crucial yet often challenging in experimental inorganic synthesis. LLMs can be fine-tuned on relevant chemical data to learn patterns and relationships that govern synthesis outcomes, effectively leveraging the vast textual and structured data available in chemical literature and databases. This approach facilitates the automation and acceleration of synthesis planning by providing data-driven predictions and suggestions [17].  

When comparing fine-tuned LLMs to bespoke machine learning (ML) models designed specifically for inorganic synthesis prediction, studies indicate that LLMs can offer competitive performance [17]. While bespoke models are often highly optimized for specific prediction tasks using tailored architectures and feature engineering, fine-tuned LLMs benefit from their extensive pre-training on large datasets, which imbues them with a broad understanding of chemical principles and language patterns. This general capability, when adapted through fine-tuning, allows LLMs to achieve accuracy levels comparable to or even exceeding those of specialized models on certain inorganic synthesis prediction tasks [17]. Related efforts in analyzing solid-state chemistry literature have employed named-entity recognition models to identify precursors and target materials, highlighting the broader application of computational techniques in understanding synthesis information from text [41].  

Moreover, the use of LLMs can serve as an effective baseline for evaluating new computational methods in inorganic synthesis prediction [17]. Their accessibility and generalizability make them practical tools for experimental chemists. LLMs can quickly provide initial insights into potential synthesis routes and compound synthesizability, guiding experimental design and reducing the need for extensive trial-and-error. This practical utility is significant for researchers working with various inorganic materials, such as oxides relevant to applications like protonic solid oxide cells, which are often studied using materials science datasets [28]. The potential expansion of LLM capabilities into inorganic domains further underscores their growing importance in the field [13]. Thus, LLMs not only contribute to the theoretical prediction landscape but also function as readily deployable assistants for experimentalists.​  

# 4.4 Peptide Synthesis  

Peptide synthesis, particularly solid‐phase peptide synthesis, is a cornerstone technique in biochemistry and pharmaceutical research. However, challenges such as aggregation, side reactions, and incomplete coupling steps can significantly reduce yield and purity, necessitating resource‐intensive optimization efforts [43]. Traditional methods for optimizing peptide synthesis often rely on empirical approaches, extensive experimentation, and the accumulated experience of chemists. These methods can be time‐consuming and may not always generalize well to novel or complex peptide sequences.​  

The application of deep learning offers a compelling alternative for addressing these challenges, presenting specific advantages for peptide synthesis optimization [43]. Deep learning models can analyze complex relationships within experimental data to predict outcomes or identify optimal conditions more efficiently than traditional techniques. For instance, deep learning has been successfully applied to predict and minimize aggregation events during critical steps like Fmoc deprotection, which is a common bottleneck in solid‐phase peptide synthesis [43]. By training models on historical or targeted experimental data, researchers can predict sequences or conditions likely to lead to aggregation before synthesis is attempted. This predictive capability allows for proactive modification of the synthesis protocol, potentially through sequence design changes or altered reaction conditions, thereby enhancing both the efficiency and reliability of polypeptide synthesis compared to reactive troubleshooting necessitated by traditional methods. The ability to leverage specific experimental data through machine learning models allows for a data‐driven approach to protocol optimization, moving beyond generalized heuristics towards sequence‐specific or condition‐specific improvements [43].  

Beyond synthesis optimization, deep learning models are also being developed to predict other crucial peptide properties, such as self-assembly behavior, which is relevant to the downstream applications of synthesized peptides, including the formation of nanostructures [30]. Such predictions, often utilizing complex architectures like Transformer models, can further inform the design and potential synthetic viability of peptide sequences [30]. Collectively, these applications highlight the growing role of deep learning in transforming various aspects of peptide research, from fundamental property prediction to the optimization of the synthetic process itself.  

# 5. Integration with Experimental and Computational Chemistry  

The integration of Large Language Models (LLMs) and allied artificial intelligence (AI) methodologies with both experimental and computational chemistry represents a transformative paradigm shift, holding significant potential to accelerate scientific discovery and enhance the efficiency of chemical research workflows [3]. This integration is crucial for developing more autonomous and intelligent systems that can bridge the gap between theoretical predictions and physical experiments, ultimately aiming towards the realization of closed‐loop, self‐driving laboratories [12,44].  

A primary focus of this integration lies in empowering experimental chemistry through automation. LLMs are increasingly employed to automate various stages of the experimental process, from designing reaction protocols and generating executable code for laboratory instruments to controlling robotic platforms and managing high‐throughput screening campaigns [6,7,10,12,38,44]. Systems leveraging LLMs can interpret high‐level natural language instructions and translate them into low‐level commands for liquid handlers and cloud laboratories, enabling autonomous experiment execution and optimization cycles [5,6,12,38,44].​  

Simultaneously, LLMs and related AI techniques are being integrated with or utilized to enhance traditional computational chemistry methods [20]. This includes applications with Density Functional Theory (DFT) [7,15,28], molecular dynamics simulations [25], and cheminformatics/bioinformatics approaches for molecular design, property prediction, and virtual screening [8,35,36]. AI models can complement these methods by accelerating calculations, predicting outcomes, or aiding in the interpretation of complex computational results [25,28,36]. Furthermore, integrated platforms facilitate the connection between computational predictions and experimental validation, although challenges related to validation costs and expertise persist [24,30,37].​  

Beyond automation and computational enhancement, this integration fosters new modes of data analysis, interpretation, and human-AI collaboration [1,10,15,33,41]. LLMs can analyze diverse datasets, interpret spectral data, extract insights from scientific literature, and assist in identifying structure-property relationships [5,12,13,15,38,44]. Platforms integrating LLMs with specialized chemical tools and knowledge bases act as intelligent assistants, augmenting human capabilities, streamlining complex tasks, and enabling chemists to interact with AI for problem-solving, prediction, and synthesis planning [13,22,27]. While automation is a key goal, the current landscape often involves a hybrid model where human expertise is critical for oversight, validation, and guiding the AI system, underscoring the importance of developing effective human-AI collaborative workflows [1,5,38,44].​  

Achieving seamless integration across experimental platforms, computational methods, and human expertise presents ongoing challenges, including the need for robust translation between abstract concepts and physical actions, effective handling of complex and imperfect data, and ensuring the interpretability and trustworthiness of AI-driven decisions. The subsequent subsections delve deeper into the specific ways LLMs are being integrated into automated experiment design and execution, data analysis and interpretation, and the evolving landscape of human-AI collaboration within chemical research.  

# 5.1 Automated Experiment Design and Execution  

The integration of large language models (LLMs) has shown considerable promise in automating various aspects of chemical experiments, ranging from protocol design to direct hardware control [6,12,38,44]. This automation capability is a significant step towards autonomous chemical research systems [10,44].  

A prominent example in this domain is the Coscientist system, which leverages LLMs, specifically GPT-4 in demonstrated applications, to design and execute chemical experiments autonomously [5,12]. Coscientist has been successfully applied to complex tasks such as the design and execution of Suzuki–Miyaura and Sonogashira coupling reactions using liquid handlers like the Opentrons OT-2 robot and interacting with cloud laboratories [5,38]. The system achieves this by generating precise Python code and low-level instructions to control laboratory instruments based on high-level natural language commands and available documentation [5,38]. This includes writing protocols for robot operations, planning liquid handling steps, and preparing reaction mixtures as dictated by experimental procedures [38,44].  

Beyond general-purpose autonomous platforms like Coscientist, LLMs are also being explored for optimizing specific types of chemical reactions. For instance, studies have utilized LLMs to optimize electrochemical C–H oxidation reactions [6]. In this context, LLMs generate synthesis parameters and subsequently generate code necessary for preparing solutions, which are then executed by liquid handling robots, demonstrating their utility in automated optimization loops [6]. Other automated systems, such as those for fast-flow peptide synthesis, rely on detailed recording of experimental parameters (sequence, flow rates, temperatures, pressure, UV–vis data) on fully automated platforms, which could potentially be integrated with LLMs for design or optimization tasks [43]. Similarly, dedicated AI-driven robotic chemists are being developed for the autonomous synthesis of organic molecules, highlighting the broader trend towards automation in synthesis [10].​  

The integration of LLMs with diverse experimental platforms presents significant challenges. LLMs must translate high-level experimental goals described in natural language into standardized machine-readable procedures, such as JSON formats, and ultimately into low-level execution code specific to particular instruments or robotic systems [38,44]. This requires robust parsing capabilities and the ability for the LLM to interact effectively with documentation and APIs of laboratory hardware [38]. The success of systems like Coscientist in controlling liquid handlers and interacting with cloud labs underscores the feasibility of this integration, but also highlights the complexity involved in ensuring accurate and reliable execution of protocols across varying hardware platforms [12,38]. The need to parse detailed tasks, plan storage locations, and manage specific hardware operations necessitates sophisticated agentic design to bridge the gap between abstract experimental concepts and physical manipulation in the laboratory [44].​  

# 5.2 Data Analysis and Interpretation  

The application of large language models (LLMs) and related artificial intelligence (AI) techniques has significantly enhanced the capacity to extract meaningful insights from complex chemical datasets, spanning experimental, computational, and textual sources [10,15,41]. These models demonstrate proficiency in analyzing diverse chemical data, including sensing, biosensing, and diagnostics information [41], interpreting spectral data such as UV–Vis and gas chromatography chromatograms [5,12,38,44], and processing scientific literature [13,15].  

A key capability lies in identifying correlations between chemical structures, reaction conditions, and experimental outcomes. For instance, models can predict reaction outcomes and recommend optimal conditions for organic synthesis by analyzing reaction types and associated data [12,13]. This includes optimizing yield in specific reactions, such as Pdcatalyzed Suzuki and Buchwald–Hartwig couplings, through multi-variable design analysis [12]. Beyond reaction prediction, these models are employed to understand structure–property relationships, such as predicting molecular equilibrium distributions and conformational transition pathways [25], identifying factors that dominate chemisorption energy on catalyst surfaces [28], and analyzing the impact of amino acid sequences on peptide self-assembly properties [30]. Furthermore, deep learning models can analyze compound structures to identify features related to properties like toxicity [36] or understand the relationship between polymer attributes and model behavior using attention scores [32]. Automated agents powered by LLMs can interpret high-throughput screening results and explain observed patterns based on fundamental chemistry knowledge [44].​  

Interpreting the internal workings of these complex models to gain chemical insights is an active area. Techniques like examining attention scores in transformer models can reveal connections between model decisions and chemical features [32]. Similarly, visualizing gradient activation maps can help understand which parts of the input data, such as specific indices in a bit–vector representation, contribute most significantly to predictions like aggregation propensity [43]. Visualization techniques like word clouds and t–SNE are also utilized to understand patterns in analyzed textual data from scientific literature [15].  

Despite these advances, challenges persist in utilizing LLMs for chemical data analysis. A primary requirement for training effective models, particularly data–intensive deep learning architectures and LLMs, is the availability of large and well– curated datasets. Chemical data are often heterogeneous, complex, and sometimes sparse or noisy, posing difficulties for model training and reliable prediction. Handling incomplete or noisy data, commonly encountered in experimental measurements, remains a significant limitation. While some studies explore methods to recover missing information, such as deep learning techniques to infer internal material structure from limited surface data [28], the robust handling of general data imperfections is critical. Moreover, the interpretation of raw experimental data, like spectra, often requires guiding prompts or pre–defined procedures for the models to correctly identify relevant features and generate appropriate analysis code [5,38]. Overcoming these limitations is crucial for the broader and more reliable deployment of LLMs in chemical data analysis and interpretation.  

# 5.3 Human-AI Collaboration  

The integration of Large Language Models (LLMs) into chemical research workflows heralds a new paradigm characterized by synergistic human-AI collaboration, poised to significantly accelerate the pace of scientific discovery [3]. In this collaborative framework, LLMs serve to augment human intelligence by automating routine tasks, providing insightful suggestions, and lowering the technical barriers to accessing advanced computational tools [17,27]. Concurrently, human chemists contribute their indispensable domain expertise, critical judgment for validating AI-generated results, and strategic guidance to steer the research direction [1,33,44].  

Examples of successful human-AI collaboration models are emerging across various chemical disciplines. Platforms like ChemCrow function effectively as chemical assistants, streamlining complex tasks through a simple interface integrated with chemical tools, thereby making advanced research more accessible [27]. SynAsk further exemplifies interactive collaboration through a question-and-answer format, enabling chemists to leverage the LLM's capabilities for information retrieval, predictive analysis, and synthesis planning [22]. This platform also showcases how the collaboration of intelligent agents, guided by human interaction, can decompose complex problems, utilize specialized tools, and generate comprehensive solutions [13]. LLMs are proving to be practical tools for experimental chemists, assisting, for instance, in the design and execution of inorganic syntheses [17]. The acceleration of MOF synthesis research using ChatGPT as an AI assistant illustrates how such models can help bridge the gap between chemistry and computing/data science fields by minimizing the prerequisite coding expertise.  

Beyond functioning as assistants, LLMs contribute to automation. In electrochemical C-H oxidation optimization, LLMs can generate code based on natural language prompts from chemists, enabling iterative refinement of reaction conditions to improve yields [6]. While systems like Coscientist aim for autonomous experiment design and execution [5,38], current implementations often still involve human physical intervention, such as manual microplate handling [5,38]. This hybrid setup highlights a practical collaboration model where AI handles complex decision-making and experimental design logic, while humans provide necessary manual assistance.  

The human role remains paramount in maintaining accuracy and relevance. In the context of autonomous synthesis platforms, chemists are essential for evaluating the correctness and completeness of AI agent responses, managing the interconnection of agents, and ultimately deciding whether to implement suggestions or refine them through further interaction [44]. In molecular design and optimization, such as structure-directed lead optimization, LLMs can generate large sets of candidate compounds, but human expertise is crucial for defining screening rules and evaluating drug properties [1]. Furthermore, incorporating the specific design preferences of medicinal chemists into AI models is critical for successful outcomes [33]. Educating future chemists to effectively interact with AI experts is also seen as a key aspect of fostering this collaborative ecosystem [41]. This collaborative paradigm allows AI to process vast amounts of information and present learned knowledge, including model reasoning, in more accessible formats like reports and visualizations, facilitating human understanding and insight [3]. This synergistic relationship between human intuition and AI efficiency is fundamental to navigating the increasing complexity of modern chemical research.​  

# 6. Challenges and Limitations  

<html><body><table><tr><td>Challenge Category</td><td>Key Issues</td><td>Impact on Application</td></tr><tr><td>Data</td><td>Scarcity (domain-specific, complex systems), Quality (noise,annotation), Bias (imbalance), Heterogeneity, Representation format</td><td>Limits model training, reliability, generalizability, interpretability</td></tr><tr><td>Interpretability/Explainabilit y</td><td>"Black Box" nature, Difficulty understanding reasoning/decisions</td><td>Hinders scientific insight, reduces trust, requires exp. validation</td></tr><tr><td>Generalizability/Robustness</td><td>Extrapolating beyond training data, Handling novelty/variations, Learning subtle properties (chirality)</td><td>Limits reliable prediction on new systems, conditions, tasks</td></tr><tr><td>Ethical Considerations</td><td>Dual-use potential, Bias propagation,Intellectual Property</td><td>Requires responsible development,safety measures,fairness, IP policies</td></tr><tr><td>Computational Cost</td><td>High cost for training and deployment</td><td>Limits accessibility, scalability</td></tr></table></body></html>  

While Large Language Models (LLMs) demonstrate significant potential for revolutionizing chemistry, their widespread adoption and effectiveness are contingent upon addressing a range of substantial challenges and limitations. Providing a balanced perspective requires acknowledging both the exciting possibilities and the inherent difficulties in applying these powerful models to complex chemical problems [20,37].  

A fundamental obstacle is the challenge of data scarcity, quality, and bias in chemical datasets [20,37]. Many chemical tasks necessitate vast amounts of high-quality, domain-specific data for successful model training, which are often unavailable, particularly for complex systems or novel properties [30,40,41]. The vastness of chemical space also contributes to data sparsity, impacting model uncertainty and generalization [45]. Furthermore, existing data often suffers from quality issues, including poor characterization or annotation [15,39], and inherent biases introduced through collection or processing, such as class imbalance [14,18,45]. The specific format of chemical information, often stored in structured databases or represented with specialized symbols like SMILES, also poses challenges for models primarily trained on natural language [4]. Reproducibility challenges with publicly available datasets due to ongoing updates further complicate data preparation.  

Another critical limitation is the "black box" nature of LLMs and the resulting difficulty in interpreting and explaining their predictions [20,21,37]. Understanding whya model makes a particular prediction or proposes a specific solution is crucial for scientific validation, building trust, and generating new chemical insights [2,3,36,41]. This lack of transparency hinders chemists' ability to rely on model outputs without experimental verification, especially as models grow in complexity [3,44]. The current interpretability methods are often insufficient to fully unveil the complex internal reasoning processes of LLMs [5,38].​  

Ensuring the generalizability and robustness of LLMs to new chemical systems, conditions, and tasks remains a significant hurdle [21,41]. Models trained on specific datasets may struggle to perform reliably when applied to data distributions outside their training domain [1,16,37,43]. Challenges include difficulties in learning specific chemical properties like chirality [21], handling the vast combinatorial space in molecular design [11,33], avoiding overfitting that leads to limited novelty [2,11], and the limited transferability of certain molecular representations [42]. The need for robust validation against experimental data is paramount to confirm model applicability in real-world scenarios [30].​  

Beyond technical challenges, significant ethical considerations surround the deployment of LLMs in chemistry. Concerns regarding the potential for dual-use and misuse of these powerful tools, particularly in autonomous systems and research platforms, are paramount and necessitate responsible development and deployment strategies [5,27,38]. Furthermore, the propagation of biases present in training data can lead to unfair or skewed research outcomes, highlighting the need for careful data curation and bias mitigation [27]. Issues related to intellectual property rights for LLM-generated discoveries also require careful consideration [27]. Establishing and adhering to clear ethical guidelines is crucial for ensuring beneficial and safe application [21].​  

Finally, the computational costs associated with training and deploying large transformer-based models are substantial [2,3,20,42]. The resource intensity limits accessibility and scalability for many researchers and institutions.  

Addressing these multifaceted challenges related to data, interpretability, generalizability, ethics, and computational requirements is essential for unlocking the full potential of LLMs and ensuring their responsible and effective integration into the field of chemistry.​  

# 6.1 Data Scarcity and Quality/Bias  

The effective application and performance of Large Language Models (LLMs) in chemistry are fundamentally dependent on the availability of large, diverse, and unbiased datasets [37]. However, researchers frequently encounter significant challenges related to data scarcity, quality, and inherent biases across various chemical domains.​  

Data scarcity represents a pervasive issue. Many chemical tasks require substantial amounts of data for successful model training; for instance, predicting the aggregation propensity (AP) of decapeptides necessitates approximately 3.2 million peptide sequence data points [30]. Similarly, deep learning strategies generally demand comprehensive datasets for optimal performance [41]. This scarcity is particularly acute for specific tasks such as machine learning-based information extraction from chemical text, which requires large quantities of annotated data [40], or the optimization of drug molecule structures, where extracting dedicated datasets from literature proves cumbersome [1]. While the volume of data for entities like DNA and protein sequences is growing exponentially, the availability of corresponding structural and functional data remains limited, highlighting a scarcity in specific data typesrather than overall volume [9]. This issue is compounded by the vastness of the chemical space, which leads to many chemical structures being unique or under-represented in available datasets, making data size and coverage a significant source of error and impacting model uncertainty and generalization [45]. The need for curated, large databases, particularly for materials properties, is also emphasized for the successful application of machine learning in discovery efforts [41].  

Beyond quantity, data quality and bias also pose considerable challenges. Significant portions of microbial taxa, functional elements, ecologies, and environments are poorly characterized, annotated, or cultured, leading to a lack of adequate representations in AI model training data [15,39]. Furthermore, biases can be introduced through data collection or processing. For example, an imbalance in labeled data, such as the scarcity $( 1 3 \% )$ of "Maybe" answers in automatically labeled datasets, can bias models towards more frequent categories [18]. Data processing choices, such as excessively low or high filtering thresholds, have also been shown to negatively impact machine learning model performance [14]. Moreover, reliance on publicly available datasets like arXiv and PubChem, while valuable, introduces reproducibility challenges as these resources are subject to ongoing updates and version control [15].​  

These data limitations directly impact the performance and generalizability of LLMs and other machine learning models in chemistry, restricting their effectiveness in prediction, generation, and analysis tasks. Consequently, various strategies are employed to mitigate the effects of data scarcity, quality issues, and biases. Data balancing techniques, such as label rebalancing, can be applied to address biases arising from imbalanced class distributions [18]. Complementing limited experimental data with information extracted from literature is another approach to balance and enhance datasets for model training [6]. Transfer learning is highlighted as a particularly effective strategy for addressing data scarcity, enabling models to perform well on small datasets by leveraging knowledge gained from pre-training on larger, related datasets [24,41]. For instance, the performance of a Transformer model on a small Heck reaction dataset was improved by pretraining it on a dataset of 370,000 chemical reactions to instill basic chemical knowledge before fine-tuning on the specific task dataset [24]. Careful preparation of pre-training data, such as clearing Heck reaction information before evaluating the transfer learning strategy, is crucial for fair assessment [24]. While deep learning models typically require comprehensive data, transfer learning offers a viable path when datasets from relevant domains are available [41]. It is also noted that some advanced LLMs, like GPT-3, demonstrate promising performance even with relatively small datasets, suggesting an inherent capability that may partially alleviate traditional data dependency issues in chemistry [29]. However, strategies such as data augmentation and active learning, while mentioned as general mitigation techniques, require further exploration within the context of LLMs in chemistry based on the available literature. Addressing the multifaceted challenges of data scarcity, quality, and bias through improved data collection, curation, and advanced learning strategies like transfer learning is critical for unlocking the full potential of LLMs in driving chemical research and discovery.​  

# 6.2 Interpretability and Explainability  

The successful integration and broad adoption of Large Language Models (LLMs) within chemistry critically depend on establishing trust, which is fundamentally linked to their interpretability and explainability [20,37]. Interpretability refers to the ability to understand how a model arrives at a particular outcome, while explainability focuses on providing justifications or insights into its reasoning process in a human-comprehensible manner [41]. Without these capabilities, AI models often function as "black boxes," making it challenging for chemists to understand the underlying principles or rationale behind predictions or proposed experiments [21,41]. This opacity is exacerbated as AI models, particularly LLMs, increase in size, making the alignment of learned knowledge with human understanding more difficult [3]. Therefore, simply achieving a correct result is often insufficient; understanding whythat result was obtained is crucial for scientific validation and discovery [41].​  

Despite its critical importance, interpretability remains a relatively underexplored area, particularly within molecular generation models [37]. The current interpretability of models in computational chemistry is noted as generally low [20]. Existing approaches to enhance interpretability include analyzing attention mechanisms to link model behavior to specific attributes, such as in polymer property prediction [20,32]. Visualization techniques, such as t-SNE for clustering analysis [15] and gradient activation maps to visualize decision-making processes and identify influential input features [43], are also employed. The development of interpretable algorithms, such as RetroExplainer for providing insights into retrosynthesis routes, represents another avenue [28]. Some AI systems, including LLM-driven autonomous research platforms like Coscientist and other models, can offer justifications for specific choices, such as experimental designs or reagent selections, demonstrating an operational understanding of concepts like reactivity and selectivity [5,38]. Furthermore, analyzing how models capture the influence of features like amino acid positions on peptide self-assembly can offer valuable insights into model behavior [21,30].  

However, current interpretability methods possess notable limitations. Even systems capable of providing justifications may not fully reveal the depth and complexity of the LLM's internal reasoning processes [5,38]. It is often still challenging for human experts to fully comprehend the intricate web of factors leading an algorithm to a particular outcome [41]. This necessitates the development of more effective and sophisticated techniques to understand the decision-making pathways and reasoning mechanisms of LLMs [20]. Future efforts should focus on developing highly interpretable models through advanced visualization, comprehensive attention mechanism analysis, and intrinsically interpretable algorithms [20]. The integration of formal Explainable AI (XAI) methods into models, such as those used for molecular structure optimization, is crucial to elucidate the logic behind their proposals [1]. Future research could also explore novel methods for analyzing LLM reasoning, such as systematic analysis of choices made across multiple experimental iterations [38]. Developing robust interpretability and explainability tools is paramount for translating AI capabilities into actionable chemical knowledge and fostering confidence in LLM-guided research.​  

# 6.3 Generalizability and Robustness  

A significant challenge in applying machine learning models, including large language models (LLMs) and transformer architectures, to chemistry lies in ensuring their ability to generalize effectively to novel chemical structures, reaction conditions, and experimental setups that deviate from the training data distribution [41]. Traditional ML algorithms often assume that test cases originate from the same distribution as the training data, an assumption frequently violated in complex real-world chemical systems [41]. Extrapolating beyond the domain covered by the training set remains a critical hurdle for achieving reliable predictions in diverse chemical applications.​  

Despite these challenges, several studies demonstrate that transformer-based models can exhibit notable generalization capabilities in chemistry. For instance, a transformer model incorporating transfer learning techniques achieved strong predictive performance for both intermolecular and intramolecular Heck reactions, illustrating its ability to generalize across different reaction types [24]. This model also showed robustness against site selectivity issues in reactants with multiple reactive sites, further highlighting its capacity to handle variations within a reaction class [24]. Similarly, transformer models applied to drug design tasks have demonstrated the ability to generate plausible molecular structures and identify potential high-activity ligands for protein targets that were not included in the training set [11]. Beyond molecular structures, transformer networks (TRN) validated with coarse-grained molecular dynamics (CGMD) and transmission electron microscopy (TEM) experiments suggest generalization to different experimental conditions in peptide self-assembly [30]. Predictive models for properties like $W - H$ difference have also shown generalization by performing well on held-out datasets [43].​  

To enhance the generalization ability and robustness of these models, various techniques are being explored. Transfer learning, as demonstrated in the context of Heck reaction prediction, allows models trained on large datasets to adapt to related but distinct chemical tasks or datasets [24]. Data augmentation techniques are also crucial for improving robustness. The ChemMatch model, for example, utilizes SoftMix data augmentation in the latent space during training. This approach increases semantic diversity within the training data, providing additional signals that help the model become more resilient to variations and noise in input data [18].​  

The importance of rigorous evaluation on diverse and challenging datasets cannot be overstated. Generalization is typically assessed by evaluating model performance on test sets distinct from the training data, ideally representing real-world variability. The successful application of models often relies on validation against experimental data or using held-out datasets representative of out-of-distribution scenarios [30,43]. Continued efforts in developing challenging benchmarks and systematic evaluation protocols are essential for driving progress in building truly generalizable and robust LLMs for chemistry.​  

# 6.4 Ethical Considerations  

The burgeoning application of large language models (LLMs) in chemical research necessitates careful consideration of associated ethical implications [37]. Key concerns revolve around the potential for misuse, fairness, bias, and intellectual property rights.  

A significant ethical challenge is the potential for LLMs to be exploited for malicious purposes, often referred to as the dualuse dilemma. This concern is highlighted in studies developing autonomous chemical research systems like Coscientist, emphasizing the imperative for ethical and responsible deployment of such AI tools [5,38]. Similarly, discussions  

surrounding LLM-empowered chemical research platforms underscore the potential for dual use and the critical need for robust safety measures [27]. Mitigating such risks requires proactive strategies to prevent misuse.  

Beyond dual-use, other ethical facets demand attention. The quality of training data is paramount, as biases within datasets can be propagated or even amplified by LLMs, potentially leading to unfair outcomes or skewed research directions [27]. Addressing bias and ensuring fairness in LLM-driven chemical discovery requires enhancing data quality and curation processes [27]. Furthermore, ethical considerations extend to intellectual property, with questions arising regarding the ownership of discoveries or generated content produced by LLMs [27].  

To navigate these complex issues, the development and deployment of LLMs in chemistry must prioritize responsible innovation [21]. This involves implementing technical safeguards, such as prompt engineering techniques and mechanisms for real-time updates, to control model behavior and prevent unintended or harmful outputs [27]. Crucially, there is an urgent need for the establishment and adherence to clear ethical guidelines governing the use of LLMs in the chemical sciences [5,38]. Such guidelines should address transparency, accountability, safety, and fairness to ensure that these powerful tools are developed and utilized for the benefit of society while minimizing potential harm.​  

# 7. Future Directions  

![](images/206d3af37e747b756b82cf2032a5a9882bb42013225b5f51101c298d87829137.jpg)  

Based on the identified challenges and the current state of the field, promising future research directions for applying large language models in chemistry are becoming increasingly clear. These directions encompass advancements in model architectures, synergistic integration with other AI techniques, leveraging diverse data modalities, and developing models tailored to specific chemical tasks.  

Future research should focus on developing novel LLM architectures specifically designed to capture the unique complexities of chemical data and tasks [20]. This includes designing models that effectively handle chemical structures, reactions, and experimental data. Exploration into task-specific architectures beyond generic Transformers is ongoing, with examples like GNNOpt, PBCT, and FIREANN demonstrating the potential of tailored designs for areas such as materials science [28]. Architectural modifications to existing models, such as incorporating structural information to improve descriptor generation for chirality recognition, are also critical [21]. Further foundational research is needed to understand how NLP models interpret chemical structures [21]. Training strategies like transfer learning show promise for broader applicability in reaction prediction [24]. Efforts are also directed towards automating model development and parameter selection akin to human intuition [3]. Beyond core model development, refining application methodologies like ChemPrompt engineering is crucial for practical use of existing LLMs [26]. Future work should aim to combine model development with specific applications rather than solely focusing on generative models in isolation, ensuring generated outputs are experimentally evaluable [2]. Exploring pre-training strategies and incorporating physical constraints into model design can enhance chemical space exploration [1]. Expanding the chemical space and improving the bioactivity and diversity of generated molecules remain key challenges [14,16]. Addressing theoretical, computational, and empirical challenges is essential for deploying these methods effectively [33]. The potential of open-source models also warrants further investigation to improve repeatability [27].  

A significant future direction lies in the synergistic integration of LLMs with other AI techniques [20,29,37]. This hybrid approach leverages the strengths of different AI methods, such as combining LLMs with reinforcement learning for optimized decision-making, active learning for efficient exploration of chemical space, or knowledge graphs for structured reasoning. Existing precedents in materials science demonstrate the value of integrating ML with techniques like genetic algorithms, clustering, and experimental feedback loops [28], or DNNs with NLP [28], and KNN for classification [10]. LLMs can be integrated with conventional ML algorithms for tasks like reaction condition optimization, where the LLM guides initial parameter selection and a method like Bayesian optimization refines the process [6]. Integrating foundation models with deep reasoning models that embed first principles and domain knowledge is crucial for enhancing understanding and predictive power [3]. Coupling LLMs with specialized generative models like diffusion models is also a promising avenue [9]. The development of integrated scientific tools for LLMs is anticipated to significantly accelerate discoveries [12,38]. Autonomous research systems like Coscientist exemplify this integration, using LLMs to orchestrate code execution, web search, and potentially leverage reaction databases [5,38]. Advanced prompting strategies can further improve the accuracy and performance of such systems [5,38]. Expanding integrated systems to cover more chemical fields and integrate with automated laboratories is a key goal [13,37]. Future work should also explore integrating other language-based and image processing tools to enhance system capabilities [27]. The integration of clinical data into design pipelines is also a potential future direction, especially for drug discovery [37].​  

Multi-modal data integration is poised to significantly enhance the capabilities of LLMs in chemistry [20]. Future efforts should focus on effectively combining diverse data types, including molecular structures (e.g., graphs, SMILES), imaging data, experimental results, and textual information [18,27,41]. This allows for a more comprehensive understanding of chemical entities and processes, leading to improved property prediction, molecule design, and reaction simulation. The integration of structural and textual data, for instance, can leverage both sequence and topological information. The biological field provides a model for integrating diverse data types like genomics, proteomics, and imaging, which can be adapted for chemistry [9]. Challenges remain in harmonizing data from different modalities and developing robust representation learning strategies. Future research should expand the scope of integrated modalities and enable crossmodal tasks, such as interpreting structures based on textual queries.​  

The development of more specialized LLMs tailored for specific chemical domains and tasks represents a crucial future direction [4]. While general models provide broad capabilities, specialized models, often trained on domain-specific data, can achieve higher performance and accuracy on targeted problems. Examples of specialized models like GNoME, DeepH, and FlowLLM in materials chemistry point towards this trend [28]. Injecting professional domain knowledge into these models is a key strategy for improving their scientific utility [23]. Future research should explore strategies for effectively integrating these specialized models with more general LLMs to create powerful and versatile systems capable of handling a wide range of chemical problems [20]. This integration could involve hierarchical systems or collaborative frameworks where different models contribute their unique expertise. Expanding the capabilities of models like ChemLLM with RAG functionality for deep literature mining and online search further enhances their specialized application in chemical research [4]. Similarly, expanding systems like SynAsk to cover diverse chemical fields like inorganic chemistry, materials science, and catalysis exemplifies the push towards broader specialization [13]. Aligning model design with realistic constraints and specific steps in processes like drug discovery is also crucial for creating effective specialized tools [37].  

Across these directions, several overarching themes and challenges persist. Improving the quality and availability of largescale, annotated datasets remains fundamental [18,39]. Developing rigorous evaluation protocols is essential for assessing the performance and reliability of new models [16]. Enhancing model interpretability is vital for gaining scientific insights and building trust in AI predictions [16]. The potential of AI to accelerate the entire scientific discovery cycle, including automated experimentation, is a major driving force for future research [13,37]. Ultimately, integrating AI into research and education will expand the boundaries of chemistry and aid in understanding complex concepts and locating information [10,18]. LLMs are expected to become routine tools for bootstrapping projects and providing baselines [29]. The ChemNLP package's design, allowing easy integration of new models and datasets, supports this future growth [15].  

# 7.1 New Architectures and Methodologies  

Advancements in novel architectures and training strategies are crucial for enhancing the performance of large language models and other AI methods on challenging chemical tasks and improving their ability to capture complex chemical phenomena. Researchers are exploring diverse approaches, ranging from task-specific model designs to sophisticated training paradigms and data integration techniques.​  

Specific architectures tailored for chemical problems are emerging. Examples include models like GNNOpt, a graph neural network utilized for identifying solar cell and quantum materials; PBCT, which employs semi-supervised learning for battery lifetime prediction; and FIREANN, a field-induced recursive embedded atom neural network designed for analyzing atomic responses to external fields [28]. These instances highlight the development of specialized AI architectures and methodologies aimed at specific chemical applications [28].  

Transformer models, while powerful, are also subject to architectural exploration. For instance, suggestions have been made to modify the internal structure of Transformer models to reference overall structural information as an auxiliary task, which could aid in improving descriptor generation models, particularly addressing challenges like chiral recognition [21]. Beyond architectural modifications, the application of training strategies such as transfer learning combined with Transformer models has shown potential for broader application across various chemical reaction prediction tasks, thereby enhancing the applicability of these methods [24].  

Effective data handling and integration represent another critical area of methodological advancement. Improving model performance necessitates a concerted focus on the collection and annotation of datasets from diverse sources [39]. Furthermore, future progress for LLMs in related scientific domains, such as biology, is anticipated through the combination of raw sequence data with structural and functional information during training [9], a strategy potentially transferable to chemistry for richer molecular representations.​  

On a higher level of abstraction, methodologies for automatically constructing neural network architectures and selecting appropriate hyperparameters are being investigated, leveraging principles akin to human intuition to streamline the model development process [3]. Complementing the development of core model architectures and training is the refinement of application methodologies, such as ChemPrompt engineering. Providing structured guides for prompt engineering empowers chemists to more effectively utilize existing LLMs like ChatGPT, accelerating research in areas such as MOF synthesis by improving the user interface and control over model output [26]. These diverse architectural and methodological explorations collectively contribute to building more capable and chemistry-aware AI systems.  

# 7.2 Integration with Other AI Techniques  

The burgeoning field of chemistry is increasingly leveraging the combined power of large language models (LLMs) with other artificial intelligence (AI) techniques to accelerate discovery and optimization processes [20,37]. This synergistic integration offers significant benefits, particularly in guiding exploration within vast chemical spaces and reducing the need for costly and time-consuming experimental procedures, often through approaches analogous to active learning.  

The trend towards integrating various AI methodologies is evident across chemical and materials science domains [41]. Precedent exists in combining machine learning (ML) with techniques such as genetic algorithms, clustering analysis, and experimental feedback loops for tasks like alloy design [28]. Similarly, deep neural networks (DNNs) have been integrated with natural language processing (NLP) for applications like designing corrosion-resistant alloys [28], and k-nearest neighbors (KNN) has been employed for building classification models in chemistry [10]. In the realm of biomolecules, integrating deep learning prediction models with homology search techniques is proposed for optimizing peptide and protein sequences [43]. These examples highlight a broader movement towards multi-modal or hybrid AI approaches.  

The direct integration of LLMs with other AI techniques represents a natural progression. A notable strategy involves combining LLMs with conventional ML algorithms for tasks such as reaction condition optimization [6]. In such hybrid frameworks, the LLM can play a crucial role in guiding initial parameter selection based on its vast pre-training knowledge, while a complementary ML program, such as one employing Bayesian optimization, refines subsequent synthesis parameters [6]. This specific use of Bayesian optimization, informed by LLM-generated code or suggestions, exemplifies how LLMs can initiate and streamline optimization workflows, effectively reducing the number of experiments required to find optimal conditions.​  

Beyond optimization algorithms, the integration extends to incorporating domain knowledge and reasoning capabilities. It has been suggested that foundation models like LLMs should be coupled with deep reasoning models that embed first principles, prior knowledge, and specific domain mechanisms [3]. This integration aims to enhance the models' understanding and predictive power beyond purely data-driven patterns. Furthermore, there is potential for integrating LLMs with specialized generative models used in molecular design, such as diffusion models applied in protein design tools like ProteinMPNN and RFdiffusion [9].​  

The integration of LLMs with broader AI-powered tools also enhances their utility in autonomous research workflows. Systems like Coscientist integrate LLMs with code execution modules and web search capabilities [5]. While distinct from integrating with a separate, external optimization algorithm [38], this setup allows the LLM to orchestrate complex research tasks, including multi-variable design and optimization of reactions [38], by leveraging these integrated tools. This suggests a paradigm where the LLM acts as a central intelligence, coordinating various AI components and data sources to navigate chemical challenges.  

Collectively, these examples underscore the growing recognition that combining the linguistic understanding and knowledge retrieval strengths of LLMs with the predictive, generative, or optimization capabilities of other AI techniques is a powerful pathway towards more efficient and autonomous chemical discovery, promising significant advancements in areas like drug design [11].​  

# 7.3 Multi-Modal Data Integration  

The integration of multiple data modalities represents a promising avenue for advancing the capabilities of large language models (LLMs) in chemistry. Unlike traditional models that may rely solely on text-based representations like SMILES strings, multi-modal approaches incorporate diverse data types, such as molecular graphs, imaging data, or experimental results, to build more comprehensive and robust models .​  

This paradigm is particularly beneficial for tasks requiring a holistic understanding of chemical entities and processes, including enhanced property prediction, more effective molecule design, and accurate reaction simulation .​  

A key aspect of multi-modal LLMs in chemistry involves the fusion of structural and textual information. For instance, one approach explores integrating SMILES strings with molecular graph representations, allowing models to leverage both linear sequence information and topological connectivity for tasks such as polymer property prediction .  

Transformer models have been identified as suitable architectures for handling these multi-modal tasks .​  

Despite the clear potential, integrating data from disparate sources presents significant challenges. Data from different modalities often vary widely in format, scale, and inherent noise levels. Developing effective representation learning strategies that can capture the unique features of each modality while simultaneously learning meaningful cross-modal relationships is critical yet complex. Furthermore, aligning and fusing information streams from structurally different data types (e.g., text and graphs) within a unified model architecture requires sophisticated techniques to avoid information loss or bias towards dominant modalities. The scarcity of large, high-quality datasets where multiple modalities are consistently available for the same set of chemical entities also poses a practical challenge for training multi-modal models effectively.​  

Future research directions in this domain are anticipated to include expanding the scope of integrated modalities and developing capabilities for cross-modal tasks, such as question answering that involves interpreting molecular structures alongside textual queries .​  

# 8. Conclusion  

Large Language Models (LLMs) and the broader field of artificial intelligence have demonstrated a profound and transformative potential in revolutionizing various domains of chemistry. AI is fundamentally impacting the field by providing sophisticated tools and methodologies that significantly accelerate research and development cycles, enabling enhanced simulations, optimized synthesis, automated operations, and accelerated material and drug discovery [28,41].  

The application of LLMs in chemistry has yielded promising results across numerous areas. In drug discovery, deep learning and generative models offer substantial advantages in reducing development costs and time [2,35]. They show success in tasks such as QSAR prediction, denovocompound generation, ADME property prediction, and identifying potential drug candidates, as exemplified by applications related to COVID-19 [36]. Transformer models, specifically, have proven effective for generating novel molecules for drug design, capable of producing context-dependent modifications with reasonable efficiency and lowering the barrier to entry for machine learning-based approaches [11,14]. The integration of generative AI is also shifting paradigms, such as focusing on equilibrium distribution prediction in molecular science [25]. Beyond drug design, LLMs have been applied to materials science, enhancing the prediction of polymer properties through multimodal approaches [32] and predicting the synthesizability and selecting precursors for inorganic compounds [17]. Transformer models have also been successfully applied to predict peptide self-assembly [30] and optimize peptide synthesis outcomes [43].​  

A significant advancement highlighted by recent research is the development of AI agent systems capable of planning, designing, and executing scientific experiments autonomously or semi-autonomously [12,38,44]. Platforms like SynAsk and ChemCrow demonstrate the power of domain-specific LLMs and integrated tools for tasks such as information retrieval, reaction prediction, and retrosynthesis in organic chemistry, often surpassing general models like GPT-4 in chemical  

factuality and reasoning for complex tasks [13,22,27]. The development of chemistry-specific LLMs, such as ChemLLM, further underscores the potential for models fine-tuned on chemical knowledge to achieve performance comparable to state-of-the-art general models in various chemical tasks [4,23]. Furthermore, LLMs are proving useful in extracting structured information from chemical literature, supporting tasks like chemical patent analysis and materials chemistry information processing [40]. Their utility also extends to areas like microbiome and metagenomic research, including gene cluster identification and microbe-disease association extraction [39].​  

Despite these significant advances, the path towards fully realizing the potential of LLMs in chemistry is not without challenges. A key challenge lies in the effective deployment and alignment of generative models with realistic discovery endpoints [33], along with the need for increased focus on practical applications and experimental validation [2]. The limitations of current molecular representations, particularly string-based methods, which require large datasets and powerful architectures, need to be addressed. While Transformers excel at capturing long-range dependencies, understanding their "black box" nature remains difficult, particularly concerning complex structural features like chirality, which requires extensive training [21,42]. Moreover, developing models with greater universality, interpretability, and seamless integration into existing workflows and tools presents ongoing hurdles [20]. Characterizing and addressing different sources of uncertainty in model predictions (data noise, model architecture, representation choice, etc.) is also crucial for improving reliability and performance [45]. Finally, as autonomous systems become more sophisticated, addressing safety and dual-use concerns is paramount [5].​  

Addressing these limitations necessitates continued research and development. This includes advancing model architectures and training strategies, focusing on domain-specific fine-tuning and knowledge integration to enhance chemical cognitive abilities [17,18,22]. The development of open benchmarks is essential for rigorous evaluation and comparison of different models and approaches [1,18]. Improving model interpretability, perhaps through explainable AI methods and analysis of attention mechanisms [1,32], is vital for building user trust and facilitating scientific insight. Developing interactive strategies that effectively combine AI predictions with expert human knowledge will also be critical for practical success [1]. Ultimately, the integration of LLMs with experimental platforms and tools will accelerate the cycle of hypothesis generation, experimentation, and data analysis.​  

The convergence of AI and science, with LLMs at the forefront, is poised to usher in a "golden age" of scientific discovery [3]. LLMs hold the potential to become indispensable "intelligent microscopes" for researchers, enabling autonomous end-toend chemical synthesis development and the design of novel molecules and materials with unprecedented speed and efficiency [18,44]. Realizing this potential requires sustained effort to refine models, build robust infrastructure, integrate domain expertise, and navigate ethical considerations. The long-term impact of LLMs on chemistry is expected to be profound, fundamentally changing how chemists approach research, leading to breakthroughs in medicine, materials science, and beyond [7,9,10,28,41].  

# References  

[1] 生成式AI助力药物分子结构优化：JACS综述 https://cloud.tencent.com/developer/article/2479335  

[2] 深度学习在分子生成中的应用进展综述 https://mp.weixin.qq.com/s?  
__biz=MzA5MDY3ODExNQ $\scriptstyle = =$ &mid=2651363235&idx=1&sn=4410dc08b189e802e4393d8a680a758a&chksm $\scriptstyle 1 =$ 8bfbc1afbc8c48b  
926e323b3ed5de652e39758ce482e59e9168f0082cb474303160b95327c10&scene=27  
[3] AI X Science：开启科学发现的黄金时代 https://mp.weixin.qq.com/s?  
__biz=MzIzMjQyNzQ5MA $\scriptstyle { \underline { { = } } } =$ &mid $\mathbf { \Psi } = \mathbf { \Psi }$ 2247712363&idx $\varXi$ 1&sn $\ c =$ 238499b3ead6fc30eb69b9d3edbcabfc&chksm=e93d9cba0cb7c2e7  
33f3f15cf2ae03d72612ac826977e47e0b1f18969170ebb1a7fe8f1c8d9e&scene $^ { - 2 7 }$   
[4] ChemLLM：上海AI Lab发布，百万数据加持，能力比肩GPT-4的化学大模型 https://baijiahao.baidu.com/s?  
id=1809241838013840475&wfr $\varXi$ spider&for=pc​  
[5] LLM驱动的自主化学研究：Coscientist在Nature正刊的突破 https://blog.csdn.net/Kakaxiii/article/details/146939255  
[6] 麻省理工学院：大语言模型赋能电化学C-H氧化反应探索 https://hub.baai.ac.cn/view/42270​  
[7] AI 颠覆材料化学：2024 科研成果精选 https://hub.baai.ac.cn/view/41996  

[8] Machine Learning Advances in Bio-cheminformatics:  https://pubs.acs.org/doi/full/10.1021/acs.jcim.4c00444  

[9] 大型语言模型：生物学研究的下一个前沿 https://mp.weixin.qq.com/s? _biz=MzIzMjQyNzQ5MA $\scriptstyle = =$ &mid=2247669842&idx=2&sn=74825ec86710d7958ea3511d0481113e&chksm=e89902dfdfee8bc9   
ecebaffcb191c9c7ccad8d3ddffb234e1cab97e51006df788fee76e0c5cb&scene=27   
[10] AI赋能化学：跨学科探索与KNN应用 https://mp.weixin.qq.com/s?   
__biz=MzA5NjQyMjQwNQ $\scriptstyle = =$ &mid=2649964896&idx $\mathbf { \Psi } : =$ 1&sn $=$ 1212a39f3ef7a5d9274796c80dc60c48&chksm $\scriptstyle 1 =$ 896e10be9dae7662   
c30231c28068e1bd27a63131bfc57976acfad01c7c6b2fef5098bead59dc&scene $^ { = 2 7 }$   
[11] Transformer模型：快速“翻译”生物活性分子用于药物设计 https://cloud.tencent.com/developer/article/2330816​   
[12] Nature重磅：GPT-4驱动AI自主化学实验研究！ https://baijiahao.baidu.com/s?   
id=1785943802032876429&wfr=spider&for=pc​   
[13] 广州国家实验室发布SynAsk：AI大模型赋能有机合成 https://mp.weixin.qq.com/s? _biz=MzAxNjE4NTE5MQ $\scriptstyle = =$ &mid $=$ 2651328420&idx $\mathop { : = }$ 1&sn=4d854d8b9496e2ae05a60ad2aff254ec&chksm $\mid =$ 815a61028a39e18d   
bbd4f014a2d6137d7812e974674248545ad10f803c75d517c696c8c6d95e&scene=27​   
[14] Transformer模型助力快速生成活性分子用于药物设计 https://baijiahao.baidu.com/s?   
id=1763751059754960883&wfr=spider&for=pc​   
[15] ChemNLP：用于材料化学文本数据的自然语言处理库 https://hub.baai.ac.cn/view/32316   
[16] 基因泰克：机器学习辅助分子生成前沿进展 https://cloud.tencent.com/developer/article/2434494   
[17] LLMs for Inorganic Synthesis Prediction https://pubs.acs.org/doi/epdf/10.1021/jacs.4c05840​   
[18] ChemMatch：学术级化学问答数据集与高效模型，助力语言模型化学研究   
https://blog.csdn.net/VinciB/article/details/145971949​   
[19] Deep Learning for Lead Optimization: Generative AI https://arxiv.org/abs/2404.19230?context=q-bio.BM​   
[20] Transformer在计算化学中的应用：苏大/澳科大JPCL综述 https://www.x-mol.com/news/910990​   
[21] Transformer快速学习分子结构，手性识别成“拦路虎” https://baijiahao.baidu.com/s?   
id=1791952975415220721&wfr=spider&for=pc​   
[22] SynAsk: A Domain-Specific LLM Platform for Organic   
https://pubs.rsc.org/id/content/articlelanding/2025/sc/d4sc04757e   
[23] ChemLLM：化学大语言模型及其在指令微调和基准测试中的应用   
https://blog.csdn.net/haimianxiaobao11/article/details/145639787​   
[24] 迁移学习Transformer模型助力Heck反应预测 http://baijiahao.baidu.com/s?id $\ c =$ 1686742084321301286&wfr=spider&for=pc​   
[25] Generative AI for Molecular Equilibrium Distributi https://www.microsoft.com/en-us/research/articles/generative-ai  
meets-structural-biology-equilibrium-distribution-prediction/?locale=zh-cn​   
[26] ChatGPT化学助手：加速科学发现，革新MOF合成研究 https://baijiahao.baidu.com/s?   
id=1774553488235713013&wfr=spider&for=pc​   
[27] ChemCrow：大语言模型赋能化学研究的工具集成方案 https://baijiahao.baidu.com/s?   
id=1771135984154678436&wfr=spider&for=pc​   
[28] AI 赋能材料化学：2024 年度科研成果精选 https://baijiahao.baidu.com/s?id $=$ 1818871565713526421&wfr=spider&for=pc​   
[29] AI大语言模型最新进展：图书馆前沿文献专题推荐 https://lib.bupt.edu.cn/a/zuixingonggao/2024/0409/4564.html​   
[30] 西湖大学Transformer模型破解百亿多肽自组装法则 https://36kr.com/p/2481057167071111​   
[31] DeepSA：基于深度学习的化合物合成可及性预测 https://hub.baai.ac.cn/view/34652   
[32] Multimodal Transformer for Enhanced Polymer Proper https://pubs.acs.org/doi/pdf/10.1021/acsami.4c01207   
[33] 机器学习辅助的生成式分子设计 https://www.nature.com/articles/s42256-024-00843-5​   
[34] AI Transforming Healthcare: NVIDIA's Ecosystem and https://www.nvidia.com/en-us/industries/healthcare-life-sciences/   
[35] 人工智能赋能新药研发：降本增效，潜力无限 https://baijiahao.baidu.com/s?   
id=1828361449587579109&wfr=spider&for=pc​   
[36] 药物发现中的深度学习应用与展望：以COVID-19为例 https://mp.weixin.qq.com/s? biz=MzA4ODY4MDE0NA $\scriptstyle = =$ &mid=2247554448&idx=4&sn=53a428173a5b26baabe56ce57f31eb0d&chksm=9024262fa753af39   
7f380bf648bb3e66839fafceff0af9ea84c10701d5c723dfc5f1285f10d1&scene=27​   
[37] AI小分子药物发现：康奈尔等Nature子刊综述 https://baijiahao.baidu.com/s?   
id=1802727252675993214&wfr=spider&for=pc​   
[38] Autonomous Chemical Research with Language Model-D https://www.nature.com/articles/s41586-023-06792-0   
[39] Deep Learning and Language Models for Microbiome R   
https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2024.1494474/abstract​   
[40] ChEMU 2020: NLP for Information Extraction from Ch https://www.frontiersin.org/journals/research-metrics-and  
analytics/articles/10.3389/frma.2021.654438/full​   
[41] Machine Learning and Knowledge Discovery for Mater   
https://www.frontiersin.org/articles/10.3389/fchem.2022.930369/full​   
[42] Transformers in Cheminformatics: A Review of Appli https://pubs.acs.org/doi/full/10.1021/acs.jcim.3c02070   
[43] Deep Learning for Prediction and Optimization of F https://pubs.acs.org/doi/10.1021/acscentsci.0c00979​   
[44] LLM-Powered Platform for Autonomous Chemical Synth https://www.nature.com/articles/s41467-024-54457-x​   
[45] Uncertainty Characterization in Machine Learning f https://pubs.acs.org/doi/10.1021/acs.jcim.3c00373  