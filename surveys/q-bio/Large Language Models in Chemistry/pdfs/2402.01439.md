# From Words to Molecules: A Survey of Large Language Models in Chemistry  

Chang Liao1 , Yemin $\mathbf { Y } \mathbf { u } ^ { 2 }$ , Yu Mei3 and Ying Wei1 1School of Computer Science and Engineering, Nanyang Technological University, Singapore 2City University of Hong Kong 3College of Computer Science and Technology, Zhejiang University chang019@e.ntu.edu.sg, yeminyu2-c $@$ my.cityu.edu.hk, meiyu1997 $@$ zju.edu.cn, ying.wei $@$ ntu.edu.sg  

# Abstract  

In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their application in chemistry tasks. Finally, we identify promising research directions, including further integration with chemical knowledge, advancements in continual learning, and improvements in model interpretability, paving the way for groundbreaking developments in the field.  

# 1 Introduction  

Humans understand and describe their environment using natural language, which reflects the complexity of human thought. The emergence of Large Language Models (LLMs) marks a significant advancement in artificial intelligence, showcasing remarkable abilities in various domains. These models excel at understanding and generating complex text, making them crucial for tasks that demand deep textual analysis and creation.  

Intriguingly, a scientific domain such as chemistry has its unique language, akin to the way humans utilize natural languages. In chemical processes, bonds are broken, and atoms are exchanged during reactions, similar to how syntax operates, while molecules are formed within specific physical constraints, echoing the principles of grammar. This parallel suggests the potential for encoding chemical information into LLMs in a manner comparable to natural language. Despite the conceptual parallels, the languages of chemistry and human communication differ substantially in their semantics. Consequently, incorporating chemical knowledge into LLMs presents a complex challenge, with numerous approaches being explored to leverage LLMs in the field of chemistry, making it a subject of considerable interest.  

![](images/c1e2f2476e8165fbb1b5eb664e6b783990629d9c7e826487327f8322fafcca8d.jpg)  
Figure 1: LLMs for Chemistry: Applications and Paradigms  

With numerous approaches being proposed, there’s currently no systematic survey focused specifically on the application of Large Language Models (LLMs) in the field of chemistry. [Xia et al., 2023] comes closest with a systematic survey on chemical pretrained models, encompassing LLMs along with pretrained models in other modalities such as graph and image. However, this survey primarily addresses the general objectives and utilization of pre-trained models and overlooks the nuanced application paradigms of LLMs, treating them primarily as representation learners. This perspective neglects the unique ways LLMs can be integrated into chemical research, as illustrated in Figure 1.  

To differentiate our work from [Xia et al., 2023], we summarize our contributions as follows:  

1. We provide a comprehensive review of tokenization methods for molecular sequences, categorizing them based on their granularity.   
2. We offer a systematic taxonomy of existing approaches, based on the nature of pretraining data, and discuss  

![](images/db0b927e8c1306dbd95eb84dd17736c7e9ec5e2f05af9c4012764f072ee5aefc.jpg)  
Figure 2: An overview of topics in this paper, with dash lines indicating their applicability to various downstream tasks.  

methodologies for adapting chemical data within LLM frameworks, including how to integrate chemical data with other domains or modalities to enhance LLM performance.  

3. We investigate the nuances of applying self-supervised learning on chemical data, highlighting domain-specific opportunities and examining tailored techniques for chemical tasks.   
4. We identify unique paradigms for LLM utilization in chemistry, presenting applications exclusive to their capabilities and elucidating novel contributions to chemical research.   
5. We outline several promising future research directions, exploring emerging trends in both chemistry and LLM development that could significantly advance the interdisciplinary field of chemical LLMs.  

The structure of this survey is depicted in Figure 2, serving as a guide for readers throughout this paper.  

# 2 Molecule encoding methods  

In order for LLMs to learn from molecules, molecules must be represented in a series of discrete tokens. In this section, we will provide a concise overview of contemporary methods for molecular representation and tokenization.  

# 2.1 Representations Methods of Molecules  

Fingerprint Representations Molecular fingerprints are typically represented as a binary string (a series of 0s and 1s), where each position in the string (bit) corresponds to a particular structural feature or property of the molecule. For instance, one bit might represent the presence or absence of a certain chemical group. There are several types of fingerprints, each capturing different aspects of molecular structure like molecular access system (MACCS) keys [Durant et al., 2002] and ECFP (Extended-Connectivity Fingerprints) [Rogers and Hahn, 2010].  

Sequential Representations Simplified Molecular-Input Line-Entry System (SMILES) [Weininger, 1988] is the first sequential molecular representation, it is compact and human-readable. However, SMILES suffers from (1) nonuniqueness, as a single molecule could be represented by multiple valid SMILES strings, (2) non-robustness, as SMILES strings do not inherently ensure chemically feasible structures, (3) information-loss, as SMILES doesn’t explicitly convey structural information. Several innovations have been proposed to address those limitations. SELFIES [Krenn et al., 2020] ensures robustness with strict derivation rules. International Union of Pure and Applied Chemistry (IUPAC) Chemical Identifier (InChI) [Heller et al., 2013] focuses on uniqueness through a complex hierarchical representation convention.  

Graph Representations Molecular graph representations, crucial in cheminformatics and drug design, vary from twodimensional to high-dimensional forms. Two-dimensional (2D) types, such as molecular fingerprints (ECFP) [Rogers and Hahn, 2010], condense molecular structures into vectors for simpler similarity analysis but may overlook complex conformations. To compensate, high-dimensional representations, including three-dimensional (3D) details, have been developed. These employ a 3D coordinate system to accurately represent molecular structures. Further, four-dimensional (4D) molecular graph representations [Hopfinger et al., 1997] capture weighted utilization of diverse spatial configurations of molecules, enhancing understanding of molecular structures and interactions.  

# 2.2 Tokenization Methods of Molecules  

The tokenization of molecule sequences can be primarily categorized into three lines of approaches: (1) character-level, (2) atom-level, and (3) motif-level.  

Character-level tokenization treats each character as a separate token, leading to erroneous splitting of multicharacter entities like ‘Br’. However, despite implausibility, this approach has shown effectiveness in chemical LLMs, as demonstrated in [Wang et al., 2019; Edwards et al., 2022; Lu and Zhang, 2022; Winter et al., 2022], underscoring LLMs’ impressive comprehension capabilities.  

Atom-level tokenization methods offer a more rational approach by segmenting sequences into atoms. Recent sequential representation methods have introduced customized atom-level tokenizers, as seen in [Heller et al., 2013;  

![](images/61384de0f79feffcb17bd08517d8b8cce2c02ddaefac8b1566e4585a5f03985a.jpg)  
Figure 3: An Example of Tokenized Output from Different Tokenizers for the Sequence “NC $\scriptstyle ( = 0$ )COc1ccc(Br)cc1”  

Krenn et al., 2020]. These in-house tokenizers are tailored to efficiently manage unique characters, such as “[Ring1]” in SELFIES and “/h” in InChI, which are essential for representing complex chemical structures within their respective formats. In contrast, SMILES does not come with a builtin tokenizer and is typically tokenized using regex-based expressions, as detailed in [Schwaller et al., 2018].  

Motif-level tokenization methods can be done with chemistry-driven approaches or data-driven approaches. Chemistry-driven approaches involve breaking molecules into chemically meaningful substructures with the help of expert chemical knowledge. These methods are prominently featured in graph-related studies [Jin et al., 2018; Rong et al., 2020; Zhang et al., 2021b]. For instance, [Rong et al., 2020] matches molecules to a database of functional groups, [Jin et al., 2018] forms motifs by applying customized fragmentation rules to break bonds, and [Zhang et al., 2021b] combines breaking of retrosynthetically interesting chemical substructures (BRICS) [Degen et al., 2008] with customized fragmentation rules to achieve a finer granularity. Following those methodologies, [Feng et al., 2023; Xie et al., 2023] have performed motif-level tokenization on sequential molecular representations. This involves breaking down molecules in the graph domain and transforming the fragmented graph motifs to sequential representations. Although these approaches produce chemically sound substructures, defining customized rules requires expert knowledge. Moreover, the process of sequence-graph transformation and matching predefined patterns like functional groups or BRICS, can be costly.  

Data-driven approaches are inspired by subword-level tokenization methods like byte-pair encoding (BPE) [Gage, 1994] in natural language processing, which iteratively merges the most frequent pairs of characters into a single token. A similar tokenization method was proposed by [Li and Fourches, 2021] on SMILES. Other works have also adopted this subword-level tokenization by directly applying BPE on molecular sequential representation corpora [Chithrananda et al., 2020; Zhu et al., 2022; Ahmad et al., 2022; Xue et al., 2022; Chilingaryan et al., 2022; Christofidellis et al., 2023; Li et al., 2023b; Liu et al., 2023b; Liu et al., 2023a]. The methodology of uncovering common subpatterns in subword tokenization is akin to the discovery of motifs in chemistry, justifies the classification of this method as motif-level.  

For illustrative examples of these various tokenization methods, please refer to Figure 3, which depicts a comprehensive comparison of previously mentioned tokenizers on an example SMILES string NC( $\scriptstyle - 0$ )COc1ccc(Br)cc1.  

# 3 Taxonomy  

We categorize current methodologies into three distinct groups by the knowledge within the pretraining corpus:  

1. Single-domain approaches pretrain their models purely on tokens from the chemical domain, i.e., sequential molecular representations and/or molecular properties tokens, without any tokens from the common text domain. Datasets containing vast amounts of molecules like PubChem [Kim et al., 2016] are adopted for this category. This kind of approach is mostly for representation learning and overlaps with pretrained models reviewed in [Xia et al., 2023], but more insights will be provided in the following sections for readers to appreciate the nuances of applying LLMs in the chemistry domain.  

2. Multi-Domain Approaches entail pretraining on a corpus or corpora that merges chemical and common textual tokens. The Colossal Clean Crawled Corpus (C4) [Raffel et al., 2020] is widely recognized for general text pretraining. Additionally, datasets featuring textual molecular descriptions aid in connecting chemical and text tokens. Notable datasets in this category include ChEBI-20 [Edwards et al., 2021], PCDes [Zeng et al., 2022], PubChemSTM [Liu et al., 2023b], PubChem324k [Liu et al., 2023d], and MoMu [Su et al., 2022]. Embedding specific chemical tasks within text sequences further enhances multi-domain pretraining. Mol-Instructions [Fang et al., 2023a] exemplifies this, offering a rich dataset for multi-domain pretraining with sentences crafted for tasks such as property prediction, reaction prediction, and also molecule description.  

There are multiple ways to mix tokens from different domains. MolT5 [Edwards et al., 2022] pretrains on sequences from different domains in one mini-batch. For instance, it processes a chemical sequence like $\therefore \mathrm { O N = C C C 1 = c [ N H 1 ] C 2 = C C = C C = C 1 2 ^ { \circ } }$ alongside a common text sequence such as “Lissamine fast yellow(2-) is an organosulfonate oxoanion resulting from the removal of a proton.” in one batch and perform pretraining objectives on them separately.  

A more common approach is to “wrap” tokens from different domains together in one sentence. For example, “Acetylsalicylic acid $\mathrm { C C ( = O ) O c 1 c c c c 1 C ( = O ) O }$ appears as odorless white crystals or crystalline powder with a slightly bitter taste.” contains both chemical sequence $\scriptstyle \cdot \mathbf { C C } ( = \mathrm { O } ) \mathbf { O c } 1 \csc \mathbf { c c } 1 \mathbf { C } ( = \mathrm { O } ) \mathrm { O } ^ { , , }$ and text sequence describing its properties. However, a notable challenge arises from this method: identical tokens from different domains can represent distinct entities. For example, the character “C” might denote a carbon atom in molecular sequences, cysteine in protein structures, or simply the letter “c” in text. To resolve this ambiguity, specific tokens are often employed to delineate sequential representations, marking their beginning and end like [START SMILES] and [END SMILES]. Additionally, domain-specific indicators such as [SMILES C], [PROTEINS C], and [C] can be added to the vocabulary to clearly differentiate the context of each token.  

Multi-modal approaches advance further by incorporating information from various modalities into LLM. Molecular fingerprints, molecular graphs, and their corresponding images are frequently utilized alongside general text and molecular sequences. Each data modality is processed via specialized encoders, such as Transformer [Vaswani et al., 2017] for fingerprints, GIN [Xu et al., 2019] for 2D graphs, SchNet [Sch¨utt et al., 2017] for 3D graphs, and ResNet [He et al., 2016] for images. Regarding the corresponding training data in different modalities, the previously mentioned molecular dataset, such as [Irwin et al., 2020], can be used to retrieve molecular sequences and generate data in other modalities like graphs, fingerprints, and images with the help of cheminformatics tools like RDKit. Apart from being generated with cheminformatics tools, graph data can also be retrieved from datasets such as GEOM-Drugs [Axelrod and Go´mez-Bombarelli, 2022]. Text encoders can be pretrained on datasets that include molecular descriptions like ChEBI-20 [Edwards et al., 2021]. After pretraining on different modalities, several adaptors or cross-modal attention layers are employed to align their latent spaces with that of LLMs for integration. This alignment presents a significant challenge, which will be thoroughly examined in the following sections.  

# 4 Methods  

This section delves into current pretraining methodologies, contrasting single-modal with multi-modal objectives. A detailed review of these methods is provided in Table 1.  

# 4.1 Language Modelling Objectives  

In this subsection, we review three key pretraining objectives: Masked Language Modeling (MLM), which directly applies to chemical LLMs; Molecule Property Prediction (MPP), specific to chemical domains; and Autoregressive Token Generation (ATG), adapted with chemistry-specific tasks for enhanced relevance in chemical LLMs. A graphical illustration for these objectives is shown in Figure 4.  

Masked Language Modelling (MLM) is a prevalent pretraining objective for LLMs. It randomly substitutes tokens in the input sequence with a special “[Mask]” token or another arbitrary token from the vocabulary. The models are then trained to predict these masked tokens based on the surrounding context. For chemical LLMs, MLM is conducted on molecular sequential representations such as SMILES or SELFIES for single-domain approaches, and on wrapped sentences for multi-domain and multi-modal approaches. We define this objective as:  

$$
\mathcal { L } _ { \mathrm { M L M } } = - \mathbb { E } _ { S \in \mathcal { D } } \left[ \sum _ { S ^ { \prime } \in m ( S ) } \log p ( S ^ { \prime } | S \setminus m ( S ^ { \prime } ) ) \right]
$$  

where $m ( \boldsymbol { S } )$ represents the masked tokens in input sequence $s$ . In single-domain approaches, SMILES-BERT [Wang et al., 2019], ChemBERTa [Chithrananda et al., 2020], MGBERT [Zhang et al., 2021a], Molformer [Ross et al., 2022], Selformer [Yu¨ksel et al., 2023], T5 Chem [Lu and Zhang, 2022], Chemformer [Irwin et al., 2022] and BARTSmiles [Chilingaryan et al., 2022] employ this objective on SMILES or SELFIES directly. MG-BERT [Zhang et al., 2021a] also enhances its input by incorporating graph adjacency knowledge, ensuring that attention calculations are confined to neighboring atoms only. Beyond single-domain, MLM extends to multi-domain and multi-modal approaches. KVPLM [Zeng et al., 2022], BioT5 [Pei et al., 2023], MultiTask Text+Chem T5 [Christofidellis et al., 2023] perform MLM on wrapped sentences, DMP [Zhu et al., 2023], Memo [Zhu et al., 2022], UniMap [Feng et al., 2023] leverage this objective for pretraining text encoders.  

Molecular Property Prediction (MPP) objective pretrains chemical LLMs to predict molecular properties given molecular sequential representations. This technique generates properties using cheminformatics tools such as RDKit, eliminating the requirement for manual labelling. These properties normally reflect the intrinsic semantics of molecules and typically include molecular weight, rotatable bond count, topological polar surface area, and the fraction of carbon atoms that are SP3 hybridized. It can be formally defined as  

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { M P P } } = - \mathbb { E } _ { \mathcal { M } \in \mathcal { D } } \log p ( \mathcal { P } | \mathcal { M } ) } \end{array}
$$  

where $\mathcal { P }$ is the set of calculated virtual properties for molecule $\mathcal { M }$ . ChemBERTa-2 [Ahmad et al., 2022] conducts both MLM and MPP pretraining on a subset of PubChem [Chithrananda et al., 2020], uncovering that MPP yields better converged performance albeit at the expense of increased training time. SPT [Winter et al., 2022] also utilizes the MPP at the early stage of pretraining and progresses to a laboratory-verified dataset to amplify performance. However, due to the design of MPP, MPP-based models are primarily used for molecular representation learning, as they cannot naturally generate tokens apart from properties.  

Autoregressive Token Generation (ATG) refers to the scheme of generating the next token based on previous tokens. This objective can be formally defined as  

$$
\mathcal { L } _ { \mathrm { A T G } } = - \mathbb { E } _ { S \in \mathcal { D } } \mathbb { E } _ { t \in S } p ( t _ { i } | t _ { 0 } , t _ { 1 } , . . . , t _ { i - 1 } )
$$  

where $t _ { i }$ stands for the ith token current predictin based on previous tokens $t _ { 0 } , . . . , t _ { i - 1 }$ of the same sequence $s$ . Data for downstream tasks, transformed into wrapped sentences for ATG pretraining, helps chemical LLMs adapt seamlessly. Given the significant overlap between ATG and downstream tasks, this section introduces pretraining-specific ATG tasks, with more downstream tasks usable for ATG pretraining available in Section 5.  

Table 1: Overview of Existing Approaches: The table columns on Backbone, Tokenization Methods,and Pretraining Objectives specifically address the corresponding aspects related to chemical sequences in multi-domain and multi-modal approaches, including objectives for text encoders and alignment strategies in multi-modal settings.   


<html><body><table><tr><td colspan="7"></td></tr><tr><td>Method SMILES-BERT</td><td>Backbone BERT</td><td>Input Representation SMILES</td><td>Tokenization Methods Character-level</td><td>Pretraining Objectives MLM</td><td>Applications Molecule Property Prediction</td><td>Reference [Wang et al., 2019]</td></tr><tr><td>Molecular Transformer</td><td>EAeoderesider</td><td>SMILES</td><td>Atom-level</td><td>(RectioATredicioon</td><td>Reaction Prediction</td><td>[Schwaller et al.,2019]</td></tr><tr><td></td><td>RoBERTa</td><td>SMILES/SELFIES</td><td>Atom-level</td><td>MLM</td><td>Molecule Property Prediction</td><td>[Chithranandaetal.,0]</td></tr><tr><td>ChemBERTa ChemBERTa-2</td><td>RoBERTa</td><td>SMILES</td><td>Atom-level/Motif-level</td><td>MLM/MPR</td><td>Molecule Property Prediction</td><td>[Ahmad et al., 2022]</td></tr><tr><td>MG-BERT</td><td>BERT</td><td>SMILES + Adjancency</td><td>Atom-level</td><td>MLM</td><td>Molecule Property Prediction</td><td>[Zhang et al.,2021a]</td></tr><tr><td>X-Mol</td><td></td><td>SMILES</td><td>Motif-level</td><td>(RepentaiTravsaia</td><td></td><td>[Xue et al.,2022]</td></tr><tr><td>ChemFormer</td><td>BART</td><td> SMILEs</td><td>Atom-level</td><td>(ReresSMLM/ATGnstato)</td><td></td><td>[Irwin et al., 2022]</td></tr><tr><td>BARTSmiles</td><td>BART</td><td>SMILES</td><td>Motif-level</td><td>MLM</td><td>Molecule onPre Predatcion</td><td>[Chilingaryan et al.,22]</td></tr><tr><td>SPT</td><td>GPT-3</td><td>SMILES + Temperature</td><td>Character-level</td><td>MPR</td><td>Molecule Property Prediction Reaction Type Classification</td><td>[Winter et al., 2022]</td></tr><tr><td>T5 Chem</td><td>T5</td><td>SMILES</td><td>Character-level</td><td>MLM</td><td>Reaction Yield Prediction Reaction Prediction</td><td>[Lu and Zhang,2022]</td></tr><tr><td>MM-Deacon</td><td>Transformer</td><td>SMILES + IUPAC</td><td>Motif-level</td><td>XDC</td><td>Molecule Property Prediction Cross-lingual Retrieval Drug-Drug Interaction</td><td>[Guo et al., 2022]</td></tr><tr><td>Regression Transformer</td><td>XLNet</td><td>SELFIES</td><td>Atom-level + Numerical</td><td>(PropertyPreictioon)</td><td>Molecular Property Prediction</td><td>[Born and Manica, 2023]</td></tr><tr><td>ChemGPT</td><td>GPT-3</td><td>SELFIES</td><td>Atom-level</td><td>(Molecul Gompletin)</td><td>NA</td><td>[Frey et al.,2023]</td></tr><tr><td>MolFormer</td><td>RoFormer</td><td>SMILES</td><td>Atom-level</td><td>MLM</td><td>Molecule Property Prediction</td><td>[Wu et al., 2023]</td></tr><tr><td>Selformer</td><td>RoBERTa</td><td>SELFIES</td><td>Motif-level</td><td>MLM</td><td>Molecule Property Prediction Molecule Property Prediction</td><td>[Yuksel et al.,2023]</td></tr><tr><td>KV-PLM</td><td>BERT</td><td>SMILES, Text</td><td>Motif-level</td><td>MLM</td><td>ReaMioleculeCptsicaion</td><td>[Zeng et al., 2022]</td></tr><tr><td>MolT5</td><td>T5</td><td>SMILES, Text</td><td>Character-level</td><td>MLM</td><td></td><td>[Edwards et al.,2022]</td></tr><tr><td>PrefixMol</td><td>GPT-3</td><td> Prefx Embeding</td><td>Motif-level</td><td>(De Novo Molecule Generation)</td><td>De Novo Molecule Generation Molecule Property Prediction</td><td>[Gao et al.,2023]</td></tr><tr><td>BioT5</td><td>T5</td><td>SELFIES, Text,FATA</td><td>Motif-level</td><td>De No</td><td></td><td>[Pei et al.,2023]</td></tr><tr><td>MolGen</td><td>BART</td><td>SELFIES</td><td>Motif-level</td><td>MLM + Prefix Tuning</td><td>Protein Property Prediction</td><td>[Fang et al.,2023b]</td></tr><tr><td>MolXPT</td><td>GPT2</td><td>SMILES, Text</td><td>Atom-level</td><td>ATG (Text Completion,Molecule Completion, Molecule Captioning)</td><td>Molecule Property Prediction MoleculeCaptioning De novo Molecule Generation</td><td>[Liuet al, 2023c]</td></tr><tr><td>Text+Chem T5</td><td>T5</td><td> SMILES, Text</td><td>Motif-level</td><td>MLM</td><td>Reaction Prediction Paragraph to Action</td><td>[Christofidellis et al.,2023]</td></tr><tr><td>nacho</td><td>T5</td><td>SMILES, Text</td><td>Atom-level</td><td></td><td></td><td>[Livne et al., 2023]</td></tr><tr><td>DrugGPT</td><td>GPT-2</td><td>SMILES,Text, FASTA</td><td>Motif-level</td><td>(Moleul Compltion)</td><td>Drug Discovery</td><td>[Li et al.,2023b]</td></tr><tr><td>Text2Mol</td><td>SciBERT</td><td>Graph, Text</td><td>Atom-level</td><td>XMC</td><td>Cross-modal Retrieval</td><td>[Edwards et al.,2021]</td></tr><tr><td>DMP</td><td>RoBERTa</td><td>Graph, SMILES</td><td>Atom-level</td><td>MLM + XMC</td><td></td><td>[Zhu et al.,2023]</td></tr><tr><td>MoMu</td><td>SciBERT</td><td>Graph, Text</td><td>Atom-level</td><td>XMC</td><td>Molecule Property Prediction Molecule Captioning De novo Molecule Generataion</td><td>[Su et al., 2022]</td></tr><tr><td>MolCA</td><td>Galactica/MolT5</td><td>SMILES, Graph, Text</td><td>Motif-level</td><td>(MoleMC + ATning)</td><td></td><td>[Liu e a., 2023d]</td></tr><tr><td>MolSTM</td><td>Chemformr</td><td>SMILES, Graph, Text</td><td>Atom-level</td><td>XMC</td><td></td><td>[Liu et al.,2023b]</td></tr><tr><td>GIT-Mol</td><td>SciBERT</td><td></td><td>Character-level</td><td>XMC</td><td>Molecule Property Prediction</td><td>[Liu et al., 2023a]</td></tr><tr><td>GIMLET</td><td>T5</td><td>Graph, Text</td><td>Atom-level</td><td>(MoleculePrpty redicticn)</td><td>MoleulePot t reac</td><td>[Zhao et al.,2023]</td></tr><tr><td>UniMap</td><td>RoBERTa</td><td>SMILES, Graph</td><td>Motif-level</td><td>MLM + XMC</td><td>MoleuleProperty Preieto</td><td>[Feng et al.,2023]</td></tr><tr><td>Memo</td><td>RoBERTa</td><td></td><td>Motif-level</td><td>MLM + XMC</td><td>Molecule Property Prediction</td><td>[Zhu et al.,2022]</td></tr></table></body></html>  

![](images/d001c96110c775abf12243b5301e49aa55aebe69ba4f7a24952ef701f179bd7e.jpg)  
Figure 4: Language Modelling Objectives  

Molecule Completion is the most straightforward approach wherein models are trained to complete the sequential representation of molecules. ChemGPT [Frey et al., 2023], MolXPT [Liu et al., 2023c], and DrugGPT [Li et al., 2023b] utilize this task to discern the underlying semantics of molecular sequential representations.  

Representation Translation involves generating an alternative, viable representation from a given input sequence. Pioneering examples include X-mol [Xue et al., 2022] and Chemformer [Irwin et al., 2022], which predict the canonical SMILES from a masked alternative SMILES representation.  

# 4.2 Cross-modal Objective  

Cross-modal Contrastive (XMC) learning maximizes the similarity within positive example pairs, while simultaneously emphasizes the distinction of negative pair examples. In the context of chemistry-related multi-modal contrastive learning, representations of the same molecule, when presented across different modalities, are classified as positive pairs. Conversely, representations stemming from different molecules are designated as negative pairs, regardless of whether they occur within the same modality.  

In the domain of contrastive learning, Info-NCE(NoiseContrastive Estimation) loss [Oord et al., 2019] is a common training objective. It is defined as  

$$
\mathcal { L } _ { \mathrm { I n f o - N C E } } = - \mathbb { E } \left[ \sum _ { M _ { x } , M _ { y } \in M } \log \frac { \exp ( \sin z _ { i } ^ { M _ { x } } , z _ { i } ^ { M _ { y } } ) / \tau ) } { \sum _ { k = 1 } ^ { K } \exp ( \sin ( z _ { i } ^ { M _ { x } } , z _ { i } ^ { M _ { y } } ) / \tau ) } \right]
$$  

where $M _ { x } , M _ { y }$ are two modalities of the overall modalities set $M$ and ziMx represents the hidden representation of ith example in modality $M _ { x }$ . MM-Deacon [Guo et al., 2022] applies Info-NCE objective on multi-lingual representations for molecules. Momu [Su et al., 2022], MOCO [?], MolCA [Liu et al., 2023d], MolSTM [Liu et al., 2023b] deploy InfoNCE loss to align representations from different modalities.  

Contrastive learning can also be employed by predicting whether two representations correspond to the same molecule. DMP [Zhu et al., 2023] enhances prediction accuracy by directly maximizing the cosine similarity between encodings of molecule graph and SMILES. Text2mol [Edwards et al., 2021] employs negative sampling loss. MolCA [Liu et al., 2023d], GIT-Mol [Liu et al., 2023a], and UniMap [Feng et al., 2023] introduce additional prediction layers for contrastive learning, which are subsequently removed after pretraining. It’s important to note that current cross-modal contrastive learning in chemical LLMs directly adapts from general contrastive learning approaches, potentially overlooking domain-specific nuances. Recently, the introduction of the reaction-aware contrastive learning framework PMSR [Jiang et al., 2023] proposes incorporating enhanced chemical knowledge, offering potential advancements for future multi-modal contrastive learning strategies.  

# 5 Applications  

# 5.1 LLMs as Chatbots  

In multi-domain and multi-modal frameworks, pretraining with general text equips chemical LLMs with the ability to comprehend and respond to chemistry-related inquiries in textual format. This endows them with chatbot-like functionalities for downstream chemical tasks like MolT5 [Edwards et al., 2022], BioT5 [Pei et al., 2023], MolXPT [Liu et al., 2023c], ChemGPT [Frey et al., 2023], nach0 [Livne et al., 2023], and DrugGPT [Li et al., 2023b]. These advanced systems enable users to engage in an intuitive dialogue format, posing inquiries in plain text and receiving detailed, contextually relevant responses, also in an accessible text format.  

A chatbot-like chemical LLM can also perform much more complex tasks involving understanding and reasoning. Experimental Action Extraction task requires LLMs to extract specific experimental actions from verbose and detailed descriptions. Several methodologies have been developed for this task, as exemplified by the works of [Vaucher et al., 2020], [Christofidellis et al., 2023], and [Vasˇkevicˇius et al., 2023]. Notably, [Vaucher et al., 2021] advanced this field further by proposing a method to generate a series of experimental actions only given a specific reaction.  

Automated Laboratories task enables researchers to input real-world queries, upon which the LLMs autonomously search scientific literature online, extract pivotal information, deduce practical synthesis routes, and execute experiments, all with minimal human intervention. Forefront initiatives like ChemCrow [Bran et al., 2023], Coscientist [Boiko et al., 2023], and GPT-Lab [Qin et al., 2023] are spearheading this technological advancement in laboratory automation.  

# 5.2 LLMs as In-context Learners  

LLMs also possess an innate capability for learning directly from conversation-based interactions, an approach requiring no alterations to their model weights, known as in-context learning (ICL). [Guo et al., 2023a] performed a comprehensive assessment of LLMs on chemical tasks utilizing ICL. Their experimental results underscored that in-context learnt generalist LLMs could perform on par with chemistrypretrained models in chemical tasks, thereby casting new light on the application of LLMs in the realm of chemistry. The effectiveness of in-context learning (ICL) is notably influenced by the style of prompts used, with simpler prompts often resulting in diminished performance, as evidenced in [Castro Nascimento and Pimentel, 2023]. A wellconstructed prompt in ICL should ideally comprise a general role-setting background, a concise task-specific introduction, relevant ICL examples, and a precise question as shown in Figure 1. A well-designed prompt can even perform well without any ICL examples provided, such as those conducted by Synergpt [Edwards et al., 2023] and MolReGPT [Li et al., 2023a]. This achievement in zero-shot settings underscores the remarkable capabilities of Large Language Models (LLMs) in adapting to new tasks.  

# 5.3 LLMs as Representation Learners  

Similar to other pretraining models for representation learning, as discussed in [Xia et al., 2023] and [Guo et al., 2023b], chemical LLMs are also adept at encoding molecular structures into latent spaces for training downstream models. A prevalent method involves using the [CLS] token representation at the beginning of sequences as a global molecular representation [Wang et al., 2019], or employing shallow neural networks across all output tokens [Chithrananda et al., 2020]. In MG-BERT [Zhang et al., 2021a], a virtual token that connects to all other tokens is included in the input sequence, and its representation is used to capture the global molecular structure for the input sequence. Additional task-specific modules are usually applied to encoded representations for downstream tasks. We list the common downstream applications as follows and comprehensive applications for each reviewed approach are shown in Table 1:  

• Molecule Property Prediction predicts properties with substantial industrial impact given a molecule that cannot be calculated from cheminformatic tools based on chemical sequences directly, such as blood-brain barrier permeability (BBBP) and lipophilicity.  

• Reaction Type/Yield Prediction categorizing chemical reactions into specific types or predict productivity.  

• Reaction Prediction encompasses three critical components: forward product prediction, single-step retrosynthesis, and reagent suggestion. Forward product prediction aims at forecasting the potential products of a given set of reactants. Single-step Retrosynthesis involves deducing feasible reactants from a desired product. Reagent suggestion focuses on identifying suitable reagents that facilitate a desired reaction.  

• Molecule Captioning & De Novo Molecule Generation are two novel tasks proposed in [Edwards et al., 2022]. Molecule captioning involves translating molecular representations into precise, textual descriptions and de novo molecule design presents the reverse challenge: creating novel molecules from textual descriptions.  

• Molecule Optimization involves refining molecules to augment properties such as the Quantitative Estimate of Druglikeness (QED), lipophilicity and so on.  

# 6 Conclusion & Future Directions  

To sum up, this survey offers a thorough exploration of the existing strategies for integrating LLMs into chemistry, covering the spectrum from input representation, through pretraining objectives, to diverse and unique applications. However, despite their rapid evolution, they remain in the nascent stages of development, indicating substantial room for growth and enhancement. The following future directions are pivotal for advancing the field:  

Further Integration with Chemical Knowledge Current chemical Large Language Models (LLMs) grapple with a limited grasp of the chemical universe, notably in retrosynthesis. The often-utilized USPTO 50k dataset [Schneider et al., 2016], with its 50,000 entries, barely scratches the surface of the vast and complex world of chemical retrosynthesis. This limitation significantly hampers the models’ ability to comprehend and predict chemical retrosynthesis accurately. Additionally, as chemistry evolves with quantum mechanics into quantum chemistry, chemical LLMs are still rooted in conventional theories. This gap underscores the pressing need for these models to integrate more deeply with advanced chemical knowledge, particularly from quantum chemistry, to stay relevant and effective in modern chemical research.  

Continual Learning Once deployed, chemical LLMs encounter new knowledge incrementally, highlighting the necessity for continual learning. The requirement for continual learning is more urgent in applications like chemical reaction prediction due to reasons like variable synthesis routes, uncertain reaction conditions, etc. The Continual learning approach allows LLMs to adapt to fresh data from downstream tasks without forgetting previously acquired information, keeping them relevant and effective amidst the fastpaced evolution of chemical synthesis.  

Interpretability LLMs often face criticism for their opaque, ”black-box” nature, which obscures the rationale behind their outputs, rendering the results less interpretable to humans. The LLM4SD study [Zheng et al., 2023] suggests leveraging LLMs for feature extraction, followed by the application of interpretable machine learning models, such as random forests or linear classifiers, on these features. Furthermore, the concept of Chain-of-Thought (CoT) prompting [Wei et al., 2022] has been introduced to prompt LLMs to articulate more intermediate reasoning steps, thereby enhancing their interpretability without compromising—and potentially even improving—their performance. Despite these advancements, the interpretability of chemical LLMs remains an unresolved challenge, presenting a valuable avenue for future research.  

References   
[Ahmad et al., 2022] W. Ahmad, et al. ChemBERTa-2: Towards Chemical Foundation Models, 2022.   
[Axelrod and Go´mez-Bombarelli, 2022] S. Axelrod, et al. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Sci. Data, 2022.   
[Boiko et al., 2023] D. A. Boiko, et al. Autonomous chemical research with large language models. Nature, 2023.   
[Born and Manica, 2023] J. Born, et al. Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. Nat. Mach. Intell, 2023.   
[Bran et al., 2023] A. M. Bran, et al. ChemCrow: Augmenting large-language models with chemistry tools, 2023.   
[Castro Nascimento and Pimentel, 2023] C. M. Castro Nascimento, et al. Do Large Language Models Understand Chemistry? A Conversation with ChatGPT. J. Chem. Inf. Model., 2023.   
[Chilingaryan et al., 2022] G. Chilingaryan, et al. BARTSmiles: Generative Masked Language Models for Molecular Representations, 2022.   
[Chithrananda et al., 2020] S. Chithrananda, et al. ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction, 2020.   
[Christofidellis et al., 2023] D. Christofidellis, et al. Unifying Molecular and Textual Representations via Multi-task Language Modelling, 2023.   
[Degen et al., 2008] J. Degen, et al. On the Art of Compiling and Using ’Drug-Like’ Chemical Fragment Spaces. ChemMedChem, 2008.   
[Durant et al., 2002] J. L. Durant, et al. Reoptimization of MDL Keys for Use in Drug Discovery. J. Chem. Inf. Comput., 2002.   
[Edwards et al., 2021] C. Edwards, et al. Text2Mol: CrossModal Molecule Retrieval with Natural Language Queries. In EMNLP, 2021.   
[Edwards et al., 2022] C. Edwards, et al. Translation between molecules and natural language. In EMNLP, 2022.   
[Edwards et al., 2023] C. Edwards, et al. SynerGPT: InContext Learning for Personalized Drug Synergy Prediction and Drug Design, 2023.   
[Fang et al., 2023a] Y. Fang, et al. Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models, 2023.   
[Fang et al., 2023b] Y. Fang, et al. Domain-Agnostic Molecular Generation with Self-feedback, 2023.   
[Feng et al., 2023] S. Feng, et al. UniMAP: Universal SMILES-Graph Representation Learning, 2023.   
[Frey et al., 2023] N. C. Frey, et al. Neural scaling of deep chemical models. Nat. Mach. Intell, 2023.   
[Gage, 1994] P. Gage. A new algorithm for data compression. The C Users Journal, 1994.   
[Gao et al., 2023] Z. Gao, et al. PrefixMol: Target- and Chemistry-aware Molecule Design via Prefix Embedding, 2023.   
[Guo et al., 2022] Z. Guo, et al. Multilingual molecular representation learning via contrastive pre-training. In ACL, 2022.   
[Guo et al., 2023a] T. Guo, et al. What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks, 2023.   
[Guo et al., 2023b] Z. Guo, et al. Graph-based molecular representation learning. In IJCAI, 2023.   
[He et al., 2016] K. He, et al. Deep residual learning for image recognition. In CVPR, 2016.   
[Heller et al., 2013] S. Heller, et al. InChI - the worldwide chemical structure identifier standard. J. Cheminformatics, 2013.   
[Hopfinger et al., 1997] A. Hopfinger, et al. Construction of 3d-qsar models using the 4d-qsar analysis formalism. J. Am. Chem. Soc., 1997.   
[Irwin et al., 2020] J. J. Irwin, et al. ZINC20—A Free Ultralarge-Scale Chemical Database for Ligand Discovery. J. Chem. Inf. Model., 2020.   
[Irwin et al., 2022] R. Irwin, et al. Chemformer: a pretrained transformer for computational chemistry. MLST, 2022.   
[Jiang et al., 2023] Y. Jiang, et al. Learning chemical rules of retrosynthesis with pre-training. In AAAI, 2023.   
[Jin et al., 2018] W. Jin, et al. Junction tree variational autoencoder for molecular graph generation. In ICML, 2018.   
[Kim et al., 2016] S. Kim, et al. PubChem Substance and Compound databases. Nucleic Acids Res., 2016.   
[Krenn et al., 2020] M. Krenn, et al. Self-referencing embedded strings (SELFIES): A $100 \%$ robust molecular string representation. MLST, 2020.   
[Li and Fourches, 2021] X. Li, et al. SMILES Pair Encoding: A Data-Driven Substructure Tokenization Algorithm for Deep Learning. J. Chem. Inf. Model, 2021.   
[Li et al., 2023a] J. Li, et al. Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective, 2023.   
[Li et al., 2023b] Y. Li, et al. DrugGPT: A GPT-based Strategy for Designing Potential Ligands Targeting Specific Proteins, 2023.   
[Liu et al., 2023a] P. Liu, et al. GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text, 2023.   
[Liu et al., 2023b] S. Liu, et al. Multi-modal molecule structure–text model for text-based retrieval and editing. Nat. Mach. Intell, 2023.   
[Liu et al., 2023c] Z. Liu, et al. MolXPT: Wrapping Molecules with Text for Generative Pre-training. ACL, 2023.   
[Liu et al., 2023d] Z. Liu, et al. MolCA: Molecular graphlanguage modeling with cross-modal projector and unimodal adapter. In EMNLP, 2023.   
[Livne et al., 2023] M. Livne, et al. nach0: Multimodal Natural and Chemical Languages Foundation Model, 2023.   
[Lu and Zhang, 2022] J. Lu, et al. Unified deep learning model for multitask reaction predictions with explanation. J. Chem. Inf. Model, 2022.   
[Oord et al., 2019] A. v. d. Oord, et al. Representation Learning with Contrastive Predictive Coding, 2019.   
[Pei et al., 2023] Q. Pei, et al. BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. In EMNLP, 2023.   
[Qin et al., 2023] X. Qin, et al. GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab, 2023.   
[Raffel et al., 2020] C. Raffel, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.   
[Rogers and Hahn, 2010] D. Rogers, et al. ExtendedConnectivity Fingerprints. J. Chem. Inf. Model, 2010.   
[Rong et al., 2020] Y. Rong, et al. Self-supervised graph transformer on large-scale molecular data. In NeurIPS, NIPS’20, 2020.   
[Ross et al., 2022] J. Ross, et al. Large-scale chemical language representations capture molecular structure and properties. Nat. Mach. Intell, 2022.   
[Schneider et al., 2016] N. Schneider, et al. What’s What: The (Nearly) Definitive Guide to Reaction Role Assignment. J. Chem. Inf. Model, 2016.   
[Schu¨tt et al., 2017] K. Schu¨tt, et al. Schnet: A continuousfilter convolutional neural network for modeling quantum interactions. In NIPS, 2017.   
[Schwaller et al., 2018] P. Schwaller, et al. “found in translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. Chem. Sci, 2018.   
[Schwaller et al., 2019] P. Schwaller, et al. Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. ACS Cent. Sci, 2019.   
[Su et al., 2022] B. Su, et al. A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language, 2022.   
[Vaswani et al., 2017] A. Vaswani, et al. Attention is all you need. In NIPS, 2017.   
[Vaucher et al., 2020] A. C. Vaucher, et al. Automated extraction of chemical synthesis actions from experimental procedures. Nat. Commun, 2020.   
[Vaucher et al., 2021] A. C. Vaucher, et al. Inferring experimental procedures from text-based representations of chemical reactions. Nat. Commun, 2021.   
[Vasˇkevicˇius et al., 2023] M. Vaˇskevicˇius, et al. Generative LLMs in Organic Chemistry: Transforming Esterification Reactions into Natural Language Procedures. Appl. Sci., 2023.   
[Wang et al., 2019] S. Wang, et al. SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction. BCB, 2019.   
[Wei et al., 2022] J. Wei, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022.   
[Weininger, 1988] D. Weininger. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput., 1988.   
[Winter et al., 2022] B. Winter, et al. A smile is all you need: predicting limiting activity coefficients from SMILES with natural language processing. Digital Discovery, 2022.   
[Wu et al., 2023] F. Wu, et al. Molformer: Motif-Based Transformer on 3D Heterogeneous Molecular Graphs. AAAI, 2023.   
[Xia et al., 2023] J. Xia, et al. A systematic survey of chemical pre-trained models. In IJCAI, 2023.   
[Xie et al., 2023] A. Xie, et al. Self-supervised learning with chemistry-aware fragmentation for effective molecular property prediction. Brief. Bioinform, 2023.   
[Xu et al., 2019] K. Xu, et al. How powerful are graph neural networks? In ICLR, 2019.   
[Xue et al., 2022] D. Xue, et al. X-MOL: large-scale pretraining for molecular understanding and diverse molecular analysis. Sci. Bull., 2022.   
[Y¨uksel et al., 2023] A. Yu¨ksel, et al. SELFormer: molecular representation learning via SELFIES language models. MLST, 2023.   
[Zeng et al., 2022] Z. Zeng, et al. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nat. Commun, 2022.   
[Zhang et al., 2021a] X.-C. Zhang, et al. MG-BERT: leveraging unsupervised atomic representation learning for molecular property prediction. Brief. Bioinform, 2021.   
[Zhang et al., 2021b] Z. Zhang, et al. Motif-based graph self-supervised learning for molecular property prediction. NeurIPS, 2021.   
[Zhao et al., 2023] H. Zhao, et al. GIMLET: A unified graph-text model for instruction-based molecule zero-shot learning. In NeurIPS, 2023.   
[Zheng et al., 2023] Y. Zheng, et al. Large language models for scientific synthesis, inference and explanation, 2023.   
[Zhu et al., 2022] Y. Zhu, et al. Featurizations matter: A multiview contrastive learning approach to molecular pretraining. In ICML AI for Science Workshop, 2022.   
[Zhu et al., 2023] J. Zhu, et al. Dual-view molecular pretraining. In SIGKDD, 2023.  