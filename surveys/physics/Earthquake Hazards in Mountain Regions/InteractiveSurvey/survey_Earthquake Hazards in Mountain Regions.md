# A Survey of Earthquake Hazards in Mountain Regions

# 1 Abstract


Earthquakes pose significant risks in mountainous regions due to their complex geological structures and high seismic activity. This survey paper aims to provide a comprehensive overview of advanced techniques and methodologies for monitoring, predicting, and assessing earthquake hazards in these areas, with a focus on the application of machine learning and advanced computational methods. The paper reviews the latest advancements in seismic monitoring, data processing, and hazard assessment, including the use of supervised machine learning for earthquake declustering, hybrid CNN-LSTM models for Vs30 prediction, and neural operators for seismic response modeling. It also explores the deployment and maintenance of seismic networks, SLAM-based channel detection, and efficient inversion techniques for high-dimensional data. The contributions of this survey lie in its synthesis of insights from a wide range of studies, highlighting the most promising approaches for improving seismic hazard mitigation in mountainous regions. This comprehensive review serves as a valuable resource for researchers, practitioners, and policymakers, ultimately contributing to more effective seismic hazard mitigation strategies.

# 2 Introduction
Earthquakes are among the most destructive natural disasters, capable of causing significant loss of life and property damage. Mountainous regions, characterized by their complex geological structures and high seismic activity, are particularly vulnerable to these hazards. The intricate interplay between tectonic forces, fault systems, and topography in mountain regions leads to a diverse range of seismic behaviors, from localized tremors to catastrophic earthquakes. Understanding and mitigating these hazards is crucial for the safety and resilience of communities in these areas. However, the challenges in mountain regions are compounded by the difficulty in deploying and maintaining seismic monitoring networks and the complexity of the underlying geology, which can obscure the true nature of seismic events.

This survey paper focuses on the advanced techniques and methodologies used to monitor, predict, and assess earthquake hazards in mountain regions. The paper aims to provide a comprehensive overview of the current state of the art in seismic monitoring, data processing, and hazard assessment, with a particular emphasis on the application of machine learning and advanced computational methods [1]. By integrating insights from recent research and practical applications, this survey seeks to highlight the most promising approaches for improving seismic hazard mitigation in mountainous areas.

The paper begins with an exploration of advanced seismic monitoring and prediction techniques, including the use of supervised machine learning for earthquake declustering, hybrid CNN-LSTM models for Vs30 prediction, and neural operators for seismic response modeling [2]. These sections delve into the technical details of these methods, discussing their strengths, limitations, and potential for enhancing seismic hazard assessments. The integration of physics-informed constraints and ensemble methods is also highlighted as a means to improve the robustness and physical plausibility of the models.

The discussion then shifts to advanced seismic network and data processing techniques. This includes the deployment and maintenance of seismic networks, the use of SLAM-based channel detection and mapping in debris flow environments, and efficient inversion techniques for high-dimensional seismic data. The integration of machine learning algorithms and real-time data transmission technologies is emphasized as key to enhancing the operational efficiency and accuracy of seismic networks [3]. The challenges and solutions in deploying and maintaining these networks in harsh and remote environments are also addressed.

The paper further examines seismic data synthesis and analysis, focusing on conditional diffusion models for seismic waveform synthesis, neural machine translation for seismic data interpretation, and Bayesian regularized neural networks for volcanic activity prediction [4]. These sections highlight the flexibility and scalability of these models, as well as their ability to handle the high-dimensional and noisy nature of seismic data. The use of physics-informed constraints and uncertainty quantification is discussed as essential for improving the reliability and interpretability of these models.

Finally, the survey paper explores seismic hazard assessment and mitigation strategies, including 3D dynamic rupture simulations, spatially-aware variational causal Bayesian networks, and Gaussian processes for probabilistic fusion of velocity models [1]. These sections provide a detailed look at the modeling and simulation of seismic events, the integration of spatial variability in causal mechanisms, and the probabilistic fusion of velocity models to account for uncertainties [5]. The practical applications of these methods in real-world scenarios are also discussed, emphasizing their role in enhancing seismic hazard assessments and informing disaster response planning.

The contributions of this survey paper lie in its comprehensive review of the latest advancements in earthquake hazard research, particularly in mountain regions. By synthesizing insights from a wide range of studies and methodologies, this paper provides a valuable resource for researchers, practitioners, and policymakers. The paper not only highlights the current state of the art but also identifies key areas for future research and development, ultimately contributing to more effective seismic hazard mitigation strategies in mountainous areas.

# 3 Advanced Seismic Monitoring and Prediction Techniques

## 3.1 Machine Learning and Deep Learning Techniques

### 3.1.1 Supervised Machine Learning for Earthquake Declustering
Supervised machine learning (ML) has emerged as a powerful tool for earthquake declustering, a process that separates clustered seismic events into background and triggered events. Traditional declustering methods, such as the nearest-neighbor (NN) method, rely heavily on statistical assumptions and can introduce biases, particularly in the identification of aftershocks. These biases can lead to systematic misclassification, where a large number of events are incorrectly labeled as aftershocks, even in the absence of causal connections. To address these limitations, supervised ML algorithms leverage labeled datasets to learn the complex patterns and dependencies in seismic data, thereby improving the accuracy of declustering.

In the context of earthquake declustering, supervised ML models are trained on historical seismic catalogs that include information on the timing, location, and magnitude of earthquakes [6]. These models can incorporate various features, such as the spatial and temporal proximity of events, the magnitude difference between events, and the seismicity rate in the vicinity. By learning from these features, ML algorithms can more accurately distinguish between background seismicity and triggered events. For instance, neural networks and decision trees have been successfully applied to decluster seismic catalogs, demonstrating superior performance compared to traditional methods. These models can capture non-linear relationships and complex interactions that are not easily discernible through conventional statistical approaches.

Moreover, the integration of physics-informed constraints into supervised ML models further enhances their predictive capabilities. Physics-informed ML models combine the strengths of data-driven learning with the physical laws governing seismic processes [7]. This hybrid approach ensures that the learned patterns are not only statistically robust but also physically plausible. For example, incorporating the Omori law, which describes the temporal decay of aftershock frequency, into the training process can improve the model's ability to identify true aftershocks. Additionally, the use of ensemble methods, such as random forests and gradient boosting, can help mitigate overfitting and increase the robustness of the declustering results. Overall, supervised ML offers a promising avenue for advancing earthquake declustering, ultimately contributing to more reliable seismic hazard assessments and improved earthquake forecasting.

### 3.1.2 Hybrid CNN-LSTM Models for Vs30 Prediction
Hybrid CNN-LSTM models have emerged as a powerful tool for predicting Vs30, a critical parameter in seismic hazard assessment that characterizes the average shear wave velocity in the top 30 meters of the soil. The integration of CNNs and LSTMs leverages the strengths of both architectures to address the inherent complexities of seismic data [8]. CNNs are adept at capturing spatial features from seismic records, such as the spectral characteristics and waveforms, which are essential for understanding the local soil properties [7]. Meanwhile, LSTMs excel in modeling the temporal dependencies within the data, making them ideal for handling the sequential nature of ground motion records [8]. This combination allows the model to effectively extract both spatial and temporal patterns, leading to more accurate and robust predictions of Vs30.

The architecture of a hybrid CNN-LSTM model typically involves a series of convolutional layers that process the input seismic data to extract relevant spatial features [8]. These features are then fed into LSTM layers, which capture the temporal dynamics of the extracted features. The convolutional layers can be designed to handle multi-channel inputs, such as different components of ground motion (e.g., vertical, north-south, and east-west), ensuring a comprehensive representation of the seismic data. The LSTM layers, on the other hand, maintain a memory of past inputs, allowing the model to account for the historical context of the seismic activity. This is particularly important for Vs30 prediction, as the soil properties can vary significantly over time due to environmental and anthropogenic factors.

To validate the effectiveness of hybrid CNN-LSTM models, several studies have been conducted using diverse datasets from various geographic regions. These studies have consistently shown that hybrid models outperform traditional methods and single-model approaches in terms of prediction accuracy and robustness. The ability of CNNs to capture intricate spatial patterns, combined with the temporal modeling capabilities of LSTMs, provides a comprehensive framework for understanding the complex interactions between seismic waves and soil properties. As a result, hybrid CNN-LSTM models are poised to become a standard tool in the arsenal of seismic engineers and researchers, enhancing the accuracy of Vs30 predictions and contributing to more effective seismic hazard assessments [2].

### 3.1.3 Neural Operators for Seismic Response Modeling
Neural operators, a class of machine learning models designed to solve partial differential equations (PDEs) and learn mappings between function spaces, have emerged as powerful tools in seismic response modeling [2]. Unlike traditional physics-informed neural networks (PINNs), which are trained to solve specific instances of PDEs, neural operators are capable of learning the solution operator for an entire family of PDEs [9]. This capability allows them to generalize well to unseen data and different physical scenarios, making them particularly suitable for seismic applications where the underlying physics can vary significantly across different geological settings. In the context of seismic response modeling, neural operators can be used to predict the dynamic response of structures to seismic excitations, which is crucial for earthquake engineering and hazard assessment [2].

Two prominent neural operator architectures, the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO), have been extensively studied in recent years [10]. DeepONet is a deep learning framework that can learn the mapping between infinite-dimensional function spaces, making it highly versatile for modeling complex, nonlinear systems. FNO, on the other hand, leverages the Fourier transform to efficiently capture the global dependencies in the data, which is particularly beneficial for problems involving wave propagation and seismic response. These models have been applied to a variety of seismic tasks, including the prediction of ground motion, structural response, and the identification of seismic sources [3]. By integrating these neural operators with traditional seismic data, researchers can achieve more accurate and computationally efficient predictions, which are essential for real-time earthquake early warning systems and structural health monitoring [2].

In a recent study, a hybrid approach combining DeepONet and FNO, termed DeepFNOnet, was developed to model the nonlinear dynamic response of a six-story shear building under stochastic ground acceleration [10]. This integrated framework demonstrated superior performance in capturing the complex, time-varying behavior of the structure, outperforming traditional methods in terms of both accuracy and computational efficiency. The adaptability of neural operators to different types of seismic hazards, such as earthquakes and wind, further underscores their potential in advancing the field of structural dynamics and response prediction [10]. As these models continue to evolve, they are expected to play a pivotal role in enhancing our understanding and mitigation of seismic risks.

## 3.2 Advanced Seismic Network and Data Processing

### 3.2.1 Deployment and Maintenance of Seismic Networks
The deployment and maintenance of seismic networks are critical components in the monitoring and understanding of seismic activity, particularly in volcanic and earthquake-prone regions [11]. The layout of these networks, often referred to as the experimental design, is a crucial factor that influences the quality and reliability of the recorded data. An optimal network design maximizes the information content of the data, thereby enhancing the accuracy of subsequent analyses such as hazard assessment, eruption forecasting, and early warning systems. However, despite the availability of well-established methods for optimizing network design, practical constraints often lead to suboptimal configurations. These constraints include logistical challenges, budget limitations, and the need to balance coverage with the density of sensors.

Seismic networks are typically composed of a variety of sensor types, including broadband seismometers, strong-motion sensors, and geophones, each with its own strengths and limitations [11]. Broadband seismometers are essential for detecting low-frequency signals and are often deployed to monitor regional and global seismic activity. Strong-motion sensors, on the other hand, are designed to withstand and accurately record high-amplitude ground motions, making them crucial for assessing the impact of strong earthquakes on structures. The maintenance of these networks involves regular calibration, data quality checks, and timely repairs to ensure continuous and reliable data collection. Challenges in maintenance can arise from harsh environmental conditions, such as extreme weather, volcanic activity, and remote locations, which can damage equipment and disrupt data transmission.

In recent years, advancements in technology and data processing have significantly improved the deployment and maintenance of seismic networks. For example, the integration of machine learning algorithms has enhanced the ability to detect and locate seismic events with greater precision [3]. Additionally, the use of wireless and satellite communication technologies has facilitated real-time data transmission, even from remote and inaccessible areas. These technological improvements not only enhance the operational efficiency of seismic networks but also contribute to more accurate and timely hazard assessments, ultimately improving public safety and disaster response efforts.

### 3.2.2 SLAM-Based Channel Detection and Mapping
SLAM (Simultaneous Localization and Mapping) technology has been increasingly adopted for environmental monitoring, including the detection and mapping of debris flow channels. Traditional SLAM algorithms, however, face significant challenges in such environments due to the unique characteristics of debris flow channels. These channels often feature complex terrain, dense vegetation, and dynamic changes, which make it difficult for SLAM algorithms to accurately map and localize. Moreover, the accumulation of systematic errors over long distances and large datasets exacerbates the issue, leading to significant deviations in the final maps. To address these limitations, an advanced SLAM-based channel detection and mapping system (AscDAMs) has been developed, incorporating several novel contributions.

AscDAMs introduces a multi-stage processing pipeline that enhances the robustness and accuracy of SLAM in debris flow environments [12]. One of the key innovations is the integration of feature-rich sensor data, such as LiDAR and RGB-D cameras, which provide a more comprehensive representation of the environment. This rich data fusion helps in extracting meaningful features even in challenging conditions, reducing the reliance on point cloud matching alone. Additionally, AscDAMs employs advanced filtering techniques, such as Kalman filters and particle filters, to correct for the accumulated errors and maintain the integrity of the map over extended periods. These filters dynamically adjust the weight of new measurements based on their reliability, ensuring that the mapping remains accurate and consistent.

Another critical aspect of AscDAMs is its ability to handle the dynamic nature of debris flow channels [12]. The system incorporates real-time data processing and adaptive algorithms that can quickly respond to changes in the environment, such as sudden shifts in the channel morphology or the appearance of new obstacles. This adaptability is crucial for applications such as disaster response and environmental monitoring, where timely and accurate information is essential. Furthermore, AscDAMs leverages machine learning techniques to continuously improve its performance, learning from past data to better predict and handle future scenarios. By combining these advanced features, AscDAMs represents a significant step forward in the application of SLAM technology to complex and dynamic environments like debris flow channels [12].

### 3.2.3 Efficient Inversion Techniques for High-Dimensional Data
Efficient inversion techniques for high-dimensional data are crucial for optimizing the computational and memory requirements in seismic data processing, particularly in the context of full waveform inversion (FWI) [4]. Traditional FWI methods, which rely on iterative gradient-based optimization, often struggle with the curse of dimensionality, leading to prohibitive computational costs and convergence issues. To address these challenges, recent advancements have focused on leveraging machine learning and deep neural networks (DNNs) to accelerate the inversion process [4]. Specifically, neural operators, such as the Fourier Neural Operator (FNO), have been introduced to efficiently approximate the complex mappings between seismic data and subsurface models. These operators can be trained on large datasets to generalize well across different seismic scenarios, thereby reducing the need for extensive manual intervention and significantly speeding up the inversion process.

One of the key techniques in this domain is the use of neural networks to perform instant inversion, where a pre-trained model can rapidly generate subsurface models from observed seismic data [4]. This approach requires extensive offline training on synthetic or real datasets to ensure the model's accuracy and robustness. The training data must be carefully selected to cover a wide range of geological scenarios and seismic conditions, ensuring that the model can generalize well to new, unseen data. Additionally, the distribution of the training data should closely match the conditions expected in real-world applications to avoid overfitting and ensure reliable performance. Techniques such as data augmentation and physics-informed training can further enhance the model's ability to capture the underlying physical processes, leading to more accurate and physically consistent inversion results.

Another significant development is the integration of hybrid models that combine physics-based and data-driven approaches. These hybrid models leverage the strengths of both methodologies to achieve a balance between computational efficiency and physical accuracy. For example, the DeepFNOnet framework, which integrates the DeepONet and FNO, has shown promising results in predicting the seismic nonlinear dynamic response of structures [10]. This framework can handle high-dimensional data by breaking down the problem into smaller, more manageable components, each of which can be processed efficiently using specialized neural network architectures. By effectively capturing the spatial and temporal dependencies in seismic data, these hybrid models can provide more accurate and detailed subsurface models, thereby enhancing the overall effectiveness of seismic monitoring and inversion techniques.

## 3.3 Seismic Data Synthesis and Analysis

### 3.3.1 Conditional Diffusion Models for Seismic Waveform Synthesis
Conditional diffusion models represent a significant advancement in the synthesis of seismic waveforms, offering a powerful framework for generating realistic and diverse seismic signals [4]. These models leverage the principles of diffusion processes, where a noise signal is gradually transformed into a structured output through a series of learned denoising steps. In the context of seismic waveform synthesis, the model is conditioned on various input parameters such as earthquake magnitude, hypocentral distance, and site-specific characteristics like VS30. This conditioning allows the model to generate waveforms that are not only statistically similar to real seismic data but also physically plausible, making them valuable for both research and practical applications in seismic hazard assessment and earthquake engineering.

The architecture of conditional diffusion models typically involves an autoencoder that maps the input seismic data into a lower-dimensional latent space [3]. The diffusion process is then applied in this latent space, where the model learns to denoise and refine the latent representations iteratively. By conditioning the diffusion process on the aforementioned parameters, the model can generate synthetic waveforms that closely match the desired conditions. For instance, the model can synthesize waveforms for specific magnitudes and distances, which is crucial for testing and validating seismic hazard models. The use of a lower-dimensional latent space not only reduces computational complexity but also enhances the model's ability to capture the intricate patterns and dependencies present in seismic data.

One of the key advantages of conditional diffusion models is their flexibility and scalability. They can be trained on large datasets of seismic recordings, allowing them to learn a wide range of waveform characteristics and variations. This capability is particularly important for simulating rare or extreme seismic events, which are often underrepresented in observational data. Additionally, the models can be fine-tuned for specific regions or geological settings by incorporating local site conditions and structural information. The resulting synthetic waveforms can be used to augment existing datasets, improve the robustness of seismic models, and support the development of more accurate and reliable early warning systems [3]. The open-source nature of these models, as exemplified by the 'This-Quake-Does-Not-Exist' library, encourages community collaboration and continuous improvement in the field of seismic waveform synthesis [3].

### 3.3.2 Neural Machine Translation for Seismic Data Interpretation
Neural Machine Translation (NMT) has emerged as a powerful tool in the interpretation of seismic data, leveraging deep learning architectures to transform raw seismic signals into actionable insights [4]. Unlike traditional methods that rely heavily on hand-crafted features and domain-specific knowledge, NMT models, particularly those based on Transformer architectures, can automatically learn complex mappings between input seismic data and desired outputs such as seismic event classification, magnitude estimation, and structural health monitoring. These models are particularly adept at handling the high-dimensional and noisy nature of seismic data, which often poses significant challenges for conventional machine learning techniques [4].

In the context of seismic data interpretation, NMT models have been applied to a variety of tasks, including the translation of seismic waveforms into interpretable features and the generation of synthetic seismic data for training and validation purposes. For instance, the attention mechanism, a key component of Transformer models, has been used to improve the capture of long-range dependencies in seismic time series, enabling more accurate predictions of seismic events and their characteristics. This is particularly important for tasks such as earthquake early warning systems, where the ability to quickly and accurately interpret incoming seismic signals can significantly enhance public safety and disaster response efforts [3].

Moreover, the integration of physics-informed constraints into NMT models has shown promise in improving the robustness and interpretability of these systems. By incorporating prior knowledge about the physical processes underlying seismic phenomena, these hybrid models can better handle the inherent uncertainties and variabilities in seismic data. For example, physics-informed NMT models have been used to predict the nonlinear dynamic response of structures under seismic excitation, demonstrating the potential of these techniques to bridge the gap between data-driven and physics-based approaches in seismic engineering. This integration not only enhances the accuracy of predictions but also provides a more transparent and interpretable framework for understanding the underlying physical processes.

### 3.3.3 Bayesian Regularized Neural Networks for Volcanic Activity Prediction
Bayesian Regularized Neural Networks (BRNNs) offer a robust framework for predicting volcanic activity by integrating prior knowledge and uncertainty quantification into the model training process [13]. In the context of volcanic activity prediction, BRNNs are particularly advantageous due to their ability to handle the inherent nonlinearity and high dimensionality of seismic data. The Bayesian regularization technique helps in mitigating overfitting, a common issue in neural networks, by incorporating a prior distribution over the model parameters. This regularization ensures that the model remains generalizable, even when trained on limited data, which is often the case in volcanic monitoring due to the infrequent nature of significant events.

In the specific application to volcanic activity prediction, BRNNs are trained on historical seismic data, including parameters such as seismic waveforms, frequency content, and temporal patterns. The prior distribution in the Bayesian framework can be informed by geological and geophysical knowledge, such as the expected distribution of seismic activity around a volcano [14]. This integration of domain-specific knowledge enhances the model's predictive power and reliability. The expected information gain from new data is calculated to optimize the experimental design, ensuring that the network is trained on the most informative data points. This approach not only improves the accuracy of predictions but also provides a measure of uncertainty, which is crucial for risk assessment and decision-making in volcanic hazard management.

The effectiveness of BRNNs in volcanic activity prediction is further demonstrated through their ability to adapt to changing conditions. Volcanic systems are dynamic, and the characteristics of seismic activity can vary significantly over time. BRNNs can be continuously updated with new data, allowing the model to evolve and maintain its predictive accuracy. This adaptability is particularly important for short-term eruption forecasting, where rapid changes in seismic patterns can indicate an impending eruption [14]. By combining the strengths of neural networks with Bayesian inference, BRNNs provide a powerful tool for enhancing the monitoring and prediction capabilities in volcanic environments, ultimately contributing to more effective hazard mitigation strategies.

# 4 Seismic Hazard Assessment and Mitigation Strategies

## 4.1 Modeling and Simulation of Seismic Events

### 4.1.1 3D Dynamic Rupture Simulations for Complex Fault Geometries
3D dynamic rupture simulations have emerged as a powerful tool for understanding the complexities of earthquake generation and propagation, especially in regions with intricate fault geometries [15]. These simulations incorporate the physics of faulting, including the effects of frictional behavior, stress heterogeneity, and the dynamic evolution of rupture fronts. By accounting for the three-dimensional nature of fault zones, these models can more accurately predict the distribution of ground motion and the potential for strong shaking in specific areas. The inclusion of complex fault geometries, such as bends, step-overs, and branching, allows for a more realistic representation of the stress and strain fields that develop during an earthquake, leading to a better understanding of the mechanisms that control rupture propagation and arrest [15].

Recent advancements in computational resources and numerical methods have enabled the development of sophisticated 3D dynamic rupture models that can handle the geometric and mechanical complexities of real-world fault systems. These models often employ high-resolution discretizations and advanced constitutive laws, such as rate-and-state friction, to capture the dynamic behavior of faults during rupture. The integration of geodetic data, seismic observations, and geological information has further enhanced the accuracy of these simulations. For instance, the use of seismic cycle models and quasi-dynamic algorithms has allowed researchers to simulate the long-term evolution of fault systems, including the accumulation of stress, the initiation of rupture, and the subsequent relaxation phases [16]. These simulations have revealed that the geometry of fault zones plays a critical role in modulating the rupture process, influencing factors such as the duration of the earthquake, the distribution of slip, and the generation of high-frequency ground motions.

However, despite these advances, several challenges remain in the field of 3D dynamic rupture simulations. One of the primary challenges is the computational cost associated with running high-resolution simulations over large domains and long time periods. Additionally, the lack of detailed and accurate data on fault geometry and material properties, particularly at depth, limits the predictive power of these models. Efforts to address these issues include the development of more efficient numerical algorithms, the use of machine learning techniques to infer fault properties from sparse data, and the integration of multi-scale modeling approaches that combine regional and local simulations. As these challenges are overcome, 3D dynamic rupture simulations will continue to provide valuable insights into the physics of earthquakes and contribute to more robust seismic hazard assessments [15].

### 4.1.2 Spatially-Aware Variational Causal Bayesian Networks
Spatially-aware variational causal Bayesian networks (SpatialVCBN) represent a significant advancement in the modeling of spatially distributed phenomena, particularly in the context of earthquake impact assessment [17]. Unlike traditional approaches that treat spatial correlation as a secondary consideration, SpatialVCBN integrates spatial variability directly into the causal mechanisms of the network. This integration is essential for accurately capturing the nuanced interactions between seismic events and their environmental impacts, such as ground shaking, soil liquefaction, and structural damage. By explicitly modeling the spatial heterogeneity of these causal relationships, SpatialVCBN provides a more robust framework for predicting and understanding the cascading effects of earthquakes across different geographical scales [5].

For inference, SpatialVCBN employs a stochastic variational approach combined with an expectation-maximization (EM) algorithm. This method iteratively updates the posterior distributions of unobserved variables and refines the model parameters, allowing for efficient and scalable computation even in large-scale scenarios. The EM algorithm alternates between an E-step, where the posterior probabilities of latent variables are estimated, and an M-step, where the model parameters are updated to maximize the likelihood of the observed data. To enhance computational efficiency, a local pruning strategy is applied, which leverages the inherent sparsity of real-world causal networks. This strategy significantly reduces the computational burden by focusing on the most relevant nodes and edges, thereby enabling the analysis of extensive geographical regions with high spatial resolution.

The practical utility of SpatialVCBN is demonstrated through its application in various earthquake impact assessments, where it has shown superior performance in predicting the spatial distribution of seismic hazards and their subsequent impacts. By incorporating spatially-aware causal mechanisms, the model can better account for the complex interactions between different types of hazards and their propagation through the environment. This enhanced capability is particularly valuable for disaster response planning, where accurate and timely predictions are critical for effective resource allocation and emergency management. Overall, SpatialVCBN offers a powerful tool for advancing our understanding and mitigation of earthquake risks in spatially complex environments.

### 4.1.3 Gaussian Processes for Probabilistic Fusion of Velocity Models
Gaussian Processes (GPs) have emerged as a powerful tool for the probabilistic fusion of velocity models, addressing the inconsistencies and uncertainties inherent in seismic velocity models [1]. By treating the velocity model as a random function, GPs provide a natural framework for incorporating prior knowledge and observational data, while also quantifying the uncertainty in the model predictions. This is particularly important in seismic hazard assessment, where the accuracy of ground motion predictions is crucial for effective disaster mitigation [11].

In the proposed workflow, GPs are used to fuse multiple 1-D seismic velocity models, each representing a different interpretation of the subsurface structure. The fusion process involves constructing a GP prior that captures the spatial correlations in the velocity models, and then updating this prior with observed data to obtain a posterior distribution over the velocity model. This posterior distribution not only provides a best estimate of the velocity model but also quantifies the uncertainty associated with this estimate. The scalability of GPs is crucial for handling the large datasets typical in seismic studies, and recent advances in scalable GP methods have made this approach feasible for practical applications.

The probabilistic velocity model obtained through GP fusion can then be used in earthquake simulations to generate ground motion predictions that account for the uncertainties in the velocity model [1]. This approach has been demonstrated through a synthetic example, where the GP-fused velocity model was used to simulate ground motions for a hypothetical earthquake scenario [1]. The results showed that the GP-fused model produced more reliable and robust ground motion predictions compared to using a single, deterministic velocity model [3]. The ability to quantify and propagate uncertainties through the simulation process is a significant advantage of this method, providing a more comprehensive assessment of seismic hazards.

## 4.2 Statistical and Probabilistic Methods

### 4.2.1 Non-Parametric Kernel Density Estimation of Magnitude Distribution
Non-parametric kernel density estimation (KDE) has emerged as a robust technique for characterizing the magnitude distribution of seismic events, particularly in scenarios where traditional parametric models fall short. Unlike parametric methods, KDE does not assume a specific functional form for the underlying distribution, making it particularly suitable for handling the complex and often multimodal nature of seismic magnitude data. The core idea behind KDE is to estimate the probability density function (PDF) of a random variable, such as earthquake magnitude, by placing a kernel function at each data point and summing these contributions. This approach allows for a flexible and data-driven estimation of the distribution, which is crucial when dealing with the inherently variable and sometimes sparse seismic data.

One of the key challenges in applying KDE to seismic magnitude distributions is the presence of a natural lower bound, typically at a minimum detectable magnitude. This boundary can introduce bias in the estimated PDF, leading to spurious modes near the lower limit. To mitigate this issue, a common practice is to mirror the data symmetrically around the lower bound, effectively extending the dataset and reducing edge effects. This mirrored data is then used to construct a more accurate and unbiased estimate of the magnitude distribution. Additionally, the choice of kernel function and bandwidth is critical in KDE. While the Gaussian kernel is widely used due to its smoothness and mathematical convenience, the bandwidth selection is particularly important. Adaptive bandwidth methods, such as those based on local data density, can further improve the accuracy of the KDE by adjusting the smoothing level according to the local variability of the data.

In the context of seismic hazard analysis, the ability of KDE to accurately capture the tail of the magnitude distribution is of paramount importance. The tail represents the rare but potentially catastrophic events, and its accurate estimation is essential for reliable risk assessment. Traditional methods, such as the Gutenberg-Richter law, often fail to adequately describe the tail, especially in regions with anthropogenic seismicity where the magnitude distribution can deviate significantly from exponential behavior. KDE, with its flexibility and data-driven nature, provides a powerful alternative for modeling these complex distributions. By leveraging advanced techniques for bandwidth selection and edge correction, KDE can offer a more nuanced and realistic representation of seismic magnitude distributions, thereby enhancing the overall reliability of seismic hazard assessments.

### 4.2.2 Multi-Level Bayesian Hierarchical Models for Ground Motion Prediction
Multi-Level Bayesian Hierarchical Models (BHM) have emerged as a robust framework for predicting ground motion parameters, particularly in regions with complex geological and seismological characteristics. These models integrate data from multiple sources, including seismic recordings, geological surveys, and historical earthquake catalogs, to provide a comprehensive and probabilistic assessment of ground motion [11]. By decomposing the ground motion into event-specific, station-specific, and event-station interaction components, BHMs can capture the variability and uncertainty inherent in ground motion prediction. This hierarchical structure allows for the incorporation of both aleatory and epistemic uncertainties, enhancing the reliability of seismic hazard assessments [2].

One of the key advantages of using BHMs in ground motion prediction is their ability to handle spatial and temporal correlations effectively. Traditional ground motion prediction equations (GMPEs) often assume independence between observations, which can lead to underestimation of uncertainties, especially in regions with dense seismic networks [18]. BHMs, on the other hand, can model the spatial correlation of ground motions across different stations, thereby providing more accurate and spatially consistent predictions. This is particularly important in regions with complex fault systems, where the spatial distribution of ground motion can vary significantly over short distances. Additionally, BHMs can incorporate temporal dependencies, such as the effects of aftershocks and seismic sequences, which are critical for understanding the evolution of ground motion over time.

Recent applications of BHMs in ground motion prediction have demonstrated their effectiveness in various contexts. For example, in the SISZ-RPOR and TFZ regions of Iceland, a multi-level BHM was developed to estimate site-to-site amplification terms for peak ground acceleration (PGA) and 5%-damped pseudo-spectral acceleration (PSA) at multiple periods [19]. This model not only provided more accurate ground motion predictions but also allowed for the identification of sites with anomalously high amplification, which can inform targeted mitigation strategies. Similarly, in the Raton Basin of Colorado and New Mexico, a BHM was used to optimize injection activities to reduce seismic hazard while maximizing economic benefits. These applications highlight the versatility and practical utility of BHMs in both natural and anthropogenic seismic settings.

### 4.2.3 Monte Carlo Approaches to Probabilistic Seismic Hazard Analysis
Monte Carlo approaches to probabilistic seismic hazard analysis (PSHA) offer a robust framework for incorporating uncertainties in seismic source characterization, ground motion prediction, and site response [20]. Unlike deterministic methods, Monte Carlo simulations allow for the stochastic sampling of input parameters, thereby providing a more comprehensive assessment of potential seismic hazards. This method involves generating a large number of earthquake scenarios, each with varying magnitudes, locations, and rupture characteristics, and then propagating these scenarios through ground motion prediction equations (GMPEs) to estimate the resulting ground motions at specific sites [18]. The cumulative distribution of these ground motions is then used to derive the annual probability of exceedance (PoE) for a given level of ground shaking.

The key advantage of Monte Carlo methods lies in their ability to handle complex and interdependent uncertainties, such as those associated with earthquake recurrence rates, fault geometry, and soil conditions. By simulating a wide range of possible scenarios, these methods provide a more realistic representation of the probabilistic nature of seismic hazards. For instance, the spatial and temporal variability of seismic activity can be captured by incorporating spatially correlated ground motions and time-dependent seismicity models. Additionally, Monte Carlo simulations can account for epistemic uncertainties, such as those arising from limited data or model assumptions, by sampling from probability distributions that reflect the current state of knowledge.

In practice, Monte Carlo approaches to PSHA involve several steps: defining the seismic source zone model, selecting appropriate GMPEs, and specifying the site conditions [20]. Each step introduces its own set of uncertainties, which are propagated through the simulation process. The final output is a probabilistic seismic hazard curve, which provides the PoE of exceeding various levels of ground motion over a specified period. This curve is crucial for designing earthquake-resistant structures and developing seismic risk mitigation strategies. The flexibility and adaptability of Monte Carlo methods make them particularly suitable for regions with complex seismo-tectonic settings, where traditional deterministic approaches may be inadequate.

## 4.3 Field and Experimental Studies

### 4.3.1 Shake-Table Tests for Structural Seismic Performance
Shake-table tests are a critical component in the evaluation of structural seismic performance, providing a controlled environment to simulate earthquake-induced ground motions and assess the dynamic response of structures [21]. These tests involve the use of a shaking table, which can replicate various earthquake scenarios, allowing researchers to study the behavior of scaled structural models under controlled conditions. The primary objective is to validate the seismic performance of structures, particularly focusing on their ability to withstand and recover from significant seismic events [2]. The tests can be designed to evaluate different aspects of structural behavior, such as the impact of various earthquake intensities, the effectiveness of seismic retrofit measures, and the influence of infill panels on the overall structural integrity.

One of the key applications of shake-table tests is the evaluation of reinforced concrete (RC) frames, which are commonly used in building construction [21]. For instance, researchers have conducted extensive tests on under-designed 1/2.5 scaled RC frames to investigate their seismic behavior under different scenarios, including with and without seismic retrofit interventions and with and without infill panels [21]. These tests have provided valuable insights into the performance of RC frames, highlighting the importance of proper joint design and the role of infill panels in enhancing structural stability. The results from these tests are crucial for developing and refining building codes and design practices, ensuring that structures can better withstand seismic events.

Shake-table tests also play a vital role in the development of performance-based earthquake engineering (PBEE) frameworks, which aim to provide a systematic method for risk-informed seismic design and post-hazard decision-making [22]. By simulating a range of earthquake scenarios, these tests help in generating ground motion intensity measures (IMs) and engineering demand parameters (EDPs), which are essential for assessing the seismic performance of structures. The data obtained from shake-table tests can be used to calibrate and validate ground motion prediction equations (GMPEs) and fragility curves, which are fundamental tools in PBEE. Overall, shake-table tests are indispensable for advancing our understanding of structural seismic performance and improving the resilience of buildings in earthquake-prone regions.

### 4.3.2 Seismological Analysis of Anthropogenic Seismicity
Seismological analysis of anthropogenic seismicity involves a comprehensive examination of the causes, mechanisms, and impacts of earthquakes induced by human activities. These activities, such as hydraulic fracturing, wastewater disposal, and gas extraction, can significantly alter the stress state in the subsurface, leading to induced seismicity [23]. The seismological analysis of these events often requires a multidisciplinary approach, integrating data from seismology, geology, and engineering. Key factors in this analysis include the spatial and temporal distribution of seismic events, the magnitude and frequency of induced earthquakes, and the physical properties of the subsurface, such as pore pressure and fault geometry. Advanced techniques, such as high-resolution seismicity catalogs and seismic imaging, are crucial for accurately characterizing the fault zones and understanding the mechanisms of rupture propagation [24].

The seismological analysis of anthropogenic seismicity also involves the assessment of the seismogenic index (SI), a proxy for the number and stress state of pre-existing basement faults, which can significantly influence the susceptibility of a region to induced earthquakes. For instance, in regions like Oklahoma and Kansas, spatiotemporal variations in induced seismic hazard have been linked to changes in pore pressure and the spatial distribution of subsurface susceptibility. The use of bilateral filters in seismic data analysis has been particularly effective in preserving sharp intensity shifts, thereby enhancing the model's ability to capture and represent the complex spatial relationships of seismic activity [17]. This approach allows for a more nuanced understanding of the seismic landscape, integrating data on ground shaking, seismic ground failures, and the impacts visible through satellite imagery [17].

Moreover, the seismological analysis of anthropogenic seismicity often includes the integration of ground deformation data, earthquake relocations, and refined statistical analysis of seismicity. For example, a detailed analysis of a seismic sequence in the offshore area between Santorini and Amorgos Islands in early 2025 revealed a gradual evolution over several weeks, culminating in a classical aftershock sequence [25]. This sequence provided insights into the fluid-induced earthquake dynamics, challenging current models of seismic and volcanic hazard assessment [25]. The findings from such studies highlight the importance of considering the complex interactions between volcanic processes and tectonic-like seismicity, which can offer new perspectives on the mechanisms of induced seismicity and improve the accuracy of seismic hazard assessments [25].

### 4.3.3 Spatial-Variant Causal Bayesian Inference for Ground Failure Assessment
Spatial-variant causal Bayesian inference represents a significant advancement in the assessment of ground failure, particularly in the context of seismic events [17]. This approach leverages Bayesian networks to model the causal relationships between ground shaking, seismic ground failures, and their subsequent impacts, such as building damage and changes in ground surface morphology [17]. Unlike traditional methods that often assume uniform causal relationships across a region, spatial-variant causal Bayesian inference accounts for the spatial heterogeneity of these relationships, recognizing that the strength and direction of causal links can vary significantly from one location to another [5]. This is particularly important in regions with complex geological structures, where the impact of an earthquake can differ markedly over short distances.

The core of this methodology lies in the construction of a spatial-variant Bayesian network, which integrates both observable and latent variables [5]. Observable variables include geospatial features such as slope, lithology, and ground motion data, as well as damage proxy maps derived from satellite imagery [5]. Latent variables, on the other hand, represent unobserved hazards and impacts, such as landslides and liquefaction, which are critical for a comprehensive understanding of the causal chain leading to building damage. The spatial-variant causal coefficients, a key component of this approach, quantify the strength of causal relationships at each location, allowing for a more nuanced and accurate assessment of ground failure risk [17]. These coefficients are estimated using variational inference techniques, which enable the model to handle the high-dimensional and often sparse data typical of seismic studies.

To further enhance the robustness of spatial-variant causal Bayesian inference, researchers have developed methods to calibrate the model using historical data from prior ground failures. This calibration process involves estimating the parameters of the Bayesian network by fitting the model to observed data, thereby refining the estimates of seismic ground failure and building damage. The integration of satellite imagery and geospatial data provides a rich source of information for this calibration, allowing the model to capture the spatial variability of ground conditions and their response to seismic events. This approach not only improves the accuracy of ground failure predictions but also facilitates the identification of areas with higher susceptibility to seismic hazards, thereby informing risk mitigation strategies and urban planning efforts.

# 5 Geophysical Methods for Subsurface Characterization

## 5.1 Numerical and Computational Techniques

### 5.1.1 Discontinuous Galerkin Method for 3D Seismic Wave Propagation
The discontinuous Galerkin (DG) method has emerged as a powerful tool for simulating 3D seismic wave propagation, particularly in complex geological settings. This method combines the flexibility of finite element methods with the efficiency of finite volume methods, making it well-suited for handling the heterogeneous and anisotropic properties of Earth's subsurface. The DG method discretizes the domain into elements, where the solution is approximated by piecewise polynomial functions that are allowed to be discontinuous across element boundaries. This discontinuity is managed through numerical fluxes, which ensure conservation and stability of the solution. The method's ability to handle complex geometries and varying material properties makes it particularly valuable for seismic simulations in regions with intricate geological structures, such as subduction zones and fault systems.

In the context of 3D seismic wave propagation, the DG method is capable of accurately capturing the dynamic behavior of waves across a wide range of frequencies. Traditional methods, such as finite difference schemes, often struggle with high-frequency simulations due to their computational demands and limitations in handling complex boundary conditions. The DG method, however, can efficiently simulate high-frequency waves by leveraging high-order polynomial approximations and parallel computing architectures. This capability is crucial for applications such as earthquake engineering, where accurate predictions of ground motion are essential for assessing structural safety and designing resilient infrastructure [3]. The method's robustness in dealing with nonlinear rock rheologies further enhances its applicability, allowing for the simulation of phenomena such as plastic deformation and fracture propagation.

To implement the DG method for 3D seismic wave propagation, several key considerations must be addressed. These include the choice of basis functions, the treatment of boundary conditions, and the efficient management of computational resources. High-order basis functions are typically used to achieve higher accuracy, while specialized techniques, such as perfectly matched layers (PMLs), are employed to absorb outgoing waves at the boundaries and prevent artificial reflections. The computational efficiency of the method is enhanced through parallelization strategies, such as domain decomposition and load balancing, which are essential for running large-scale simulations on high-performance computing systems. Validation of the DG method is performed through comparisons with analytical solutions and benchmark problems, ensuring its reliability and accuracy in practical applications.

### 5.1.2 Joint Inversion of Receiver Functions and Surface Wave Dispersion
Joint inversion of receiver functions (RFs) and surface wave dispersion (SWD) has emerged as a powerful technique for improving the resolution and reliability of subsurface imaging in geophysical studies [26]. By integrating the complementary information from RFs, which are sensitive to sharp velocity contrasts and crustal interfaces, and SWD, which provides constraints on the average velocity structure over longer wavelengths, joint inversion methods can significantly reduce the non-uniqueness of the inversion results. This approach is particularly advantageous in regions with complex geological structures, such as sedimentary basins and crustal interfaces, where traditional single-dataset inversions often struggle to provide accurate and stable solutions [27].

The joint inversion process typically involves the simultaneous fitting of RF and SWD data to a common velocity model, which is iteratively refined to minimize the misfit between the observed and predicted data. Advanced inversion algorithms, such as the transdimensional Bayesian approach and the unscented Kalman filter (UKI), have been developed to handle the high-dimensional parameter space and data uncertainties inherent in joint inversion [26]. These algorithms can effectively account for the trade-offs between different data types and provide robust estimates of subsurface properties, including the depths of sedimentary basins and the Moho discontinuity [27]. The UKI method, in particular, has shown promise in its ability to handle nonlinear relationships and provide uncertainty quantification, making it a valuable tool for geophysical imaging.

Despite the advantages of joint inversion, several challenges remain. One of the primary challenges is the computational complexity associated with handling large datasets and the high-dimensional parameter space. Efficient parallel computing and optimization techniques are essential for making joint inversion feasible in practical applications. Additionally, the integration of multiple geophysical datasets requires careful consideration of data weighting and the development of robust error models to ensure that the inversion results are not biased by one dataset over the other. As computational resources continue to improve and new algorithms are developed, the joint inversion of RFs and SWD is expected to play an increasingly important role in geophysical exploration and subsurface characterization [26].

### 5.1.3 Hybrid Models for Induced Seismicity Forecasting
Hybrid models for induced seismicity forecasting integrate multiple data sources and methodologies to improve the accuracy and reliability of predictions [23]. These models typically combine geophysical, geological, and hydrological data with advanced computational techniques, such as machine learning and numerical simulations. The primary goal is to capture the complex interactions between fluid injection or extraction, subsurface stress changes, and seismic activity. By leveraging the strengths of both data-driven and physics-based approaches, hybrid models can provide more comprehensive insights into the mechanisms driving induced seismicity. For instance, machine learning algorithms can identify patterns and correlations in large datasets that are not immediately apparent through traditional methods, while numerical simulations can help validate these findings by simulating the physical processes involved.

One of the key challenges in induced seismicity forecasting is the heterogeneity of subsurface conditions and the variability in the response to fluid injection or extraction. Hybrid models address this by incorporating detailed 3D geological models and dynamic simulations of fluid flow and stress changes. These models can simulate the propagation of seismic waves through complex subsurface structures, accounting for factors such as anisotropy, heterogeneity, and non-linear behavior [28]. For example, the integration of seismic tomography with geomechanical models allows for a more accurate representation of the subsurface environment, which is crucial for predicting the locations and magnitudes of potential induced earthquakes. Additionally, the use of real-time monitoring data, such as microseismic events and pressure changes, can further refine these models and improve their predictive capabilities.

Another important aspect of hybrid models is their ability to handle the uncertainty and variability inherent in induced seismicity [3]. By combining probabilistic methods with deterministic simulations, these models can provide probabilistic forecasts that account for the range of possible outcomes. This is particularly useful for risk assessment and decision-making in industries such as oil and gas, geothermal energy, and waste disposal. For instance, Bayesian approaches can be used to update model parameters based on new data, allowing for continuous improvement and adaptation. Furthermore, the integration of machine learning techniques, such as neural networks and decision trees, can enhance the model's ability to handle non-linear relationships and high-dimensional data. Overall, hybrid models represent a significant advancement in the field of induced seismicity forecasting, offering a more robust and reliable framework for understanding and mitigating seismic risks [23].

## 5.2 Geophysical Survey and Imaging

### 5.2.1 Electrical Resistivity Tomography and Seismic Refraction Tomography
Electrical Resistivity Tomography (ERT) and Seismic Refraction Tomography (SRT) are advanced geophysical techniques that have revolutionized subsurface imaging, providing high-resolution and detailed information about the subsurface structure [27]. ERT measures the electrical resistivity distribution within the ground, which is influenced by factors such as lithology, fluid content, and porosity. This method involves injecting electrical currents into the ground through electrodes and measuring the resulting voltage differences. The data collected are then inverted to produce a two-dimensional (2D) or three-dimensional (3D) resistivity model of the subsurface. ERT is particularly useful for identifying variations in subsurface materials, such as different rock types and groundwater content, making it a valuable tool in hydrogeological and environmental studies.

Seismic Refraction Tomography (SRT), on the other hand, utilizes the principles of seismic wave propagation to map subsurface structures. SRT involves generating seismic waves at the surface and recording the arrival times of these waves at multiple receiver locations. The primary advantage of SRT is its ability to delineate the boundaries between different geological layers based on the velocity of seismic waves. This technique is particularly effective in areas with distinct velocity contrasts, such as between sedimentary layers and bedrock. The combination of ERT and SRT can provide a comprehensive understanding of the subsurface, as the complementary nature of the data helps to overcome the limitations of each individual method [27]. For instance, while ERT is sensitive to changes in fluid content and porosity, SRT is more effective in resolving deeper structures and providing information on the mechanical properties of the subsurface.

Recent advancements in computational algorithms and machine learning (ML) have significantly enhanced the inversion and interpretation of ERT and SRT data. ML-driven models can process large datasets more efficiently, reducing the non-uniqueness of solutions and improving the accuracy of subsurface imaging. These models can also integrate data from multiple sources, such as geophysical surveys and borehole logs, to create more reliable and detailed subsurface models. Case studies have demonstrated the effectiveness of these integrated approaches in various applications, including groundwater management, environmental monitoring, and engineering site investigations. By leveraging the strengths of both ERT and SRT, researchers and practitioners can achieve a more robust and comprehensive characterization of the subsurface, which is essential for informed decision-making in fields such as resource exploration, infrastructure development, and natural hazard mitigation.

### 5.2.2 Ray Tracing and Grid-Based Travel Time Computations
Ray tracing and grid-based travel time computations are fundamental techniques in seismology and geophysics, providing essential insights into the Earth's subsurface structure. Ray tracing involves solving the eikonal equation to compute the travel times of seismic waves through a given velocity model. This method is particularly effective in homogeneous or smoothly varying media, where rays can be traced from sources to receivers using Fermat's principle of least time. Despite its efficiency, ray tracing can struggle with complex, heterogeneous media, leading to inaccuracies in travel time predictions. To address these limitations, grid-based methods have been developed, which discretize the subsurface into a grid and solve the eikonal equation numerically. These methods, including finite-difference and fast marching algorithms, are better suited for handling complex velocity structures and can provide more accurate travel time estimates.

Grid-based travel time computations, such as those implemented in finite-difference methods, offer a robust alternative to ray tracing by solving the eikonal equation on a regular grid [29]. These methods iteratively update travel times at each grid point until convergence is achieved, ensuring that the computed travel times accurately reflect the underlying velocity model. The fast marching method, a variant of the finite-difference approach, is particularly efficient for large-scale problems, as it propagates the front of the wavefront in a manner similar to Dijkstra's algorithm for finding the shortest path in a graph. This method has been widely adopted in seismic tomography and earthquake location studies, where accurate travel time calculations are crucial for resolving subsurface structures and determining hypocenters [29]. However, the computational cost of grid-based methods can be high, especially for 3D models, which often require significant computational resources and parallel processing capabilities.

Despite the advancements in computational methods, the practical application of 3D travel time calculations remains challenging due to the computational demands and the need for high-resolution velocity models [29]. To facilitate broader adoption, researchers have developed open-source software tools that simplify the process of setting up and running travel time computations. These tools, such as the SeisComP3 and ObsPy packages, provide user-friendly interfaces and pre-built algorithms for both ray tracing and grid-based methods, making it easier for geoscientists to perform complex travel time calculations. Additionally, the integration of these methods with high-performance computing platforms has enabled more detailed and accurate simulations, enhancing our understanding of subsurface structures and improving the reliability of seismic hazard assessments.

### 5.2.3 Theoretical and Experimental Studies of Shear Zones and SSEs
Theoretical and experimental studies of shear zones and slow slip events (SSEs) have significantly advanced our understanding of the mechanics and dynamics of subduction zones [30]. Theoretical models have provided insights into the frictional behavior of faults, the role of fluid pressure, and the evolution of stress and strain during the seismic cycle [15]. For instance, rate-and-state friction laws have been instrumental in explaining the stick-slip behavior observed in laboratory experiments and natural fault systems. These models predict that the stability of fault slip is highly sensitive to the rate of slip and the state of the fault surface, which can lead to a range of behaviors from stable creep to catastrophic rupture. Additionally, numerical simulations have been used to explore the effects of heterogeneities in fault properties, such as variations in frictional strength and fluid pressure, on the nucleation and propagation of seismic events [15].

Experimental studies have complemented theoretical work by providing direct observations of fault behavior under controlled conditions. Laboratory experiments on rock samples have revealed the complex interplay between friction, temperature, and fluid pressure, which can significantly influence the mechanical properties of faults. For example, experiments have shown that the presence of fluids can reduce the effective normal stress on a fault, leading to a decrease in the frictional strength and an increase in the likelihood of slip. Furthermore, high-pressure, high-temperature experiments have demonstrated that the rheological behavior of fault materials can change dramatically under conditions similar to those found in deep subduction zones, where temperatures and pressures are much higher than at the surface. These findings have important implications for understanding the mechanisms that control the initiation and propagation of SSEs.

In the context of subduction zones, SSEs have emerged as a critical component of the seismic cycle, contributing to the overall deformation and potentially triggering larger earthquakes [30]. Observational studies using geodetic techniques, such as GPS and InSAR, have identified SSEs in various subduction zones around the world, including Cascadia, Japan, and New Zealand. These events are characterized by slow, aseismic slip over periods ranging from days to months, and they often occur in regions where the plate interface is partially locked. Recent research has shown that SSEs can exhibit deterministic behavior, with repeatable patterns of slip and timing, suggesting that they may be predictable [30]. Understanding the physical processes that govern SSEs, such as the role of fluid pressure and the mechanical properties of the fault zone, is essential for improving our ability to forecast seismic hazards in subduction zones.

## 5.3 Remote Sensing and Satellite Data

### 5.3.1 Normalized Differential Water Index for Soil Water Content
The Normalized Differential Water Index (NDWI) is a remote sensing technique designed to assess the wetness level of an area, which is closely related to soil water content [31]. This index is derived by comparing the reflectance values in the near-infrared (NIR) band, typically centered around 832 nm, and the short-wave infrared (SWIR) band, usually around 1613 nm. The NIR band is highly sensitive to water content, as water molecules strongly absorb light in this spectral region, while the SWIR band is less affected by water but more influenced by soil minerals and organic matter. By normalizing the difference between these two bands, NDWI provides a quantitative measure of the relative moisture content in the soil, making it a valuable tool for hydrological and agricultural applications [31].

In practice, the NDWI is calculated using the formula: \( \text{NDWI} = \frac{(NIR - SWIR)}{(NIR + SWIR)} \). This equation enhances the contrast between wet and dry areas, allowing for more accurate delineation of soil moisture levels. The effectiveness of NDWI in soil water content estimation is further improved by comparing different combinations of NIR and SWIR bands [31]. For instance, the combination of NIR and SWIR1 (around 1613 nm) can be compared with the combination of NIR and SWIR2 (another SWIR band, often around 2190 nm), each providing unique insights into the soil's moisture conditions. These comparisons help in validating the results and ensuring the reliability of the soil water content assessments.

The application of NDWI in soil water content mapping is particularly significant in regions prone to natural hazards such as earthquakes, where soil moisture plays a critical role in the vulnerability to liquefaction. By integrating NDWI with other geospatial data, such as vegetation indices, a comprehensive understanding of the area's hydrological and ecological conditions can be achieved. This integrated approach not only aids in the assessment of soil moisture but also supports broader environmental monitoring and management efforts, enhancing the resilience of communities to environmental challenges.

### 5.3.2 High-Resolution Seismicity Data for Fault Zone Dip Measurement
High-resolution seismicity data play a critical role in accurately measuring fault zone dip, which is essential for understanding the structural and mechanical properties of fault zones [24]. Traditional seismic methods, such as active source surveys, provide detailed images of subsurface structures but are often limited by high costs and logistical constraints, especially in large-scale studies. Recent advancements in passive seismic techniques, including the deployment of dense seismic arrays, have significantly enhanced the resolution and coverage of seismic data, making it feasible to map fault zones with unprecedented detail.

Surface wave dispersion (SWD) and ambient noise tomography (ANT) are two key techniques that have revolutionized the field of high-resolution seismicity data analysis [26]. SWD involves analyzing the dispersion characteristics of surface waves to infer shear wave velocity structures, which are crucial for understanding the mechanical properties of fault zones. ANT, on the other hand, utilizes continuous seismic noise recordings to generate high-resolution images of the subsurface, providing valuable insights into the spatial distribution of seismic velocities. The integration of these techniques with advanced inversion methods has enabled researchers to more accurately delineate fault zone structures and measure their dip angles, thereby improving our understanding of fault zone behavior and seismic hazard assessment.

To further enhance the resolution and accuracy of fault zone dip measurements, multifrequency receiver function (RF) inversion techniques have been developed. These methods involve analyzing the direct P phase within P-to-S converted waves (P-RFs) to provide detailed information about the shallow crustal structure [26]. By combining RF inversion with SWD and ANT, researchers can achieve a more comprehensive and precise characterization of fault zones. This integrated approach not only improves the resolution of fault zone dip measurements but also provides valuable insights into the physical properties of the fault zone, such as porosity and fracture density, which are critical for assessing seismic hazards and understanding the mechanics of faulting processes.

### 5.3.3 Satellite Imagery for Rapid Seismic Ground Failure Assessment
Satellite imagery, particularly from platforms like the European Space Agencys Sentinel-2, has emerged as a critical tool for rapid seismic ground failure assessment. Sentinel-2, with its twelve spectral bands and spatial resolutions of 10, 20, and 60 meters, provides detailed and frequent coverage of large areas, making it ideal for monitoring seismic events and their aftermath. The high-resolution imagery allows for the identification of ground deformation, landslides, and other surface changes that occur during and after earthquakes. This capability is particularly valuable in regions with limited ground-based monitoring infrastructure, such as the megathrust zone in the western part of Padang, where the risk of seismic activity is high and the need for rapid assessment is critical [31].

The use of satellite imagery in seismic ground failure assessment is not limited to post-event analysis; it also plays a crucial role in pre-event hazard mapping and vulnerability assessment [17]. By analyzing historical imagery and vegetation indices, such as the Normalized Difference Water Index (NDWI), researchers can map water content and identify areas prone to liquefaction and landslides [31]. This information is essential for urban planning and the evaluation of building structures in seismically active regions. The integration of multi-temporal satellite data with ground-based measurements and geophysical surveys enhances the accuracy and reliability of these assessments, providing a comprehensive understanding of the seismic hazards and risks.

Moreover, the rapid acquisition and processing capabilities of satellite imagery enable real-time monitoring and rapid response to seismic events. Machine learning techniques, when applied to satellite data, can automate the detection of ground failures and provide near-instantaneous alerts to emergency responders and decision-makers. This real-time monitoring is particularly important in densely populated areas like Padang City, where the timely identification of affected regions can significantly reduce the impact of seismic events on human life and infrastructure. The combination of satellite imagery with advanced computational methods represents a significant advancement in the field of seismic ground failure assessment, offering a powerful tool for both research and practical applications.

# 6 Future Directions


The current survey highlights several limitations and gaps in the field of seismic monitoring, prediction, and hazard assessment in mountainous regions. One of the primary limitations is the difficulty in deploying and maintaining seismic networks in remote and harsh environments, which often leads to incomplete and sporadic data collection. Additionally, while advanced machine learning and deep learning techniques have shown promise in enhancing seismic data processing and prediction, they are still limited by the availability of high-quality, labeled datasets and the need for robust validation in real-world scenarios. Furthermore, the integration of physics-informed constraints into these models remains a challenge, as it requires a deep understanding of the underlying physical processes and the ability to accurately model their interactions. The complexity of the geological structures in mountain regions, including the presence of multiple fault systems and varying soil conditions, also poses significant challenges in accurately modeling seismic behavior and predicting ground motion.

To address these limitations, several directions for future research are proposed. First, there is a need to develop more resilient and autonomous seismic monitoring systems that can operate effectively in challenging environments. This includes the integration of advanced power management systems, robust communication technologies, and self-maintenance capabilities. Additionally, the use of drone and satellite-based monitoring systems can complement ground-based networks, providing a more comprehensive and continuous coverage of seismic activity. Second, the development of large, high-quality, and diverse seismic datasets is crucial for training and validating machine learning models. Collaborative efforts between researchers, government agencies, and industry stakeholders can facilitate the creation of open-source datasets that are representative of various seismic environments. Third, the integration of physics-informed constraints into machine learning models should be further explored. This can be achieved by developing hybrid models that combine the strengths of data-driven and physics-based approaches, ensuring that the learned patterns are not only statistically robust but also physically plausible. Finally, the development of explainable AI techniques can enhance the transparency and interpretability of machine learning models, making them more trustworthy and actionable for decision-makers in the field of seismic hazard mitigation.

The potential impact of these proposed future research directions is significant. Improved seismic monitoring systems and data collection methods will lead to more accurate and timely data, which is essential for enhancing our understanding of seismic behavior and improving early warning systems. The development of large and diverse datasets, coupled with the integration of physics-informed constraints, will enable the creation of more robust and reliable machine learning models. These models can provide more accurate predictions of seismic events and ground motion, ultimately contributing to more effective seismic hazard assessments and mitigation strategies. Furthermore, the use of explainable AI techniques will increase the trust and adoption of these models by stakeholders, including policymakers, emergency responders, and the public. Overall, these advances will enhance the resilience of communities in mountainous regions, reducing the risk of loss of life and property damage from seismic hazards.

# 7 Conclusion



The survey paper provides a comprehensive overview of the advanced techniques and methodologies used to monitor, predict, and assess earthquake hazards in mountain regions. The paper highlights the integration of machine learning and deep learning techniques, such as supervised machine learning for earthquake declustering, hybrid CNN-LSTM models for Vs30 prediction, and neural operators for seismic response modeling. These methods have shown significant potential in enhancing the accuracy and robustness of seismic hazard assessments. Additionally, the paper discusses advanced seismic network and data processing techniques, including the deployment and maintenance of seismic networks, SLAM-based channel detection and mapping, and efficient inversion techniques for high-dimensional data. These advancements are crucial for improving the operational efficiency and accuracy of seismic networks, particularly in harsh and remote environments. The paper also explores seismic data synthesis and analysis, focusing on conditional diffusion models for seismic waveform synthesis, neural machine translation for seismic data interpretation, and Bayesian regularized neural networks for volcanic activity prediction. These models offer flexible and scalable solutions for handling the high-dimensional and noisy nature of seismic data, enhancing the reliability and interpretability of seismic hazard assessments. Finally, the paper delves into seismic hazard assessment and mitigation strategies, including 3D dynamic rupture simulations, spatially-aware variational causal Bayesian networks, and Gaussian processes for probabilistic fusion of velocity models. These sections provide a detailed look at the modeling and simulation of seismic events, the integration of spatial variability in causal mechanisms, and the probabilistic fusion of velocity models to account for uncertainties, all of which are essential for enhancing seismic hazard assessments and informing disaster response planning.

The significance of this survey paper lies in its comprehensive review of the latest advancements in earthquake hazard research, particularly in mountain regions. By synthesizing insights from a wide range of studies and methodologies, this paper serves as a valuable resource for researchers, practitioners, and policymakers. The paper not only highlights the current state of the art but also identifies key areas for future research and development. The integration of machine learning and advanced computational methods is shown to have the potential to significantly improve seismic hazard mitigation strategies, ultimately contributing to the safety and resilience of communities in mountainous areas. The paper's focus on practical applications and real-world scenarios underscores its relevance and impact in the field of seismology.

In conclusion, this survey paper emphasizes the importance of continued research and innovation in seismic hazard assessment and mitigation. The rapid advancements in machine learning and computational techniques offer promising avenues for improving the accuracy and reliability of seismic hazard assessments. Researchers and practitioners are encouraged to build upon the methodologies discussed in this paper, exploring new techniques and interdisciplinary approaches to address the unique challenges of mountain regions. Policymakers and stakeholders are also urged to invest in the deployment and maintenance of advanced seismic monitoring networks and to adopt the latest technologies for real-time data processing and hazard assessment. By fostering collaboration and knowledge sharing, the scientific community can make significant strides in enhancing seismic hazard mitigation and protecting vulnerable populations in mountainous areas.

# References
[1] Gaussian Processes for Probabilistic Estimates of Earthquake Ground  Shaking  A 1-D Proof-of-Concept  
[2] SeisGPT  A Physics-Informed Data-Driven Large Model for Real-Time  Seismic Response Prediction  
[3] High Resolution Seismic Waveform Generation using Denoising Diffusion  
[4] Controllable seismic velocity synthesis using generative diffusion  models  
[5] Spatially-Heterogeneous Causal Bayesian Networks for Seismic  Multi-Hazard Estimation  A Variational  
[6] Do earthquakes  know  how big they will be  a neural-net aided study  
[7] MC-GRU a Multi-Channel GRU network for generalized nonlinear structural  response prediction across  
[8] Deep Sequence Models for Predicting Average Shear Wave Velocity from  Strong Motion Records  
[9] Ambient Noise Full Waveform Inversion with Neural Operators  
[10] Neural Operators for Stochastic Modeling of Nonlinear Structural System  Response to Natural Hazards  
[11] Deep Learning-based Average Shear Wave Velocity Prediction using  Accelerometer Records  
[12] AscDAMs  Advanced SLAM-based channel detection and mapping system  
[13] Forecasting Volcanic Radiative Power (VPR) at Fuego Volcano Using  Bayesian Regularized Neural Netwo  
[14] Near-real-time design of experiments for seismic monitoring of volcanoes  
[15] Ground Motion Characteristics of Cascading Earthquakes in a Multiscale  Fracture Network  
[16] Probability of earthquake fault jumps from physics based criterion  
[17] Spatial-variant causal Bayesian inference for rapid seismic ground  failures and impacts estimation  
[18] QuakeFormer  A Uniform Approach to Earthquake Ground Motion Prediction  Using Masked Transformers  
[19] Nationwide frequency-dependent seismic site amplification models for  Iceland  
[20] CyberShake Earthquake Fault Rupture Modeling and Ground Motion  Simulations for the Southwest Icelan  
[21] Monitoring the Seismic Behavior of a Scaled RC Frame with Intermediate  Ductility in a Shaking Table  
[22] Long-range Ising model for regional-scale seismic risk analysis  
[23] Mitigation and optimization of induced seismicity using physics-based  forecasting  
[24] Insights on the dip of fault zones in Southern California from modeling  of seismicity with anisotro  
[25] 2025 Santorini-Amorgos crisis triggered by a transition from volcanic to  regular tectonic activity  
[26] Enhancing Crustal Velocity Structure in Sedimentary Basin by joint  inversion of Teleseismic P-Wave  
[27] Electrical and seismic refraction methods  fundamental concepts, current  trends, and emerging machi  
[28] A Discontinuous Galerkin Method for Simulating 3D Seismic Wave  Propagation in Nonlinear Rock Models  
[29] HypoNet Nankai  Rapid hypocenter determination tool for the Nankai  Trough subduction zone using phy  
[30] Chaotic Slow Slip Events in New Zealand from two coupled slip patches  a  proof of concept  
[31] Vulnerability Liquefaction Mapping in Padang City Based on Cloud  Computing Using Optical Satellite  