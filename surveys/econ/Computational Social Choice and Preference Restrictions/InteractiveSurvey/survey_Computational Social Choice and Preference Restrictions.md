# A Survey of Computational Social Choice and Preference Restrictions

# 1 Abstract


The field of computational social choice (ComSoC) integrates insights from computer science, economics, and political science to address fundamental questions in collective decision-making, particularly in the design and analysis of algorithms and mechanisms for aggregating individual preferences. This survey paper focuses on the intersection of ComSoC with preference restrictions, a topic that has gained significant attention due to its practical implications and theoretical richness. The survey provides a comprehensive overview of the current state of research, highlighting key theoretical results, empirical findings, and computational methods. Key contributions include an in-depth analysis of real-world elections, the use of integer linear programming and probabilistic analysis, and the exploration of game-theoretic and algorithmic techniques for addressing computational challenges. The survey also delves into the theoretical and axiomatic analysis of social choice mechanisms, including the study of impossibility results and the design of strategy-proof mechanisms. Finally, the survey highlights the importance of preference restrictions in guiding future research and fostering interdisciplinary collaboration, ultimately contributing to the development of more effective and fair collective decision-making processes.

# 2 Introduction
The field of computational social choice (ComSoC) has emerged as a critical interdisciplinary area, integrating insights from computer science, economics, and political science to address fundamental questions in collective decision-making [1]. This field focuses on the design and analysis of algorithms and mechanisms that aggregate individual preferences to make collective decisions, such as in voting systems, resource allocation, and participatory budgeting [2]. The rise of ComSoC is driven by the increasing availability of computational resources and the need for more efficient and fair methods of decision-making in complex, diverse societies. Theoretical and empirical advances in this area have led to a deeper understanding of the properties and limitations of various social choice mechanisms, as well as the development of new algorithms and techniques to optimize these processes.

This survey paper focuses on the intersection of computational social choice with preference restrictions, a topic that has gained significant attention due to its practical implications and theoretical richness [3]. Preference restrictions refer to the constraints placed on the set of possible preferences that individuals can express, which can significantly impact the performance and fairness of social choice mechanisms [4]. By analyzing the effects of these restrictions, researchers can design more robust and efficient voting systems and other collective decision-making processes. This survey aims to provide a comprehensive overview of the current state of research in this area, highlighting key theoretical results, empirical findings, and computational methods.

The survey begins with an in-depth analysis of real-world elections and voting systems, focusing on the theoretical optimization of voting rules and the use of integer linear programming (ILP) and probabilistic analysis [3]. The concept of expected distortion and its relationship to social welfare optimization is a central theme, with a detailed exploration of how different voting rules perform under various distributional assumptions [5]. The use of ILP and probabilistic methods is discussed, emphasizing their role in deriving exact and approximate solutions to complex election-related problems. Additionally, the survey delves into the computational complexity of voting systems, particularly in the context of semi-random models, which offer a middle ground between worst-case and average-case analyses.

The empirical analysis of voting systems is another key component of the survey, with a focus on the use of synthetic data and real-world datasets. The PrefLib platform and the generation of synthetic datasets are discussed, highlighting their importance in testing theoretical models and algorithms under controlled conditions. The survey also covers the development of election indices and visualization tools, which facilitate the classification and comparison of different datasets [3]. The intersection of ComSoC with abstract argumentation (AA) is explored, emphasizing the role of AA in modeling and analyzing the acceptability of voting outcomes, especially in scenarios involving conflicting preferences.

The survey then transitions to a detailed examination of algorithmic and computational methods, including linear programming, game-theoretic analysis of strategic interactions, and probabilistic models of misalignment. The use of linear programming and rounding schemes in addressing the metric distortion problem is discussed, along with the application of game-theoretic models to understand strategic behaviors such as manipulation, bribery, and control. The survey also explores the use of probabilistic models to capture the discrepancies between agent preferences and collective outcomes, providing a framework for understanding the trade-offs between fairness and efficiency.

Finally, the survey concludes with an overview of theoretical and axiomatic analysis, including the characterization of voting rules, the study of impossibility results, and the development of network solutions and graph theory. The survey highlights the importance of axiomatic properties such as strategy-proofness and Pareto efficiency, and the role of computational complexity in verifying and implementing these properties. The survey also discusses the application of graph theory to problems in social choice, such as the graph isomorphism problem and canonical labeling, and the use of network solutions in resource allocation and scheduling.

The contributions of this survey paper are multifaceted [6]. It provides a comprehensive and up-to-date overview of the current research landscape in computational social choice, with a particular focus on preference restrictions [1]. The survey synthesizes key findings from theoretical, empirical, and computational studies, offering a unified perspective on the challenges and opportunities in this field. By highlighting recent advancements and open problems, the survey aims to guide future research and foster interdisciplinary collaboration, ultimately contributing to the development of more effective and fair collective decision-making processes.

# 3 Analyzing Real-World Elections and Voting Systems

## 3.1 Theoretical Analysis and Optimization of Voting Rules

### 3.1.1 Expected Distortion and Social Welfare
The concept of expected distortion in the context of social welfare optimization is a critical aspect of evaluating the performance of voting mechanisms. Expected distortion measures the discrepancy between the optimal social welfare achievable and the social welfare realized by a given voting rule [5]. In scenarios with a finite number of alternatives and voters, the expected distortion provides a quantitative assessment of how well a voting rule approximates the optimal social welfare [5]. This is particularly important in settings where the true preferences of the electorate are uncertain or where the voting rule must operate under incomplete information.

In the analysis of expected distortion, the majority rule emerges as a benchmark for two-alternative elections. For any underlying distribution of voter preferences, the majority rule not only optimizes expected social welfare but also minimizes expected distortion. This result underscores the robustness of the majority rule in aligning with the collective preferences of the electorate. However, as the number of alternatives increases, the complexity of optimizing expected social welfare and minimizing expected distortion grows, necessitating more sophisticated mechanisms. These mechanisms often involve probabilistic approaches or the use of scoring rules that adapt to the underlying distribution of preferences.

The relationship between expected distortion and social welfare is further nuanced by the distributional assumptions made about voter preferences. Under mild conditions, such as the assumption of a smooth distribution of preferences, the voting rule that maximizes expected social welfare also tends to minimize expected distortion [5]. This finding is significant because it suggests that optimizing for social welfare can serve as a proxy for minimizing distortion, thereby simplifying the design of voting mechanisms. However, the effectiveness of this approach depends on the specific characteristics of the distribution, highlighting the need for careful empirical validation in different contexts.

### 3.1.2 Integer Linear Programming and Probabilistic Analysis
Integer Linear Programming (ILP) has emerged as a powerful tool in computational social choice, particularly in the context of election-related problems [7]. By formulating these problems as ILPs, researchers can leverage the rich theory and efficient algorithms developed in the field of integer programming to derive both exact and approximate solutions. For instance, the problem of determining the winner under various voting rules can often be cast as an ILP, where the decision variables represent the allocation of votes or the selection of candidates. This approach has been particularly useful in settings where the number of candidates or voters is relatively small, allowing for the application of exact algorithms to find optimal solutions.

Probabilistic analysis complements the ILP approach by providing insights into the behavior of these algorithms under uncertainty. In many practical scenarios, the preferences of voters or the structure of the election may be subject to noise or randomness. Probabilistic models, such as the semi-random model, allow researchers to analyze the performance of ILP-based algorithms in these settings. For example, the smoothed complexity of an ILP can be studied to understand how small perturbations in the input data affect the algorithm's performance. This analysis has shown that, under certain conditions, ILP formulations of election problems can have polynomial smoothed complexity, even if they are NP-hard in the worst case. This probabilistic perspective is crucial for understanding the practical applicability of ILP-based methods in real-world elections.

Moreover, the combination of ILP and probabilistic analysis has led to the development of robust algorithms that can handle a wide range of input distributions. For instance, the manipulation cost of voting rules can be analyzed using probabilistic methods to determine the likelihood of successful manipulation under different scenarios. This has important implications for the design of voting mechanisms that are resistant to strategic behavior. Additionally, the use of probabilistic techniques in the context of ILP has enabled the derivation of tight bounds on the performance of approximation algorithms, providing a theoretical foundation for their practical use in computational social choice.

### 3.1.3 Semi-Random Models and Computational Complexity
Semi-random models have emerged as a powerful tool for analyzing the computational complexity of voting systems, offering a middle ground between the worst-case and average-case analyses. These models assume that the input data is generated by a combination of adversarial and random processes, thereby capturing more realistic scenarios than purely adversarial or purely random models. In the context of voting, semi-random models have been particularly useful in understanding the robustness of voting rules against strategic manipulation and control. By incorporating elements of randomness, these models can reveal how the presence of noise affects the computational hardness of various voting-related problems, such as determining the winner or identifying manipulative strategies.

The computational complexity of problems in semi-random models often differs significantly from their worst-case counterparts. For instance, while many voting problems are NP-hard in the worst case, they may become tractable under semi-random assumptions. This is because the random component can introduce enough structure to simplify the problem, making it amenable to efficient algorithms. However, the exact nature of this transition from intractability to tractability is highly dependent on the specific semi-random model used. For example, models that assume a small amount of adversarial influence combined with a large amount of randomness may yield different results compared to models with a higher degree of adversarial control.

Moreover, the study of semi-random models has led to the development of new algorithmic techniques that exploit the inherent structure introduced by the random component. These techniques often involve probabilistic methods and concentration inequalities, which help in bounding the likelihood of certain events and in designing algorithms that perform well on average. The insights gained from semi-random models have practical implications for the design of voting systems, as they can guide the selection of voting rules that are both computationally efficient and resistant to manipulation. Additionally, these models provide a framework for evaluating the robustness of existing voting mechanisms in more realistic settings, thereby contributing to the broader goal of ensuring fair and effective collective decision-making processes.

## 3.2 Empirical Studies and Data Analysis

### 3.2.1 Survey of Voting Studies and Synthetic Data
The use of synthetic data in voting studies has become increasingly prevalent, addressing the challenge of data scarcity and enabling researchers to test theoretical models and algorithms under controlled conditions. Since the inception of the PrefLib platform in 2013, which provides a comprehensive database of real-world election data, the field has seen a surge in the generation and utilization of synthetic datasets [3]. These synthetic datasets are designed to mimic various aspects of real elections, such as voter preferences, election outcomes, and the structure of the electorate [1]. Researchers often employ statistical models to generate synthetic data, ensuring that it reflects the complexity and variability of real-world scenarios. For instance, the Mallows model is frequently used to simulate elections with varying degrees of voter agreement and disagreement, allowing for a nuanced exploration of how different voting rules perform under diverse conditions.

The analysis of synthetic data has provided valuable insights into the behavior of voting systems, particularly in scenarios where empirical data is limited. For example, studies have used synthetic data to investigate the frequency of Condorcet winners, the stability of election outcomes under different voting rules, and the impact of strategic voting. These studies have often confirmed findings from empirical research, such as the high prevalence of Condorcet winners in many elections and the high degree of consensus among different voting rules [3]. However, synthetic data has also revealed new phenomena, such as the conditions under which certain voting rules are more susceptible to manipulation or the effects of noise on the stability of election outcomes. By systematically varying parameters in synthetic datasets, researchers can explore the robustness of voting systems to changes in voter behavior and election structure.

Despite the advantages of synthetic data, its use also comes with limitations. One key challenge is ensuring that synthetic data accurately captures the complexities of real-world elections, including the diverse preferences and strategic behaviors of voters [1]. To address this, researchers have developed a range of statistical cultures, each designed to simulate different aspects of voter behavior. For example, the impartial culture model assumes that all preference orders are equally likely, while the Mallows model introduces a notion of central tendency in voter preferences. The choice of statistical culture can significantly impact the results of a study, and researchers must carefully justify their selection based on the specific research question and the characteristics of the real-world data they aim to emulate. Additionally, the use of synthetic data in conjunction with real-world data, as facilitated by platforms like PrefLib, can provide a more comprehensive understanding of voting systems by allowing researchers to validate their findings across multiple data sources [3].

### 3.2.2 Election Indices and Visualization Tools
Election indices and visualization tools play a pivotal role in understanding the structure and properties of elections. These tools facilitate the classification and comparison of different datasets by mapping elections onto a geometric space, where each point represents an election [3]. The map of elections, as developed by Szufa et al. and Boehmer et al., provides a powerful framework for visualizing the relationships between various elections. By plotting elections as points in a multidimensional space, researchers can identify clusters and outliers, which can reveal underlying patterns and anomalies in the data. This visualization technique is particularly useful for detecting datasets that occupy previously unexplored regions of the election space, indicating novel or unique characteristics of the data.

One of the key applications of election indices is in the classification of datasets. By quantifying the distances between elections on the map, these indices enable the categorization of datasets into distinct groups based on their structural similarities [3]. For instance, in our study, we classify our 25 datasets into three categories using the map of elections [3]. Each category corresponds to a different region on the map, reflecting variations in the distribution of voter preferences and candidate rankings. This classification helps in identifying datasets that share common features, such as high agreement or polarization, which can be crucial for understanding the dynamics of different electoral systems.

Furthermore, the development of new visualization techniques, such as the single-election map, enhances the interpretability of election data [8]. Unlike traditional maps that compare multiple elections, the single-election map focuses on the internal structure of a single election, providing insights into the relationships between individual votes [8]. This approach allows researchers to explore the statistical cultures that underlie the generation of the dataset, validating intuitions about the agreement, diversity, and polarization within the election [8]. Such detailed visualizations are essential for validating the robustness of election indices and for developing a deeper understanding of the mechanisms that drive voter behavior in different contexts.

### 3.2.3 ComSoC and Abstract Argumentation
The intersection of Computational Social Choice (ComSoC) and Abstract Argumentation (AA) presents a rich area of research that bridges the gap between collective decision-making and formal reasoning about conflicting arguments. Abstract Argumentation Frameworks (AFs), introduced by Dung, provide a foundational model for representing and reasoning about arguments and their interactions, typically through a directed graph where nodes represent arguments and edges represent attacks between them [9]. In the context of ComSoC, these frameworks are particularly useful for modeling and analyzing the acceptability of different voting outcomes, especially in scenarios where voters have conflicting preferences or where the decision-making process involves a complex interplay of arguments and counterarguments.

One of the key challenges in this intersection is the computational complexity of determining acceptable sets of arguments, which can be seen as analogous to identifying winning outcomes in voting systems. While classical voting rules often focus on aggregating preferences to select a single winner or a set of winners, the AA perspective allows for a more nuanced analysis by considering the acceptability of different sets of arguments under various semantics [2]. For example, the grounded semantics, which selects the largest set of arguments that can be defended against all attacks, can be used to identify a core-stable outcome in a voting scenario, where no coalition of voters has a justified complaint against the selected outcome. This connection between AA and ComSoC has led to the development of new algorithms and complexity results that help in understanding the practical applicability of theoretical models.

Moreover, the integration of AA into ComSoC has also facilitated the study of fairness and manipulation in collective decision-making processes. By modeling the arguments and attacks that might arise in a voting scenario, researchers can analyze the robustness of different voting rules against strategic behavior. For instance, the concept of weak priceability, which relaxes the conditions for core stability, can be used to study the stability of voting outcomes under various forms of manipulation. This approach not only provides a deeper understanding of the theoretical properties of voting rules but also offers practical insights into how these rules perform in real-world settings, where voters may have diverse and conflicting interests. The combination of AA and ComSoC thus represents a powerful tool for advancing the field of collective decision-making, offering both theoretical depth and practical relevance.

## 3.3 Algorithmic and Computational Methods

### 3.3.1 Linear Programming and Rounding Schemes
Linear programming (LP) has emerged as a powerful tool in the computational social choice domain, particularly for addressing the metric distortion problem. Unlike previous approaches that focused on the worst-case computational complexity, LP formulations allow for a more nuanced exploration of the trade-offs involved in collective decision-making. By framing the problem in terms of linear constraints and objectives, LPs enable the identification of solutions that optimize certain criteria while respecting the underlying structure of the preferences. For instance, in the context of metric distortion, the LP formulation can be used to find the worst-case metric for a given election mechanism, thereby providing a theoretical upper bound on the distortion [10].

The effectiveness of LP formulations in computational social choice is further enhanced through the use of rounding schemes. These schemes convert the fractional solutions obtained from the LP relaxation into integer solutions that are feasible for the original problem. One common approach is to use randomized rounding, where the fractional values are interpreted as probabilities for selecting the corresponding options. This method leverages the probabilistic nature of the LP solution to produce a near-optimal integer solution with high probability. Another approach involves deterministic rounding, which uses specific rules to round the fractional values to integers. For example, in the context of committee selection, deterministic rounding can be used to ensure that the selected committee respects certain fairness or diversity constraints.

In addition to these basic rounding techniques, more sophisticated methods have been developed to address specific challenges in computational social choice. For instance, the use of knapsack cover inequalities in the LP formulation helps to tighten the relaxation, leading to better approximation guarantees. Furthermore, the combination of LP with other combinatorial techniques, such as the greedy algorithm, has proven effective in achieving low distortion in large-scale settings. The integrality gap of the LP relaxation, which measures the worst-case ratio between the optimal fractional solution and the optimal integer solution, is a critical parameter in assessing the quality of the rounding schemes. By carefully analyzing this gap, researchers can derive tight bounds on the performance of their algorithms, thereby providing a solid theoretical foundation for practical applications in collective decision-making.

### 3.3.2 Game-Theoretic Analysis of Strategic Interactions
Game-theoretic analysis of strategic interactions in voting systems has been a focal point for understanding how agents with conflicting goals can manipulate the outcomes of collective decision-making processes [11]. This analysis often involves the study of strategic behaviors such as manipulation, bribery, and control, where agents may alter the preferences or the structure of the election to achieve their desired outcomes [7]. One of the key models in this context is the Stackelberg game, where a leader (often a socially-minded actor) makes a decision, and a follower (a manipulator) responds strategically. This model is particularly relevant in scenarios where a defender must counteract the actions of an attacker who seeks to manipulate the election results [11]. For instance, in the context of vote recounting, the defender aims to restore the correct outcome after the attacker has modified some votes to favor a preferred candidate. The strategic interaction between these two players can be analyzed using game-theoretic tools to determine the optimal strategies for both parties and the conditions under which the defender can successfully counteract the manipulation.

The literature on game-theoretic analysis of strategic interactions in voting systems has also explored the dynamics of multiple manipulators with conflicting goals [11]. In such settings, the interactions become more complex, as each manipulator must consider the potential actions of the others. This can lead to a rich set of strategic behaviors, including coalitions and counter-manipulations. For example, in multi-party campaigning, several parties may attempt to influence the election outcome by bribing voters or altering the preferences of certain groups. The strategic interactions in such scenarios can be modeled using non-cooperative game theory, where the goal is to find Nash equilibria or other solution concepts that describe the stable outcomes of these interactions. The computational complexity of finding such equilibria is a significant challenge, and recent work has focused on developing algorithms and heuristics to approximate these solutions in practical settings.

Moreover, the game-theoretic analysis of strategic interactions in voting systems has also considered the role of external constraints, such as budget limitations or the presence of multiple rounds of manipulation. For instance, in the context of bribery, the manipulator may have a limited budget to spend on altering votes, and the defender may have a budget to counteract these actions. The strategic interactions in such settings can be modeled using resource allocation games, where the goal is to optimize the allocation of resources to achieve the desired outcome. The analysis of these games often involves the use of Presburger arithmetic to formalize the constraints and the strategic decisions of the agents. This approach allows for the computation of optimal strategies that respect the budget constraints and can be used to predict the outcomes of the strategic interactions in real-world voting scenarios.

### 3.3.3 Probabilistic Models and Misalignment
In the context of probabilistic models and misalignment, a critical aspect is the development of models that can accurately capture the discrepancies between the preferences of agents and the outcomes of collective decision-making processes. These discrepancies, or misalignments, can arise due to various factors, including the inherent randomness in agent behavior, the strategic manipulation of votes, or the structural properties of the voting rules themselves. To address these issues, we propose a probabilistic framework that extends traditional models by incorporating the dynamic and often unpredictable nature of agent interactions.

The proposed model is designed to account for the heterogeneity in agent populations, where different agents may have varying levels of understanding and engagement with the decision-making process. This is particularly important in settings where agents can be both human and artificial, each bringing their own biases and heuristics to the table. By integrating these factors into a probabilistic framework, we can better understand how misalignments emerge and propagate through the system. For instance, in the context of election manipulation, the model can help predict the likelihood of successful manipulation attempts by quantifying the uncertainty in voter preferences and the effectiveness of different manipulation strategies.

Furthermore, the probabilistic model allows for a more nuanced analysis of the trade-offs between fairness and efficiency in collective decision-making. By simulating different scenarios and parameter settings, we can explore how changes in the model parameters affect the degree of misalignment and the overall performance of the decision-making process. This approach not only provides a theoretical foundation for understanding misalignment but also offers practical insights into designing more robust and fair voting systems. The model's flexibility in accommodating various types of agent populations and decision-making contexts makes it a valuable tool for both theoretical research and real-world applications.

# 4 Theoretical and Axiomatic Analysis of Social Choice Mechanisms

## 4.1 Axiomatic Characterization and Properties of Voting Rules

### 4.1.1 Valid Inequalities and Facet Defining Inequalities
Valid inequalities and facet defining inequalities (FDIs) play a crucial role in the formulation and solution of combinatorial optimization problems, particularly in the context of polyhedral theory [12]. These inequalities help in tightening the linear programming relaxation of the problem, thereby improving the efficiency of exact algorithms such as branch-and-cut. In the realm of social choice theory, valid inequalities can be used to model constraints on the preferences and decisions of agents, ensuring that the resulting social choice functions adhere to certain desirable properties.

The derivation of valid inequalities often involves identifying structural properties of the problem that can be expressed as linear constraints. For instance, in the context of committee selection problems, valid inequalities can capture the relationships between the preferences of agents over different committees. These inequalities ensure that the selected committee respects the preferences of the agents in a structured manner. The process of identifying valid inequalities typically involves analyzing the combinatorial structure of the problem, such as the lattice of preferences or the graph of pairwise comparisons.

Facet defining inequalities are a subset of valid inequalities that define the facets of the polytope associated with the problem [12]. A facet is a maximal face of the polytope, and FDIs are essential for understanding the geometric structure of the feasible region [12]. The identification of FDIs is more challenging than that of general valid inequalities, as it requires proving that the inequality cannot be further strengthened without excluding feasible solutions. In the context of social choice, FDIs can provide deep insights into the nature of the constraints that govern the admissible social choice functions. For example, they can reveal the minimal conditions under which a social choice function satisfies certain axioms, such as Pareto efficiency or strategy-proofness.

### 4.1.2 Axiomatic Properties of Scheduling Rules
In the context of scheduling rules, axiomatic properties play a crucial role in ensuring that the rules are fair, efficient, and robust. One of the fundamental axioms is **neutrality**, which requires that the scheduling rule should treat all tasks or agents equally, without favoring any particular task or agent. This axiom ensures that the scheduling rule does not introduce biases that could lead to unfair outcomes. For instance, in a setting where tasks have different processing times, a neutral scheduling rule would not prioritize tasks based on their processing times alone but would consider other factors such as the agents' preferences and the overall efficiency of the schedule.

Another important axiom is **consistency**, which demands that the scheduling rule should produce consistent outcomes when the set of tasks or agents is modified in a specific way. For example, if a task is added or removed, the scheduling rule should not drastically change the schedule for the remaining tasks. This property is particularly relevant in dynamic environments where tasks may be added or removed over time. Consistency ensures that the scheduling rule is stable and predictable, which is essential for maintaining the trust of the agents in the system. Additionally, the **cancellation** axiom, which is closely related to consistency, requires that the scheduling rule should not be influenced by the presence of tasks that do not affect the final schedule. This ensures that the rule is efficient and does not waste computational resources on irrelevant tasks.

Finally, the **PTA Condorcet consistency** property, which stands for "Processing Time Aware Condorcet consistency," is a specialized axiom that is particularly relevant in scheduling problems where tasks have different processing times. This axiom requires that if a task is preferred over another by a majority of agents, and this preference is consistent with the processing times, then the preferred task should be scheduled before the less preferred one. This property ensures that the scheduling rule respects the agents' preferences while also taking into account the practical constraints of the tasks. However, as shown in the literature, achieving PTA Condorcet consistency can be challenging, and some scheduling rules may fail to satisfy this property, leading to schedules that are far from optimal in terms of both fairness and efficiency.

### 4.1.3 Impossibility Results and Strategy-Proofness
Impossibility results in social choice theory, particularly those stemming from Arrow's Impossibility Theorem, highlight fundamental limitations in designing social welfare functions that satisfy a set of desirable properties simultaneously [13]. Arrow's theorem states that no social welfare function can satisfy Pareto efficiency, independence of irrelevant alternatives, and non-dictatorship when the number of alternatives exceeds two [4]. These results underscore the inherent trade-offs in aggregating individual preferences into a collective decision. Subsequent research has explored various relaxations and extensions of these conditions to identify more permissive settings where desirable social choice functions can exist.

Strategy-proofness, a critical property in social choice mechanisms, ensures that individuals cannot benefit by misrepresenting their preferences [14]. This concept is particularly important in settings where strategic behavior can lead to suboptimal outcomes. While the Gibbard-Satterthwaite theorem extends Arrow's result to show that no non-dictatorial social choice function can be strategy-proof when preferences are unrestricted, recent work has focused on identifying restricted domains where strategy-proof mechanisms can be designed [15]. For instance, single-peaked preferences and other structured domains allow for the design of strategy-proof mechanisms that satisfy additional fairness and efficiency criteria [16]. These findings highlight the importance of domain restrictions in overcoming the limitations posed by classical impossibility results.

Moreover, the computational complexity of verifying and implementing strategy-proof mechanisms has become a focal point in algorithmic social choice [1]. Recent advances have shown that while finding a most equitable tie-breaking mechanism can be computationally challenging, it is feasible in quasipolynomial time for a broad class of preferences. This progress not only enhances the practical applicability of strategy-proof mechanisms but also deepens our understanding of the theoretical boundaries within which such mechanisms can operate. The interplay between computational feasibility and strategic robustness remains a rich area for future research, with potential applications in various domains, including resource allocation, voting systems, and market design.

## 4.2 Theoretical Models and Frameworks

### 4.2.1 Price-Based Iterative Double Auctions
Price-based iterative double auctions (PIDA) are a class of market mechanisms designed to facilitate efficient and fair allocation of resources in two-sided markets. In these auctions, both buyers and sellers submit bids and asks, respectively, which are iteratively adjusted based on the market clearing price. The primary goal of PIDA is to converge to a market equilibrium where supply meets demand, ensuring that the allocation is both Pareto efficient and strategy-proof. The iterative nature of these auctions allows for dynamic adjustments, making them particularly suitable for environments where preferences and availability can change over time, such as in charger sharing scheduling (CSS) problems.

In the context of CSS, charger owners (sellers) submit asks indicating their available charging times, locations, and time unit costs, while electric vehicle (EV) drivers (buyers) place bids expressing their charging time requirements and the prices they are willing to pay. The auction mechanism then iteratively updates the market clearing price until a stable allocation is reached. This process not only ensures that the charging needs of EV drivers are met but also maximizes the utility for charger owners by optimizing the allocation of charging slots. The computational efficiency of PIDA is further enhanced by leveraging specific bidding languages that naturally model scheduling constraints, thereby reducing the computational burden associated with bid evaluation and winner determination.

The design of PIDA mechanisms is grounded in theoretical foundations that ensure desirable properties such as strategy-proofness and Pareto efficiency. Strategy-proofness is achieved by designing the auction rules such that truthful bidding is the dominant strategy for both buyers and sellers, thereby preventing strategic manipulation. Pareto efficiency is guaranteed by the market clearing process, which ensures that resources are allocated in a way that no participant can be made better off without making another participant worse off. These properties make PIDA a robust and practical solution for resource allocation problems in dynamic and complex market environments.

### 4.2.2 Network Solutions and Graph Theory
Network solutions and graph theory provide a robust framework for addressing complex problems in computational social choice and beyond [17]. In the context of graph isomorphism (GI) and canonical labeling (CL), these problems are deeply intertwined with the structure and properties of graphs. The GI problem, which asks whether two given undirected unweighted graphs are isomorphic, is a cornerstone of computational complexity theory. Despite extensive research, the exact complexity of GI remains unresolved, as it is neither known to be NP-complete nor to have a polynomial-time algorithm. This ambiguity places GI in a unique position within the hierarchy of computational problems, making it a subject of ongoing interest and investigation.

Graph canonical labeling (CL) is closely related to GI, as it involves finding a unique representative (canonical form) for each isomorphism class of graphs. A canonical form serves as a standardized representation that can be used to efficiently compare graphs for isomorphism. Various algorithms have been developed to compute canonical forms, such as the popular Nauty algorithm, which employs a combination of partition refinement and individualization techniques. These methods leverage the structural properties of graphs to systematically reduce the search space, thereby improving the efficiency of the canonical labeling process. The interplay between GI and CL highlights the importance of graph theory in understanding and solving computational problems in network analysis.

In the realm of network solutions, graph theory provides a powerful toolset for designing and analyzing algorithms that operate on networks. Network solutions, such as those used in social choice and resource allocation, often involve assigning scores to nodes or edges based on specific criteria, and then selecting optimal configurations according to these scores. For instance, in the context of voting systems, network solutions can be used to determine the most preferred candidate or policy by aggregating individual preferences into a collective decision [2]. Graph-theoretic concepts, such as centrality measures, clustering coefficients, and community detection, are crucial for understanding the behavior of these systems and optimizing their performance. By integrating graph theory with network solutions, researchers can develop more sophisticated and effective algorithms for a wide range of applications, from social networks to transportation systems.

### 4.2.3 Intensinist Social Welfare Functions
Intensinist social welfare functions (ISWFs) represent a significant advancement in the field of social choice theory by incorporating the ordinal intensity of preferences alongside traditional ordinal rankings. Unlike conventional social welfare functions that rely solely on the ranking of alternatives, ISWFs take into account the varying degrees of preference intensity expressed by individuals. This extension allows for a more nuanced representation of individual preferences, potentially leading to more equitable and representative social decisions [10]. The scoring procedure used in ISWFs assigns a score to each alternative based on the cumulative intensity ranks provided by all agents. An alternative is deemed socially preferred if its total intensity rank exceeds that of another alternative.

The introduction of ISWFs addresses a critical gap in the traditional framework, where the intensity of preferences is often overlooked, leading to potential inefficiencies and inequities in social choice outcomes. By integrating intensity data, ISWFs can better reflect the true preferences of the population, thereby enhancing the fairness and acceptability of collective decisions. For instance, in scenarios involving the allocation of indivisible goods, where Pareto efficiency is paramount, ISWFs can help identify allocations that not only maximize overall welfare but also distribute resources in a manner that respects the varying intensities of individual needs and desires. This approach is particularly valuable in contexts where the stakes are high, and the distribution of resources has significant implications for individual well-being.

Moreover, the application of ISWFs extends beyond theoretical interest to practical implementation in various domains, including public policy, resource allocation, and collective decision-making processes. The ability to incorporate intensity data into the decision-making process can lead to more robust and resilient social choices, as it accounts for the depth of individual preferences. However, the implementation of ISWFs also presents challenges, such as the need for accurate and reliable methods to elicit and measure preference intensities, and the computational complexity associated with processing and aggregating this additional information. Despite these challenges, the potential benefits of ISWFs in enhancing the quality and equity of social decisions make them a promising area of research and development in social choice theory.

## 4.3 Computational and Empirical Analysis

### 4.3.1 Computational Complexity and Graph Theory
The graph isomorphism (GI) problem, which asks whether two given undirected unweighted graphs are isomorphic, is a fundamental problem in computational complexity and graph theory. Despite its apparent simplicity, the complexity of the GI problem remains an open question in theoretical computer science. It is one of the few natural problems in the complexity class NP that has not been classified as either NP-complete or solvable in polynomial time. This ambiguity has led to extensive research, with significant efforts aimed at understanding its computational complexity and its relationship to other problems in computational graph theory.

The study of graph isomorphism is deeply intertwined with the graph canonical labeling (CL) problem, which involves computing a canonical form of a given graph. A canonical form is a unique representation of a graph that can serve as a "representative" for all graphs isomorphic to it. The CL problem is at least as hard as the GI problem, as a polynomial-time algorithm for CL would immediately yield a polynomial-time algorithm for GI. Conversely, any improvement in algorithms for the GI problem can potentially lead to advancements in solving the CL problem, highlighting the strong connection between these two problems. This interplay has driven the development of various algorithms and heuristics, such as the Weisfeiler-Leman algorithm, which have been instrumental in advancing our understanding of the computational aspects of graph isomorphism.

Recent research has also explored the computational complexity of related problems, such as computing the maximum eigenvalue rank (MERV) and assessing the possibility of achieving a certain ranking (ANR-possibility). These problems are of interest to the theory community because they offer natural extensions to the study of graph isomorphism and canonical labeling. Efficient algorithms for these problems could provide new insights into the structure of graphs and the complexity of graph-related computations. Moreover, the connections between these problems and the GI and CL problems suggest that advances in one area can have significant implications for the others, making the study of computational complexity and graph theory a rich and dynamic field of research.

### 4.3.2 Theoretical Analysis and Computer-Aided Proofs
Theoretical analysis and computer-aided proofs have played a pivotal role in advancing the understanding of social choice functions, particularly in the context of majority decision rules and their implications on social preferences [6]. One of the key contributions in this area is the reformulation of Sen’s value restriction (VR) and the not-strict value restriction (NSVR) in terms of the PPM [13]. This reformulation not only provides a quantitative description of these conditions but also facilitates a deeper analysis of their implications on the transitivity of social preferences. By leveraging mathematical tools such as linear algebra and graph theory, researchers have been able to generalize existing results and develop more robust characterizations of social choice functions.

In the realm of computational social choice, the development of efficient algorithms for verifying and computing most equitable rules (MERVs) has been a significant focus [18]. The complexity of these problems often depends on the number of alternatives \( m \) and the specific social choice settings. For instance, when \( m \) is bounded, MERV can be computed in polynomial time. However, for variable \( m \), the problem becomes more challenging, and recent work has explored the computational hardness of these tasks. Computer-aided proofs have been instrumental in establishing the NP-hardness of computing MERV in certain settings, thereby highlighting the need for heuristic and approximation algorithms. These algorithms not only provide practical solutions but also contribute to the theoretical understanding of the problem by revealing the underlying structure and properties of MERV.

Moreover, the integration of computer-aided proofs with theoretical analysis has enabled researchers to explore the connections between different social choice functions and their properties. For example, the study of intensity-efficient allocations has benefited from the use of computational tools to verify the Pareto efficiency and intensity conditions. Similarly, the analysis of axiomatic properties such as neutrality, consistency, and cancellation has been enhanced by the application of linear algebra and network theory. These theoretical and computational advancements collectively contribute to a more comprehensive understanding of social choice functions and their implications for decision-making processes in various social and economic contexts [6].

### 4.3.3 Participatory Budgeting Models and Frameworks
Participatory budgeting (PB) is a democratic process that allows citizens to directly decide on the allocation of a portion of the public budget [19]. This process typically involves several stages, including idea generation, proposal development, and voting, which collectively ensure that the community's preferences are effectively represented. The primary goal of PB is to enhance transparency, inclusivity, and accountability in public spending decisions. In many municipalities, a single PB cycle can span an entire year, reflecting the complexity and deliberative nature of the process. Cities such as Paris and Madrid have implemented large-scale PB initiatives, allocating significant portions of their budgets—often exceeding $100 million annually—to projects chosen by residents.

The design of PB frameworks has evolved to accommodate a wide range of preferences and decision-making scenarios. Traditional models often rely on simple majority voting or approval voting, where residents vote for their preferred projects, and those with the highest votes are funded. However, these methods can lead to inefficiencies and inequities, particularly when the budget is limited and the number of projects is large. To address these issues, more sophisticated models have been developed, such as the use of scoring rules, where projects are assigned scores based on various criteria, and the budget is allocated to maximize the total score. Additionally, recent advancements in computational social choice have introduced algorithms that optimize the selection of projects to better align with the community's preferences, ensuring a more equitable distribution of resources.

Another critical aspect of PB frameworks is the incorporation of fairness and participation principles. These models often include mechanisms to ensure that marginalized groups have a voice in the decision-making process and that the outcomes reflect the diverse needs of the community. For instance, some PB processes use weighted voting systems, where the influence of each vote is adjusted based on demographic factors or historical underrepresentation. Furthermore, the development of online platforms, such as Decide Madrid, has facilitated greater participation by allowing residents to engage in the PB process remotely. These technological advancements have not only increased accessibility but also enhanced the transparency and efficiency of the PB process, making it a powerful tool for democratic governance.

# 5 Algorithmic and Complexity Analysis of Voting Problems

## 5.1 Fine-Grained Complexity and Algorithm Design

### 5.1.1 Tight Reductions and Equivalences
Tight reductions and equivalences play a crucial role in understanding the computational landscape of various voting and optimization problems, particularly in the context of Kemeny voting schemes and the Positional Winner (PW) problem [20]. These reductions not only help in establishing the computational complexity of problems but also in uncovering deep structural relationships between them. In the realm of Kemeny voting, tight reductions have been instrumental in demonstrating that many classical space reduction techniques, such as the Major Order Theorems and the Condorcet criterion, do not directly translate to the 3-wise Kemeny setting [20]. This highlights the unique challenges posed by higher-order Kemeny voting schemes and underscores the need for novel reduction techniques tailored to these settings [20].

Moreover, the study of tight reductions has led to significant advancements in the understanding of the PW problem on various types of partial orders. For instance, reductions from the 3-DIMENSIONAL MATCHING problem have been used to classify the complexity of the PW problem on arbitrary partial orders, providing a unified framework that simplifies previous proofs [21]. These reductions are particularly powerful because they demonstrate that the complexity of the PW problem is preserved across different classes of partial orders, including doubly-truncated and partitioned partial orders [21]. This equivalence not only enriches the theoretical foundations of the field but also has practical implications for algorithm design and implementation.

In the context of quantum computing, tight reductions are equally important, especially given the current limitations in qubit availability. By implementing data reduction rules as a preprocessing step, large instances of problems like the Quadratic Unconstrained Binary Optimization (QUBO) and the Knapsack Resource Allocation (KRA) problem can be broken down into smaller, more manageable subproblems. This approach not only enhances the feasibility of solving these problems on current quantum hardware but also provides a systematic way to aggregate solutions from multiple subproblems. The effectiveness of these reductions is further validated through empirical studies, which compare the performance of different preprocessing modes and highlight the benefits of using tight reductions in terms of both runtime and solution quality.

### 5.1.2 Faster Combinatorial Algorithms
Faster combinatorial algorithms have been a focal point in the optimization of various computational problems, particularly in the realm of preference aggregation and rank aggregation. These algorithms aim to reduce the computational complexity of determining a consensus ranking from a set of individual rankings, which is a common task in applications such as voting systems, recommendation systems, and data fusion. The development of faster combinatorial algorithms is crucial for handling large-scale datasets and real-time applications where efficiency is paramount.

One of the key advancements in this area involves the use of efficient space reduction techniques and parallel processing. For instance, the Iterated 3-wise Major Order Theorems (3MOT) have shown significant improvements in solving the relative order of pairs in political elections and competitions, where the similarity between votes is high. These algorithms can achieve a proportion of correctly solved pairs ranging from 0.6 to 0.9 after only a few iterations, making them highly effective for practical applications. Moreover, the algorithms perform well even on uniformly generated instances, demonstrating their robustness and versatility.

Another important development is the reduction in complexity for the Positional Weight (PW) problem on doubly-truncated partial orders and partitioned partial orders [21]. This reduction not only speeds up the computation but also provides a new, self-contained proof of the classification of the complexity of PW on arbitrary partial orders [21]. By leveraging reductions from a single NP-complete problem, such as the 3-DIMENSIONAL MATCHING problem, these algorithms offer a more streamlined approach to solving complex ranking problems. Additionally, the use of quadratic unconstrained binary optimization (QUBO) formulations has shown promise in further accelerating the computation, especially in the context of quantum computing and specialized hardware [22].

### 5.1.3 Streaming Algorithms and Coresets
Streaming algorithms and coresets play a pivotal role in handling large-scale data efficiently, particularly in scenarios where the data volume exceeds the memory capacity. In the context of set-wise Kemeny voting schemes, these techniques are crucial for reducing the computational complexity and space requirements [20]. A core concept in this domain is the use of coresets, which are small, weighted subsets of the input data that approximate the behavior of the entire dataset with respect to a specific optimization problem. By constructing coresets, it becomes feasible to apply computationally intensive algorithms to large datasets in a streaming environment, thereby significantly reducing both the storage and processing demands.

One notable approach involves the development of streaming algorithms that can process permutations in real-time while maintaining a compact summary of the data. These algorithms often leverage sampling techniques to select representative elements from the stream, ensuring that the summary remains accurate and useful for subsequent analysis. For instance, a (2 − δ)-approximation algorithm for the k-median problem has been designed, which stores only O(k^2 log^4 n log d) permutations [23]. This algorithm not only achieves a near-optimal approximation but also operates within the constraints of limited memory, making it suitable for applications with stringent resource limitations.

Moreover, the integration of these streaming algorithms with other space reduction techniques, such as the ≤3/4-majority rule and the extended Condorcet rule, can further enhance the efficiency and effectiveness of the overall system. These complementary methods can pre-process the data to eliminate redundant or irrelevant information, thereby reducing the input size before applying the streaming algorithm. This synergistic approach not only accelerates the computation but also improves the robustness of the solution, making it more resilient to noise and outliers in the data. The combination of these techniques represents a promising direction for future research, aiming to develop more scalable and efficient algorithms for large-scale preference aggregation tasks.

## 5.2 Computational Methods and Heuristics

### 5.2.1 Quantum Heuristics for KRA
In the pursuit of efficient heuristics for the Kemeny Rank Aggregation (KRA) problem, quantum computing offers a promising avenue [22]. The KRA problem, which involves finding a consensus ranking that minimizes the sum of Kendall-tau distances to a set of input rankings, is known to be NP-hard [22]. To tackle this challenge, we leverage the Adiabatic Quantum Model, which is particularly well-suited for solving quadratic unconstrained binary optimization (QUBO) problems [22]. By formulating the KRA problem as a QUBO, we can map it onto a quantum annealer, which is designed to find the global minimum of the energy landscape corresponding to the QUBO problem.

The QUBO formulation of the KRA problem involves encoding the rankings as binary variables and constructing a quadratic objective function that reflects the Kendall-tau distance. This transformation allows us to utilize the quantum annealer's ability to explore the solution space more efficiently than classical algorithms. We compare the performance of our quantum-based heuristics with classical heuristics, such as simulated annealing and genetic algorithms, which are commonly used to sample different solutions in the KRA problem. Our experiments focus on the runtime and solution quality, as well as the diversity of solutions generated by each method.

Our results indicate that the quantum heuristics provide a significant speedup in finding high-quality solutions, especially for larger instances of the KRA problem. This is attributed to the quantum annealer's capability to escape local minima and explore a wider range of the solution space. Moreover, the diversity of solutions obtained from the quantum heuristics is higher compared to classical methods, which is crucial for applications requiring multiple diverse rankings. These findings suggest that quantum computing can be a valuable tool in the development of more refined and efficient algorithms for KRA, paving the way for future research in this domain.

### 5.2.2 Data Reduction and Preprocessing
Data reduction and preprocessing are crucial steps in the pipeline for solving the Kemeny consensus problem, especially when dealing with large datasets or complex partial orders. These techniques aim to reduce the computational complexity by simplifying the input while preserving the essential features necessary for the solution. One of the primary methods involves the application of data reduction rules, which are designed to identify and eliminate redundant or trivial constraints. For instance, the ≤3/4-majority rule and the extended Condorcet rule are two such rules that have been effectively utilized. The ≤3/4-majority rule allows for the removal of candidates that are ranked below the 3/4 threshold in a majority of the votes, significantly reducing the search space. Similarly, the extended Condorcet rule can identify and eliminate candidates that are dominated by others in a pairwise comparison, further refining the input.

The effectiveness of these data reduction rules is particularly evident when applied to doubly-truncated and partitioned partial orders. Doubly-truncated partial orders, where the top and bottom elements are fixed, often exhibit a drop in complexity for the Kemeny voting scheme [21]. Partitioned partial orders, characterized by disjoint sets with linear orders between them, are common in real-life datasets and have been shown to benefit from these reduction techniques [21]. By breaking down the problem into smaller, more manageable subproblems, these rules enable more efficient computation and can be particularly useful in scenarios where the original problem size is prohibitive for direct computation. The application of these rules not only accelerates the solution process but also enhances the scalability of the algorithms, making them more practical for real-world applications.

In our experimental evaluation, we compared three preprocessing modes: no preprocessing, applying the ≤3/4-majority rule exhaustively, and applying the extended Condorcet rule [22]. The results demonstrated that both the ≤3/4-majority rule and the extended Condorcet rule significantly reduced the computational time and improved the solution quality, especially for large and complex datasets [22]. The extended Condorcet rule, in particular, showed a notable advantage in reducing the number of candidate solutions, leading to faster convergence and more accurate results. These findings underscore the importance of data reduction and preprocessing in enhancing the performance of Kemeny consensus algorithms and highlight the potential for further research in developing more sophisticated and efficient reduction techniques.

### 5.2.3 3-Wise Major Order Theorems
The 3-Wise Major Order Theorems (3MOT) represent a significant advancement in the field of social choice theory, particularly in the context of Kemeny voting schemes [20]. These theorems extend the classical 2-wise Major Order Theorem by considering the preferences among three candidates simultaneously, thereby providing a more nuanced understanding of the collective preferences. The 3MOT quantifies the preference for one candidate over another not only in direct pairwise comparisons but also in the context of additional alternatives. This extension allows for a more refined space reduction method, which is crucial for efficiently solving the Kemeny consensus problem.

At the core of the 3MOT is the identification of constraints on 3-wise medians, which are essential for determining the optimal ranking of candidates. The Iterated 3MOT further refines these constraints by iteratively applying the 3MOT, leading to a substantial reduction in the search space. This iterative approach not only accelerates the computation but also ensures that the solution remains consistent with the majoritarian principles underlying the Kemeny rule. By leveraging these theorems, the computational complexity of finding the Kemeny consensus can be significantly reduced, making it feasible to handle larger datasets and more complex voting scenarios [24].

Moreover, the 3MOT and its iterated version provide a unified framework that can be applied to various k-wise Kemeny voting schemes, where \( k \geq 2 \) [20]. This framework not only extends the applicability of the 2-wise Major Order Theorem but also opens new avenues for developing more sophisticated algorithms and analyzing the quantitative properties of these voting schemes [20]. The ability to handle more complex preference structures through 3-wise and higher-order theorems is particularly valuable in real-world applications where voter preferences are often multifaceted and intricate.

## 5.3 Complexity Analysis and Reductions

### 5.3.1 NP-Complete Problems and Reductions
NP-Complete Problems and Reductions form a cornerstone in the study of computational complexity, particularly in the context of Kemeny voting schemes and related problems. The Kemeny consensus problem, which involves finding a ranking that minimizes the total number of pairwise disagreements with a set of input rankings, is a prime example of an NP-hard problem [24]. This hardness persists even when the number of voters is fixed, as shown by various reductions from known NP-complete problems such as 3-DIMENSIONAL MATCHING [25]. These reductions not only establish the NP-hardness of the Kemeny consensus problem but also provide a framework for understanding the complexity of related voting and ranking problems [24].

In the context of 3-wise Kemeny voting schemes, the hardness results are further refined through reductions that exploit the structure of the 3-wise Kendall-tau distance [20]. These reductions demonstrate that the problem remains NP-hard even under specific constraints, such as when the input rankings are partial orders or when the number of candidates is limited. The use of reductions from 3-DIMENSIONAL MATCHING to the Kemeny consensus problem on partial chains, for instance, not only confirms the NP-completeness of the problem but also provides a new, self-contained proof that is more streamlined and accessible compared to previous approaches. This approach highlights the versatility of reductions in establishing complexity results and in providing a deeper understanding of the problem's structure.

Moreover, the study of NP-complete problems and reductions in Kemeny voting schemes has led to the development of efficient data reduction techniques and heuristics [20]. These techniques, such as the use of space reduction methods and the Iterated 3MOT algorithm, aim to preprocess the input to reduce the problem size, making it more tractable for exact or approximate solutions. The effectiveness of these methods is often evaluated through experimental studies, where they are compared against other solvers in terms of runtime and solution quality. The interplay between theoretical hardness results and practical algorithmic techniques underscores the importance of reductions in both theoretical and applied aspects of computational complexity.

### 5.3.2 Polynomial-Time Algorithms and Hardness Results
In the realm of computational social choice, particularly within the context of Kemeny voting schemes, the development of polynomial-time algorithms and the identification of hardness results play a pivotal role in understanding the tractability of various voting-related problems [3]. For instance, the Kemeny Rank Aggregation (KRA) problem, which seeks to find a consensus ranking that minimizes the sum of Kendall-tau distances to a set of input rankings, is known to be NP-hard [22]. Despite this hardness, significant efforts have been devoted to devising efficient algorithms for special cases and approximations. One notable approach involves leveraging the structure of the input rankings, such as when the rankings are doubly-truncated or partitioned, to achieve polynomial-time solutions.

The complexity landscape of the KRA problem is further enriched by the study of its variants and related problems [22]. For example, the problem of determining the winner under the Plurality and Veto rules remains NP-complete, even when restricted to partial chains, highlighting the inherent difficulty in these settings. However, the introduction of specific constraints or the use of advanced data reduction techniques can sometimes lead to a drop in complexity, making certain instances of the problem tractable. This interplay between hardness and tractability is crucial for the practical application of voting schemes, as it guides the design of heuristic and approximation algorithms that can handle real-world datasets efficiently.

Moreover, the exploration of polynomial-time algorithms and hardness results extends beyond the KRA problem to encompass a broader class of preference aggregation tasks. For instance, the verification of weak or strong popularity of a given ranking among a set of voters' preferences is another area where the distinction between polynomial-time solvability and NP-hardness is critical [24]. Recent advances have shown that the complexity of these problems can vary significantly depending on the number of voters involved, with polynomial-time solutions being possible for small numbers of voters but NP-hardness emerging as the number increases [25]. These findings not only contribute to the theoretical foundations of computational social choice but also inform the development of practical tools for decision-making processes in multi-agent systems [1].

### 5.3.3 Parameterized Complexity and Safe Bribery
Parameterized complexity offers a nuanced approach to understanding the computational challenges inherent in safe bribery problems, particularly within the context of voting systems [7]. By focusing on specific parameters, such as the number of candidates or the number of votes, researchers can identify tractable instances of otherwise intractable problems. For example, the problem of determining whether a given candidate can be made a winner through bribery (Safe Bribery) is generally NP-hard. However, when the number of candidates is fixed, the problem becomes polynomial-time solvable for a wide range of voting rules, including those that are anonymous and efficient. This parameterization not only simplifies the problem but also provides practical insights into the feasibility of strategic manipulations in real-world voting scenarios.

In the realm of shift bribery, where the focus is on moving a preferred candidate up in the rankings through minimal changes, the parameterized complexity analysis reveals a similar dichotomy [2]. While the general problem remains computationally challenging, parameterizing by the number of shifts or the number of voters can lead to fixed-parameter tractability (FPT) results. For instance, when the number of shifts required to achieve the desired outcome is bounded, the problem can often be solved efficiently. This is particularly relevant in settings where the manipulator has limited resources or influence. The parameterized approach thus provides a framework for understanding the trade-offs between the complexity of the problem and the extent of manipulation required.

Moreover, the concept of "safe" bribery introduces an additional layer of complexity by requiring that the manipulation not only ensures the preferred candidate's victory but also guarantees that the manipulation cannot be detected or challenged [2]. The parameterized complexity analysis of safe bribery and safe shift bribery reveals that while these problems are generally harder than their non-safe counterparts, they too exhibit FPT behavior under certain parameterizations. For example, when the number of candidates is constant, both Safe Bribery and Safe Shift Bribery can be solved in polynomial time [2]. This highlights the importance of parameter selection in designing algorithms that can handle strategic behaviors in voting systems, ensuring both efficiency and robustness against detection.

# 6 Future Directions


The field of computational social choice (ComSoC) has made significant strides in understanding the theoretical and practical aspects of preference aggregation and collective decision-making. However, several limitations and gaps remain that hinder the full realization of its potential. One of the primary limitations is the computational complexity of many voting and ranking problems, particularly in large-scale and real-time scenarios. Despite advancements in approximation algorithms and heuristics, the exact solution of many problems remains intractable, especially when the number of alternatives or voters is large. Additionally, the current models often assume a static environment, which does not reflect the dynamic and evolving nature of real-world decision-making processes. The lack of robust methods for handling noisy or incomplete data is another significant gap, as real-world preferences are often subject to uncertainty and change.

To address these limitations, several directions for future research are proposed. First, there is a need to develop more efficient and scalable algorithms for solving computationally hard problems in ComSoC. This includes exploring the use of advanced optimization techniques, such as quantum computing and parallel processing, to handle large-scale datasets. Additionally, the development of adaptive algorithms that can dynamically adjust to changes in the input data and the decision-making environment is crucial. These algorithms should be capable of incorporating real-time data and updating the decision-making process accordingly, thereby enhancing the responsiveness and adaptability of voting systems.

Second, the integration of machine learning and artificial intelligence (AI) techniques into ComSoC can significantly enhance the handling of noisy and incomplete data. Machine learning models, such as deep neural networks and probabilistic graphical models, can be used to predict and impute missing preferences, thereby improving the accuracy and robustness of the decision-making process. Furthermore, the use of reinforcement learning can enable the development of systems that learn from past decisions and improve over time, leading to more informed and efficient collective decisions. The combination of AI with traditional ComSoC methods can also facilitate the design of more user-friendly and accessible voting systems, particularly for non-expert users.

Third, the theoretical foundations of ComSoC can be further strengthened by exploring new axiomatic properties and impossibility results. The identification of novel axioms that capture the ethical and fairness considerations in collective decision-making is essential for designing more equitable and transparent mechanisms. Additionally, the study of domain restrictions and structured preferences can lead to the development of voting rules that are both computationally feasible and strategy-proof. The exploration of these theoretical aspects can also inform the practical design of voting systems, ensuring that they align with the values and preferences of the community.

The potential impact of the proposed future work is substantial. By addressing the computational challenges and enhancing the robustness and adaptability of voting systems, we can significantly improve the efficiency and fairness of collective decision-making processes. This can have far-reaching implications in various domains, including political elections, resource allocation, and participatory budgeting. The integration of AI and machine learning can make these systems more accessible and user-friendly, thereby increasing participation and trust in the decision-making process. Furthermore, the development of more equitable and transparent mechanisms can foster greater social cohesion and reduce conflicts, ultimately contributing to more just and inclusive societies.

# 7 Conclusion



This survey has provided a comprehensive overview of the current state of research in computational social choice (ComSoC) with a focus on preference restrictions. Key findings include the critical role of preference restrictions in optimizing the performance and fairness of social choice mechanisms, the use of integer linear programming (ILP) and probabilistic analysis in solving complex election-related problems, and the significance of semi-random models in understanding the computational complexity of voting systems. The survey also highlighted the importance of empirical studies using synthetic data and real-world datasets, the development of election indices and visualization tools, and the integration of abstract argumentation (AA) in modeling and analyzing the acceptability of voting outcomes. Additionally, the survey explored algorithmic and computational methods, including linear programming, game-theoretic analysis of strategic interactions, and probabilistic models of misalignment. Theoretical and axiomatic analysis covered the characterization of voting rules, the study of impossibility results, and the application of network solutions and graph theory. The survey also discussed the computational and empirical analysis of voting problems, including fine-grained complexity, faster combinatorial algorithms, and quantum heuristics.

The significance of this survey lies in its contribution to the interdisciplinary field of ComSoC by synthesizing key theoretical results, empirical findings, and computational methods. By providing a unified perspective on the challenges and opportunities in this field, the survey aims to guide future research and foster collaboration across disciplines. The insights gained from this survey are crucial for the development of more effective and fair collective decision-making processes, particularly in the context of voting systems, resource allocation, and participatory budgeting. The survey underscores the importance of considering preference restrictions and the computational aspects of social choice mechanisms to ensure that these processes are robust, efficient, and aligned with the collective preferences of the electorate.

In conclusion, this survey calls for continued research and innovation in the field of computational social choice. Future directions should focus on addressing open problems, such as the development of more sophisticated preference restriction models, the design of algorithms that can handle large-scale and dynamic datasets, and the exploration of new theoretical frameworks that can capture the complexities of real-world decision-making processes. Additionally, there is a need for interdisciplinary collaboration to integrate insights from computer science, economics, and political science, ensuring that the theoretical advancements are translated into practical applications. By advancing the field of ComSoC, we can contribute to the creation of more inclusive, transparent, and equitable collective decision-making processes, ultimately benefiting society as a whole.

# References
[1] Guide to Numerical Experiments on Elections in Computational Social  Choice  
[2] How Hard is Safe Bribery   
[3] Collecting, Classifying, Analyzing, and Using Real-World Elections  
[4] Describing Sen's Transitivity Condition in Inequalities and Equations  
[5] The Distortion of Binomial Voting Defies Expectation  
[6] Preference Restrictions in Computational Social Choice  A Survey  
[7] Multi-Party Campaigning  
[8] Diversity, Agreement, and Polarization in Elections  
[9] Implementing Ranking-Based Semantics in ConArg  a Preliminary Report  
[10] Metric Distortion Bounds for Randomized Social Choice  
[11] Protecting Elections by Recounting Ballots  
[12] Derivations of large classes of facet-defining inequalities of the weak  order polytope using rankin  
[13] Reformulating the Value Restriction and the Not-Strict Value Restriction  in Terms of Possibility Pr  
[14] On the Indecisiveness of Kelly-Strategyproof Social Choice Functions  
[15] Local Diversity of Condorcet Domains  
[16] Probabilistic Fixed Ballot Rules and Hybrid Domains  
[17] Collective schedules  axioms and algorithms  
[18] Computing Most Equitable Voting Rules  
[19] Participatory Budgeting  Models and Approaches  
[20] Space reduction techniques for the $3$-wise Kemeny problem  
[21] The Complexity of Possible Winners on Partial Chains  
[22] Heuristic for Diverse Kemeny Rank Aggregation based on Quantum Annealing  
[23] Clustering Permutations  New Techniques with Streaming Applications  
[24] On weakly and strongly popular rankings  
[25] Auditing for Core Stability in Participatory Budgeting  