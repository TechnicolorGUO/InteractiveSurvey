# A Survey of Causal Modeling for Longitudinal and Panel Data

# 1 Abstract


Causal inference is a fundamental area of research that seeks to understand cause-and-effect relationships in various fields, including economics, epidemiology, and social sciences. This survey paper provides a comprehensive overview of the latest developments in causal modeling for longitudinal and panel data, focusing on advanced estimation techniques, quasi-experimental designs, and methods for handling spillover effects and spatial data. The main findings highlight the integration of doubly robust estimators, which combine the strengths of outcome regression and propensity score weighting to address potential biases and improve the reliability of causal estimates. The paper also explores the application of panel clustering and de-biased estimation techniques to identify subgroups with similar treatment effects and handle non-random missingness. Additionally, it discusses the integration of quasi-experimental designs, such as difference-in-differences and synthetic control methods, with machine learning techniques to enhance the robustness of causal inferences in complex settings. Finally, the survey addresses the challenges of spatiotemporal data and hidden Markov models, emphasizing the importance of modeling arbitrary spillover and carryover effects. This survey aims to guide researchers and practitioners in selecting the most appropriate techniques for their specific applications, ultimately contributing to more reliable and robust causal inferences in complex data environments.

# 2 Introduction
Causal inference is a fundamental area of research that seeks to understand the cause-and-effect relationships between variables, playing a crucial role in fields ranging from economics and epidemiology to social sciences and policy evaluation. The ability to accurately estimate causal effects is essential for making informed decisions and designing effective interventions. However, the complexity of real-world data, characterized by non-random missingness, time-varying confounders, and intricate spatiotemporal dependencies, poses significant challenges to traditional causal inference methods. In recent years, the advent of advanced statistical and machine learning techniques has opened new avenues for addressing these challenges, particularly in the context of longitudinal and panel data. This survey paper aims to provide a comprehensive overview of the latest developments in causal modeling for longitudinal and panel data, focusing on advanced estimation techniques, quasi-experimental designs, and extensions for handling spillover effects and spatial data [1].

The primary focus of this survey is on the integration of advanced statistical methods and machine learning algorithms to enhance the robustness and accuracy of causal inferences in longitudinal and panel data settings [2]. We delve into the use of doubly robust estimators, which combine the strengths of outcome regression and propensity score weighting to address potential biases and improve the reliability of causal estimates. These methods are particularly valuable in settings where model misspecification is a concern, as they remain consistent if either the outcome model or the propensity score model is correctly specified. We also explore the application of these techniques to handle non-random missingness and time-varying covariates, which are common in panel data.

Additionally, we examine the role of panel clustering and de-biased estimation in identifying subgroups within observational data that exhibit similar treatment effects [3]. Clustering observations using regression trees and low-rank matrix completion for missing data are discussed as powerful tools for segmenting data and recovering low-rank structures, respectively. These methods not only enhance the accuracy of causal estimates but also provide valuable insights into the heterogeneity of treatment effects across different segments of the population [4].

The survey also covers the integration of quasi-experimental designs, such as difference-in-differences (DiD) and synthetic control methods, with machine learning techniques. We discuss the use of double/debiased machine learning (DML) for fixed effects and semiparametric estimators for the triple difference framework, which extend traditional DiD methods to handle more complex settings with staggered treatment adoption and network interference. These advanced techniques provide a flexible and robust approach to estimating causal effects in intricate settings, ensuring that the estimates remain consistent even under model misspecification.

Furthermore, the paper explores the application of Bayesian inference and deep learning methods to causal inference in longitudinal and panel data. Bayesian networks and neural basis expansion analysis (N-BEATS) are highlighted as powerful tools for modeling complex causal relationships and capturing temporal patterns in time series data. We also discuss the use of generalized synthetic control methods that incorporate probability distributions to account for the full range of variability in the data, enhancing the reliability of causal estimates [5].

Finally, the survey addresses the challenges of spatiotemporal data and hidden Markov models, emphasizing the importance of modeling arbitrary spillover and carryover effects in spatiotemporal settings [1]. Feedback-augmented non-homogeneous hidden Markov models (FAN-HMMs) are presented as a flexible framework for capturing the intricate dynamics of spatiotemporal data, providing more accurate and interpretable results.

The contributions of this survey paper lie in its comprehensive review of the latest advancements in causal modeling for longitudinal and panel data, providing a unified framework for understanding the integration of advanced statistical and machine learning techniques [4]. By highlighting the strengths and limitations of various methods, this survey aims to guide researchers and practitioners in selecting the most appropriate techniques for their specific applications, ultimately contributing to more reliable and robust causal inferences in complex data environments.

# 3 Advanced Estimation Techniques for Panel Data

## 3.1 Doubly Robust Estimators

### 3.1.1 Integration of Outcome Regression and Propensity Score Weighting
The integration of outcome regression and propensity score weighting represents a powerful approach to estimating the average treatment effect (ATE) and the average treatment effect on the treated (ATT) in observational studies. This method leverages the strengths of both techniques to address potential biases and improve the robustness of causal inferences. Outcome regression models the relationship between the treatment and the outcome, adjusting for observed covariates, while propensity score weighting accounts for the probability of receiving the treatment based on these covariates. The combination of these methods results in a doubly robust estimator, which remains consistent if either the outcome regression model or the propensity score model is correctly specified, but not necessarily both. This property is particularly valuable in settings where model misspecification is a concern.

In the context of panel data, the integration of outcome regression and propensity score weighting is extended to handle the complexities of repeated observations and potential time-varying covariates. The doubly robust estimator in this setting involves first estimating the propensity scores for each unit at each time point, typically using logistic regression or machine learning algorithms. These propensity scores are then used to weight the outcome regression, ensuring that the weighted sample is balanced in terms of observed covariates. This approach helps to mitigate the effects of time-invariant and time-varying confounders, thereby providing more reliable estimates of the treatment effect [6]. The cross-fitting scheme, a key component of this method, further enhances the robustness and efficiency of the estimator by reducing the impact of overfitting and ensuring that the estimates are not overly sensitive to the choice of nuisance parameters.

Moreover, the integration of outcome regression and propensity score weighting is particularly useful in scenarios where the data exhibit non-random missingness or where the treatment assignment is influenced by unobserved factors. By incorporating both the outcome regression and the propensity score weighting, the method can account for these complexities and provide more accurate and reliable estimates of the treatment effect. The doubly robust property ensures that the estimator remains valid even if one of the models is misspecified, making it a versatile tool for a wide range of empirical applications. This approach has been successfully applied in various fields, including health economics, political science, and social policy, where the evaluation of causal effects is crucial for informing decision-making and policy design.

### 3.1.2 Robustness Against Model Misspecifications
Robustness against model misspecifications is a critical aspect of causal inference, particularly in settings where the assumptions underlying the models may not hold. In the context of matrix completion and causal inference, the traditional assumption that missing entries are generated randomly is often unrealistic, especially in panel data settings where missingness patterns can be highly structured [3]. To address this, recent work has extended the matrix completion framework to handle non-random missingness patterns, providing theoretical guarantees for the recovery of counterfactual outcomes under minimal assumptions on the intervention patterns [3]. This extension is crucial for ensuring that the estimated average treatment effects (ATE) remain valid even when the missing data mechanism is not random.

The robustness of causal estimators against model misspecifications is further enhanced through the use of doubly robust (DR) methods, which combine the strengths of both outcome regression and propensity score weighting [7]. Specifically, a DR estimator remains consistent if either the outcome model or the propensity score model is correctly specified, thereby providing a safeguard against potential misspecifications. This property is particularly valuable in empirical applications where the true data-generating process is often unknown and complex. The introduction of a novel de-biasing technique in recent literature has further improved the accuracy of these estimators, allowing for more reliable inference even in the presence of non-random missingness and unobserved confounding.

Moreover, the development of methods that can handle general non-random missingness patterns has opened new avenues for robust causal inference in a variety of settings, including those with bipartite network interference. These methods not only provide more accurate estimates of the ATE but also offer a higher degree of robustness against potential violations of key assumptions, such as parallel trends. By leveraging advanced techniques from matrix completion and doubly robust estimation, researchers can now develop more reliable and robust causal inferences, even in complex and challenging data environments [8]. This advancement is particularly significant for policy evaluation and decision-making, where the reliability of causal estimates is paramount.

### 3.1.3 Group Structure in Microeconomic Panel Data
In microeconomic panel data, the grouping of units into higher-level aggregates, such as households within states or firms within industries, introduces a rich structure that can be leveraged for causal inference [5]. This group structure is particularly useful when treatments are assigned at the group level, as it allows for the exploitation of within-group variation over time. By focusing on the dynamics within and between these groups, researchers can better control for unobserved heterogeneity and improve the identification of causal effects. For instance, in state-level policy evaluations, the group structure can help address issues of spatial correlation and spillovers, which are common in settings where policies affect neighboring regions.

The integration of group structure into panel data models enhances the robustness of causal inference by providing additional layers of information and heterogeneity. This is especially important in microeconomic datasets, such as the Current Population Survey and the Panel Study of Income Dynamics, where individual-level data are nested within larger aggregate units [5]. By accounting for the group-level variation, researchers can more accurately estimate treatment effects while controlling for time-invariant and time-varying confounders [9]. For example, in the context of evaluating the impact of a new economic policy, the group structure can help disentangle the effects of the policy from other concurrent changes that may affect the outcome.

Moreover, the group structure in microeconomic panel data facilitates the use of advanced econometric techniques, such as synthetic control methods and fixed effects models, which can be adapted to handle complex treatment patterns. These methods leverage the group-level data to construct counterfactual outcomes and estimate the average treatment effect on the treated (ATT). By incorporating group-level covariates and interactions, these models can provide more precise and reliable estimates of causal effects, even in the presence of unobserved confounders. This approach is particularly valuable in policy evaluation, where the goal is to understand the impact of interventions on specific subpopulations within a broader economic context.

## 3.2 Panel Clustering and De-biased Estimation

### 3.2.1 Clustering Observations Using Regression Trees
Clustering observations using regression trees is a powerful approach for identifying subgroups within observational data that exhibit similar treatment effects. This method leverages the hierarchical partitioning capabilities of regression trees to segment the data into clusters where the treatment effect is relatively homogeneous. Unlike traditional clustering methods, which often rely on distance metrics or density-based criteria, regression trees explicitly model the relationship between the treatment effect and covariates, allowing for a more nuanced understanding of how treatment effects vary across different segments of the population.

The process begins by constructing a regression tree where the splitting criterion is designed to maximize the heterogeneity in treatment effects between child nodes. This is achieved through a greedy algorithm that iteratively selects the best split based on a measure of treatment effect variance reduction. Once the tree is fully grown, each terminal node represents a cluster of observations with similar treatment effects. The advantage of this approach is that it can capture complex interactions between covariates and treatment effects, leading to more accurate and interpretable clusters compared to linear models or simpler clustering techniques.

To further enhance the robustness and reliability of the estimated treatment effects, the method incorporates a doubly robust estimation framework. Specifically, the average treatment effect within each cluster is estimated using a weighted combination of regression-based and propensity score weighting estimators. This ensures that the method remains robust to potential misspecifications of either the outcome model or the propensity score model. Additionally, the use of double/debiased machine learning (DML) techniques allows for flexible estimation of nuisance parameters, thereby reducing bias and improving the overall performance of the method. The resulting clusters and treatment effect estimates provide valuable insights into the heterogeneity of treatment effects, facilitating more targeted and effective policy interventions.

### 3.2.2 Low-Rank Matrix Completion for Missing Data
Low-rank matrix completion has emerged as a powerful tool for handling missing data in various applications, particularly in settings where the underlying data matrix is believed to have an intrinsic low-rank structure. This section delves into the methodologies and theoretical foundations of low-rank matrix completion, focusing on its application to non-random missingness patterns [3]. Traditional matrix completion methods often assume that missing entries are missing at random (MAR), which is a strong assumption and may not hold in many real-world scenarios [10]. For instance, in panel data settings, missingness can be driven by complex, non-random mechanisms such as unit-specific or time-specific factors.

Recent advances in matrix completion have sought to relax the MAR assumption by considering more general missingness patterns. One notable approach is the use of convex optimization techniques, which aim to recover the low-rank matrix by solving a convex relaxation of the rank minimization problem. These methods typically involve nuclear norm minimization, which serves as a convex surrogate for the rank function. However, while these methods provide strong theoretical guarantees under certain conditions, they are often computationally intensive and may not scale well to large datasets. Moreover, they may not be well-suited for handling the structured missingness patterns often encountered in social science applications.

To address these limitations, recent work has explored non-convex optimization methods that can handle more general missingness patterns and are computationally more efficient. These methods often leverage iterative algorithms, such as alternating minimization or gradient descent, to directly optimize the low-rank factorization of the matrix. This approach not only allows for more flexible modeling of missing data but also provides a natural framework for incorporating additional constraints or regularizations, such as sparsity or smoothness. Furthermore, these methods have been shown to achieve state-of-the-art performance in both synthetic and real-world datasets, making them a promising direction for future research in the field of low-rank matrix completion for missing data.

### 3.2.3 Theoretical Guarantees on Accuracy and Interpretability
The theoretical guarantees on accuracy and interpretability in causal inference are crucial for ensuring the reliability and transparency of estimated treatment effects. In the context of bipartite network interference (BNI), recent advancements have focused on developing estimators that can recover the average treatment effect (ATE) under minimal assumptions. These estimators leverage a novel de-biasing identity and extend the entry-wise uncertainty quantification analysis to handle general non-random missingness patterns. This extension is significant because it addresses the challenge of counterfactual recovery, which is essential for accurately estimating treatment effects in settings with complex interference structures.

To ensure the accuracy of these estimators, theoretical guarantees are established through rigorous mathematical derivations. One key result is the proof that the bias introduced by approximating the treatment effect function with a piece-wise constant function, produced by a regression tree, decreases polynomially as the number of leaves in the tree increases [3]. This finding is supported by mild assumptions on the regression tree and a density assumption on the covariates. Additionally, the framework allows for the relaxation of the assumption that entries are observed at the same rate as missing entries, which is a common limitation in traditional matrix completion techniques [10]. By accommodating unit-specific features in the missing pattern, the proposed methods enhance the robustness of the estimators against potential biases.

Interpretability is another critical aspect of these theoretical guarantees. The use of regression trees and piece-wise constant functions not only ensures that the bias decreases with increased model complexity but also provides a clear and interpretable representation of the treatment effect [3]. This interpretability is particularly valuable in policy-making and clinical settings, where stakeholders need to understand the underlying mechanisms driving the estimated effects. Furthermore, the double-robust (DR) method, which combines inverse probability weighting (IPW) with outcome regression, enhances the robustness of the estimators by ensuring consistency even if one of the models is misspecified [7]. This dual approach not only improves the accuracy of the estimates but also provides a transparent and reliable basis for inference, making the results more credible and actionable.

## 3.3 Quasi-Experimental Designs and Machine Learning

### 3.3.1 Analyzing Effects of Policy Changes in Sports
Analyzing the effects of policy changes in sports requires a robust methodological framework capable of isolating the impact of these interventions from other confounding factors. One of the most widely used approaches in this context is the Difference-in-Differences (DiD) method, which leverages panel data to estimate the causal effects of policy changes [11]. The DiD method relies on the assumption that, in the absence of the intervention, the trends in the outcomes of the treated and control groups would have followed a similar path [7]. This assumption, known as the parallel trends assumption, is crucial for the validity of DiD estimates. In the context of sports, this method can be applied to evaluate the impact of policies such as rule changes, financial regulations, or anti-doping measures on performance metrics, fan engagement, and economic outcomes.

To enhance the robustness of DiD estimates in sports policy analysis, researchers have developed various extensions and modifications. For instance, the synthetic control method (SCM) has been adapted to create a synthetic control group that mimics the behavior of the treated group in the absence of the intervention. This approach is particularly useful when there are few suitable control units available, a common issue in sports where the number of comparable leagues or teams may be limited. The SCM constructs a weighted average of control units to form a synthetic control, which is then used to estimate the counterfactual outcomes for the treated unit. This method has been applied to evaluate the effects of major policy changes, such as the introduction of salary caps in professional sports leagues, by comparing the performance of the league with the synthetic control over time.

Moreover, recent advancements in causal inference have introduced techniques to address the limitations of traditional DiD and SCM methods [12]. For example, the use of machine learning algorithms, such as those based on doubly robust estimators, can help account for complex heterogeneity in treatment effects and improve the accuracy of causal estimates. These methods combine outcome regression and propensity score weighting to provide estimates that are robust to model misspecification. In the context of sports, this can be particularly valuable when evaluating policies that may have differential impacts across different segments of the population, such as players of varying skill levels or fans from different demographic groups. By integrating these advanced techniques, researchers can provide more reliable and nuanced insights into the effects of policy changes in sports, thereby informing better decision-making by policymakers and stakeholders.

### 3.3.2 Double/Debiased Machine Learning for Fixed Effects
Double/Debiased Machine Learning (DML) has emerged as a powerful tool for estimating causal effects in settings with fixed effects, particularly in panel data contexts. The core idea behind DML is to leverage machine learning techniques to estimate nuisance parameters, such as the propensity score and the outcome regression, while ensuring that the resulting estimators are orthogonal to the nuisance parameters. This orthogonality ensures that the estimators are robust to the first-stage estimation errors, thereby improving the accuracy and reliability of the causal effect estimates. By using machine learning algorithms, DML can flexibly handle high-dimensional and complex data structures, making it particularly suitable for settings with many fixed effects or interactive fixed effects.

In the context of fixed effects, DML addresses the issue of bias that arises from the use of traditional estimators, such as those based on nuclear norm regularization, which can introduce bias in the estimated matrix. The de-biasing technique in DML involves constructing orthogonalized equations that are insensitive to the first-stage estimation errors. This is achieved by using cross-fitting schemes, where the data is split into multiple folds, and the nuisance parameters are estimated in one fold while the causal effect is estimated in another. This process is repeated across folds, and the final estimate is averaged, leading to an estimator that is both consistent and asymptotically normal. The double-robust property of DML ensures that the estimator remains consistent even if one of the nuisance models is misspecified, provided the other is correctly specified.

Moreover, DML extends the applicability of fixed effects models to more complex settings, such as those with staggered treatment adoption or repeated cross-sectional data. In these settings, the parallel trends assumption, which is crucial for the validity of traditional difference-in-differences (DiD) estimators, may be violated due to unobserved confounders or time-varying effects [6]. DML provides a flexible framework to address these challenges by incorporating machine learning algorithms that can adapt to the data structure and capture complex interactions. The use of DML in these settings not only enhances the robustness of the causal effect estimates but also allows for the estimation of heterogeneous treatment effects, providing a more nuanced understanding of the treatment impact across different subgroups [13].

### 3.3.3 Semiparametric Estimators for Triple Difference Framework
Semiparametric estimators for the triple difference framework represent a significant advancement in causal inference, particularly in settings where traditional difference-in-differences (DiD) methods may not suffice [12]. These estimators leverage the robust properties of semiparametric methods to address the complexities introduced by the third dimension, often representing an additional source of variation or heterogeneity. By extending the identification and estimation techniques from the DiD literature, semiparametric estimators for triple difference provide a flexible and robust approach to estimating causal effects in more intricate settings [12]. Specifically, these estimators are designed to handle scenarios where the parallel trends assumption may hold across multiple dimensions, such as in panel data with staggered treatment adoption or in settings with network interference.

The development of semiparametric estimators for the triple difference framework builds on the foundational work in semiparametric panel data and DiD literature [12]. These estimators are derived using influence function-based methods, which allow for the construction of doubly robust estimators. Doubly robust estimators are particularly valuable because they remain consistent if either the propensity score model or the outcome regression model is correctly specified, providing a safeguard against model misspecification. This property is crucial in empirical applications where the true data-generating process is often unknown. Moreover, these estimators are designed to be asymptotically normal and efficient, achieving the semiparametric efficiency bound under certain regularity conditions [8]. This efficiency is particularly important in small and moderate sample sizes, where the performance of traditional estimators may be suboptimal.

Empirically, semiparametric estimators for the triple difference framework have been shown to outperform alternative methods in both synthetic and real-world data applications. For instance, in the context of panel data with staggered treatment adoption, these estimators have demonstrated superior performance in estimating treatment effects compared to matrix completion-based methods and synthetic control estimators. The robustness and efficiency of these estimators make them particularly suitable for applications in microeconometrics and policy evaluation, where the data often exhibit complex patterns of missingness and heterogeneity. Additionally, the ability to incorporate inverse probability weighting (IPW) in the estimation process further enhances the robustness of these estimators, making them a powerful tool for causal inference in a wide range of empirical settings.

# 4 Synthetic Control Methods and Extensions

## 4.1 Automated Causal Inference Frameworks

### 4.1.1 OpportunityFinder for Panel and Cross-Sectional Data
OpportunityFinder (OPF) represents a significant advancement in the democratization of causal inference techniques, particularly for panel and cross-sectional data [14]. This tool is designed to automate the selection and application of causal inference algorithms, thereby making sophisticated analysis accessible to non-expert users [14]. By supporting a wide array of causal inference methods, OPF can handle both panel data, which typically involves multiple units observed over time, and cross-sectional data, which captures a snapshot of different units at a single point in time. The auto-selection feature of OPF ensures that the most appropriate algorithm is chosen based on the characteristics of the dataset, such as the number of units, the length of the observation period, and the presence of confounding variables.

One of the key strengths of OPF is its ability to manage the complexities inherent in panel data, where the number of units or the length of the pretreatment period may be insufficient for traditional frequentist approaches. In such scenarios, Bayesian inference methods, which are integrated into OPF, offer several advantages. Bayesian methods can provide more accurate and robust estimates by incorporating prior knowledge and uncertainty into the model. This is particularly useful when dealing with small sample sizes or when the data exhibit high variability. Moreover, OPF's Bayesian framework allows for the estimation of spillover effects, which are crucial in settings where the outcomes of treated units may influence the outcomes of untreated units, thus relaxing the assumption of no interference (SUTVA).

In addition to its robust handling of panel data, OPF also excels in cross-sectional data analysis, where the primary challenge is often the lack of temporal information. By leveraging a variety of causal inference algorithms, OPF can estimate the average treatment effect (ATE) for binary actions, providing users with a clear and interpretable measure of the causal impact [14]. The tool's user-friendly interface and automated workflow make it an invaluable resource for researchers and practitioners who need to conduct causal analysis without deep expertise in statistical methods. As OPF continues to evolve, it is expected to become a standard tool in the toolkit of data scientists and researchers working with both panel and cross-sectional data.

### 4.1.2 Proximal Causal Inference with Synthetic Controls
Proximal causal inference with synthetic controls (SC) extends traditional SC methods by incorporating elements of proximal causal inference to address issues of unmeasured confounding and overfitting [5]. This approach leverages the concept of proxies—variables that capture the effects of unmeasured confounders—to improve the robustness of causal estimates [15]. By integrating Bayesian regularization techniques, such as the horseshoe prior, the method effectively regularizes the synthetic control weights, thereby reducing the risk of overfitting, especially in settings with a large number of control units and short pretreatment periods [16]. This regularization is crucial because it helps maintain the balance between fitting the pre-treatment outcomes well and avoiding spurious correlations that can arise from overfitting.

The Bayesian framework allows for the incorporation of prior knowledge and uncertainty in the estimation process, making it particularly suitable for settings where data is limited or noisy. The use of Bayesian horseshoe priors ensures that the synthetic control weights are sparse, meaning that only a subset of the control units significantly contributes to the synthetic control [16]. This sparsity is beneficial in reducing the complexity of the model and improving interpretability. Moreover, the Bayesian approach facilitates the estimation of credible intervals for the causal effects, providing a measure of uncertainty that is often lacking in frequentist methods. The simulation studies conducted in the literature demonstrate the effectiveness of this approach in producing accurate and reliable causal estimates, even in small sample sizes and complex data structures [2].

In practical applications, the proximal causal inference with synthetic controls has been shown to be particularly useful in policy evaluation, where the treated unit is often a single entity, such as a state or a city, and the number of available control units is limited [5]. By leveraging proxies and Bayesian regularization, this method can provide more robust and credible causal estimates compared to traditional SC methods. The ability to handle unmeasured confounding and reduce overfitting makes it a valuable tool for researchers and policymakers seeking to understand the impact of interventions in real-world settings [17]. Additionally, the method's flexibility in handling different types of data, including aggregate and individual-level data, enhances its applicability across various domains, from economics to public health.

### 4.1.3 Conformal Inference for Counterfactual Analysis
Conformal inference for counterfactual analysis represents a robust and versatile approach to estimating the uncertainty associated with counterfactual predictions in causal inference. Unlike traditional methods that often rely on parametric assumptions and may suffer from model misspecification, conformal inference leverages the principles of conformal prediction to provide valid and distribution-free confidence intervals for counterfactual outcomes. By treating the counterfactual prediction problem as a conditional distribution estimation task, conformal inference methods can be applied to a wide range of causal inference settings, including those involving synthetic control methods (SCM) and difference-in-differences (DID) designs [18].

The core idea behind conformal inference is to construct prediction sets that are guaranteed to cover the true counterfactual outcome with a pre-specified probability, regardless of the underlying data distribution. This is achieved by calibrating the prediction intervals using a hold-out sample or cross-validation, ensuring that the coverage properties are maintained even in small samples or under model misspecification. In the context of causal inference, this approach is particularly valuable when dealing with complex, high-dimensional data where traditional parametric methods may fail to capture the true data-generating process. Moreover, conformal inference can be seamlessly integrated with various machine learning algorithms, allowing for flexible and adaptive modeling of counterfactual outcomes.

Recent developments in conformal inference have extended its applicability to dynamic settings, where the relationships between units and their outcomes evolve over time. By incorporating temporal dependencies and structural breaks, these methods can provide more accurate and reliable estimates of counterfactual outcomes, especially in scenarios where the treatment effects are heterogeneous or time-varying. Additionally, conformal inference can be used to test hypotheses about the entire counterfactual distribution, enabling researchers to conduct comprehensive analyses of causal effects beyond simple point estimates. This makes conformal inference a powerful tool for robust causal inference in both observational and experimental studies.

## 4.2 Deep Learning and Bayesian Approaches

### 4.2.1 Neural Basis Expansion Analysis for Time Series
Neural Basis Expansion Analysis for Time Series (N-BEATS) is a deep learning architecture designed to forecast future values in time series data based on historical observations [19]. Unlike traditional time series models, N-BEATS employs a stack of fully connected layers to decompose the input time series into a series of basis functions, which are then combined to form the final forecast. This architecture is particularly effective in capturing complex temporal patterns and nonlinear relationships within the data, making it a powerful tool for various forecasting tasks. The key innovation of N-BEATS lies in its ability to learn interpretable basis functions, which can provide insights into the underlying dynamics of the time series.

The N-BEATS model is structured into a stack of blocks, each consisting of a series of fully connected layers followed by a skip connection. Each block is responsible for learning a specific aspect of the time series, such as trend, seasonality, or other latent patterns. The outputs of these blocks are summed to produce the final forecast. This modular design allows the model to adapt to different types of time series data and to scale efficiently as the complexity of the data increases. Moreover, the use of skip connections helps to mitigate the vanishing gradient problem, ensuring that the model can effectively learn long-term dependencies in the data.

In the context of causal inference, N-BEATS can be particularly useful for imputing missing data and estimating counterfactual outcomes in panel data settings [8]. By leveraging the time series dimension, N-BEATS can provide accurate predictions of the potential outcomes for treated units, even in the presence of latent confounding variables [19]. This capability is crucial for methods like synthetic control, where the goal is to construct a counterfactual control unit that closely matches the treated unit's behavior in the absence of treatment. The robustness and flexibility of N-BEATS make it a valuable addition to the toolkit of researchers and practitioners working on causal inference problems involving time series data.

### 4.2.2 Bayesian Networks for Pathway Analysis
Bayesian networks have emerged as a powerful tool for pathway analysis, offering a structured approach to model complex causal relationships among variables. These networks represent probabilistic dependencies through directed acyclic graphs (DAGs), where nodes correspond to variables and edges denote conditional dependencies. In the context of pathway analysis, Bayesian networks enable the integration of prior knowledge and empirical data, facilitating the identification of causal pathways that might not be apparent through traditional statistical methods. By incorporating Bayesian principles, these models can handle uncertainty in a principled manner, making them particularly suitable for scenarios with limited data or high-dimensional feature spaces.

One of the key advantages of using Bayesian networks in pathway analysis is their ability to perform probabilistic inference, allowing researchers to estimate the likelihood of different causal pathways given the observed data. This is achieved through the application of Bayes' theorem, which updates the prior probabilities of hypotheses based on the evidence provided by the data. In the context of synthetic control methods, Bayesian networks can be used to construct synthetic weights that reflect the underlying causal structure of the data. For instance, the Bayesian horseshoe prior has been effectively utilized to regularize the synthetic weights, thereby reducing overfitting and improving the accuracy of counterfactual predictions [16]. This regularization is crucial in synthetic control applications, where the number of potential control units can be large relative to the amount of available data.

Moreover, Bayesian networks can accommodate dynamic and temporal aspects of pathway analysis, which are often critical in understanding the evolution of causal relationships over time. By extending the static Bayesian network framework to dynamic Bayesian networks (DBNs), researchers can model how variables interact and influence each other across different time points. This extension is particularly relevant in fields such as epidemiology and economics, where interventions and policies can have delayed or evolving impacts. DBNs provide a flexible and robust framework for analyzing such dynamic systems, enabling the identification of time-varying causal pathways and the assessment of long-term intervention effects. Overall, the integration of Bayesian networks into pathway analysis offers a comprehensive and rigorous approach to uncovering the intricate causal structures underlying complex data.

### 4.2.3 Generalized Synthetic Control with Probability Distributions
Generalized synthetic control methods that incorporate probability distributions offer a robust framework for causal inference, particularly in settings with complex data structures and limited sample sizes [5]. By extending traditional synthetic control methods, which typically rely on point estimates, these generalized approaches utilize the full distribution of outcomes to construct synthetic controls [5]. This approach allows for a more nuanced representation of uncertainty and variability in the data, thereby enhancing the reliability of causal estimates. Specifically, the use of probability distributions enables the modeling of the entire outcome distribution of the control units, rather than just their means, leading to more accurate and stable synthetic controls.

The incorporation of Bayesian methods, such as the Bayesian horseshoe prior, further enhances the robustness of these generalized synthetic control methods [16]. The Bayesian horseshoe prior is particularly effective in regularizing the synthetic weights, which helps to mitigate overfitting—a common issue in synthetic control applications, especially when dealing with a large number of control units and short pretreatment periods [16]. This regularization is crucial because it ensures that the synthetic control remains a reliable counterfactual, even in the presence of noisy or sparse data. The Bayesian framework also allows for the integration of prior knowledge and the quantification of uncertainty, providing a more comprehensive assessment of the causal effect.

Moreover, the use of Wasserstein barycenters in generalized synthetic control methods provides a principled way to match the entire distribution of the control units to the target unit. Unlike traditional linear point-wise approaches, which match only the means of the control units, Wasserstein barycenters match the full distribution, ensuring that the synthetic control unit accurately reflects the target unit's characteristics across all quantiles. This distributional matching is particularly useful in settings where the outcome distribution is skewed or multimodal, as it captures the full range of variability in the data. The resulting synthetic control unit is thus more representative and leads to more precise and reliable causal inferences [5].

## 4.3 Extensions for Spillover Effects and Spatial Data

### 4.3.1 Spatial Autoregressive Models for Spillover Effects
Spatial autoregressive (SAR) models are fundamental in spatial econometrics for capturing the spatial dependence and spillover effects among units [16]. These models extend the standard linear regression framework by incorporating a spatial lag of the dependent variable, which accounts for the influence of neighboring units' outcomes on the outcome of a given unit. The spatial lag is typically represented by \( \rho W y \), where \( \rho \) is the spatial autoregressive coefficient and \( W \) is the spatial weights matrix that defines the spatial relationships between units. This extension is crucial in settings where the outcomes of units are not independent but are influenced by their neighbors, such as in the study of economic activities, disease spread, or social behaviors.

The integration of SAR models into the Synthetic Control Method (SCM) allows for a more nuanced estimation of treatment effects in the presence of spillovers. By incorporating the spatial lag, the model can account for the indirect effects of the treatment that propagate through the network of units. For example, in the context of public health interventions, the effectiveness of a vaccination program in one region may be influenced by the vaccination rates in neighboring regions. The SAR-SCM approach can estimate both the direct effect of the intervention in the treated region and the spillover effects in the neighboring regions, providing a more comprehensive understanding of the intervention's impact [16]. This is particularly important in policy evaluation, where ignoring spillovers can lead to biased estimates of treatment effects.

Moreover, the Bayesian structural time series (BSTS) method can be adapted to incorporate SAR models, allowing for the estimation of predictive distributions of counterfactual outcomes that account for spillover effects. This approach not only provides a more accurate estimate of the treatment effect but also quantifies the uncertainty associated with the spillover effects [16]. By leveraging the Bayesian framework, researchers can incorporate prior knowledge about the spatial relationships and the magnitude of spillovers, leading to more robust and reliable estimates. This is particularly useful in settings with limited data or when the spatial relationships are complex and nonlinear. The ability to construct confidence intervals for both treatment and average spillover effects enhances the interpretability and practical utility of the model, making it a valuable tool for policymakers and researchers alike.

### 4.3.2 Bayesian Inference for Synthetic Controls
Bayesian inference has emerged as a robust framework for enhancing the synthetic control method (SCM), particularly in scenarios characterized by a large number of control units and short pretreatment periods, which are prone to overfitting. By incorporating Bayesian regularization, specifically through the use of Bayesian horseshoe priors, the synthetic control model can effectively mitigate overfitting bias [16]. The Bayesian horseshoe prior is known for its strong shrinkage properties, which help in reducing the influence of irrelevant or noisy control units, thereby improving the accuracy of the synthetic control weights.

Several studies have demonstrated the effectiveness of Bayesian methods in SCM through the application of Markov Chain Monte Carlo (MCMC) sampling techniques, which facilitate the estimation of complex models and provide a principled way to quantify uncertainty. For instance, Kim et al. (2020) introduced a Bayesian synthetic control method that leverages the Bayesian horseshoe prior to model the synthetic weights [16]. Their simulation studies have shown that this approach leads to more accurate predictions of counterfactual outcomes compared to traditional frequentist methods, especially in settings with limited pretreatment data. The Bayesian framework also allows for the incorporation of prior knowledge, which can further enhance the robustness of the synthetic control model.

Moreover, the Bayesian approach to SCM is not limited to the use of horseshoe priors; it can be extended to include a variety of flexible modeling techniques. For example, the use of hierarchical Bayesian models can account for heterogeneity across control units, while nonparametric Bayesian methods can capture complex patterns in the data without making strong parametric assumptions. These advancements in Bayesian inference for synthetic controls have broadened the applicability of SCM to a wider range of empirical settings, including those with dynamic and nonlinear relationships [16]. The ability to handle such complexities through Bayesian methods underscores the potential of this approach to advance the field of causal inference in policy evaluation and beyond.

### 4.3.3 Dynamical Systems Theory for Policy Analysis
Dynamical systems theory offers a robust framework for understanding and modeling the temporal evolution of systems, which is particularly relevant in the context of policy analysis. This theory is characterized by its ability to capture the complex interactions and feedback loops that are often present in social and economic systems. By integrating dynamical systems theory with synthetic control methods, researchers can better account for the dynamic nature of policy impacts, which may evolve over time and interact with other concurrent changes [20]. The key advantage of this approach is its capacity to model not only the immediate effects of a policy but also its long-term consequences and the potential for nonlinear responses.

In the application of dynamical systems theory to policy analysis, the synthetic control method serves as a critical tool for constructing a counterfactual scenario that reflects what would have happened in the absence of the policy intervention [20]. This is achieved by creating a weighted combination of control units that closely matches the pre-intervention characteristics of the treated unit. However, traditional synthetic control methods often assume static relationships between units, which may not accurately reflect the dynamic and evolving nature of real-world systems. By incorporating dynamical systems theory, the synthetic control method can be extended to account for time-varying relationships and the potential for spillover effects, where the treatment of one unit may influence the outcomes of other units.

This integration also allows for a more nuanced understanding of the policy's impact across different quantiles of the outcome distribution, rather than focusing solely on average effects. For instance, in the context of economic policies, it can provide insights into how the policy affects different segments of the population, such as low-income versus high-income households. Additionally, by modeling the system as a dynamical process, researchers can explore the stability and robustness of the policy effects over time, which is crucial for long-term policy planning and evaluation. This approach not only enhances the accuracy of causal inference but also provides a more comprehensive picture of the policy's broader implications.

# 5 Causal Inference in Complex Longitudinal Settings

## 5.1 Dynamic Causal Modeling and Functional MRI

### 5.1.1 Nonlinear State-Space Framework for Effective Connectivity
The nonlinear state-space framework, particularly Dynamic Causal Modeling (DCM), provides a robust method for inferring effective connectivity in neural systems from resting-state functional magnetic resonance imaging (rs-fMRI) data [21]. DCM is grounded in the principle that the observed neural activity in brain regions is driven by underlying, unobserved neuronal states that interact according to a set of dynamic equations. These equations capture the influence of one region on another, allowing for the estimation of directed connectivity patterns. The framework incorporates both deterministic and stochastic components, where the deterministic part describes the expected changes in neural states due to intrinsic dynamics and external inputs, while the stochastic part accounts for random fluctuations and measurement noise.

In the context of rs-fMRI, DCM models the blood-oxygen-level dependent (BOLD) signal as a nonlinear function of the underlying neural activity, which is influenced by both local and remote interactions. The model parameters, including connection strengths and modulatory effects, are estimated using Bayesian inference techniques, which allow for the integration of prior knowledge about the system and the quantification of uncertainty in the parameter estimates. This approach enables researchers to make probabilistic statements about the presence and strength of effective connectivity between brain regions, providing a more nuanced understanding of neural interactions compared to traditional correlation-based methods.

The application of DCM to rs-fMRI data involves several steps, including the selection of regions of interest (ROIs) based on prior knowledge or data-driven approaches, the specification of the model structure, and the estimation of model parameters. The choice of ROIs is critical, as it determines the nodes of the network and the potential interactions to be investigated. Once the ROIs are selected, the model structure is defined, typically by specifying the connectivity matrix and the form of the dynamic equations. Parameter estimation is then performed using algorithms implemented in software packages such as SPM12, which provide tools for model fitting and comparison. The resulting connectivity estimates can be used to test hypotheses about the organization and function of neural networks, offering insights into the mechanisms underlying cognitive processes and neurological disorders.

### 5.1.2 Linear Mixed Effect Models for Genetic Data
Linear Mixed Effect (LME) models have become a cornerstone in the analysis of genetic data, particularly in the context of longitudinal studies where repeated measurements are taken from the same subjects over time. These models are particularly useful for accounting for the hierarchical structure of genetic data, where genetic effects can vary both within and between individuals. By incorporating both fixed and random effects, LME models can effectively capture the complex dependencies and heterogeneity inherent in genetic data. The fixed effects in these models typically represent the average genetic effects across the population, while the random effects capture individual-specific deviations from these averages, thus providing a more nuanced understanding of genetic influences.

In the context of genetic data, LME models are often used to assess the impact of Single Nucleotide Polymorphisms (SNPs) on various phenotypic outcomes. The random effects in these models can account for the unobserved genetic and environmental factors that may influence the phenotype, thereby reducing the risk of confounding. This is particularly important in genetic studies, where the genetic architecture of complex traits is often polygenic, involving multiple genetic variants with small effects. By modeling these effects simultaneously, LME models can provide more accurate and reliable estimates of the genetic contributions to the phenotype. Additionally, the use of random effects allows for the modeling of within-subject correlations, which is crucial for longitudinal data where repeated measurements are taken over time.

The application of LME models in genetic studies also extends to the integration of multi-omics data, where the goal is to understand the interplay between different layers of biological information, such as genomics, transcriptomics, and epigenomics. In these settings, LME models can be extended to include multiple random effects to account for the hierarchical structure of the data, such as the clustering of genes within pathways or the grouping of individuals within families. This extension not only enhances the model's ability to capture complex biological relationships but also improves the interpretability of the results. Furthermore, the flexibility of LME models allows for the incorporation of various types of covariates, including time-varying and time-invariant factors, making them a versatile tool for the analysis of genetic data in a wide range of biological and clinical contexts.

### 5.1.3 Parametric Bootstrap for Null Distributions
The parametric bootstrap is a powerful resampling technique used to approximate the sampling distribution of a statistic under the null hypothesis, particularly when the asymptotic distribution is either unknown or poorly approximates the finite-sample distribution. In the context of complex models, such as those involving mixed effects or functional data, the parametric bootstrap offers a flexible and robust alternative to traditional asymptotic methods. By simulating data under the null hypothesis, the parametric bootstrap can accurately capture the variability and structure of the data, leading to more reliable hypothesis testing and confidence interval construction.

In practice, the parametric bootstrap involves fitting the model to the observed data to estimate the parameters under the null hypothesis. These estimates are then used to generate synthetic datasets that mimic the characteristics of the original data. Each synthetic dataset is analyzed using the same model, and the test statistic of interest is computed. By repeating this process many times, a distribution of the test statistic under the null hypothesis is constructed. This empirical distribution can be used to calculate p-values and construct confidence intervals, providing a more accurate assessment of statistical significance compared to asymptotic approximations, especially in small sample sizes or when the data exhibit complex dependencies.

The parametric bootstrap is particularly useful in scenarios where the null hypothesis involves specific constraints on the model parameters, such as zero effects or equality of means. It allows for the direct simulation of these constraints, ensuring that the null distribution is correctly specified. Additionally, the method can handle various types of data, including longitudinal, spatial, and functional data, making it a versatile tool in modern statistical analysis. Despite its computational demands, the parametric bootstrap provides a principled approach to hypothesis testing and inference, enhancing the reliability and interpretability of statistical results in complex modeling contexts.

## 5.2 Simulation Studies and Marginal Structural Models

### 5.2.1 Generating Synthetic Data for MSMs
Generating synthetic data for Marginal Structural Models (MSMs) is crucial for evaluating the performance of these models under controlled conditions and for understanding the implications of various assumptions. The process typically involves simulating data that mimic real-world scenarios, including time-varying treatments, confounders, and outcomes. This section outlines the key steps and considerations in generating such synthetic data.

First, the data generation process must accurately reflect the underlying causal structure of the system being studied. This involves specifying the distributions of baseline covariates, the treatment assignment mechanism, and the outcome model. For instance, in a longitudinal setting, one might simulate a series of time points where treatments and covariates are updated at each step, and outcomes are generated based on the current state of the system. The treatment assignment mechanism is particularly important, as it must account for potential confounding and ensure that the assumptions of consistency, no unmeasured confounding, and positivity are met.

Second, the simulation should incorporate realistic levels of variability and complexity to ensure that the synthetic data are representative of real-world data. This includes considering different levels of treatment effect heterogeneity, varying degrees of confounding, and potential violations of the positivity assumption [9]. For example, in scenarios where certain treatment levels are rarely observed, the synthetic data should reflect these rare events to test the robustness of the MSM estimation methods. Additionally, the simulation can be extended to include multiple data sets to assess the stability and reproducibility of the results across different data configurations.

### 5.2.2 Addressing Time-Varying Confounding and g-Null Paradox
Addressing time-varying confounding in longitudinal studies is a critical issue, as standard methods often fail to provide consistent estimators due to the complex interplay between time-varying exposures and confounders [9]. Time-varying confounding occurs when the effect of a treatment on the outcome is influenced by time-dependent covariates that are themselves affected by past treatments [9]. This creates a feedback loop that can lead to biased estimates of causal effects. Robins and colleagues have developed several methods to address this issue, including marginal structural models (MSMs) and structural nested models (SNMs). MSMs, in particular, use inverse probability of treatment weighting (IPTW) to adjust for time-varying confounders, thereby providing a consistent estimate of the causal effect under certain assumptions [9].

However, the application of these methods is complicated by the g-null paradox, a phenomenon that arises when attempting to simulate data under the null hypothesis of no treatment effect. The g-null paradox occurs because the conditional distributions used in the simulation process do not align with the marginal distribution of the null hypothesis. This misalignment makes it difficult to generate data that accurately reflects the null scenario, leading to potential biases in hypothesis testing. To address this issue, researchers have proposed various strategies, such as using marginal structural models with carefully specified weights or employing alternative simulation techniques that can better handle the complexities of time-varying confounding.

Despite these advances, the g-null paradox remains a significant challenge in the analysis of longitudinal data with time-varying confounders. Future research should focus on developing more robust methods for simulating data under the null hypothesis and on extending existing causal inference techniques to handle a broader range of data structures and assumptions. Additionally, there is a need for more comprehensive empirical evaluations of these methods to assess their performance in real-world settings, particularly in studies with complex time-dependent covariates and small sample sizes.

### 5.2.3 Evaluating Bias and Confidence Interval Coverage
Evaluating bias and confidence interval coverage is a critical component in assessing the reliability and validity of causal inference models. In the context of econometrics and biostatistics, the presence of bias can severely undermine the accuracy of estimated causal effects, especially in observational studies where treatment assignment is not randomized. Bias can arise from various sources, including omitted variable bias, measurement error, and model misspecification. For instance, in a study examining the impact of fertilizer on crop yield, if the amount of fertilizer is not randomly assigned and instead depends on the quality of the land, the error term \( u \) will be correlated with the regressor, leading to biased estimates of the treatment effect.

To address these issues, researchers often employ methods such as propensity score matching, inverse probability weighting (IPW), and doubly robust estimators to adjust for confounding variables. However, the effectiveness of these methods in reducing bias and ensuring valid confidence interval coverage depends on the correct specification of the propensity score model and the absence of unmeasured confounders. Simulation studies play a crucial role in evaluating the performance of these methods under various scenarios. These studies typically assess the degree of bias, the coverage probability of confidence intervals, and the efficiency of estimators. For example, in settings with time-varying treatments and confounders, such as in longitudinal studies of HIV progression, conventional methods like regression adjustment can be biased due to time-dependent confounding [2]. Simulation studies have shown that methods like the g-computation algorithm and targeted maximum likelihood estimation (TMLE) can provide less biased estimates and better coverage probabilities.

Moreover, the robustness of these methods to violations of key assumptions, such as the assumption of no unmeasured confounders, is also a critical aspect of evaluation. In practice, assumptions such as causal sufficiency, stationarity, and the correct functional form of the model are often difficult to verify. Therefore, sensitivity analyses are essential to assess the impact of potential violations on the estimated causal effects. For instance, in the context of hidden Markov models (HMMs) applied to causal inference, the assumption of causal sufficiency and the correct specification of the number of hidden states can significantly affect the estimation of causal effects. Through simulation experiments, researchers can explore how misspecifications in these assumptions influence the bias and coverage properties of the estimators, providing valuable insights into the robustness of the methods in real-world applications.

## 5.3 Spatiotemporal Data and Hidden Markov Models

### 5.3.1 General Causal Inference for Arbitrary Spillover Effects
General causal inference for arbitrary spillover effects addresses the complexities inherent in spatiotemporal data where interventions can affect not only the targeted units but also neighboring units, and these effects can persist over time [1]. Traditional methods often assume that spillover effects are limited to immediate neighbors and carryover effects are confined to short time lags, which may not accurately reflect real-world scenarios [1]. The proposed methodology in this section relaxes these assumptions by modeling micro-level data directly, thereby capturing the intricate patterns of spillover and carryover effects without aggregation or restrictive assumptions [1].

This approach leverages advanced statistical techniques to handle the heterogeneity and complexity of spatiotemporal data. By avoiding the aggregation of data, the methodology can account for individual-level variations and interactions that are crucial for understanding the true causal mechanisms. The flexibility in modeling spillover and carryover effects allows for a more realistic representation of how interventions spread through a population and evolve over time. This is particularly important in fields such as epidemiology, urban planning, and environmental science, where the spatial and temporal dimensions of interventions play a critical role in their effectiveness.

The methodology also addresses the challenges associated with identifying and estimating causal effects in the presence of complex dependencies. It employs robust statistical methods to ensure that the estimated causal effects are reliable and valid, even in the face of potential confounding factors and measurement errors. Through simulation studies and real-world applications, the approach demonstrates its ability to provide accurate and interpretable results, making it a valuable tool for researchers and practitioners dealing with spatiotemporal data. The ability to model arbitrary spillover and carryover effects without restrictive assumptions enhances the applicability of causal inference in a wide range of empirical settings.

### 5.3.2 Feedback-Augmented Non-Homogeneous Hidden Markov Models
Feedback-Augmented Non-Homogeneous Hidden Markov Models (FAN-HMMs) represent a significant advancement in the modeling of complex temporal data, particularly in scenarios where standard Hidden Markov Models (HMMs) fall short due to their restrictive assumptions. Traditional HMMs assume that the observed data are conditionally independent given the hidden states and covariates, which is often an oversimplification in real-world applications [22]. FAN-HMMs extend this framework by incorporating feedback mechanisms and allowing for time-varying covariates, thereby capturing more nuanced and realistic dynamics in the data. This extension is crucial for accurately modeling systems where past observations and hidden states influence future states and observations, such as in ecological monitoring, speech recognition, and life course research.

In FAN-HMMs, the feedback mechanism is introduced to account for the dependencies between consecutive observations and hidden states, which are often present in non-homogeneous time series. This is achieved by modifying the transition and emission probabilities to include lagged observations and hidden states, thus relaxing the conditional independence assumption. The inclusion of time-varying covariates further enhances the model's flexibility, allowing it to adapt to changes in the underlying system over time. For instance, in ecological studies, environmental factors such as temperature and precipitation can vary seasonally and impact the behavior of species, making the use of time-varying covariates essential for accurate modeling. Similarly, in speech recognition, the acoustic environment can change, and the speaker's voice characteristics can evolve, necessitating a model that can adapt to these variations.

The estimation of FAN-HMMs involves more complex algorithms compared to standard HMMs, as the feedback and non-homogeneity introduce additional layers of complexity. Techniques such as the Expectation-Maximization (EM) algorithm and Bayesian methods are commonly employed to estimate the parameters of FAN-HMMs. The EM algorithm iteratively refines the estimates of the transition and emission probabilities by alternating between the E-step, where the expected values of the hidden states are computed, and the M-step, where the parameters are updated to maximize the likelihood. Bayesian methods, on the other hand, provide a principled way to incorporate prior knowledge and handle uncertainty in the parameter estimates. Despite the increased computational demands, the enhanced modeling capabilities of FAN-HMMs make them a valuable tool for analyzing a wide range of temporal data, offering more accurate and reliable insights into the underlying processes.

### 5.3.3 Simulation Experiments for Model Estimation
Simulation experiments play a crucial role in evaluating the performance of model estimation techniques, particularly in the context of causal inference and longitudinal data analysis [2]. These experiments allow researchers to systematically explore the behavior of estimators under controlled conditions, providing insights into their accuracy, precision, and robustness. By generating synthetic datasets that mimic real-world scenarios, simulation studies can help identify the strengths and weaknesses of different estimation methods, thereby guiding the selection of appropriate techniques for specific applications. For instance, in the context of linear instrumental variable (IV) and panel data models, simulation experiments have been used to assess the impact of omitted variable bias and the validity of the zero conditional mean error assumption.

In the specific case of the S3L (Structural Stability Selection for Longitudinal Data) framework, simulation experiments are essential for validating the multi-objective optimization and stability selection procedures used to estimate causal structures [23]. These experiments typically involve generating data from known causal models with varying levels of complexity, including different numbers of time slices, types of noise, and degrees of confounding. The performance of S3L is then evaluated by comparing the estimated causal structures to the true underlying models [23]. Key metrics of interest include the accuracy of the estimated causal relationships, the stability of the selected models across multiple runs, and the computational efficiency of the algorithm. Simulation studies have shown that S3L can effectively recover causal structures even in the presence of high-dimensional data and complex temporal dependencies, making it a valuable tool for causal inference in longitudinal settings.

Moreover, simulation experiments are indispensable for assessing the performance of model-based inference methods in scenarios with limited data, such as those involving few treated units or short time periods. These studies often focus on evaluating the robustness of estimators to violations of key assumptions, such as independence and identically distributed (iid) samples, linear relationships, and additive Gaussian noise. By systematically varying these assumptions, researchers can gain a deeper understanding of the conditions under which different methods are most reliable. For example, simulations have demonstrated that certain design-based approaches, while conceptually distinct from model-based methods, share similar limitations when applied to small sample sizes or sparse data. This insight underscores the importance of carefully considering the trade-offs between different estimation strategies and the specific characteristics of the data at hand.

# 6 Future Directions


The current landscape of causal inference for longitudinal and panel data, while rich with advanced statistical and machine learning techniques, still faces several limitations and gaps. One major limitation is the assumption of no unmeasured confounding, which is often difficult to justify in real-world settings. Additionally, the handling of non-random missingness and time-varying covariates remains challenging, particularly in settings with complex spatiotemporal dependencies. Existing methods, while powerful, often require strong assumptions about the data-generating process, which may not always hold in practice. Furthermore, the integration of causal inference with deep learning and Bayesian methods, while promising, is still in its early stages and lacks comprehensive theoretical foundations and practical guidelines.

To address these limitations, several directions for future research are proposed. First, developing methods that can more effectively handle unmeasured confounding is crucial. This could involve the integration of external data sources, such as administrative records or social media data, to enrich the covariate set and reduce the risk of omitted variable bias. Additionally, the use of instrumental variables (IVs) and negative control outcomes can help to identify and adjust for unmeasured confounders. Second, there is a need for more robust methods to handle non-random missingness and time-varying covariates. This could involve the development of advanced imputation techniques and the extension of existing methods, such as doubly robust estimators, to better account for these complexities. Third, the integration of causal inference with deep learning and Bayesian methods should be further explored. This could involve the development of hybrid models that combine the strengths of deep learning for capturing complex patterns with the robustness of Bayesian methods for uncertainty quantification. Finally, the development of methods for causal inference in spatiotemporal settings, particularly those that can handle arbitrary spillover and carryover effects, is an important area for future research. This could involve the extension of existing models, such as feedback-augmented non-homogeneous hidden Markov models (FAN-HMMs), to more complex and realistic scenarios.

The potential impact of these proposed future directions is significant. Addressing the limitations of current methods can lead to more reliable and robust causal inferences, which are essential for making informed decisions in fields such as healthcare, economics, and policy evaluation. For example, more effective handling of unmeasured confounding can improve the accuracy of treatment effect estimates, leading to better-targeted interventions and policies. Robust methods for handling non-random missingness and time-varying covariates can enhance the reliability of longitudinal studies, providing more accurate insights into the dynamics of complex systems. The integration of causal inference with deep learning and Bayesian methods can open new avenues for modeling complex causal relationships, particularly in settings with high-dimensional and heterogeneous data. Finally, the development of methods for causal inference in spatiotemporal settings can provide valuable insights into the spread of diseases, the impact of environmental policies, and the dynamics of social and economic phenomena, ultimately contributing to more effective and evidence-based decision-making.

# 7 Conclusion



The survey paper provides a comprehensive overview of the latest advancements in causal modeling for longitudinal and panel data, focusing on the integration of advanced statistical methods and machine learning techniques. Key findings include the robustness and accuracy of doubly robust estimators, which combine outcome regression and propensity score weighting to address potential biases and improve the reliability of causal estimates. The paper also highlights the importance of panel clustering and de-biased estimation in identifying subgroups with similar treatment effects, enhancing the accuracy of causal inferences. Additionally, the integration of quasi-experimental designs, such as difference-in-differences and synthetic control methods, with machine learning techniques is discussed, emphasizing their flexibility and robustness in handling complex settings with staggered treatment adoption and network interference. The survey further explores the application of Bayesian inference and deep learning methods to causal inference, particularly in the context of spatiotemporal data and hidden Markov models, which are crucial for capturing the intricate dynamics of such data.

The significance of this survey lies in its comprehensive review and synthesis of the latest developments in causal modeling for longitudinal and panel data. By providing a unified framework for understanding the integration of advanced statistical and machine learning techniques, the survey aims to guide researchers and practitioners in selecting the most appropriate methods for their specific applications. This is particularly important in fields such as economics, epidemiology, and social sciences, where the ability to accurately estimate causal effects is essential for making informed decisions and designing effective interventions. The survey also addresses the challenges posed by non-random missingness, time-varying confounders, and spatiotemporal dependencies, offering practical solutions and methodological insights that can enhance the robustness and reliability of causal inferences in complex data environments.

Finally, this survey calls for continued research and innovation in causal inference methods, particularly in the areas of spatiotemporal data and hidden Markov models. The development of more flexible and robust methods for handling arbitrary spillover and carryover effects, as well as the integration of feedback mechanisms in non-homogeneous hidden Markov models, represents a promising direction for future work. Additionally, there is a need for more comprehensive empirical evaluations of these methods to assess their performance in real-world settings, particularly in studies with complex time-dependent covariates and small sample sizes. By addressing these challenges, researchers can further advance the field of causal inference and contribute to more reliable and actionable insights in a wide range of empirical applications.

# References
[1] Spatiotemporal causal inference with arbitrary spillover and carryover  effects  
[2] Exact Simulation of Longitudinal Data from Marginal Structural Models  
[3] Heterogeneous Treatment Effects in Panel Data  
[4] PanelMatch  Matching Methods for Causal Inference with Time-Series  Cross-Section Data  
[5] Difference-in-Differences Meets Synthetic Control  Doubly Robust  Identification and Estimation  
[6] A bracketing relationship between difference-in-differences and  lagged-dependent-variable adjustmen  
[7] Double-Robust Estimation in Difference-in-Differences with an  Application to Traffic Safety Evaluat  
[8] Inferring Treatment Effects in Large Panels by Uncovering Latent  Similarities  
[9] Positivity violations in marginal structural survival models with  time-dependent confounding  a sim  
[10] Large Dimensional Latent Factor Modeling with Missing Observations and  Applications to Causal Infer  
[11] Causal Data Fusion for Panel Data without Pre-Intervention Period  
[12] Semiparametric Triple Difference Estimators  
[13] Double Machine Learning for Static Panel Models with Fixed Effects  
[14] OpportunityFinder  A Framework for Automated Causal Inference  
[15] Doubly Robust Proximal Synthetic Controls  
[16] Identification and Inference for Synthetic Control Methods with  Spillover Effects  Estimating the E  
[17] Difference-in-Differences under Bipartite Network Interference  A  Framework for Quasi-Experimental  
[18] An Exact and Robust Conformal Inference Method for Counterfactual and  Synthetic Controls  
[19] Forecasting Algorithms for Causal Inference with Panel Data  
[20] Dynamical systems theory for causal inference with application to  synthetic control methods  
[21] Spectral Dynamic Causal Modelling of Resting-State fMRI  Relating  Effective Brain Connectivity in t  
[22] Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal  Causal Inference  
[23] Causality on Longitudinal Data  Stable Specification Search in  Constrained Structural Equation Mode  