# 5/1/2025, 6:20:35 PM_Causal Modeling for Longitudinal and Panel Data  

# 0. Causal Modeling for Longitudinal and Panel Data  

# 1. Introduction  

Longitudinal data, often referred to as panel data or cross-sectional time-series data, comprises observations collected on the same individuals or entities across multiple time points [10,14,17,24,34]. This structure combines the characteristics of both cross-sectional and time-series data, providing richer information, increased degrees of freedom, and improved estimation efficiency compared to purely cross-sectional or time-series analyses [17]. Panel data captures temporal dynamics, allowing researchers to observe and analyze changes over time, while also accounting for individual heterogeneity, which refers to unobserved, time-invariant characteristics that differ across entities [14,17,23,24]. Panel datasets can be balanced, where every individual has observations at every time point, or unbalanced, with missing observations for some individuals at certain times [17,23].​  

Establishing causality is a fundamental objective in scientific research and data analysis, aiming to determine if a change in one phenomenon directly causes a change in another, as opposed to mere correlation [5,13,21]. In the context of longitudinal and panel data, causal inference is particularly crucial for deriving informed insights, evaluating the effectiveness of interventions, understanding dynamic relationships, and guiding policy decisions across diverse fields such as economics, finance, public policy, social sciences, and biostatistics [10,14,21,24,26,27,28]. For instance, it is vital for assessing the impact of government policies or analyzing user behavior in industry data [24,26,28].  

![](images/cbeca6bafe5554a418c8e0eed9edf26cd279f20f7f5ae6ea95b1056ecbac3e6e.jpg)  

![](images/3e65594299cd4b5fa3647e5cef283bda57fca21ced2885485f97d3e167838f9f.jpg)  

Despite the advantages offered by longitudinal data, inferring causality presents significant challenges, particularly when working with observational data where randomized experiments are infeasible or unethical [5,11,33]. A primary challenge distinguishing genuine causal effects from spurious associations driven by confounding [5,13,21]. Unobserved heterogeneity, specifically time-invariant confounders that are not measured but influence both the treatment and the outcome, poses a substantial threat to causal inference [2,14,17,23,24]. Furthermore, endogeneity, which occurs when explanatory variables are correlated with the error term often due to omitted variables or reverse causality, must be addressed for valid causal claims [2,34]. Longitudinal data structures also introduce complexities such as time-varying confounders that change over time and potentially mediate or modify treatment effects, and feedback loops where outcomes from one period influence treatments or covariates in subsequent periods. Successfully navigating these challenges requires careful consideration of causal assumptions, such as consistency, exchangeability (conditional independence), and positivity, particularly in observational settings [33].  

To address these methodological challenges, a range of causal modeling approaches has been developed for longitudinal and panel data. These methods leverage the temporal and cross-sectional dimensions of the data to enhance the ability to identify causal effects. Key approaches include Difference-in-Differences (DID) strategies, which are effective for evaluating the impact of interventions on treated groups compared to control groups over time [26,28]. Fixed effects models are commonly used to control for time-invariant unobserved heterogeneity by analyzing within-individual variations [2,24]. Instrumental variables (IV) methods are employed to handle endogeneity when valid instruments can be identified, although challenges exist in finding relevant and valid instruments, especially in high-dimensional settings [2,3,14]. Quasiexperimental methods adapt designs from experimental settings to observational data by exploiting natural experiments or policy variations [2,5]. Econometric panel models offer frameworks for estimating linear relationships and conducting robust inference while accounting for panel-specific issues [23]. More advanced techniques, including Bayesian causal models [7] and approaches from machine learning integrated with quasi-experimental designs, are also emerging to tackle complex data structures and confounding patterns [5].​  

This survey aims to provide a comprehensive overview of key causal modeling approaches applicable to longitudinal and panel data. It will define these data structures, elaborate on the unique challenges they pose for causal inference, and systematically review the methodologies developed to overcome these obstacles. The subsequent sections are organized as follows: Section 1 defines longitudinal and panel data and their characteristics. Section 2 details the core challenges for causal inference, including confounding, endogeneity, time-varying factors, and feedback loops. Section 3 surveys various causal modeling techniques, analyzing their underlying assumptions, strengths, and limitations in the context of longitudinal data. Finally, Section 4 concludes with a summary and potential directions for future research.  

# 2. Longitudinal and Panel Data Basics  

Longitudinal or panel data represents a dataset structure where observations are collected on the same units (individuals, firms, countries, etc.) across multiple time periods [14,24]. This structure combines features of both cross-sectional data (multiple units at a single time point) and time-series data (a single unit across multiple time points), offering richer information [9].  

Panel data can be categorized into balanced and unbalanced panels [14,17]. A balanced panel contains observations for all units at all time periods, resulting in a rectangular data structure. In contrast, an unbalanced panel has missing observations for some units at certain time periods, leading to an irregular structure [14,17]. The general form of a linear panel data model is often expressed as  

$$
Y _ { i t } = X _ { i t } ^ { \prime } \beta + u _ { i t } ,
$$  

where $Y _ { i t }$ is the dependent variable for unit $\mathbf { \chi } _ { i }$ at time $t$ , $X _ { i t }$ ​ is a vector of explanatory variables, $\beta$ is a vector of coefficients, and $u _ { i t }$ ​ is the error term [23].  

A fundamental challenge in analyzing panel data, particularly in causal inference, is addressing unobserved heterogeneity [24]. This refers to unmeasured characteristics or factors specific to each individual unit that might influence the outcome variable and potentially be correlated with the explanatory variables. If not properly accounted for, unobserved heterogeneity can lead to omitted variable bias and inconsistent parameter estimates [14,34].​  

To address this challenge, several econometric models are employed. The Fixed Effects (FE) model is a cornerstone approach designed to control for time-invariant unobserved heterogeneity [17,24]. The core assumption is that these unobserved individual characteristics, denoted as $\alpha _ { i }$ , are constant over time for each unit but can vary across units, and importantly, may be correlated with the regressors [4,14]. The basic FE specification is  

$$
Y _ { i t } = \alpha _ { i } + \beta X _ { i t } + \epsilon _ { i t } .
$$  

The FE model eliminates the time-invariant $\alpha _ { i }$ ​ through data transformations, allowing consistent estimation of $\beta$ [24,34]. Common methods include the within transformation (de-meaning), which subtracts the individual's mean over time from each observation, and the first-difference transformation, which subtracts the lagged observation [4]. This allows the FE model to identify the effects of time-varying predictors based on within-individual variation [30]. However, a limitation is that the FE model cannot estimate the effects of time-invariant variables, as they are removed in the transformation [4,24].  

In contrast, the Random Effects (RE) model treats the unobserved individual-specific effects, often denoted as $\mu _ { i }$ ​ , as random variables that are uncorrelated with the explanatory variables [17,34]. The basic RE model is  

$$
Y _ { i t } = \alpha + \beta X _ { i t } + \mu _ { i } + \epsilon _ { i t } ,
$$  

where $\mu _ { i }$ is the random individual effect and $\alpha$ is a grand mean intercept [4]. The key assumption distinguishing RE from FE is the uncorrelatedness of $\mu _ { i }$ and $X _ { i t }$ ​ [4,32]. Under this assumption, the RE model can estimate the effects of both timevarying and time-invariant variables. Estimation is typically performed using Generalized Least Squares (GLS) or Feasible Generalized Least Squares (FGLS) to account for the correlation in the error term  

$$
u _ { i t } = \mu _ { i } + \epsilon _ { i t }
$$  

induced by the random effect [4,23,34]. While potentially more efficient than FE when its assumptions hold, the RE estimator is inconsistent if $\mu _ { i }$ is correlated with $X _ { i t }$ ​ [4].  

Mixed Effects (ME) models, also known as hierarchical linear models or multilevel models, provide a flexible framework that can encompass aspects of both FE and RE [17]. These models are suitable for data with nested structures, such as individuals nested within groups or observations nested within individuals over time. They allow for the estimation of both fixed effects (parameters constant across individuals) and random effects (parameters that vary randomly across individuals, often assumed to follow a distribution) [17]. For panel data, this can involve specifying individual-specific random intercepts or even random slopes for covariates, allowing for variation in relationships across units.  

![](images/97b454f4eea5132302966a19269e7efc20842c19202f8c5a0c7cdc2890187c9c.jpg)  

![](images/a577a48e618dd2f850cf8b4110e5452028b0e2fe3e508a819f1067beff2883a3.jpg)  

The choice between FE and RE models is crucial and hinges on the assumption regarding the correlation between the unobserved individual effects and the regressors [34]. The FE model is consistent even if this correlation exists, but it cannot estimate time-invariant effects. The RE model is more efficient if the correlation does not exist and can estimate timeinvariant effects, but it is inconsistent if the correlation is present [4]. This trade-off between consistency and efficiency is often formally evaluated using the Hausman test. The Hausman test compares the coefficient estimates of the FE and RE  

models; if they differ significantly, it suggests that the unobserved effects are correlated with the regressors, favoring the FE model for consistency [34]. If they do not differ significantly, the RE model is preferred for its efficiency [34].  

# 2.1 Fixed Effects (FE) Model  

The Fixed Effects (FE) model is a widely used technique for analyzing panel data, particularly when addressing concerns about unobserved heterogeneity [17,24]. The core assumption of the FE model is that unobserved characteristics specific to each individual or cross-sectional unit are constant over time and may be correlated with the explanatory variables [4,14,23,34]. These time-invariant unobservable factors are treated as individual-specific parameters, denoted as $\alpha _ { i }$ ​ , that need to be accounted for in the estimation [4,23]. The basic structural form of the FE model is typically represented as:​  

$$
Y _ { i t } = \alpha _ { i } + \beta X _ { i t } + \epsilon _ { i t }
$$  

where $Y _ { i t }$ is the dependent variable for individual $\mathbf { \chi } _ { i }$ at time $t$ , $X _ { i t }$ ​ is a vector of explanatory variables, $\alpha _ { i }$ is the timeinvariant individual fixed effect, $\beta$ is the vector of regression coefficients for the time-varying covariates, and $\epsilon _ { i t }$ ​ is the idiosyncratic error term [17]. This specification allows for different intercepts $\left( \alpha _ { i } \right)$ across individuals but assumes common slopes $( \beta )$ [32]. The primary goal is to control for these unobserved individual-specific effects to mitigate omitted variable bias stemming from time-invariant confounders [14,24,34].​  

The strength of the FE model lies in its ability to eliminate these time-invariant unobserved factors through data transformations prior to estimation [24,34]. Two principal estimation methods achieve this: the within transformation (or de-mean method) and the first-difference transformation [4].​  

The within transformation involves subtracting the time-averaged value for each variable for individual $\mathbf { \chi } _ { i }$ from the observation at time $t$ [4,30,32]. The mean over time for individual $\mathbf { \chi } _ { i }$ is $\bar { Y } _ { i } = \alpha _ { i } + \beta \bar { X } _ { i } + \bar { \epsilon } _ { i }$ , where $\bar { Y _ { i } } = \frac { 1 } { T } \sum _ { t = 1 } ^ { T } Y _ { i t }$ ​ and similarly for $X _ { i t }$ ​ and $\epsilon _ { i t }$ ​ [4]. Subtracting this mean equation from the original model yields the transformed equation:  

$$
Y _ { i t } - { \bar { Y } } _ { i } = \beta ( X _ { i t } - { \bar { X } } _ { i } ) + ( \epsilon _ { i t } - { \bar { \epsilon } } _ { i } )
$$  

or​  

$$
y _ { i t } ^ { * } = \beta x _ { i t } ^ { * } + \epsilon _ { i t } ^ { * }
$$  

where $y _ { i t } ^ { * } = Y _ { i t } - { \bar { Y } } _ { i }$ , $x _ { i t } ^ { * } = X _ { i t } - \bar { X } _ { i }$ , and $\epsilon _ { i t } ^ { * } = \epsilon _ { i t } - \bar { \epsilon } _ { i }$ [4]. Crucially, the time-invariant individual effect $\alpha _ { i }$ is removed in this process $\mathbf { \Phi } ^ { \prime } \alpha _ { i } - \alpha _ { i } = 0$ ), allowing consistent estimation of $\beta$ using OLS on the demeaned data [23]. This method utilizes the time variation within each individual unit [30].  

The first-difference transformation provides an alternative approach, particularly suitable for panels with only two time periods or short time series [4]. This method involves subtracting the observation at time $t - 1$ from the observation at time $t$ for each individual:​  

$$
Y _ { i t } - Y _ { i , t - 1 } = \beta ( X _ { i t } - X _ { i , t - 1 } ) + ( \epsilon _ { i t } - \epsilon _ { i , t - 1 } )
$$  

This transformation also eliminates the fixed effect $\alpha _ { i }$ (since $\alpha _ { i } - \alpha _ { i } = 0$ ) and allows estimation of $\beta$ from the differenced data [4]. Conceptually, the Least Squares Dummy Variable (LSDV) method, which includes a dummy variable for each individual, also estimates the FE model, providing the same slope coefficients as the within estimator when the time dimension is sufficient [30,32].  

<html><body><table><tr><td>Aspect</td><td>Descriptio n</td><td>Key Assumptio n</td><td>Model Form</td><td>Estimation Methods</td><td>Strengths</td><td>Limitation S</td></tr><tr><td>Definition</td><td>Controls for time- invariant unobserve d heterogen eity (individual effects).</td><td>Individual effects correlated with regressors.</td><td>Yit= αi+ βXit+ eit</td><td>Within Transform ation (De- meaning), First- Difference, LSDV</td><td>Controls time- invariant confoundi ng, Consistent estimates for time-</td><td>Cannot estimate time- invariant effects, Sensitive to measurem ent error,</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td></td><td></td><td></td><td>varying effects.</td><td>Assumes no time- varying</td></tr></table></body></html>  

<html><body><table><tr><td>Aspect</td><td>Descriptio n</td><td>Key Assumptio n</td><td>Model Form</td><td>Estimation Methods</td><td>Strengths</td><td>Limitation S</td></tr><tr><td>Definition</td><td>Controls for time- invariant unobserve d heterogen eity (individual effects).</td><td>Individual effects correlated with regressors.</td><td>Yit= αi+ βXit+ Eit</td><td>Within Transform ation (De- meaning), First- Difference, LSDV</td><td>Controls time- invariant confoundi Consistent estimates for time- varying effects. ng,</td><td>Cannot estimate time- invariant effects, Sensitive to measurem ent error, Assumes no time- varying confoundi ng.</td></tr></table></body></html>  

By effectively removing time-invariant confounding, FE models are valuable for estimating causal effects of time-varying variables [4,24]. They isolate the impact of changes in explanatory variables withinan individual over time, controlling for any stable, unmeasured individual characteristics that might otherwise bias results [22,24]. This makes them suitable for analyzing how changes in a time-varying treatment or exposure affect an outcome, net of stable confounders like inherent ability, background, or fixed environmental factors [24]. For instance, they can be used to estimate the causal effects of stressors on perceived stress, accounting for stable personal traits [22]. The Difference-in-Differences (DID) approach, a cornerstone for policy evaluation and causal inference in observational studies, is fundamentally based on the FE estimator [25]. An illustrative example shows how accounting for individual-specific effects via de-meaning can reveal a stronger correlation between time-varying variables, such as reminder frequency and healthy eating scores, by controlling for hidden stable factors [32]. Model selection between FE and pooled OLS or Random Effects models is often guided by tests like the Ftest or Hausman test, evaluating the significance of the individual effects or the correlation between effects and regressors [4].​  

Despite their considerable strengths in handling time-invariant heterogeneity, FE models have notable limitations. A significant drawback is that they cannot estimate the effects of any variable that is constant over time for a given individual [4,24]. Such variables are eliminated during the de-meaning or differencing transformations, making their coefficients unidentifiable [24]. This precludes analyzing the impact of time-invariant characteristics like gender, ethnicity, or stable aspects of education using the FE model directly [24]. Furthermore, while FE models address time-invariant confounding, they do notcontrol for unobserved confounders that vary over time [22]. If such time-varying confounders exist and are correlated with both the time-varying regressor and the outcome, the FE estimates may still be biased. The causal interpretation also typically assumes no feedback loops, meaning past outcomes should not directly influence current explanatory variables, and past explanatory variables should not directly influence current outcomes beyond their direct effects [22]. Additionally, FE estimates can be sensitive to measurement error in the time-varying explanatory variables, which may be amplified by the within transformation. Finally, sufficient within-individual variability in the explanatory variables is necessary for their effects to be precisely estimated [22].  

# 2.2 Random Effects (RE) Model  

The Random Effects (RE) model is an econometric approach for analyzing panel data, which assumes that unobserved individual heterogeneity can be represented by random variables [17,34]. Specifically, it treats individual-specific effects as  

random and assumes they are uncorrelated with the explanatory variables [17,23,34]. This contrasts with the fixed effects model, where individual effects are treated as fixed parameters. The core assumption of the RE model is that the individualspecific random effect $( \mu _ { i } )$ is independent of the regressors ( $X _ { i t }$ ) [4,32].  

The basic form of the Random Effects model is typically expressed as:  

$$
Y _ { i t } = \alpha + \beta X _ { i t } + u _ { i t }
$$  

where $u _ { i t } = \mu _ { i } + \epsilon _ { i t }$ [4,17]. In this formulation, $Y _ { i t }$ is the dependent variable for individual $\mathbf { \chi } _ { i }$ at time $t$ , $X _ { i t }$ ​ is a vector of explanatory variables, $\alpha$ is the constant term, $\beta$ is the vector of regression coefficients, $\mu _ { i }$ is the individual-specific random effect, and $\epsilon _ { i t }$ ​ is the idiosyncratic error term, assumed to be independent and identically distributed [4]. The presence of the individual random effect $\mu _ { i }$ in the composite error term $u _ { i t }$ ​ introduces correlation across the error terms for a given individual over time [4,23].  

Due to the complex covariance structure induced by $\mu _ { i }$ , applying Ordinary Least Squares (OLS) directly to the RE model will yield consistent but inefficient parameter estimates [4,23]. The OLS estimator is no longer the Best Linear Unbiased Estimator (BLUE) because the assumption of independent and identically distributed errors is violated [4]. To address this inefficiency, the Generalized Least Squares (GLS) method, or more commonly, Feasible Generalized Least Squares (FGLS), is typically employed for parameter estimation [4,23,34]. FGLS provides more efficient estimators compared to OLS by explicitly modeling the covariance structure of the error terms [23].  

The RE model is suitable for applications where the individual-specific effects are considered random draws from a larger population, and the primary interest lies in estimating the average effect across this population [32]. A key advantage of the RE model over the Fixed Effects (FE) model is its ability to estimate the coefficients of time-invariant explanatory variables, which are otherwise absorbed into the individual effects in the FE framework. By integrating both within-individual and between-individual variation, the RE model can offer a more complete picture when its underlying assumption holds [32].  

However, the critical limitation of the Random Effects model lies in its core assumption: that the individual random effect ( $\mu _ { i }$ ) is uncorrelated with the explanatory variables $( X _ { i t }$ ​ ) [4,34]. If this assumption is violated, meaning there is a correlation between the unobserved individual characteristics and the regressors, the RE estimator becomes inconsistent and biased [4]. This potential endogeneity issue requires careful consideration, and diagnostic tests, such as the Hausman test, are often used to formally compare the RE and FE models based on whether this assumption is likely to hold.  

# 2.3 Mixed Effects (ME) Model  

The Mixed Effects (ME) model represents a statistical framework designed to analyze data with nested or clustered structures, such as longitudinal or panel data where observations are grouped by individuals over time. This approach is particularly useful for modeling outcomes that are influenced by both population-level factors (fixed effects) and individualspecific variations (random effects). The ME model is often presented as combining the strengths of both Fixed Effects (FE) and Random Effects (RE) models [17]. Specifically, it aims to control for individual-specific constant terms while simultaneously accounting for individual-specific random deviations from the overall mean [17].  

The basic structure of a Mixed Effects model for panel data can be represented as:  

$$
Y _ { i t } = \alpha _ { i } + \beta X _ { i t } + u _ { i } + \epsilon _ { i t }
$$  

In this formulation, $Y _ { i t }$ ​ denotes the dependent variable for individual $\mathbf { \chi } _ { i }$ at time $t$ . The term $\alpha _ { i }$ represents the individualspecific constant term, capturing time-invariant heterogeneity that is treated as a fixed effect or absorbed into the individual intercept [17]. $\beta$ represents the regression coefficient(s) associated with the explanatory variable(s) $X _ { i t }$ ​ , indicating the fixed effects of these covariates on the outcome [17]. The component $u _ { i }$ ​ signifies the individual-specific random effect, which captures unobserved heterogeneity specific to individual $\mathbf { \chi } _ { i }$ that is assumed to be randomly sampled from a distribution, typically a normal distribution with a mean of zero [17]. Finally, $\epsilon _ { i t }$ ​ is the idiosyncratic error term, representing the remaining unexplained variance that varies across individuals and over time [17].  

This structure allows the model to estimate common effects across the population $( \beta )$ while simultaneously permitting individuals to have their own baseline levels $\left( \alpha _ { i } \right)$ and deviations from predicted values $\left( u _ { i } \right)$ , thereby accommodating the inherent dependency within individual trajectories in longitudinal settings.  

# 2.4 Hausman Test for Model Selection  

The Hausman test is a widely utilized statistical procedure in panel data analysis for selecting between the fixed effects (FE) model and the random effects (RE) model . This choice is critically dependent on the assumption regarding the correlation between the unobserved individual-specific effects and the explanatory variables . The random effects model inherently assumes that these unobserved effects are uncorrelated with the covariates, which, if true, allows the RE estimator to be consistent and efficient . In contrast, the fixed effects model accommodates the possibility of correlation between the unobserved effects and regressors, providing consistent estimates even when the RE assumption is violated, although typically at the cost of efficiency compared to a valid RE estimator .​  

The primary function of the Hausman test is to formally evaluate this critical assumption of uncorrelatedness . The null hypothesis $\left( H _ { 0 } \right)$ for the Hausman test states that the unobserved individual-specific effects are uncorrelated with the explanatory variables ( $\mathrm { C o v } ( \mu _ { i } , X _ { i t } ) = 0 .$ ) . Under this null hypothesis, both the fixed effects and random effects estimators are consistent, but the random effects estimator is also efficient, making it the preferred choice . The alternative hypothesis ( $H _ { 1 }$ ​ ) is that a correlation exists between the individual effects and the explanatory variables ( $\mathrm { C o v } ( \mu _ { i } , X _ { i t } ) \neq 0$ ) . If the alternative hypothesis holds, the random effects estimator is inconsistent, whereas the fixed effects estimator remains consistent .​  

Performing the Hausman test involves comparing the coefficient estimates obtained from both the FE and RE models . The specific steps include estimating the fixed effects model to obtain $\hat { \beta } _ { F E }$ ​ and estimating the random effects model to obtain $\hat { \beta } _ { R E }$ . A test statistic is then calculated based on the difference between these estimates . A common representation of the Hausman test statistic is:​  

$$
H = ( b _ { F E } - b _ { R E } ) ^ { \prime } \Sigma ^ { - 1 } ( b _ { F E } - b _ { R E } )
$$  

or, equivalently,  

# 无效公式  

where $b _ { F E }$ ​ and $b _ { R E }$ ​ represent the parameter estimates from the fixed and random effects models, respectively, and $\Sigma$ (or the appropriate covariance difference matrix) is related to the difference in their covariance matrices . Under the null hypothesis, this statistic asymptotically follows a chi-square distribution with degrees of freedom equal to the number of explanatory variables .​  

The interpretation of the test result dictates the appropriate model choice . If the calculated test statistic does not exceed the critical value from the chi-square distribution at the chosen significance level (i.e., the null hypothesis $H _ { 0 }$ is not rejected), it indicates insufficient evidence to conclude that the unobserved effects are correlated with the explanatory variables . In this case, the assumption of the random effects model is considered valid, and the RE model is preferred due to its relative efficiency . Conversely, if the test statistic is statistically significant (i.e., the null hypothesis $H _ { 0 }$ ​ is rejected), it provides evidence that the unobserved effects are correlated with the explanatory variables . Consequently, the random effects estimator is inconsistent, and the fixed effects model must be chosen to obtain consistent estimates of the parameters .  

The Hausman test thus provides a data-driven basis for navigating the trade-off between potential efficiency and consistency when modeling panel data.  

# 3. Addressing Endogeneity in Panel Data  

![](images/781d5a8bafeebd85b84d95477626dfa1186d0a31a743b2ced8b99924d4ebd7a8.jpg)  

Endogeneity represents a fundamental challenge in causal inference, particularly in observational studies using longitudinal and panel data. It arises when an explanatory variable is correlated with the error term in a regression model, violating a core assumption of ordinary least squares (OLS) estimation [1,2,23]. This correlation typically stems from unobserved factors that influence both the explanatory variable and the outcome, or from structural features of the model itself, leading to biased and inconsistent OLS estimates [1,2].​  

Common sources of endogeneity include omitted variable bias, where relevant confounders are not included in the model [5,6,13,22,27], reverse causality, measurement error, and the inclusion of lagged dependent variables in dynamic models [6,22,27]. In panel data, while fixed effects models can address endogeneity caused by unobserved, time-invariant confounders and certain forms of reverse causality [24,34], endogeneity can persist due to time-varying omitted variables or the inherent structure of dynamic processes where lagged outcomes are correlated with subsequent error terms [34].  

Addressing endogeneity is crucial for obtaining valid causal inferences. A range of econometric techniques are employed for this purpose. Instrumental Variables (IV) methods are a primary approach designed to handle unobserved confounding and other sources of endogeneity by leveraging an exogenous variable that influences the endogenous regressor but affects the outcome only through that regressor [1,2,3,5,6,13,14,21,27,31,33]. For dynamic panel data models, where the lagged dependent variable creates specific endogeneity challenges, Generalized Method of Moments (GMM) techniques, such as  

Difference GMM and System GMM, are widely used to provide consistent estimates [23,35]. Additionally, methods like fixed effects, as noted, address time-invariant unobserved heterogeneity, and various quasi-experimental designs applied to observational data, such as difference-in-differences or regression discontinuity, offer strategies to identify causal effects under specific assumptions by exploiting exogenous variation [5]. The following sections delve into the details of Instrumental Variables and Generalized Method of Moments estimation as key methods for addressing endogeneity in panel data contexts.​  

# 3.1 Instrumental Variables (IV)  

nstrumental Variables (IV) methods are widely employed in causal inference to address the challenge of endogeneity, which arises when an explanatory variable is correlated with the error term, often due to unmeasured confounding [13,21].  

![](images/47420ed4cec14a7623e293b20a7b5d604f2b0152d8fd3172e5de96bc48d913a3.jpg)  

The core intuition behind IV is to find an exogenous variable, termed an instrumental variable (Z), that is correlated with the endogenous regressor (X) but is uncorrelated with the unobserved confounders or the error term (u or $\epsilon$ ) [1,2,6]. This instrument Z effectively isolates the exogenous variation in X, which can then be used to estimate the causal effect of X on the outcome variable (Y) [5]. The IV estimator is fundamentally derived from the ratio of the covariance between the instrument and the outcome to the covariance between the instrument and the endogenous variable: \​  

[2]. The key requirement is that the instrument influences the outcome only through its effect on the endogenous variable [2,5,34].  

Valid IV estimation relies on several crucial assumptions [5,6]:  

1. Relevance: The instrument Z must be sufficiently correlated with the endogenous variable X \​ [6]. A weak correlation violates this condition, leading to the "weak instrument" problem [1,6].  

1. Exogeneity: The instrument Z must be uncorrelated with the error term or any unmeasured confounders that affect the outcome \ [2,6]. This means the instrument should not be confounded with other factors influencing the outcome [5,13].  

1. Exclusion Restriction: The instrument Z must affect the outcome Y only through its effect on the endogenous variable [2,5,34]. This assumption is critical and often difficult to satisfy in practice [31].  

For certain interpretations like the Average Treatment Effect (ATE) or Local Average Treatment Effect (LATE), an additional assumption of monotonicity may be required, implying no individuals receive the reverse treatment when affected by the instrument [5].​  

Satisfying the exogeneity and exclusion restriction assumptions presents significant challenges, as researchers must often rely on prior knowledge and theoretical arguments which are not always empirically verifiable [13]. The selection of valid instruments is considered a challenging area of research [13].  

<html><body><table><tr><td>xtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivreg</td></tr><tr><td>xtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivreg;</td></tr></table></body></html>  

<html><body><table><tr><td>tivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxt</td></tr><tr><td>ivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxti</td></tr><tr><td>vregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtiv regxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtivregxtiv</td></tr><tr><td>regivreg' automatically adjust standard errors in panel data estimation [31].</td></tr><tr><td>firstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstf</td></tr><tr><td></td></tr><tr><td>irstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfir</td></tr><tr><td>stfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirst</td></tr><tr><td>firstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirstfirst in</td></tr><tr><td>xtivreg g' isalsoinformative[31].Methodsfor corecting bias due to weak instruments areanactive areaofresearch,</td></tr></table></body></html>  

Testing for instrument validity, particularly exogeneity, is crucial. When multiple instruments are available (an overidentified model), the Sargan test (or Hansen test, which is robust to heteroskedasticity) can be used to test the joint validity of the instruments by examining the correlation between the instruments and the residuals from the second-stage regression [2,31,34]. However, exogeneity is generally untestable in the just-identified case (number of instruments equals the number of endogenous variables) [2]. The Hausman test is commonly used to compare the IV estimates with OLS estimates; a statistically significant difference between them suggests the presence of endogeneity in the original equation and supports the use of IV [2,34]. A significant p-value from the Hausman test indicates that the assumptions underlying the OLS estimator are violated, lending credence to the IV approach [34].  

Selecting appropriate and valid instruments, especially in panel data settings with potential dynamic effects or complex structures, remains a key challenge [13]. Guidance for selecting instruments in panel data emphasizes ensuring the instrument is theoretically exogenous and significantly related to the endogenous variable [34]. With the availability of large datasets, data-driven methods are being explored to identify valid instruments from a pool of many candidates, addressing issues of irrelevant or invalid instruments through techniques like resampling [3,40]. Sensitivity analysis, such as frameworks based on omitted variable bias, can also be used to assess the robustness of IV results to potential violations of assumptions [27]. Examples like using search queries in recommendation systems [1] or season of birth in educational studies [5] illustrate the application of IVs, while studies applying IV methods in areas like finance [21] and health (e.g., Mendelian Randomization [27,40]) highlight their diverse utility and associated methodological challenges. Methodological resources like "基本无害的计量经济学" provide detailed explanations and applications of IV methods [29].​  

# 3.2 Dynamic Panel Data Models and GMM Estimation  

![](images/a6088d2559b3cb8d413182cd43c12bf0e66dd6486c4a6d4a6fef8af8f77bfa25.jpg)  

![](images/f29fcf0829f448af9d4f15f2c6ee3acff9e72cb33eaad796fba70f1f863e9325.jpg)  

Dynamic panel data (DPD) models are characterized by the inclusion of a lagged dependent variable among the regressors A common representation is a first-order autoregressive (AR(1)) model for panel data:  

$$
Y _ { i t } = \alpha _ { i } + \rho Y _ { i , t - 1 } + \epsilon _ { i t }
$$  

where $Y _ { i t }$ is the dependent variable for individual $\mathbf { \chi } _ { i }$ at time $t$ , $\alpha _ { i }$ ​ represents the unobserved individual-specific effect, $Y _ { i , t - 1 }$ is the lagged dependent variable, $\rho$ is the autoregressive coefficient, and $\epsilon _ { i t }$ ​ is the idiosyncratic error term .​  

The presence of the lagged dependent variable $Y _ { i , t - 1 }$ ​ introduces a significant endogeneity problem. In models with individual fixed effects $\left( \alpha _ { i } \right)$ , standard estimators like the within‐group estimator are inconsistent, particularly when the time dimension $( T )$ is small . This inconsistency, known as Nickell bias, arises because $Y _ { i , t - 1 }$ ​ is correlated with the mean of the idiosyncratic error term $\epsilon _ { i t }$ over time, which is part of the composite error in the fixed effects transformation.  

To address the fixed effects, a common approach is first-order differencing:  

$$
\Delta Y _ { i t } = \rho \Delta Y _ { i , t - 1 } + \Delta X _ { i t } ^ { \prime } \beta + \Delta \epsilon _ { i t }
$$  

. While this transformation eliminates the individual effects $\alpha _ { i }$ , it creates a new source of endogeneity. The differenced lagged dependent variable,  

$$
\Delta Y _ { i , t - 1 } \equiv Y _ { i , t - 1 } - Y _ { i , t - 2 } ,
$$  

is correlated with the differenced error term,  

$$
\Delta \epsilon _ { i t } \equiv \epsilon _ { i t } - \epsilon _ { i , t - 1 } ,
$$  

because $Y _ { i , t - 1 }$ is by definition correlated with $\epsilon _ { i , t - 1 }$ ​ . This endogeneity necessitates the use of instrumental variables.  

Generalized Method of Moments (GMM) estimation techniques are widely employed to consistently estimate DPD models by using suitably lagged values of the variables as instruments . GMM is particularly relevant in settings where regressors, such as the lagged dependent variable, are not strictly exogenous .​  

The Arellano-Bond estimator, also known as Difference GMM, addresses the endogeneity in the differenced equation . It uses lagged levels of the dependent variable (e.g., $Y _ { i , t - 2 } , Y _ { i , t - 3 } , . . . \rangle$ as instruments for the differenced equation, as these lagged levels are correlated with $\Delta Y _ { i , t - 1 }$ ​ but uncorrelated with $\Delta \epsilon _ { i t }$ ​ under the assumption that the idiosyncratic errors $\epsilon _ { i t }$ ​ are not serially correlated . The estimator can utilize "all possible lags" as instrumental variables to maximize efficiency .  

A limitation of the Difference GMM estimator arises when the dependent variable is highly persistent, leading to weak instruments. In such cases, the lagged levels may be poor instruments for the differenced variables. The Blundell-Bond (System GMM) estimator, introduced by Blundell and Bond (1998), improves upon Difference GMM by treating the original equation in levels and the differenced equation as a system . This method supplements the instruments used in Difference GMM (lagged levels for the differenced equation) with additional instruments for the level equation: lagged differences of the dependent variable (e.g., $\Delta Y _ { i , t - 1 }$ ) . The validity of these additional instruments for the level equation relies on the assumption that the first differences of the instruments are uncorrelated with the individual effects, which is often satisfied if the initial conditions of the process are not systematically related to the fixed effects. By combining information from both levels and differences, System GMM typically achieves significant efficiency gains compared to Difference GMM, especially when instruments are weak . A key premise for System GMM is the absence of autocorrelation in the original model's disturbance term .​  

The validity of GMM estimators depends crucially on the validity of the chosen instruments and the assumptions regarding the error term. Key assumptions include the absence of serial correlation in the idiosyncratic error term and the exogeneity of the instruments. These assumptions are typically tested using diagnostic procedures such as the Sargan or Hansen tests for overidentifying restrictions and tests for serial correlation in the residuals.  

# 4. Quasi-Experimental Designs for Panel Data  

Quasi-experimental designs represent a class of approaches utilized to estimate causal effects from observational data by leveraging naturally occurring variations or policy changes [5,6]. Unlike randomized controlled trials where treatment assignment is determined randomly by the researcher, quasi-experiments analyze settings where the intervention is assigned based on observable characteristics, specific rules, or external events, yet in a manner that approximates random assignment around the point of intervention [5,21]. These designs aim to mimic experimental conditions as closely as possible within observational settings, thereby bridging the methodological gap between true experiments and purely observational studies where confounding is typically more challenging to address rigorously [5,6]. This section focuses on key quasi-experimental designs particularly well-suited for application with longitudinal and panel data, detailing their underlying principles and requirements [6]. We will specifically examine the Difference-in-Differences (DID) method and Regression Discontinuity (RD) designs [5,6,21]. For each design, we will describe its core intuition, identification strategy, critical assumptions, common applications, and specific considerations when applied within a panel data framework [6].  

# 4.1 Difference-in-Differences (DID)  

The Difference-in-Differences (DID) method is a widely employed quasi-experimental design used to estimate the causal effect of an intervention or policy change, particularly relevant in observational settings where randomized controlled trials are not feasible [5,6,26]. The core intuition of DID is to compare the change in an outcome variable over time for a group exposed to a treatment (the treatment group) against the change over the same period for a comparable group not exposed to the treatment (the control group) [5,24,25,26,28]. This approach essentially uses the observed trend in the control group to impute the counterfactual trend that the treated group would have experienced in the absence of the intervention [5,24]. The difference between the actual outcome change in the treatment group and the estimated counterfactual change provides an estimate of the causal effect [5].​  

Mathematically, the basic $2 \times 2$ DID setup involves observations from a treatment group ( $D _ { i } = 1$ ) and a control group ( $D _ { i } = 0$ ) across two time periods: pre-treatment ( $T = 0$ ) and post-treatment ( $T = 1$ ) [25]. The causal effect is estimated as the difference in the outcome change in the treatment group minus the outcome change in the control group:​  

( $\scriptstyle \cdot \sum _ { T = 1 }$ , treatment ​ − $Y _ { T = 0 }$ , treatment ​) − ( $\stackrel { \cdot } { Y } _ { T = 1 }$ , control ​ − YT =0, control ​)  

[5].​  

This can be equivalently formulated within a linear regression framework, which is extensively discussed in econometric literature [29]. A standard specification for the $2 \times 2$ design is:  

$$
y _ { i t } = \beta _ { 0 } + \beta _ { 1 } T r e a t { _ i } + \beta _ { 2 } P o s t { _ t } + \beta _ { 3 } \left( T r e a t { _ i } \times P o s t _ { t } \right) + \epsilon _ { i t }
$$  

where $y _ { i t }$ ​ is the outcome for individual $\mathbf { \Xi } _ { i }$ at time $t$ , ${ { T r e a t } _ { i } }$ is a dummy variable indicating membership in the treatment group, $P o s t _ { t }$ ​ is a dummy variable indicating the post-treatment period, and $T r e a t _ { i } \times P o s t _ { t }$ ​ is an interaction term that is 1 only for the treatment group in the post-treatment period [1,12,28]. The coefficient $\beta _ { 3 }$ ​ represents the DID estimator, capturing the estimated average treatment effect on the treated (ATT) under the model assumptions [1,12,24]. This regression approach can also incorporate additional covariates $X _ { i t }$ ​ [25].  

A critical assumption for the validity of the DID estimator is the parallel trends assumption [1,5,6,24,25,26]. This assumption posits that, in the absence of the intervention, the average outcome trends for the treatment and control groups would have been the same over time [5,6,24,25]. While DID permits time-invariant differences between groups, it requires that the diferencesdo not change systematically over time in the absence of treatment [6]. The plausibility of the parallel trends assumption cannot be directly tested as the counterfactual is unobservable. However, researchers commonly assess its credibility by examining the trends in the outcome variable for both groups during the pre-treatment period [28]. Graphical analysis showing similar pre-intervention trends (often referred to as event studies) provides visual evidence. In a dynamic DID model, specifying pre-treatment period dummies allows for testing if the coefficients for these periods are statistically indistinguishable from zero, which supports the parallel trends assumption [25].  

DID can be extended beyond the basic $2 \times 2$ design to settings with multiple time periods, multiple treatment and control groups, and scenarios with staggered treatment adoption [25]. For multiple time periods, the regression framework can include time fixed effects (e.g., year dummies) to account for common temporal shocks affecting both groups, and group fixed effects to control for time-invariant group characteristics [12,25]. A common specification is the Two-Way Fixed Effects (TWFE) model:​  

$$
Y _ { i g t } = \alpha _ { g } + \gamma _ { t } + \beta I _ { g t } + \delta X _ { i g t } + \epsilon _ { i g t }
$$  

where $\alpha _ { g }$ are group-specific fixed effects, $\gamma _ { t }$ ​ are time-specific fixed effects, $I _ { g t }$ ​ is the interaction term indicating treatment exposure, and $\beta$ is the estimated DID effect [25]. To analyze the dynamic effects of the treatment over time, a model can be specified with separate interaction terms for each time period relative to the treatment initiation date:  

$$
Y _ { i t } = \beta _ { 0 } + \sum _ { t = - T _ { 1 } } ^ { - 1 } \beta _ { t } D _ { i t } ^ { t } + \sum _ { t = 1 } ^ { T _ { 2 } } \beta _ { t } D _ { i t } ^ { t } + \delta X _ { i t } + \epsilon _ { i t }
$$  

where $D _ { i t } ^ { t }$ ​ are dummies for time relative to treatment (with the period immediately preceding treatment often omitted as the reference) [25]. In this dynamic setting, checking the parallel trends assumption involves verifying that the coefficients for the pre-treatment periods $( \beta _ { - T _ { 1 } } , . . . , \beta _ { - 1 } )$ are close to zero [25]. Another extension is the triple difference method, which adds a third differencing dimension to control for trends that differ between groups but are common to a subset of units within each group [26,28].​  

Despite its utility, DID is subject to several threats to validity. The most significant threat is the violation of the parallel trends assumption [26]. If the trends in the treatment and control groups would have diverged even without the intervention, the DID estimate will be biased. Other threats include violations of the Stable Unit Treatment Value Assumption (SUTVA), such as spillover effects where the treatment of one unit affects the outcomes of other units, including those in the control group, or where the intervention causes changes in group composition [5,26]. Temporary shocks that are correlated with the treatment dummy variable can also pose a threat, particularly in TWFE models [6]. Over-parameterization can be an issue in some implementations, such as Bayesian models, especially with limited data [12]. While DID inherently addresses selection bias stemming from time-invariant unobserved confounders by differencing them out, it does not account for selection bias based on factors that change over time and are correlated with both treatment and the outcome [6]. Strategies to address violations of parallel trends include selecting more comparable control groups, potentially using methods like propensity score matching or synthetic control methods to construct a weighted control group that better matches the pre-treatment trends of the treated group [26]. As mentioned, the triple difference method can also help control for certain types of differential trends [26].​  

DID has been widely applied across various fields, including economics, public policy, education, and health. Classic examples include Card's (1990) study on the labor market effects of the Mariel Boatlift in Miami compared to other cities [28], the analysis of minimum wage effects in New Jersey and Pennsylvania [5], studies on education reforms [28], and evaluation of health policies [28]. These applications demonstrate the method's flexibility in evaluating the causal impact of sharp policy changes or natural experiments [28]. Implementation is typically done using regression software with appropriate dummy and interaction terms, or specialized commands [6].  

# 4.2 Regression Discontinuity (RD)  

Regression Discontinuity (RD) designs constitute another quasi‐experimental approach for inferring causal effects from observational data [5]. Pioneered by Thistlewaite and Campbell (1960) and gaining prominence among economists in the late 1990s, RD leverages a discontinuity in the treatment assignment rule based on whether a continuous “running” or “forcing” variable crosses a specific threshold or cutoff point [6]. Treatment $( X )$ is assigned based on the value of the running variable $( R )$ ; specifically, individuals receive the intervention if $R \geq c$ for a threshold $\boldsymbol { c }$ , or with a different probability if $R < c$ [5,6]. The crucial insight of RD is that individuals with running variable values just above the cutoff are expected to be highly similar, or quasi‐randomly assigned, compared to those just below the cutoff [5,6]. This localized quasi‐randomness near the cutoff allows for the estimation of the local average treatment effect at that specific threshold [5].​  

RD designs can be categorized as either sharp or fuzzy [6]. In a sharp RD design, treatment assignment is deterministic: all individuals with $R \geq c$ receive treatment, and none with $R < c$ do. In contrast, a fuzzy RD design features a discontinuous jump in the probabilityof receiving treatment at the cutoff, rather than a deterministic assignment [6].  

The fundamental approach to estimating the causal effect in RD involves fitting regression models to the outcome variable ( $Y$ ) as a function of the running variable $( R )$ separately on either side of the cutoff [5]. The estimated causal effect at the threshold is then the difference between the predicted outcome values immediately to the right and left of the cutoff based on these fitted models [5]. For instance, in the context of academic achievement, RD can be used to estimate the effect of receiving a merit certificate (assigned based on a test score threshold) on subsequent academic performance by comparing students scoring just above the cutoff (who receive the certificate) with those scoring just below (who do not) [5].  

The validity of RD relies on key assumptions. A primary assumption is that individuals cannot precisely manipulate their value of the running variable to be just above or below the cutoff [5,6]. If individuals can sort themselves perfectly around the threshold based on their potential outcomes, the quasi‐randomness assumption is violated [6]. Another critical assumption is the continuity of the conditional expectation of the outcome and any relevant covariates with respect to the running variable at the cutoff, absent the treatment effect.  

<html><body><table><tr><td>rdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdr</td></tr><tr><td>drdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdr</td></tr><tr><td>drdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdr</td></tr><tr><td>drdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdr</td></tr><tr><td>drdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdr</td></tr><tr><td>drdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrdrd,</td></tr></table></body></html>  

rdrobustrdlocrand, and rddensity  to facilitate these analyses and tests [6]. This methodology is a standard topic covered in econometrics literature [29].  

While the provided digests do not explicitly detail applications to longitudinal or panel data, RD designs can be effectively applied in such contexts. By using the initial cutoff assignment, researchers can track outcomes over multiple time periods after individuals cross the threshold. This allows for the examination of how the initial causal effect at the cutoff evolves over time, distinguishing between immediate impacts and effects that manifest or persist longitudinally. For example, one could study the long-term academic trajectory of students who barely received the merit certificate compared to those who barely missed it [5].​  

# 5. Advanced Causal Inference Techniques for Longitudinal and Panel Data  

This section delves into advanced methodologies for causal inference tailored to the unique challenges and opportunities presented by longitudinal and panel data. Moving beyond foundational methods, these techniques address more intricate causal questions, leverage distinct modeling paradigms, or integrate modern computational approaches to enhance robustness and flexibility [11,27]. We explore four key areas: the potential outcomes framework and associated g-methods, Bayesian approaches, causal discovery algorithms, and the integration of machine learning techniques.​  

The Potential Outcomes Framework, including methods like g-computation, g-estimation, and Marginal Structural Models (MSMs), provides a rigorous theoretical basis for defining and estimating causal effects, particularly in the presence of timevarying treatments and confounding that depends on past exposure history [11,27,33]. It offers powerful tools for handling complex causal pathways and mediation, relying on key assumptions such as sequential ignorability and positivity.​  

Bayesian Methods offer a probabilistic approach to causal inference, naturally incorporating uncertainty quantification and allowing for the integration of prior knowledge. Techniques such as Bayesian Networks (BNs) and their dynamic extensions are valuable for modeling complex dependency structures and time-varying relationships, while Bayesian nonparametrics provides flexibility for estimating heterogeneous effects [7,8,12,18].  

Causal Discovery Algorithms aim to infer the underlying causal structure directly from observational data, leveraging the temporal dimension inherent in longitudinal and panel datasets. These algorithms, spanning constraint-based and scorebased methods, are crucial for hypothesis generation and understanding the network of relationships among variables, though challenges like unmeasured confounding and feedback loops persist [1,7,13,27].​  

Finally, the Integration with Machine Learning focuses on harnessing powerful predictive modeling tools to enhance causal inference. Approaches like Double Machine Learning (DML) and the use of deep learning techniques enable flexible modeling of complex relationships and management of high-dimensional data, contributing to more robust confounding adjustment and heterogeneous effect estimation [1,27].​  

Collectively, these advanced techniques represent the forefront of causal analysis for longitudinal and panel data, offering sophisticated solutions to estimate causal effects, quantify uncertainty, discover causal structures, and leverage complex data characteristics that are not adequately addressed by simpler methods.  

# 5.1 Potential Outcomes Framework (G-Methods and MSMs)  

The potential outcomes framework, also known as the counterfactual framework, serves as a foundational language for modern causal inference, providing a formal basis for defining causal effects, particularly in complex settings such as those involving longitudinal data with time-varying treatments and confounders [11,21,29]. At its core, this framework posits that for each individual, there exist multiple potential outcomes corresponding to different hypothetical treatment or exposure states [21,29]. The causal effect for an individual is conceptually defined as a comparison between their potential outcomes under different interventions, such as  

$Y ^ { a = 1 }$ (outcome under treatment) and $Y ^ { a = 0 }$ (outcome under no treatment), where a causal effect exists if $Y ^ { a = 1 } \neq Y ^ { a = 0 }$ [33]. Common measures of causal effect derived from this framework include the causal risk difference, risk ratio, and odds ratio [33].  

A key challenge in this framework is the fundamental problem of causal inference: for each individual, only the outcome corresponding to the treatment they actually received is observed, while the other potential outcomes remain unobservable [13]. Causal inference then becomes a problem of estimating these unobserved potential outcomes or average causal effects across a population based on observed data [13].​  

For longitudinal data, where treatments and confounders can change over time and past treatments can influence future confounding, defining and estimating causal effects becomes more intricate. The potential outcomes framework extends to this setting by considering potential outcomes under different sequences of treatments over time. Estimating these effects in the presence of time-varying confounding affected by past treatment necessitates specialized techniques, often referred to collectively as g-methods. These include g-computation, g-estimation (used with Structural Nested Models), and Inverse Probability of Treatment Weighting (IPTW).​  

Marginal Structural Models (MSMs) are a class of models within the potential outcomes framework specifically designed to estimate the causal effect of a time-varying treatment on an outcome in the presence of time-dependent confounding that is itself affected by past treatment. MSMs typically model the relationship between time-varying treatment and the outcome under hypothetical treatment regimes, often employing IPTW to create a synthetic dataset where the treatment assignment at each time point is independent of measured past confounders, thereby enabling standard regression techniques to estimate the causal effect.​  

Applying the potential outcomes framework and its associated methods relies on critical assumptions. The most prominent is sequential ignorability (also known as sequential exchangeability or no unmeasured confounding conditional on past history). This assumption states that, conditional on the observed past treatment, confounder, and outcome history, the treatment assignment at the current time point is independent of the potential outcomes under different future treatment regimes. Another crucial assumption is positivity (or experimental treatment assumption), which requires that for every combination of past history, there is a non-zero probability of receiving any of the treatment options at the current time point. Violations of these assumptions can lead to biased causal effect estimates.​  

The potential outcomes framework is also extensively applied in causal mediation analysis, which seeks to decompose the total causal effect of an exposure into direct and indirect effects mediated through an intermediate variable [13,15]. The framework is advantageous in mediation for its ability to handle exposure-mediator interactions and non-continuous variables [15]. Approaches like principal stratification, which stratify individuals based on potential mediator values under different treatments, are used to address comparability challenges in mediation [13]. Advanced estimation techniques compatible with the potential outcomes framework for complex longitudinal data include Targeted Maximum Likelihood Estimation (TMLE) and methods for Structural Nested Mean Models (SNMMs) [27], further highlighting the framework's versatility. Comprehensive texts like "Causal Inference: What If" provide detailed expositions of concepts and methods, including those tailored for complex longitudinal data within this framework [29].​  

# 5.2 Bayesian Methods for Causal Inference  

Bayesian methods offer a flexible and powerful framework for causal inference, particularly advantageous for their capacity to incorporate prior information and provide transparent quantification of parameter and predictive uncertainty [8]. This approach models causal relationships through probability distributions, allowing for a comprehensive representation of uncertainty in causal effect estimates.  

A fundamental tool within the Bayesian framework for causal discovery and modeling is the Bayesian Network (BN). BNs represent causal relationships between variables using a directed acyclic graph (DAG), where nodes correspond to variables and directed edges indicate probabilistic dependencies that can be interpreted causally under certain assumptions [7]. The structure of the DAG is typically learned from data using specific algorithms, and the strength of relationships is quantified through conditional probability tables (CPTs) [7]. The joint probability distribution over the variables  

$$
X _ { 1 } , \ldots , X _ { n }
$$  

in a BN can be factorized as the product of the conditional probabilities of each node given its parents in the DAG:  

# 无效公式  

where ​Parents $( X _ { i } )$  

denotes the set of parents of node $X _ { i }$ in the DAG [7]. BNs have been applied to study causal relationships among multiple factors, such as in constructing gene regulatory networks from biological data [13]. However, challenges remain, particularly in learning network structures from high-dimensional, small-sample data and addressing the ongoing debate about the feasibility of discovering causal relationships solely from observational data [13].​  

Bayesian Structural Equation Modeling (BSEM) extends traditional SEM by incorporating Bayesian inference, making it particularly suitable for analyzing complex causal pathways and mediation effects. BSEM facilitates the handling of latent variables and the integration of prior knowledge into the model estimation process [18].  

dynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedyn amitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamit edynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedyn amitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamitedynamit edynamitedynamitedynamite amite\` R package, designed for Bayesian modeling in longitudinal and panel data contexts [8].​  

Furthermore, Bayesian nonparametrics offers flexible modeling approaches for causal inference, enabling the estimation of heterogeneous treatment effects and the modeling of non-linear relationships without assuming a rigid parametric form. Examples mentioned include the use of multi-task Gaussian processes and Bayesian Additive Regression Trees (BART) for estimating heterogeneous effects [18]. Bayesian inference is also applied within specific causal inference designs for longitudinal data, such as using PyMC for estimating causal effects in Difference-in-Difference (DID) models by obtaining posterior estimates of the causal impact [12]. These diverse applications underscore the versatility of Bayesian methods in addressing various challenges in causal inference with longitudinal and panel data.​  

# 5.3 Causal Discovery Algorithms  

Causal discovery is the process of inferring the structure of a causal graph directly from observational data [13]. In the context of longitudinal and panel data, causal discovery plays a crucial role in generating causal hypotheses by leveraging the inherent temporal order of observations, which provides valuable information about potential causal directions. This task is distinct from causal inference, which typically focuses on estimating the magnitude of a specific causal effect given a known or assumed causal structure.  

Algorithmic approaches to causal discovery broadly fall into two main categories: constraint-based methods and scorebased methods [1,7]. Constraint-based algorithms, such as the PC algorithm and the IC algorithm, rely on performing statistical tests to identify conditional independencies among variables and then constructing a graph that is consistent with these detected independencies [1,7].​  

Score-based methods, conversely, involve searching over a space of possible causal graph structures and evaluating each candidate structure using a scoring function, such as the Bayesian Information Criterion (BIC) or the BDeu score [7]. Examples of score-based algorithms include ExhaustiveSearch, Hillclimbsearch, Chow-Liu, and Tree-augmented Naive Bayes (TAN) [7]. Some score-based approaches, like NOTEARS and CGNN, transform the discrete search problem into a continuous optimization problem, which can significantly improve computational speed [1]. Causal relationships are often represented using Directed Acyclic Graphs (DAGs), particularly within the framework of causal Bayesian networks [13], where parent nodes in the graph represent direct causes of their children [18].  

While powerful, causal discovery algorithms operate under certain assumptions, such as the Causal Markov Condition and Faithfulness, and face significant limitations. A major challenge is the presence of unmeasured confounding, where latent variables influence multiple observed variables, potentially leading to spurious associations being misinterpreted as direct causal links; recent work explores hidden causal representation learning to address this [27]. Feedback loops (cycles in the causal structure) also pose difficulties for algorithms designed for DAGs, requiring extensions to handle such cases. Furthermore, practical applications encounter challenges such as high-dimensional data and small sample sizes, which affect both the computational and statistical efficiency of discovery algorithms [13]. There is ongoing research debate regarding the extent to which causal relationships can be reliably discovered solely from observational data, with efforts exploring methods that combine both observational and experimental data to improve discovery accuracy [13,27].  

# 5.4 Integration with Machine Learning  

The integration of machine learning (ML) techniques with causal inference methods has become increasingly important, particularly when analyzing complex relationships and high-dimensional data in longitudinal and panel settings. ML offers powerful tools for flexible modeling and prediction, which can enhance the stages of causal inference—from confounding adjustment to heterogeneous effect estimation. The growing interest in this area is reflected in academic discussions and conferences dedicated to the topic [27].​  

One significant area of integration involves using ML algorithms to model complex relationships and handle highdimensional confounders. The Double Machine Learning (DML) framework, for instance, is designed to flexibly model the relationships between confounders, treatment variables, and outcome variables in the presence of high-dimensional confounding covariates [1]. DML often leverages the principles of Doubly Robust (DR) estimation. DR frameworks provide a degree of protection against model misspecification; specifically, causal effect estimates remain unbiased if at least one of the models—the outcome model $( g _ { t } ( X , W )$ ) or the treatment model $( p _ { t } ( X , W )$ )—is correctly specified [1]. ML methods can be employed to estimate both $g _ { t } ( X , W )$ (outcome modeling, relevant in approaches like g-computation) and $p _ { t } ( X , W )$ (propensity score modeling), thus capitalizing on the robustness property of DR estimators while accommodating non-linearities and interactions.​  

Deep learning methods also present opportunities for causal inference by capturing complex interactions and learning rich representations from multimodal data. In contexts like drug discovery, which involves causal questions about interventions and health outcomes, deep generative models and deep representation learning are used to transform discrete spaces for efficient problem solving and predict outcomes by capturing intricate feature interactions [40]. These capabilities are pertinent to causal inference when dealing with high-dimensional feature spaces and aiming to understand how interventions affect complex, interacting systems.  

Beyond modeling relationships, ML is also applied in causal discovery and inference algorithms. Methods like Bayesian Networks can be integrated with machine learning for tasks such as causal structure learning, parameter estimation, and inference [7]. Furthermore, research explores the experimental evaluation of machine learning algorithms specifically for causal inference tasks, emphasizing the need to understand their performance characteristics in identifying causal effects rather than just predictive accuracy [27]. Efforts such as Deep Stable Learning and Heterogeneous Risk Minimization also highlight the use of deep learning to address issues pertinent to causal inference, including handling distribution shifts and estimating effects that vary across individuals or subgroups [27].​  

The integration of prediction-focused ML with identification-focused causal inference offers significant potential, particularly for automating complex modeling tasks and handling large, intricate datasets. However, it also poses challenges, such as ensuring that ML models, optimized for predictive performance, satisfy the assumptions necessary for valid causal identification. Careful consideration is required to bridge the gap between algorithmic prediction accuracy and the theoretical guarantees needed for unbiased and reliable causal effect estimation.  

# 6. Specific Causal Topics in Longitudinal Data  

This section delves into specific questions concerning causal effects within longitudinal settings, focusing on understanding the howand forwhomof these effects. The investigation centers on mechanisms by which treatments or exposures exert their influence and the variability of these effects across different individuals or subgroups, particularly as they evolve over time.​  

Causal mediation analysis provides a structured framework for understanding the howby decomposing a total causal effect into direct and indirect components [15,25]. The indirect effect operates through an intermediate variable, or mediator, shedding light on the underlying pathways or processes [15]. While traditional methods rely on strong assumptions and can face challenges with discrete variables or non-linear models, sophisticated approaches—including those adapted for multiple mediators and high-dimensional data using techniques like Bayesian methods and targeted penalization—are necessary [16,27]. A significant challenge in applying mediation analysis to longitudinal data involves addressing timevarying confounding and potential feedback loops, which complicate the identification and estimation of causal pathways over time [13].​  

Complementing the understanding of how, the study of Heterogeneous Treatment Effects (HTE) addresses the forwhom, exploring how causal effects vary across individuals or subgroups based on their characteristics [1]. This variation is crucial in longitudinal contexts where individual attributes and environmental factors change over time. Key concepts include the Individual Treatment Effect (ITE), denoted as $\tau _ { i } = Y _ { i 1 } - Y _ { i 0 }$ ​ , and the Conditional Average Treatment Effect (CATE) for a subgroup with characteristics $X$ , expressed as $E$ [1]. Identifying effect modifiers—variables that alter the magnitude of a treatment effect across their levels—is fundamental to HTE analysis [33]. While simple stratification and regression with interaction terms offer initial insights [25,33], advanced methods, including various machine learning techniques like causal forests, Meta Learners (S-learner, T-learner, X-learner), Double Machine Learning, Deep Reinforcement Learning, Bayesian nonparametrics, and deep learning approaches, are increasingly employed to estimate HTEs, particularly with complex or high-dimensional data [1,18,27]. The ability to accurately estimate HTEs has significant implications for developing tailored interventions, optimizing treatment regimes, and informing targeted policy decisions [16]. Together, these two areas represent critical advancements in moving beyond average effect estimation to a more granular understanding of causal processes and their variability in dynamic longitudinal settings.​  

# 6.1 Causal Mediation Analysis  

Causal mediation analysis serves as a framework for dissecting the total causal influence of an exposure or treatment into its constituent components: a direct effect and an indirect effect, the latter operating through one or more intermediate variables known as mediators [15]. This approach is pivotal for understanding the mechanisms or processes by which an independent variable impacts a dependent variable, facilitating the testing of hypotheses regarding these underlying pathways [15,25]. A key characteristic of a mediator is its endogenous nature; it must be influenced by the treatment or study condition, which is the exogenous variable [15].  

Identifying and estimating mediation effects requires stringent assumptions. Traditional mediation analysis methods, such as those based on the product of coefficients or the difference in coefficients, often presuppose conditions like normally distributed residuals, homoscedasticity, independence of observations, and the absence of interference from effect modifiers [15]. However, these conventional techniques prove inadequate when dealing with discrete mediators or outcomes and can introduce ambiguities in non-linear model specifications [15]. Furthermore, a critical challenge arises when attempting to assess direct and indirect effects by conditioning on the mediator. As highlighted in studies discussing challenges in causal inference, evaluating the effect of a cause (A) on an outcome (B) by holding the intermediate factor (C) constant at the same value for both treated and control groups can lead to flawed conclusions if these groups are inherently not comparable under this conditioning scheme [13]. For longitudinal data, where variables are measured repeatedly over time, the assumptions required for identifying mediation, such as sequential ignorability (which involves conditional independence assumptions regarding treatment, mediator, and outcome over time), become particularly strong and challenging to satisfy in practice. The presence of time-varying confounders and feedback loops (where an outcome at one time point can influence a mediator at a subsequent time point, and vice versa) further complicates estimation and necessitates advanced methods beyond simple cross-sectional approaches.  

Given the limitations of traditional methods and the complexities inherent in longitudinal data and other intricate scenarios, more sophisticated approaches have been developed. These include methods tailored for multiple mediators [27] and highdimensional settings, such as those encountered in omics studies. For instance, Bayesian methods have been proposed for high-dimensional mediation analysis, incorporating techniques like coordinated selection of correlated mediators and sparse mediation analysis through targeted penalization of natural indirect effects [16]. These advancements are crucial for reliably identifying significant mediators and estimating their effects in complex data structures, although addressing timevarying confounding and feedback loops in longitudinal mediation remains an active area of research.​  

# 6.2 Heterogeneous Treatment Effects  

Understanding how causal effects vary across different individuals or contexts is crucial, particularly in longitudinal studies where subject characteristics and contexts evolve over time. This phenomenon is termed Heterogeneous Treatment Effect (HTE) and refers to the differing effects of a treatment on individuals due to variations in their characteristics [1]. While some modeling approaches, such as basic fixed effects models, often assume a constant causal effect across all entities, this can be a significant limitation when the research objective extends beyond estimating an overall average impact to understanding individual-level or subgroup-specific effects [24].​  

The concept of HTE builds upon the individual treatment effect (ITE), defined for an individual ​i as   
\​   
where $Y _ { i 1 }$ and $Y _ { i 0 }$ represent the potential outcomes under treatment and control, respectively. Since only one of these outcomes is observed, $\tau _ { i }$ must be estimated through modeling [1]. More commonly estimated is the Conditional Average Treatment Effect (CATE), which represents the average causal effect for a subgroup defined by specific characteristics $X$ . The CATE is formally expressed as   
$\backslash ,$  

providing a conditional expectation of the treatment effect [1].  

Identifying effect modifiers is a fundamental step in exploring HTEs. An effect modifier $M$ for the effect of a treatment $A$ on an outcome $Y$ is defined as a variable where the average causal effect of $A$ on $Y$ varies across the different levels of $M$ [33]. A natural and straightforward approach to identifying potential effect modifiers involves stratified analysis, where the treatment effect is estimated separately within strata defined by the levels of the suspected modifier [33].  

Beyond simple stratification, various methods exist for estimating HTEs. Traditional regression‐based approaches can incorporate interaction terms between the treatment variable and potential effect modifiers. For instance, moderation analyses within Difference-in-Differences (DID) frameworks can explore how treatment effects vary across different subgroups [25]. However, estimating HTEs, particularly CATEs, becomes more complex with many covariates. Advanced machine learning–based approaches have emerged to address this challenge. These include methods like causal forests, which use random forest principles adapted for causal inference, and various Meta Learners such as the S-learner, T-learner, and X-learner [1]. Other estimation frameworks based on Double Machine Learning (DML) and Deep Reinforcement Learning (DRL) are also utilized [1].​  

Furthermore, Bayesian nonparametrics have been suggested as a flexible approach for estimating heterogeneous effects [18]. Research focusing on estimating covariate-specific treatment effect (CSTE) curves, especially with high-dimensional covariates from observational data, is directly relevant to this area [27]. Deep learning techniques are also being explored in areas like Deep Stable Learning and Heterogeneous Risk Minimization, contributing to the estimation and understanding of varying effects [27].​  

The exploration of HTEs has significant implications for targeted interventions and policy recommendations. By understanding which subgroups benefit most (or least) from a treatment or policy, resources can be allocated more effectively, and interventions can be tailored to specific populations. This is particularly relevant in fields like medicine for developing optimal treatment regimes. Methods have been developed for variable selection when estimating optimal treatment regimes, even in the presence of numerous covariates, and frameworks like C-learning have been proposed for estimating optimal dynamic treatment regimes [16]. Furthermore, research specifically addresses subgroup identification and variable selection aimed at improving treatment decision making [16]. Robust methods have also been developed for optimal treatment decision making, including those based on survival data [16].​  

# 7. Applications in Various Fields  

Causal modeling techniques applied to longitudinal and panel data are instrumental in uncovering complex relationships and informing decision‐making across a wide array of disciplines.  

<html><body><table><tr><td>Field</td><td>Key Areas of Application</td><td>Example Applications</td></tr><tr><td>Economics</td><td>Policy Evaluation,Labor Markets, Corporate Behavior, Consumption</td><td>Macroeconomic policy effects,Tax/welfare reform impact, Minimum wage effects, Impact of institutions, Marriage on earnings.</td></tr><tr><td>Finance</td><td>Corporate Behavior, Financial Markets, Regulations</td><td>Firm performance, Asset price drivers, Impact of regulations/policies on markets, Banking interventions.</td></tr><tr><td>Sociology</td><td>Social Inequality, Individual Behavior, Social Dynamics, Family Structures</td><td>Evolution of inequalities, Changes in consumption/labor, Impact of social policies, Peer effects, Family size on employment, China's One-</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>Child Policy psychological impacts.</td></tr><tr><td>Public Health & Epidemiology</td><td>Health Outcomes, Disease Progression, Intervention Effectiveness</td><td>Medical policy impact, Disease development, Obesity on health, Stressors on perceived stress, Causal mediation (e.g., gut bacteria & asthma).</td></tr><tr><td>Political Science</td><td>Policy Evaluation,Political Behavior, Institutions</td><td>Impact of political institutions, Voting behavior, Political advertising effectiveness.</td></tr><tr><td>Business & Marketing</td><td>Online Strategy Evaluation, Impact of Campaigns/Policies</td><td>Measuring impact of advertising, Competitive actions, Policies on sales/customer behavior (e.g., using DID where A/B testing is infeasible).</td></tr><tr><td>Other Fields</td><td>Neuroscience, Environmental Economics, Education, Recommender Systems, Autonomous Driving, NLP</td><td>Studying brain changes over time, Energy-economy relationship, Education reform evaluation, Bias correction in recommender systems,Improving NLP robustness.</td></tr></table></body></html>  

<html><body><table><tr><td>Field</td><td>Key Areas of Application</td><td>Example Applications</td></tr><tr><td>Economics</td><td>Policy Evaluation,Labor Markets, Corporate Behavior, Consumption</td><td>Macroeconomic policy effects,Tax/welfare reform impact, Minimum wage effects, Impact of institutions, Marriage on earnings.</td></tr><tr><td>Finance</td><td>Corporate Behavior, Financial Markets, Regulations</td><td>Firm performance, Asset price drivers, Impact of regulations/policies on markets, Banking interventions.</td></tr><tr><td>Sociology</td><td>Social Inequality, Individual Behavior, Social Dynamics, Family Structures</td><td>Evolution of inequalities, Changes in consumption/labor, lmpact of social policies, Peer effects, Family size on employment, China's One- Child Policy psychological impacts.</td></tr><tr><td>Public Health& Epidemiology</td><td>Health Outcomes, Disease Progression,Intervention Effectiveness</td><td>Medical policy impact, Disease development, Obesity on health, Stressors on perceived stress, Causal mediation (e.g., gut bacteria &asthma).</td></tr></table></body></html>  

<html><body><table><tr><td>Political Science</td><td>Policy Evaluation,Political Behavior, Institutions</td><td>Impact of political institutions, Voting behavior, Political advertising effectiveness.</td></tr><tr><td>Business & Marketing</td><td>Online Strategy Evaluation, Impact of Campaigns/Policies</td><td>Measuring impact of advertising, Competitive actions, Policies on sales/customer behavior (e.g., using DID where A/B testing is infeasible).</td></tr><tr><td>Other Fields</td><td>Neuroscience, Environmental Economics, Education, Recommender Systems,Autonomous Driving, NLP</td><td>Studying brain changes over time, Energy-economy relationship,Education reform evaluation, Bias correction in recommender systems, Improving NLP robustness.</td></tr></table></body></html>  

Their ability to account for unobserved heterogeneity and temporal dynamics makes them particularly valuable for estimating causal effects in observational settings. Applications span economics, finance, sociology, public health, political science, and beyond, addressing critical research questions that require understanding how interventions, policies, or characteristics causally influence outcomes over time [4,5,10,11,14,17].​  

In economics, causal modeling with longitudinal and panel data is frequently employed for policy evaluation and labor market analysis. Researchers use these methods to assess the effectiveness of macroeconomic policies, analyze changes in corporate behavior, study individual and household consumption habits, and evaluate the impact of tax and welfare reforms [4,14,17]. For instance, panel data models can be used to analyze how different countries or regions respond to monetary or fiscal policies over time, controlling for country‐specific factors [4]. The Difference‐in‐Differences (DID) method is widely used in policy evaluation within economics, including labor economics and macroeconomics [25]. Instrumental Variables (IV) approaches are also critical for studying causal effects in economics, such as the impact of institutions on per capita income; a seminal example utilized colonial‐era mortality rates as an instrument for current institutions [34]. Labor economics provides further examples, including the application of fixed effects models to estimate the causal impact of marriage on men's earnings [24] and the use of dynamic panel threshold models to examine the effect of obesity (measured by BMI) on worker productivity across different countries [35]. Causal inference training is increasingly integrated into economics curricula, highlighting its importance in the field [38].​  

Finance also heavily utilizes panel data models to study corporate behavior, financial markets, and the effects of regulations [4,14,17,21]. Applications include analyzing firm performance under varying market conditions, studying dynamic changes in stock and bond markets, understanding drivers of asset prices, and investigating the impact of policy changes on markets [21]. Panel data analysis allows researchers to identify performance differences across companies over time and study how systemic and individual risks affect stock price fluctuations [4]. DID analysis, for example, has been applied using banking data to estimate the impact of interventions on the number of banks in operation within specific regions [12].​  

In sociology, causal modeling for longitudinal and panel data is essential for studying social inequality, individual behavior, social dynamics, and family structures [14,17]. Researchers analyze the formation and evolution of inequalities in income, education, and health, as well as changes in individual consumption habits, labor supply, and income fluctuations over time [4,17]. Panel data models are used to track long‐term changes in behavior and the impact of social policies [4]. Causal inference has been applied to study sensitive topics such as the psychological health impacts of China's One‐Child Policy [27]. Studies on relationship dynamics, including the interplay of power and gratitude, also employ fixed effects models on panel data [32]. Furthermore, IV approaches are utilized in sociology to address confounding in studies of peer effects, using aggregate data or natural phenomena as instruments, and to examine the impact of family size on maternal employment using birth‐related events as instruments [2]. The field of youth development leverages quantitative methods, including potentially causal approaches, to understand thriving among diverse youth populations [37].​  

Public health and epidemiology are significant areas where causal inference with longitudinal data is crucial for understanding health outcomes, disease progression, and intervention effectiveness [3,4,5,10,14,15,16,40]. Causal models analyze the impact of medical policies on population health, track individual health status changes, and analyze disease development over many years [4,14]. Examples include estimating the effect of obesity on health‐related quality of life using Mendelian randomization, where genetic variations serve as instrumental variables [3,5,40]. Fixed effects regression has been applied to examine the causal effects of various work‐related and non‐work‐related stressors on perceived stress using longitudinal health survey data [22]. Causal mediation analysis is employed to quantify indirect effects in complex biological systems, such as assessing the role of gut bacteria in mediating the link between the infant gut virome and asthma risk [15]. The field addresses a wide range of health issues, including cardiovascular diseases (LVAD outcomes, infections), kidney transplantation, respiratory diseases, and cognitive dysfunction in cancer patients [16]. Quasi‐ experimental methods like DID and Regression Discontinuity are used to assess the impact of health interventions and policies, such as changes in tax law affecting health insurance purchases or Zero Tolerance laws impacting underage drinking [5,28]. Causal inference is also used to estimate outcomes like return to work after sick leave or the effect of wealth shocks on cognition, accounting for challenges like selection bias due to death [27].​  

Beyond these core areas, causal modeling for longitudinal and panel data finds applications in other fields. Political science utilizes these methods for policy evaluation and potentially analyzing political advertising and voting behavior, or the impact of political institutions [4,10,25,34]. Neuroscience is another field engaging with causal inference techniques for analyzing longitudinal data [10]. The business and marketing domain widely adopts DID and other panel data techniques for evaluating online strategies where A/B testing is not feasible [26]. These methods are used to measure the impact of advertising campaigns, competitive actions, and policy changes on outcomes like sales, customer spending, readership, and website usage [24,25]. Furthermore, causal inference is applied in advanced technical domains such as correcting bias in recommender systems using causal graphs or IVs, integrating causal understanding into autonomous driving trajectory prediction platforms, and enhancing the robustness and interpretability of Natural Language Processing (NLP) models [1,5]. Environmental economics applies panel data analysis to study the relationship between energy consumption and economic growth and evaluate environmental policies [14,17]. In education research, panel data analysis is used to study education achievement and social mobility, while DID is applied to evaluate the impact of education reforms [14,28]. The diversity of these applications underscores the broad utility of causal modeling approaches in extracting meaningful insights from longitudinal and panel data across a wide range of disciplines.  

# 8. Software and Implementation  

<html><body><table><tr><td>Software/Platform</td><td>Key Capabilities& Packages</td><td>Relevant Commands/Functions/Librar ies</td></tr><tr><td>R</td><td>Linear Panel Models (FE, RE), DID, Bayesian Longitudinal Models, General Causal Inference Toolboxes</td><td>plmpanelViewpanelViewpa nelview, fixestdynamite (Bayesian Longitudinal)，NlinTS, ，TigramiteDoWhy y.</td></tr><tr><td>Stata</td><td>Standard Panel Models (FE, RE), Endogeneity (IV, GMM), DID, RD, Diagnostic Tests</td><td>xtsetxtregxtregxtreg (FE，RE), xtivregxtabond, xtdpdsys （GMM）, (DID)，hausman (Hausman Test)，rd, rdrobust (RD).</td></tr><tr><td>Python</td><td>Standard Panel Models (FE, RE), Data Manipulation, Bayesian Networks, ML- based Causal Inference, DID</td><td>statsmodelslinearmodels (FE，RE，Hausman）, pandasbnlearnbnlearnbnl earn， pgmpy (Bayesian Networks),</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>EconMLCausalML (ML/DR), causalpyCausalDiscovery Toolbox，,</td></tr><tr><td>Other</td><td>Complex Data Processing, SEM, Multilevel Modeling, Specific Applications</td><td>Tigramite. SAS, SPSS, EViews (Econometrics), MATLAB (Numerical), Mplus (SEM, Multilevel), MicrobiomeStatPlot</td></tr></table></body></html>  

<html><body><table><tr><td>Software/Platform</td><td>Key Capabilities & Packages</td><td>Commands/Functions/Librar</td></tr><tr><td>R</td><td>Linear Panel Models (FE, RE), DID, Bayesian Longitudinal Models, General Causal Inference Toolboxes</td><td>plmpanelViewpanelViewpa nelview, fixestdynamite (Bayesian Longitudinal)，NlinTS, ，TigramiteDoWhy y'.</td></tr><tr><td>Stata</td><td>Standard Panel Models (FE, RE), Endogeneity (IV, GMM), DID, RD, Diagnostic Tests</td><td>xtsetxtregxtregxtreg (FE，RE), xtivregxtabond，， xtdpdsys （GMM）， (DID)，hausman (Hausman Test)，rd, rdrobust (RD).</td></tr><tr><td>Python</td><td>Standard Panel Models (FE, RE), Data Manipulation, Bayesian Networks, ML- based Causal Inference, DID</td><td>statsmodelslinearmodels (FE，RE，Hausman）, pandasbnlearnbnlearnbnl earn， pgmpy (Bayesian Networks), EconMLCausalML (ML/DR), causalpyCausalDiscovery Toolbox，,</td></tr><tr><td>Other</td><td>Complex Data Processing, SEM, Multilevel Modeling, Specific Applications</td><td>SAS, SPSS, EViews (Econometrics), MATLAB (Numerical), Mplus (SEM, Multilevel), MicrobiomeStatPlot (Mediation).</td></tr></table></body></html>  

Implementing causal modeling techniques for longitudinal and panel data necessitates specialized software and libraries equipped to handle their unique structure and complexities. Widely used statistical programming environments such as R, Stata, and Python offer extensive capabilities for this purpose, alongside other dedicated software packages.  

R provides a robust ecosystem for panel data analysis and causal inference. The plm package is a foundational tool for linear panel models, offering functionalities for data management specific to panel structures, such as creating pdata.frame objects and applying transformations like Between, Within, diff, and lag [23]. It supports the estimation of various models including fixed effects, random effects, between-group, first-difference, and error components models [23]. Beyond  

standard panel regressions, R offers packages for specific causal methods. For instance, Difference-in-Differences (DID) analysis can be implemented using packages like panelView and fixest [25]. For Bayesian approaches to multivariate longitudinal data, the dynamite package provides an efficient interface leveraging Stan backends (rstan, cmdstanr) and supports estimation for a wide array of distributions, including Gaussian, Student t, Categorical, and various count and duration models [8]. General causal inference toolboxes available in R include NlinTS, Tigramite, and DoWhy [21].  

Stata is another widely adopted platform known for its extensive and well-documented commands for panel data analysis. Core commands like xtreg are standard for estimating fixed effects (FE) and random effects (RE) models [14,30,34]. Researchers can specify the panel structure using the xtset command [35]. For handling endogeneity using instrumental variables in panel contexts, xtivreg is available, which can display first-stage results via the first option and automatically adjusts standard errors [31]. Generalized Method of Moments (GMM) estimation for dynamic panel models is supported by commands such as xtabond, xtdpdsys, xtdpd, and xtabond2 [6]. Stata also includes commands for DID (diff) and provides capabilities for Regression Discontinuity (RD) designs [6]. More specialized methods, like dynamic panel threshold models, can be implemented using user-written commands such as xtendothresdpd, which requires installation and specific options for threshold variable, instruments, and estimation grids [35]. Diagnostic tests commonly used in panel data analysis, such as the Hausman test for comparing FE and RE models, are also directly supported by commands like hausman [34]. Data preparation and descriptive statistics are routinely performed in Stata [22].​  

Python has rapidly evolved into a powerful environment for econometric and causal inference analysis, leveraging its versatility and machine learning ecosystem. Libraries like statsmodels and linearmodels enable the estimation of fixed and random effects models, including code snippets for data preparation steps like time-demeaning and performing Hausman tests [4,24]. The pandas library is essential for efficient data loading, manipulation, and preprocessing, which are crucial steps before model estimation [4,12]. For causal inference rooted in graphical models, the bnlearn library (built on pgmpy) facilitates the implementation of Bayesian Networks, supporting structure learning with various algorithms (e.g., hillclimbsearch, exhaustive search) and scoring functions (e.g., BIC, K2, BDeu), as well as parameter learning (MLE, Bayesian estimation) for Conditional Probability Tables (CPTs) [7]. Python is also prominent in machine learning-based causal inference, with libraries like Microsoft's EconML providing tools for Double Machine Learning (DML) and Double Robust Learning (DRL), and Uber's CausalML for comparing different Meta Learners [1]. Other Python libraries for general causal inference include CausalDiscoveryToolbox, DoWhy, and Tigramite [21]. Bayesian DID analysis can be conducted using libraries like causalpy, which integrates with arviz and pymc for model specification, estimation, and visualization [12].  

Beyond these primary platforms, several other software packages and tools are utilized for specific aspects of longitudinal and panel data analysis with causal implications. These include SAS and SPSS for complex data processing and statistical analysis, EViews for time series and panel data econometrics (though with specific limitations for certain tests like Granger causality on pooled data without data transformation) [14,19,21], MATLAB for numerical computations, and Mplus for advanced techniques like structural equation modeling and multilevel modeling often applied to longitudinal data [21,37]. Specialized packages, such as MicrobiomeStatPlot, may offer source code and data on platforms like GitHub for specific causal methods like causal mediation analysis [15]. Data preparation steps, including data cleaning and handling missing data, are typically performed using the data manipulation capabilities within these software packages (e.g., pandas in Python, data management commands in Stata, or base R functions), followed by model diagnostics appropriate for the chosen method. Clustering standard errors at the entity level, for example, is an important diagnostic consideration, particularly when using fixed effects models [24].  

# 9. Challenges, Limitations, and Future Directions  

<html><body><table><tr><td>Category</td><td>Key Challenges/Limitations</td><td>Specific Examples / Context</td></tr><tr><td>Assumption Validity</td><td>Reliance on untestable assumptions</td><td>Parallel trends (DID), Exogeneity/Exclusion (IV), Sequential ignorability (Potential Outcomes), Linearity/Normality (Mediation).</td></tr><tr><td>Bias</td><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td></td><td>Unmeasured confounding, Selection bias, Time-varying confounding, Reverse causality, Endogeneity</td><td>Observational studies lack randomization, Standard FE doesn't handle time-varying or reverse causality.</td></tr><tr><td>Validation</td><td>Dificulty validating causal claims/structures from observational data</td><td>Requires expert knowledge (BNs), RCTs are gold standard but costly/unethical.</td></tr><tr><td>Data Requirements</td><td>High data needs, Data quality issues (errors, missing data,measurement error, unbalanced)</td><td>Sufficient T for each N needed, Impacts reliability, Adds analytical complexity.</td></tr><tr><td>Method Specific</td><td>Limitations of specific methods</td><td>Cannot estimate time- invariant effects (FE),Weak instruments (IV), Small N/Serial correlation issues (DID).</td></tr><tr><td>Complexity</td><td>Required specialized knowledge, Computational complexity</td><td>Proper application is non- trivial, Algorithmic search (BNs) can be intensive.</td></tr><tr><td>Model Selection</td><td>Uncertainty and dependence on choices (model,assumptions, estimator)</td><td>FE vs RE choice is complex, Different methods yield different results.</td></tr><tr><td>Generalizability</td><td>External validity limited to specific subpopulations/contexts</td><td>LATE (IV),effects near cutoff (RD), Context-dependent findings.</td></tr></table></body></html>  

<html><body><table><tr><td>Category</td><td>Key Challenges / Limitations</td><td>Specific Examples / Context</td></tr><tr><td>Assumption Validity</td><td>Reliance on untestable assumptions</td><td>Parallel trends (DID), Exogeneity/Exclusion (IV), Sequential ignorability (Potential Outcomes), Linearity/Normality (Mediation).</td></tr><tr><td>Bias</td><td>Unmeasured confounding, Selection bias, Time-varying confounding, Reverse causality, Endogeneity</td><td>Observational studies lack randomization, Standard FE doesn't handle time-varying or reverse causality.</td></tr><tr><td>Validation</td><td>Difficulty validating causal claims/structures from observational data</td><td>Requires expert knowledge (BNs), RCTs are gold standard but costly/unethical.</td></tr><tr><td>Data Requirements</td><td>High data needs, Data quality issues (errors, missing data,measurement error, unbalanced)</td><td>Sufficient T for each N needed, Impacts reliability, Adds analytical complexity.</td></tr><tr><td>Method Specific</td><td>Limitations of specific methods</td><td>Cannot estimate time- invariant effects (FE),Weak instruments (IV), Small</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td>N/Serial correlation issues</td></tr><tr><td></td><td>knowledge, Computational</td><td>trivial, Algorithmic search</td></tr><tr><td></td><td>dependence on choices (model,assumptions,</td><td>Different methods yield different results.</td></tr><tr><td></td><td>specific</td><td>LATE (IV), effects near cutoff (RD),Context-dependent findings.</td></tr></table></body></html>  

Despite significant advancements, causal inference in longitudinal and panel data settings faces numerous inherent challenges and limitations. A pervasive issue is the sensitivity of causal estimates to untestable or difficult-to-verify assumptions. Methods such as Difference-in-Differences (DID) fundamentally rely on the parallel trends assumption, which posits that, in the absence of treatment, the average outcomes for treated and control groups would have followed parallel trajectories over time [26,28]. While placebo tests on pre-treatment data can offer some validation [25], the assumption itself remains ultimately untestable in the post-treatment period. Critiques of standard DID methods, particularly regarding linearity assumptions, have led to the development of alternatives like the changes-in-changes estimator that relax some requirements [28]. Similarly, Instrumental Variables (IV) analysis hinges on the exogeneity and exclusion restriction of the instruments—assumptions that the instrument affects the outcome only through the treatment [31,34]. Identifying valid instruments is challenging, and finding alternative instruments for robustness checks is often difficult [34]. The interpretation of IV estimates is also often limited to the Local Average Treatment Effect (LATE), applicable only to the subpopulation whose treatment status is affected by the instrument, highlighting the assumption of heterogeneous treatment effects [2]. Traditional mediation analysis frequently relies on assumptions such as normality and linearity, which are often violated in empirical applications [15]. More broadly, the conclusions drawn from causal models are contingent on the correctness of underlying model assumptions [21]. In Bayesian analysis, model over-parameterization can lead to estimation challenges such as divergent samples [12]. The common Independent and Identically Distributed (IID) assumption underlying many machine learning and AI models is often violated in real-world scenarios, leading to Out-OfDistribution (OOD) problems that causal inference seeks to address [1]. Quasi-experimental methods, while powerful, require that the data structure aligns with specific design assumptions, and inappropriate application without careful consideration can yield misleading results [5].​  

Causal inference in observational longitudinal data is particularly susceptible to various biases. Selection bias and unmeasured confounding are significant concerns [33], as controlling for all relevant confounding variables in observational studies is inherently difficult [21]. This difficulty stems from the lack of guaranteed exchangeability between groups in observational settings compared to randomized controlled trials [11]. Time-varying confounders that are themselves affected by prior treatment pose a challenge that standard fixed effects models cannot adequately address [24]. Similarly, fixed effects models may not eliminate bias in the presence of reverse causality [24]. The endogeneity of interventions themselves is another source of potential bias not accounted for in standard DID estimation [28]. Furthermore, accurately accounting for and controlling individual heterogeneity remains a challenge in panel data regression analysis [17]. Issues like irrelevant or invalid instruments can also introduce bias in IV estimation [3].​  

Validating causal claims derived from observational studies is challenging due to these inherent biases and the reliance on untestable assumptions. The high cost and ethical considerations of Randomized Controlled Trials (RCTs) necessitate the use of observational data [11], but this requires accurate control of confounding [21]. Fully validating complex causal structures, such as those represented by Bayesian Networks, solely from data is difficult, underscoring the value of incorporating prior expert knowledge [7].  

Practical challenges also impede the application of causal inference methods to longitudinal and panel data. High data requirements, including sufficient observations over time for each individual, can be difficult to meet in practice [14,21]. Data quality issues, including errors, biases from collection and processing, missing data, and measurement error, can significantly affect the reliability of results [17]. Unbalanced panel data also poses analytical challenges [14]. Many existing panel data techniques are designed for "short" panels (many individuals, few time periods), creating challenges for "long" panels (few individuals, many time periods) [14]. Issues specific to certain methods include the breakdown of standard asymptotics for DID with a small number of treatment groups and the inefficiency of DID and FE estimators in the presence of serial correlation, which can also lead to underestimated standard errors [28]. The complexity of causal inference methods often requires specialized knowledge for proper application [21]. Computational complexity, such as the intensive search required to find optimal Directed Acyclic Graphs (DAGs) in Bayesian Networks, can be prohibitive for large variable sets [7], although advances in computing have mitigated some issues like those in GLS estimation [23]. Model selection uncertainty is another significant challenge, as different model specifications, assumptions, and estimation methods can yield disparate results [17]. Selecting between fixed and random effects models, for instance, is complex, and an incorrect choice can lead to biased estimates [14].  

Furthermore, external validity and the generalizability of findings are often limited. Causal estimates from quasiexperimental methods, such as IV or Regression Discontinuity (RD), frequently represent effects for specific subpopulations (e.g., compliers in IV, individuals near the threshold in RD) rather than the entire population [2,5]. The generalizability of study findings across different contexts, such as societies or specific types of stressors, depends on replication and validation in diverse settings [22]. Ensuring relevance often requires contextually sensitive measures [37]. Other complexities include the potential for multiple causal pathways influencing an outcome, complicating the identification of isolated causal relationships [21], and the challenge of modeling complex interactions between numerous factors, as seen in predicting clinical trial outcomes [40]. There is also a recognized trade-off between achieving high model fit (R-squared) and ensuring causal validity, particularly when using instrumental variables, emphasizing the priority of satisfying the exclusion restriction [31]. Finally, the development of causal inference methods across different fields has often occurred in relative isolation, hindering the cross-pollination of ideas necessary for innovation [10]. A fundamental challenge lies in the broader lack of universally applicable methods for reliably discovering causal relationships directly from complex data [13].  

These challenges highlight several promising avenues for future research and development. A critical direction is the development of methods more robust to assumption violations, such as exploring alternative model structures and priors in Bayesian DID [12], developing theoretical guarantees for resampling methods to handle irrelevant instruments [3], and advancing causal mediation analysis to relax traditional assumptions like normality and linearity [15]. Developing techniques for high-dimensional panel data that mitigate issues like multicollinearity and data sparsity is essential [17]. This includes exploring alternative methods for causal discovery in high-dimensional settings, such as tree-based or constraintbased approaches [7]. Integrating different causal paradigms and combining causal inference with machine learning and AI is a fertile ground for innovation [1,10]. This involves leveraging machine learning for tasks like prediction in the first stage of IV analysis or identifying target populations [5], integrating causal effect estimates into reinforcement learning frameworks [5], and applying techniques like geometric deep learning to complex data structures in domains like drug discovery [40]. Future work also includes researching nonlinear panel data models to capture complex relationships [17] and extending IV methods to handle non-linear relationships or weak instruments [3]. Leveraging new data sources and combining them with big data and machine learning technologies promises to enhance data processing and model estimation [17]. Specific areas of development for panel data include spatial panel data analysis to account for spatial dependence, dynamic panel data analysis to model temporal dynamics, and multilevel panel data analysis to investigate interactions across different hierarchical levels [17]. Further research is needed on evaluating causal effects from complex observational data, developing techniques for investigating causal mechanisms (direct and indirect effects), building models capable of predicting the effects of external interventions, and mining causal relationships and networks from diverse and complex data types [13]. Refining theoretical models based on new empirical insights and dimensions is also crucial for advancing causal understanding in specific fields [37]. Addressing these challenges and pursuing these future directions will be vital for advancing the rigor and applicability of causal modeling for longitudinal and panel data.  

# 10. Conclusion  

This survey has explored key concepts, methodologies, and challenges inherent in causal modeling for longitudinal and panel data. The analysis highlights that drawing valid causal inferences from such data requires a sophisticated understanding and careful application of specialized techniques [11,21].  

The core objective of this field is to understand causal relationships, distinguishing between frameworks such as the sufficient-component-cause approach, which addresses mechanisms ("how does it happen?"), and the counterfactual (potential outcomes) framework, which focuses on effects ("what happens?") [33]. Longitudinal and panel data, with their unique time and individual dimensions, offer significant advantages, including improved estimation efficiency, control for unobserved individual-specific heterogeneity, analysis of dynamic effects, and reduced sample selection bias compared to cross-sectional or time series data alone [4,14].​  

Key techniques discussed for leveraging these advantages include panel data regression analysis [17], particularly Fixed Effects (FE) and Random Effects (RE) models, which are commonly employed to address heterogeneity [4,34]. FE models, in particular, are noted as powerful tools for causal inference, especially when randomized data or strong instruments are unavailable, by controlling for time-invariant unobserved confounders [22,24]. Instrumental Variable (IV) techniques are crucial for addressing endogeneity more broadly, requiring careful selection and validation of instruments [3,34]. The Difference-in-Differences (DID) method emerges as a widely used quasi-experimental approach for estimating the causal effect of interventions or shocks, particularly valuable in non-randomized or non-A/B testing scenarios by comparing changes over time between treatment and control groups [25,26,28]. Other quasi-experimental methods and techniques like Bayesian Networks are also recognized as practical tools for inferring causality from observational data [5,7].  

However, these methods are not without limitations. A major challenge in causal inference is the fundamental difficulty in recognizing causal relationships from data itself [13]. Specific techniques face challenges; for instance, panel data methods, while powerful, have limitations with time-varying confounders and reverse causality [24]. The validity of the DID method fundamentally relies on the critical parallel trends assumption and can be affected by issues like serial correlation and small sample sizes [28]. Addressing endogeneity through IV requires meticulous attention to instrument selection and validation [34].​  

Therefore, when performing causal inference with longitudinal and panel data, researchers must exercise considerable diligence. This involves carefully considering the study design and its implications for causal identification, understanding and scrutinizing model assumptions (such as the parallel trends assumption for DID [28] or the exogeneity assumptions for FE/RE/IV models [24,34]), and being acutely aware of the potential limitations of the chosen methods [24,28]. Rigorous data analysis and sensitivity analyses are paramount to ensure the accuracy and reliability of the results [21]. The Hausman test, for example, plays a crucial role in guiding model selection between FE and RE models [34].  

Ultimately, the rigorous application of causal modeling techniques to longitudinal and panel data holds immense value for advancing knowledge and informing practice across a multitude of disciplines, including economics, social sciences, finance, and beyond [4,17,21,22]. Progress in causal inference is vital for enhancing our ability to understand complex phenomena in the natural sciences and humanities [13]. By providing a more accurate assessment of the impact of interventions or strategies, these methods can guide business optimization, strategic decision-making, and inform public policy [22,26]. The field continues to pose challenging research questions at the intersection of statistics, machine learning, and artificial intelligence [13], underscoring the ongoing need for methodological development and cross-disciplinary collaboration [5,10].​  

# 11. Appendix: Stationarity and Cointegration Tests for Panel Data  

Analyzing longitudinal and panel data, particularly when employing time-series methods such as Granger causality or dynamic panel models, necessitates careful consideration of time series properties like stationarity and cointegration [19]. Stationarity, also referred to as the absence of a unit root, is a critical property for time series variables. A stationary time series exhibits statistical properties such as mean, variance, and autocorrelation that remain constant over time [19]. Conversely, non-stationary series, common in economic contexts, often display trends or stochastic drifts. Regressing nonstationary variables directly can lead to spurious regression, where high R-squared values suggest a strong relationship even when no true correlation exists, driven merely by common underlying trends [19]. Strictly defined, a time series is stationary if, after removing its constant mean and time trend, the residual sequence behaves as white noise, characterized by a zero mean and constant variance [19].  

To avoid spurious regression and ensure valid inference, stationarity tests, specifically unit root tests, are essential prerequisites for panel data regression involving time series [14,19]. These tests typically assess the null hypothesis that a unit root exists (i.e., the series is non-stationary) against the alternative of stationarity. Unit root tests can be conducted under different model specifications depending on whether the series is assumed to have a constant mean (intercept) and/or a deterministic time trend; tests should ideally be performed under modes including both trend and intercept, intercept only, and neither [19].​  

Several panel unit root test methods have been developed to account for the panel structure, including the Levin, Lin & Chu (LLC) test, Im, Pesaran & Shin (IPS) test, Breitung test, ADF-Fisher test, and PP-Fisher test [19]. The LLC test assumes a common unit root process across all panel units and is considered suitable for medium-sized panels, typically with the time series dimension (T) between 25 and 250 and the cross-sectional dimension (N) between 10 and 250 [19]. The IPS test, ADFFisher test, and PP-Fisher test, among others, allow for heterogeneous unit root processes across panel units. For practical application, it is often suggested to utilize both a common root test like LLC and an individual root test like Fisher-ADF [19]. A sequence is generally considered stationary if both types of tests reject the null hypothesis of a unit root [19]. The standard procedure involves testing the level sequence first. If a unit root is found, first-order, second-order, or higher-order differencing is applied until the series becomes stationary [19]. A series that becomes stationary after $d$ differencing steps is denoted as integrated of order $d$ , or $I ( d )$ [19].  

When variables are non-stationary but integrated of the same order (e.g., all are I(1)), cointegration tests are used to investigate the existence of a long-term equilibrium relationship among them [14,19]. Cointegration implies that while individual series are non-stationary, a specific linear combination of these variables is stationary [19]. This stationary linear combination represents the deviation from the long-run equilibrium. Common panel cointegration test methods include those proposed by Pedroni, Kao, and Johansen [19]. These tests differ in their assumptions regarding cross-sectional independence and heterogeneity of cointegrating vectors. If variables are found to be cointegrated, standard regression models can be applied to the cointegrating relationship, or error correction models can be employed. Conversely, if variables are non-stationary and not cointegrated, or if they have different orders of integration, model modifications such as differencing certain variables may be necessary, provided economic significance is preserved [19].​  

![](images/4ca8d65c588a6023b871040def5ac4ce8516dfa513819d03bdf913cccc966e8c.jpg)  

![](images/4160389ba31d5af8647140b7544e52a1512b2b45fbb971aa16ffef5fc737dbb9.jpg)  

![](images/5293c4698118b674d2d39a1bc79179f26cc208d5e08f9396d34f93af245e54de.jpg)  

In summary, conducting stationarity and cointegration tests is paramount for rigorous time-series analysis with panel data. These tests inform appropriate model selection and ensure that statistical inferences drawn from models like Granger causality or dynamic panel models are valid, preventing misleading conclusions stemming from spurious correlations [19].  

# References  

[1] 金雅然：因果推断技术思想与方法总结 https://baijiahao.baidu.com/s?id $=$ 1753445414392961455&wfr=spider&for=pc   
[2] 工具变量：定义、寻觅、实证与因果推断 https://mp.weixin.qq.com/s?   
__biz=Mzg3NzU5NjMyNg==&mid=2247561892&idx $\underline { { \underline { { \mathbf { \Pi } } } } } =$ 3&sn $| =$ 918beb0842f65b076fb796efad661f59&chksm $\mid =$ cf23296df854a07bb   
dee64789e320d5aea20ebfcaa3b533434e788002576005f28c74b777049&scene=27   
[3] Causal Inference with Many Instruments: Fighting N   
https://www.stat.pku.edu.cn/kxyj/xshd/xsjz/tjysjkxxlxsjz/1e5aac1c6ee641cf949c591026a1a763.htm​   
[4] 面板数据模型：固定效应与随机效应 https://www.cnblogs.com/haohai9309/p/18501377​   
[5] 准实验：从观察数据中推断因果关系的数据科学方法 https://www.thepaper.cn/newsDetail_forward_11798559   
[6] 内生性问题解决策略汇总：工具变量、GMM、PSM、Heckman、面板数据、双重差分、断点回归   
https://mp.weixin.qq.com/s?  

_biz $: =$ Mzg3NzU5NjMyNg==&mid $\begin{array} { r } { { \bf \Pi } = \frac { \bf { \dot { \Pi } } } { \bf \Pi } } \end{array}$ 2247660688&idx=3&sn=f56bc3a1f330488a9780e4e38de8e70a&chksm $\vDash$ cf2caf59f85b264f70 2370ae2e240f9667f7a24be7e7ae65fd03bd7eb370671dc9b30ba431f7&scene=27  

[7] Python贝叶斯网络：因果关系检测实战指南 https://mp.weixin.qq.com/s?  
_biz=MzU0MDQ1NjAzNg==&mid=2247586365&id $\ L =$ 3&sn=622a60b32c0f02fa5a9ce940c20fff40&chksm=fad474b559c294d6e8  
db048dd3fa139c3feb6846cfedd0caae20b08abf603f3c3dd74c550a6a&scene=27  

[8] dynamite: Bayesian Modeling for Multivariate Longi https://docs.ropensci.org/dynamite/ [9] [原版经典] Wooldridge《横截面与面板数据计量经济学分析》2e (2010) https://bbs.pinggu.org/thread-3656367-1-1.html ] 纵向数据因果推断跨学科研讨会 https://blog.csdn.net/lazysnake666/article/details/123814999  

11] Causal Inference with Longitudinal Data: Challenge https://lmns.fudan.edu.cn/3f/2a/c26299a671530/page.htm [12] PyMC 模型进行差分-in-差分分析：Python 因果推断实践 https://blog.csdn.net/qq_32146369/article/details/140749355  

[13] 因果推断：统计学中的挑战与探索 https://mp.weixin.qq.com/s?   
_biz=Mzg4NTM2NTI5Ng==&mid=2247596657&idx=3&sn=5dbb910417301a57c310e983cfdfe8f5&chksm $\mid =$ ce1d69adb491045d   
8302d779e2b2fe9c69e8e700b135a1b480b9bca546e8ee26707aa87eccf9&scene=27  

[14] 面板数据分析：特点、流程、应用与优缺点 https://baijiahao.baidu.com/s?id $| =$ 1801609545197272898&wf $\mathbf { \bar { \rho } } = \mathbf { \rho }$ spider&for=pc [15] MicrobiomeStatPlot：因果中介分析简介及案例 https://blog.csdn.net/woodcorpse/article/details/142907772 [16] Min Zhang: Biostatistics, Causal Inference, and Ap https://vsph.tsinghua.edu.cn/en/info/1010/1471.htm  

17] 面板数据回归分析：模型选择与应用指南 https://www.fanruan.com/blog/article/359202 [18] MCMC与贝叶斯因果推断：经典文献推荐 https://www.bilibili.com/read/cv28247457  

[19] 面板数据平稳性检验、协整检验及模型选择 https://mp.weixin.qq.com/s?   
__biz=MzA3NDg2NzQzNw $\scriptstyle = =$ &mid=2650977839&idx=1&sn=f38060a2de3d3351d38e313bd933af46&chksm=85250efc0f77c16f1   
9570bea25f0de0c1e87abdbf7303134a1942cc702aec626b5f04756bd95&scene=27  

[20] 世界计量经济学会2024亚洲计量经济学和统计学暑期学校报名启动 https://mp.weixin.qq.com/s?_biz $: =$ MjM5MjIwMjMyMw $\scriptstyle = =$ &mid=2651394731&idx $\mathop { : = }$ 1&sn $=$ 893f6453184920cf5a0b3cffc744fab4&chksm=bd5459978a23d08100e714359ba5d31ebeb61b4b7720d2a0366505546d4dd604c8ce8c5d1fa4&scene=27  

[21] 金融领域的因果推断：方法、应用与挑战 https://baijiahao.baidu.com/s?id $\ c =$ 1801071496883306736&wfr=spider&for=pc [22] Work and Life Stressors: Causal Effects on Perceiv https://journals.plos.org/plosone/article? id=10.1371/journal.pone.0290410  

[23] R语言plm包：面板数据计量经济学简介 https://mirrors.sjtug.sjtu.edu.cn/cran/web/packages/plm/vignettes/A_plmPackage.html [24] Panel Data and Fixed Effects https://matheusfacure.github.io/python-causality-handbook/14-Panel-Data-and-FixedEffects.html​  

[25] Difference-in-Differences (DID): Causal Inference  https://bookdown.org/mike/data_analysis/sec-difference-indifferences.html  

[26] 双重差分模型：非ABTest情境下的策略效果评估利器 https://zhidao.baidu.com/question/573024228625168764.html [27] 2021泛太平洋因果推断大会：27场前沿报告直播 https://swarma.org/?p=28825 [28] Difference-in-Differences: A Panel Data Method for https://www.sciencedirect.com/topics/economics-econometricsand-finance/difference-in-differences  

[29] 因果推断好书推荐 https://mp.weixin.qq.com/s? biz $: =$ MzA4ODQ2NzIyMQ $\scriptstyle 1 = =$ &mid $\circleddash$ 2652948081&idx $\mathop { : = }$ 1&sn $\ c =$ 248733426063dd32f33e511daec1768d&chksm $| =$ 8bfdd40bbc8a5d1   
d6e006dc0dd8af260c401c3fe5d0d29490796c0b411e293a8b3b5236cbc02&scene=27  

[30] Stata面板数据分析：固定效应与随机效应（普林斯顿教程） https://roll.sohu.com/a/740014318_121124024 [31] Panel Data Fixed Effects Endogeneity and IV Regres https://www.statalist.org/forums/forum/general-statadiscussion/general/1743279-panel-data-fixed-effects  

[32] 面板数据分析：固定效应与随机效应模型选择及应用 https://baijiahao.baidu.com/s?   
id=1828769866800571903&wfr=spider&for=pc​   
[33] Causal Inference: Notes and Summary https://book.douban.com/review/13289092/   
[34] 面板数据、工具变量选择及Hausman检验：若干问题探讨 https://business.sohu.com/a/674940001_121124212   
[35] Stata动态面板门槛模型：xtendothresdpd命令详解与应用 https://it.sohu.com/a/850799459_121124024   
[36] 2023最新因果推断书籍汇总 https://news.sohu.com/a/726117732_121124024​   
[37] DePTh Lab: Youth Development & Quantitative Method https://health.oregonstate.edu/labs/DePTh​   
[38] 香港大学经济学硕士（MEcon）申请攻略 https://baijiahao.baidu.com/s?id=1733430495676321413&wfr=spider&for=pc   
[39] 面板数据计量分析 https://www.docin.com/touch_new/mip_preview_new.do?id $=$ 1063134959   
[40] 北大数学周学术报告一览(05.06-05.12) https://mp.weixin.qq.com/s? _biz=MzUzMzg4MzgxMQ $\scriptstyle = =$ &mid=2247498090&idx $\mathop { : = }$ 2&sn=2d16675f9e00f7a6c85cc3781d5e1ef2&chksm=fa9f87a6cde80eb0d   
7739563ed00cee2a3301ed3f8c466d85ad3fd3225c5be34be97b37987a1&scene=27   
[41] Difference-in-Differences模型讨论 https://wenku.baidu.com/view/90e68d5a270c844769eae009581b6bd97e19bc5c.html  