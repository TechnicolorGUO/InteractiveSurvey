# A Survey of Business Process Management Systems in Organizational Contexts

# 1 Abstract


The field of Business Process Management (BPM) has evolved significantly, driven by advancements in information technology and the increasing complexity of organizational processes. This survey paper focuses on the application and evolution of Business Process Management Systems (BPMS) within organizational contexts, exploring how BPMS can optimize business processes, ensure regulatory compliance, and enhance overall organizational performance. The paper synthesizes existing knowledge and identifies gaps in the literature, providing a comprehensive overview of the current state of BPMS. Key findings include the integration of digital technologies such as artificial intelligence and blockchain, the use of large language models (LLMs) for process modeling and automation, and the importance of interdisciplinary collaboration in advancing BPM research and practice. The paper highlights the practical implications of BPMS and underscores the potential of emerging technologies in transforming BPM and driving innovation in organizational processes.

# 2 Introduction
The field of Business Process Management (BPM) has evolved significantly over the past few decades, driven by advancements in information technology, the increasing complexity of organizational processes, and the need for greater efficiency and agility in a rapidly changing business environment [1]. BPM encompasses a wide range of activities, from the design and modeling of business processes to their execution, monitoring, and continuous improvement [1]. The integration of digital technologies, such as artificial intelligence, machine learning, and blockchain, has further transformed the landscape of BPM, enabling more sophisticated and data-driven approaches to process management [2]. These advancements have not only enhanced the operational efficiency of organizations but have also facilitated better alignment with strategic goals and regulatory requirements. As organizations continue to face new challenges and opportunities, the role of BPM in driving organizational success remains paramount [3].

This survey paper focuses on the application and evolution of Business Process Management Systems (BPMS) within organizational contexts [1]. Specifically, it explores how BPMS can be leveraged to optimize business processes, ensure regulatory compliance, and enhance overall organizational performance [4]. The paper delves into various methodologies and frameworks that support the development and implementation of BPMS, including empirical and qualitative research methods, quantitative approaches, and the use of large language models (LLMs) [5]. By synthesizing existing knowledge and identifying gaps in the literature, this survey aims to provide a comprehensive overview of the current state of BPMS and highlight key areas for future research and practice [6].

The paper begins by examining the role of comprehensive overviews and systematic reviews in advancing BPM research [7]. These overviews synthesize existing literature, identify gaps, and provide a structured understanding of the field, integrating insights from diverse disciplines such as information technology, operations research, and organizational behavior. The paper then explores frameworks for visual aggregation, which are essential for consolidating and presenting complex data in a comprehensible manner. These frameworks use data visualization techniques and machine learning algorithms to facilitate decision-making and process optimization. Additionally, the paper discusses the systematic synthesis of studies, which involves rigorous methodologies to aggregate and analyze data from multiple sources, ensuring the findings are comprehensive and reliable.

Next, the paper delves into quantitative and qualitative approaches in BPM, including multidimensional dynamic systems (MDS) and integral indicators. MDS models capture and analyze multiple dimensions of data and processes simultaneously, enabling a more holistic understanding of system behavior. Integral indicators, on the other hand, provide a unified view of economic health by integrating multiple dimensions of performance into a single metric. The paper also examines mixed-methods approaches, which combine qualitative and quantitative data to provide a more nuanced understanding of complex issues in BPM.

The paper further explores case studies and real-world applications of BPMS, including design science research methodology, collaborative workshops, and empirical validation of theoretical models [6]. These sections highlight the practical implications and real-world applicability of BPMS, emphasizing the importance of interdisciplinary collaboration and empirical validation in advancing the field [8]. The paper also discusses formal and prototype development in BPM, including ontology and formal specification, performance and cost analysis, and the role of blockchain in ensuring regulatory transparency and compliance [9].

Finally, the paper examines the application of large language models (LLMs) in BPM, focusing on their use in process modeling, automation, and enhancing the comprehension of process models [8]. LLMs offer advanced capabilities in natural language processing and can automate tasks such as the extraction of process-related information from unstructured text, the identification of unnecessary steps in business processes, and the generation of structured process models [10]. The paper concludes by outlining future directions for research and practice in BPM, emphasizing the need for continued innovation and the integration of emerging technologies to address the evolving challenges and opportunities in the field [6].

The contributions of this survey paper are multifaceted. First, it provides a comprehensive and structured overview of the current state of BPMS, synthesizing insights from a wide range of research and practice [11]. Second, it identifies key gaps and areas for future research, guiding researchers and practitioners in their efforts to advance the field. Third, it highlights the practical implications and real-world applications of BPMS, demonstrating how these systems can be leveraged to enhance organizational performance and compliance [4]. Finally, the paper underscores the potential of emerging technologies, such as LLMs, in transforming BPM and driving innovation in organizational processes [8].

# 3 Empirical and Qualitative Research Methods in BPM

## 3.1 Literature Reviews and Umbrella Reviews

### 3.1.1 Comprehensive Overviews of Academic Knowledge
Comprehensive overviews of academic knowledge serve as foundational pillars for advancing research and practice in various domains. These overviews synthesize existing literature, identify gaps, and provide a structured understanding of the field. In the context of business process management (BPM), comprehensive overviews have been instrumental in elucidating the complexities and interdependencies within organizational processes [4]. By integrating insights from diverse disciplines such as information technology, operations research, and organizational behavior, these overviews offer a holistic perspective that facilitates the development of more robust and adaptable BPM frameworks [3].

The process of creating comprehensive overviews involves rigorous methodologies, including systematic literature reviews, meta-analyses, and thematic synthesis. These methods ensure that the collected knowledge is not only extensive but also critically evaluated and synthesized in a manner that highlights key trends, emerging themes, and unresolved issues. For instance, in the realm of digital transformation (DT) and business processes (BPs), comprehensive overviews have helped in identifying the synergies and challenges associated with integrating DTs into BPs [12]. Such overviews often involve collaborative efforts from a multidisciplinary group of researchers, ensuring a broad and balanced perspective.

Moreover, comprehensive overviews play a crucial role in guiding future research and practice. They not only summarize the current state of knowledge but also outline potential avenues for further exploration. By highlighting areas that require more attention, such as the integration of machine learning (ML) in BPM or the sustainability of business processes, these overviews inspire new hypotheses and research questions [6]. Additionally, they provide practitioners with a clear understanding of best practices and emerging tools, facilitating the adoption of innovative solutions in real-world settings. Thus, comprehensive overviews are essential for fostering continuous improvement and innovation in the field of BPM [7].

### 3.1.2 Frameworks for Visual Aggregation
Frameworks for visual aggregation are essential in the context of data-driven business process management (BPM) as they enable the consolidation and presentation of complex data in a comprehensible manner. These frameworks typically consist of a set of methods and tools designed to transform raw data into visual representations that facilitate decision-making and process optimization. The primary goal of visual aggregation frameworks is to reduce the cognitive load on users by summarizing large datasets into meaningful insights, thereby enhancing the interpretability and usability of the data.

One of the key components of these frameworks is the use of data visualization techniques, such as charts, graphs, and dashboards, which are tailored to the specific needs of BPM. For instance, process mining tools often employ visual aggregation to highlight patterns and anomalies in process execution data, allowing managers to identify bottlenecks and inefficiencies. Additionally, these frameworks may integrate machine learning algorithms to automate the detection of trends and outliers, further enhancing the analytical capabilities of the system.

Another important aspect of visual aggregation frameworks is their ability to support collaborative decision-making. By providing a shared, interactive interface, these frameworks enable stakeholders from different departments to engage in real-time analysis and discussion, fostering a more integrated and informed approach to process management. Furthermore, the adaptability of these frameworks to various data sources and formats ensures that they can be applied across different industries and organizational contexts, making them a versatile tool in the BPM toolkit.

### 3.1.3 Systematic Synthesis of Studies
Systematic synthesis of studies in the context of e-procurement and related technologies involves a rigorous and structured approach to aggregating and analyzing data from multiple sources. This method ensures that the findings are comprehensive, reliable, and applicable to a wide range of contexts. The synthesis process typically begins with a detailed literature search, where relevant studies are identified through databases, conference proceedings, and other scholarly sources. The inclusion and exclusion criteria are meticulously defined to ensure that only high-quality, relevant studies are considered. This step is crucial for maintaining the integrity and validity of the synthesis.

Once the studies are selected, the data extraction process is initiated. This involves systematically collecting and organizing data from each study, focusing on key variables such as the type of e-procurement system, the context of implementation, the methodologies used, and the outcomes reported. Data extraction forms are often used to standardize this process, ensuring that all relevant information is captured consistently. The extracted data is then analyzed using both qualitative and quantitative methods, depending on the nature of the data and the research questions. Meta-analysis, for instance, can be used to quantitatively combine the results of multiple studies, providing a more robust estimate of the effects of e-procurement systems.

Finally, the synthesized results are interpreted and discussed, highlighting the main findings, trends, and patterns. This section often includes a critical evaluation of the strengths and limitations of the included studies, as well as an assessment of the overall quality of the evidence. The synthesis may also identify gaps in the existing literature, suggesting areas for future research. By providing a comprehensive overview of the current state of knowledge, systematic synthesis of studies serves as a valuable resource for researchers, practitioners, and policymakers, guiding the development and implementation of e-procurement systems.

## 3.2 Quantitative and Qualitative Approaches

### 3.2.1 Multidimensional Dynamic Systems
Multidimensional dynamic systems (MDS) represent a sophisticated approach to modeling complex systems, particularly in the context of business process management (BPM). These systems are characterized by their ability to capture and analyze multiple dimensions of data and processes simultaneously, enabling a more holistic understanding of system behavior. MDS integrates various types of data, including structured and unstructured, and considers the dynamic interactions between different components of the system. This multidimensionality is crucial for addressing the intricate and evolving nature of modern business environments, where processes are often inter-organizational and distributed.

In the realm of BPM, MDS can be applied to enhance process modeling and optimization. By leveraging advanced data analytics and machine learning techniques, MDS can identify patterns and anomalies in process execution, predict potential issues, and suggest corrective actions. For instance, in the e-procurement domain, MDS can be used to monitor and optimize the procurement process by analyzing data from various sources such as supplier performance, market trends, and internal operations. This comprehensive analysis allows organizations to make data-driven decisions that improve efficiency, reduce costs, and enhance compliance with regulatory requirements.

Moreover, MDS facilitates the integration of digital twins (DTs) into BPM, providing a virtual representation of physical processes and assets. DTs enable real-time monitoring and simulation, allowing organizations to test different scenarios and optimize processes without disrupting actual operations [12]. This capability is particularly valuable in industries such as manufacturing and healthcare, where the ability to predict and prevent failures can significantly impact operational performance and patient outcomes. By combining the strengths of MDS and DTs, organizations can achieve a higher level of process agility and resilience, ultimately driving sustainable business growth and innovation.

### 3.2.2 Integral Indicators for Economic Phenomena
Integral indicators represent a sophisticated method for evaluating and predicting economic phenomena, particularly in the context of financial crises and organizational performance [13]. Developed in 2008, this method has been extensively tested and applied to various economic scenarios, including the 2008 financial crisis. The core principle of integral indicators is to capture the multifaceted nature of economic systems by integrating multiple dimensions of performance into a single, comprehensive metric. This approach allows for a more nuanced and holistic assessment of economic health, which is crucial for making informed decisions and policy adjustments.

The method of integral indicators is particularly valuable in analyzing the activities of complex economic entities such as banks, stock exchanges, and insurance markets [13]. By aggregating diverse data points—ranging from financial ratios and market indices to operational metrics and external economic conditions—integral indicators provide a unified view of an organization's performance. This unified view is essential for identifying trends, detecting anomalies, and forecasting potential risks. For instance, during the 2008 financial crisis, integral indicators were instrumental in predicting market downturns and informing strategic responses by financial institutions and regulatory bodies [13].

Moreover, the application of integral indicators extends beyond crisis management to ongoing business operations. In the modern business environment, characterized by rapid technological changes and global market dynamics, organizations face a multitude of challenges that require continuous monitoring and adaptation. Integral indicators offer a robust framework for assessing the impact of these changes on organizational performance. By integrating data from various sources, including internal operations, market conditions, and regulatory environments, these indicators enable organizations to make data-driven decisions that enhance resilience and competitiveness. This method is particularly relevant for small and medium-sized businesses (SMBs) in regions like Asia-Pacific, where IT and telecommunications investments are significant, and the ability to quickly adapt to market changes is critical for survival and growth.

### 3.2.3 Mixed-Methods for Comprehensive Insights
Mixed-Methods for Comprehensive Insights

Mixed-methods approaches in the context of e-Governance and business process management (BPM) offer a holistic perspective by integrating both qualitative and quantitative data to provide a more nuanced understanding of complex issues. This methodological framework is particularly valuable in the evaluation of e-Governance initiatives, such as the Nation e-Governance Plan (NeGP), which aims to enhance the accessibility and efficiency of government services [14]. By combining structured data from service delivery metrics with qualitative insights from user feedback and stakeholder interviews, mixed-methods research can uncover the multifaceted impacts of these initiatives on various user groups, including the common man, businesses, and government agencies.

The integration of mixed-methods in BPM research and practice is essential for addressing the dynamic and multifaceted nature of organizational processes [15]. For instance, in the adoption and implementation of e-procurement systems, mixed-methods can help in identifying the technical, organizational, and human factors that influence the success of such systems. Quantitative data from system usage logs and performance metrics can be complemented with qualitative data from user surveys and focus groups to understand the barriers and facilitators of system adoption. This comprehensive approach not only enhances the validity and reliability of the findings but also provides actionable insights for stakeholders to improve system design and implementation.

Moreover, mixed-methods research is instrumental in the continuous improvement of e-Governance and BPM systems. By iteratively combining data from different sources and methods, researchers and practitioners can monitor the evolution of processes and systems over time, identify emerging trends, and adapt strategies accordingly. For example, in the context of the NeGP, mixed-methods can be used to assess the long-term sustainability and impact of e-Governance services on societal well-being. This approach supports evidence-based decision-making and ensures that the development and implementation of e-Governance and BPM systems are aligned with the evolving needs and expectations of the stakeholders.

## 3.3 Case Studies and Real-World Applications

### 3.3.1 Design Science Research Methodology
Design Science Research (DSR) methodology is employed in this study to systematically develop and evaluate the artifact designed to enhance e-procurement systems. The DSR approach emphasizes the creation of innovative solutions to address specific problems within a given context, aligning with the broader goals of the e-Procurement Mission Mode Project (MMP) to streamline and reform public procurement processes [14]. The methodology involves a cyclical process of defining the problem, designing the solution, implementing the artifact, and rigorously evaluating its effectiveness. This iterative nature ensures that the developed artifact is both theoretically grounded and practically applicable, contributing to both scientific knowledge and technological advancement.

The artifact developed in this research, the Scripting Your Process (SYP) method, is designed to facilitate the creation of web-based interactive narratives from business process models [5]. This method addresses the challenge of translating complex business processes into user-friendly, interactive formats that can be easily understood and utilized by stakeholders involved in the procurement process. The SYP method is grounded in design requirements derived from stakeholder needs and theoretical foundations in human-computer interaction and business process management. The development of the SYP method involves a series of design cycles, each refining the method based on feedback and evaluation, ensuring that the final artifact meets the specified design criteria and effectively supports the e-procurement MMP objectives.

To validate the effectiveness of the SYP method, a comprehensive evaluation framework is employed, encompassing both quantitative and qualitative metrics. Quantitative measures include system performance indicators such as processing speed and accuracy, while qualitative assessments focus on user satisfaction and the method's impact on improving the procurement process. The evaluation phase also involves comparative analysis with existing methods to highlight the unique contributions and advantages of the SYP method. Through this rigorous evaluation, the research aims to demonstrate the practical utility of the SYP method in enhancing the transparency, efficiency, and user experience of e-procurement systems, thereby supporting the broader goals of the e-Procurement MMP.

### 3.3.2 Collaborative Workshops and Interdisciplinary Collaboration
Collaborative workshops and interdisciplinary collaboration play a pivotal role in advancing the Nation e-Governance Plan (NeGP) by fostering a comprehensive understanding and integration of diverse perspectives [14]. These workshops serve as platforms where stakeholders from various domains, including government officials, technologists, and community leaders, come together to share insights and co-create solutions. The interactive nature of these sessions facilitates the identification of gaps, challenges, and opportunities, thereby enhancing the overall effectiveness and adaptability of e-governance initiatives. For instance, the integration of Digital Twins (DTs) and Business Processes (BPs) has been a focal point of recent workshops, highlighting the potential for real-time monitoring and optimization of government services [12].

The collaborative setting of these workshops is designed to break down silos and promote cross-disciplinary dialogue. Participants engage in discussions that contextualize theoretical concepts with practical applications, leading to a deeper understanding of the complexities involved in implementing e-governance systems. This approach not only enhances the technical feasibility of projects but also ensures that they are aligned with the socio-economic needs of the community. The use of interactive narratives and simulations during these sessions helps in visualizing potential scenarios and outcomes, thereby aiding in strategic planning and decision-making. Moreover, the workshops often result in the formation of working groups that continue to collaborate beyond the initial meeting, ensuring sustained progress and innovation.

Interdisciplinary collaboration extends beyond the scope of workshops, encompassing ongoing partnerships and knowledge exchange initiatives. These collaborations leverage the strengths of different disciplines, such as computer science, public administration, and social sciences, to develop holistic solutions. For example, the integration of State Wide Area Networks (SWANs) and State Data Centres (SDCs) requires a deep understanding of both technical and administrative aspects. By bringing together experts from these fields, interdisciplinary teams can address the multifaceted challenges of data security, network reliability, and user accessibility. Additionally, these collaborations facilitate the development of standardized protocols and best practices, which are essential for the scalability and sustainability of e-governance initiatives. Overall, the synergy achieved through collaborative workshops and interdisciplinary efforts is crucial for realizing the vision of efficient, transparent, and reliable government services.

### 3.3.3 Empirical Validation of Theoretical Models
Empirical validation of theoretical models is a critical step in establishing the credibility and practical utility of proposed frameworks in the field of business process management (BPM) [5]. This section delves into the methodologies and outcomes of empirical studies that test the robustness of theoretical models, particularly those related to process optimization, compliance monitoring, and environmental impact assessment. The primary objective of these validations is to bridge the gap between theoretical constructs and real-world applications, ensuring that the models can effectively guide decision-making and process improvement in diverse organizational settings.

Several key approaches are employed in the empirical validation of theoretical models. These include case studies, simulations, and experimental designs. Case studies, often conducted in specific industry contexts, provide in-depth insights into how theoretical models perform under real-world conditions. Simulations, on the other hand, allow researchers to control variables and test the models under a wide range of scenarios, thereby enhancing the generalizability of the findings. Experimental designs, particularly those involving controlled environments, help isolate the effects of specific variables on the model's performance, providing a rigorous basis for validating the theoretical assumptions. Each of these methods contributes uniquely to the validation process, and their combined use often yields a comprehensive understanding of the model's strengths and limitations.

The outcomes of empirical validation studies have significant implications for both research and practice. For instance, studies that validate models for process optimization have demonstrated the effectiveness of certain algorithms in reducing process cycle times and improving resource utilization. Similarly, empirical research on compliance monitoring has highlighted the importance of real-time data analytics in detecting and correcting deviations from regulatory standards [16]. In the context of environmental impact assessment, empirical validation has shown that integrating life cycle assessment (LCA) into BPM can lead to more sustainable business practices by identifying and mitigating environmental risks [11]. Overall, these empirical findings not only reinforce the theoretical foundations of BPM but also provide actionable insights for practitioners aiming to enhance their organizational processes [8].

# 4 Formal and Prototype Development in BPM

## 4.1 Ontology and Formal Specification

### 4.1.1 BPCM Ontology for Process Changes
The BPCM ontology is designed to address the challenges of managing changes in business processes, particularly in environments where processes are highly dynamic and collaborative [17]. This ontology serves as a structured representation of the knowledge and information related to business process changes, enabling a more systematic and coherent approach to change management [17]. By formalizing the concepts, relationships, and attributes associated with process changes, the BPCM ontology facilitates better communication and coordination among stakeholders, ensuring that changes are effectively communicated, understood, and implemented [17].

The core components of the BPCM ontology include a detailed taxonomy of change types, such as incremental changes, radical changes, and adaptive changes, each with specific characteristics and impacts on the business process. Additionally, the ontology defines the lifecycle of a change, from identification and proposal to implementation and evaluation, providing a clear framework for managing the change process. This lifecycle is further enriched by incorporating roles and responsibilities, change impact analysis, and risk assessment, which are critical for ensuring that changes are managed in a controlled and effective manner.

The BPCM ontology also integrates with existing business process management (BPM) frameworks and standards, such as BPMN, to ensure compatibility and seamless integration into existing process models [18]. By leveraging the formal structure of the ontology, organizations can more easily adapt their processes to changing business conditions, regulatory requirements, and technological advancements. Furthermore, the ontology supports the development of automated tools and systems that can assist in the identification, analysis, and implementation of process changes, thereby enhancing the overall agility and responsiveness of the organization [19].

### 4.1.2 Transparency Information Language Extensions
Transparency Information Language Extensions (TILEs) represent a critical advancement in the realm of Business Process Management (BPM) by enabling the systematic integration of transparency-related metadata into process models [20]. These extensions are designed to enhance the clarity and traceability of business processes, particularly in environments where regulatory compliance and stakeholder trust are paramount. By leveraging established mechanisms such as BPMN (Business Process Model and Notation), TILEs allow for the precise mapping of transparency information onto process elements, ensuring that all relevant data, such as data controllers, processing purposes, and retention periods, are clearly documented and accessible.

The implementation of TILEs involves the development of a dedicated editor plugin for platforms like Camunda, which supports the creation and management of transparency information alongside the process model. This plugin not only facilitates the modeling of personal data processing but also ensures that the transparency information is integrated seamlessly into the process execution. The plugin enables stakeholders to perform ex ante processing transparency checks, verifying that the process design aligns with legal and regulatory requirements before deployment. This proactive approach helps mitigate risks associated with non-compliance and enhances the overall trustworthiness of the business process.

Furthermore, the cloud-native architecture of TILEs supports the dynamic updating and versioning of process models, allowing organizations to adapt to changing regulatory landscapes and business needs. The formal specification of transparency information ensures that all stakeholders have a clear and unambiguous understanding of the process, facilitating effective communication and collaboration. This structured approach to transparency not only aids in compliance but also promotes a culture of accountability and transparency within the organization, ultimately leading to more robust and trustworthy business processes.

### 4.1.3 Metamodeling for Enhanced Flexibility
Metamodeling plays a pivotal role in enhancing the flexibility of business process models, particularly in dynamic and evolving environments [21]. By defining a higher-level abstraction, metamodels allow for the creation of flexible and adaptable process models that can be easily modified to accommodate new requirements or changes in the business environment. This is achieved through the use of customizable elements and parameters that can be adjusted without altering the underlying structure of the model. For instance, in the context of blockchain-based systems, metamodels can facilitate the seamless integration of on-chain and off-chain components, enabling the dynamic reconfiguration of processes as needed.

The flexibility introduced by metamodeling is particularly valuable in scenarios where business processes are subject to frequent changes due to regulatory updates, market dynamics, or technological advancements. Metamodels can support the definition of multiple implementation options for a single activity, allowing organizations to choose the most appropriate implementation at runtime. This capability is crucial for ensuring that processes remain aligned with business goals and can adapt to new constraints or opportunities. Furthermore, metamodels can be used to encode business rules and policies, which can be dynamically evaluated and enforced during process execution, ensuring compliance and consistency across different process instances.

In practice, the implementation of metamodeling techniques often involves the use of model-driven development (MDD) frameworks that provide tools for creating, managing, and deploying metamodels. These frameworks support the creation of domain-specific languages (DSLs) that enable process designers to express complex business logic in a structured and understandable manner. By leveraging these DSLs, organizations can achieve a higher degree of customization and flexibility in their process models, ultimately leading to more efficient and effective business operations. The ability to bind implementations at different times, such as at design time, deployment time, or runtime, further enhances the adaptability of the system, allowing for real-time adjustments to process behavior in response to changing conditions.

## 4.2 Performance and Cost Analysis

### 4.2.1 Experimental Evaluation of Execution Costs
The experimental evaluation of execution costs in blockchain-based business processes is a critical area of research, focusing on the economic feasibility and efficiency of deploying and maintaining smart contracts. This section delves into the methodologies and metrics used to assess the execution costs, highlighting the trade-offs between on-chain and off-chain execution. Initial findings indicate that while on-chain execution ensures transparency and immutability, it incurs significant gas fees, especially for complex transactions. These costs are directly influenced by the computational complexity, storage requirements, and the frequency of transactions. For instance, the deployment of a smart contract on the Ethereum network involves a fixed cost, which can be substantial for intricate contracts with numerous functions and data structures.

To mitigate these costs, researchers have explored various optimization techniques, such as code refactoring, modularization, and the use of layer-two solutions. Layer-two solutions, such as state channels and sidechains, offer a promising approach by moving the bulk of the computation and storage off the main blockchain, thereby reducing the gas fees associated with on-chain transactions [22]. However, these solutions introduce new challenges, including the need for robust security mechanisms to prevent fraud and ensure data integrity. The experimental results show that while layer-two solutions can significantly reduce execution costs, they may compromise the decentralization and security guarantees inherent to on-chain execution.

Furthermore, the evaluation also considers the long-term sustainability of blockchain-based systems, particularly in terms of resource consumption and environmental impact. The energy consumption of blockchain networks, especially those using proof-of-work (PoW) consensus algorithms, has been a subject of concern. Researchers have proposed alternative consensus mechanisms, such as proof-of-stake (PoS) and proof-of-authority (PoA), which are less energy-intensive. The experimental data suggests that these alternatives can reduce the overall execution costs and environmental footprint of blockchain-based business processes, making them more viable for widespread adoption. However, the transition to these mechanisms requires careful consideration of the trade-offs between security, decentralization, and efficiency.

### 4.2.2 Stochastic Games for Incentive Alignment
Stochastic games provide a robust framework for modeling and solving incentive alignment problems in inter-organizational business processes [23]. These games are particularly useful in settings where multiple parties with potentially conflicting interests must collaborate to achieve a common goal. By representing the interactions between these parties as a stochastic game, we can capture the dynamic and probabilistic nature of their decisions and outcomes. The key advantage of using stochastic games is their ability to model the uncertainties and strategic interactions inherent in collaborative processes, allowing for the computation of optimal strategies that align the incentives of all participants.

In the context of blockchain-based business processes, stochastic games can be employed to ensure that all participants adhere to the agreed-upon rules and incentives. This is achieved by encoding the business process model and the associated incentives into a stochastic game, where the payoffs for each player are determined by the outcomes of the process [23]. The game-theoretic approach enables the identification of Nash equilibria, where no player can unilaterally deviate from the agreed strategy without reducing their expected payoff. This alignment of incentives is crucial for maintaining the integrity and efficiency of the process, especially in environments where trust between participants is limited.

To implement this approach, we propose a method for translating BPMN (Business Process Model and Notation) models with activity-based costs into stochastic games [23]. This translation involves mapping the activities, events, and flows of the BPMN model to the states, actions, and transitions of the stochastic game. The resulting game can then be solved using algorithms developed in the machine learning community, such as those for computing Nash equilibria and correlated equilibria. This method not only ensures that the incentives are aligned but also provides a formal basis for verifying the correctness and fairness of the process execution. The approach is demonstrated through an order-to-cash (O2C) process, where the alignment of incentives among suppliers, customers, and financial institutions is critical for the success of the process.

### 4.2.3 Optimization of R&D Programs
The optimization of R&D programs is a critical aspect of enhancing the efficiency and effectiveness of high-tech enterprises (HTEs) in a rapidly evolving market. This section delves into the methodologies and frameworks employed for optimizing R&D programs, focusing on dynamic programming methods and their application in resource allocation and decision-making processes. Dynamic programming is particularly suited for R&D optimization due to its ability to break down complex problems into simpler, sequential decision-making steps, thereby facilitating the management of resources over time and under uncertainty.

In the context of HTEs, the optimization process is typically divided into discrete time periods, each representing a stage in the R&D lifecycle. At each stage, decisions are made regarding the allocation of resources such as funding, personnel, and equipment to various R&D activities. The goal is to maximize the overall value of the R&D program, which can be quantified through metrics such as innovation output, time-to-market, and return on investment. The dynamic programming approach allows for the evaluation of different scenarios and the identification of the most optimal resource allocation strategy, taking into account constraints such as budget limitations and technological feasibility.

Moreover, the optimization of R&D programs is not a one-time activity but a continuous process that requires adaptability and responsiveness to changes in the business environment. This is achieved through the use of evaluation functions that assess the performance of the R&D program at each step and provide feedback for subsequent decision-making. These functions can be designed to incorporate various factors, including market demand, regulatory changes, and technological advancements, ensuring that the R&D program remains aligned with the strategic goals of the organization. The integration of these elements into a cohesive framework supports the dynamic and iterative nature of R&D optimization, ultimately leading to more resilient and innovative HTEs.

## 4.3 Blockchain and Compliance

### 4.3.1 Blockchain for Regulatory Transparency
Blockchain technology offers a robust solution for enhancing regulatory transparency through its inherent characteristics of decentralization, immutability, and consensus mechanisms. By leveraging a distributed ledger, blockchain ensures that all transactions and records are transparently accessible to all authorized participants, thereby reducing the risk of fraudulent activities and increasing accountability [24]. This is particularly valuable in regulatory contexts where transparency and traceability are paramount. For instance, in the financial sector, blockchain can provide regulators with real-time access to transactional data, enabling them to monitor compliance and detect irregularities more efficiently. The immutable nature of blockchain records also ensures that once a transaction is recorded, it cannot be altered or deleted, providing an auditable trail that enhances trust and reduces the need for intermediaries.

Moreover, blockchain's smart contract capabilities can automate regulatory compliance by encoding rules and regulations directly into the blockchain. These smart contracts can automatically enforce compliance checks and trigger actions based on predefined conditions, such as verifying the authenticity of documents or ensuring that transactions adhere to regulatory standards. This automation not only reduces the administrative burden on organizations but also minimizes the potential for human error and manipulation. For example, in the pharmaceutical industry, smart contracts can be used to track the provenance of drugs, ensuring that they meet regulatory requirements at every stage of the supply chain. This level of transparency and automation can significantly enhance regulatory oversight and consumer safety.

However, despite these advantages, the implementation of blockchain for regulatory transparency faces several challenges. One of the primary concerns is the balance between transparency and privacy. While transparency is crucial for regulatory compliance, it must be balanced against the need to protect sensitive information, especially in industries dealing with personal data. Permissioned blockchains, which allow for controlled access to the ledger, can help address this issue by restricting access to certain data to only authorized entities. Additionally, the scalability and performance of blockchain systems remain areas of active research, as current solutions may not yet be capable of handling the high transaction volumes required in some regulatory environments. Despite these challenges, the potential benefits of blockchain for regulatory transparency make it a promising area for further exploration and development.

### 4.3.2 Policy Languages for Compliance Checking
Policy languages for compliance checking play a crucial role in ensuring that business processes adhere to regulatory, legal, and organizational requirements [16]. These languages are designed to express complex compliance rules and policies in a machine-readable format, enabling automated verification and enforcement. The SPECIAL policy language, for instance, is a notable example that supports the encoding of consent, business policies, and regulatory obligations. By leveraging the rich history of policy language research from the Semantic Web community, SPECIAL provides a machine-understandable encoding of consent, which is essential for demonstrating compliance with legal requirements such as the General Data Protection Regulation (GDPR) [25].

The effectiveness of policy languages in compliance checking is further enhanced by the integration of automated compliance checking mechanisms [16]. These mechanisms can validate that data processing activities comply with the consent provided by data subjects and ensure that business processes align with regulatory obligations [25]. For example, the compliance checking algorithm developed in conjunction with the SPECIAL policy language can automatically verify that the data processing performed by data controllers and processors adheres to the consent and regulatory requirements [25]. This not only reduces the risk of non-compliance but also provides a transparent and traceable audit trail of compliance activities.

Moreover, the use of policy languages in compliance checking is not limited to data protection and privacy. They can also be applied to a broader range of compliance scenarios, such as financial regulations, industry-specific standards, and internal business policies. The ability to express and enforce these policies in a formal and automated manner is particularly important in inter-organizational settings, where multiple parties need to ensure that their collaborative processes are compliant with various regulations and agreements. By providing a standardized and formal approach to policy specification and enforcement, policy languages facilitate trust and transparency in complex business ecosystems.

### 4.3.3 Formal Methods for Compliance Verification
Formal methods for compliance verification in the context of inter-organizational processes are critical for ensuring that business processes adhere to regulatory requirements and organizational policies [16]. These methods leverage mathematical and logical techniques to formally specify and verify the properties of processes, thereby providing a high degree of assurance that the processes will behave as intended. The primary challenge in this domain is the complexity of inter-organizational processes, which often involve multiple parties with varying interests and constraints. Formal methods address this by providing a precise and unambiguous representation of the process logic, enabling rigorous analysis and verification.

One of the key approaches in formal methods for compliance verification is the use of formal languages and models, such as Petri nets, process algebras, and temporal logics, to represent the process and its compliance requirements. These formalisms allow for the specification of both the process flow and the constraints that must be satisfied. For instance, Petri nets can be used to model the sequence and concurrency of activities, while temporal logics can express timing and ordering constraints. Model checking, a prominent technique in formal verification, can then be applied to automatically verify that the process model satisfies the specified properties. This approach is particularly useful in detecting potential violations of compliance rules before the process is executed, thus preventing costly errors and legal issues.

Another significant aspect of formal methods for compliance verification is the integration of these methods into the broader lifecycle of business process management [16]. This includes the modeling, simulation, and execution phases of the process. By embedding formal verification techniques within these phases, organizations can ensure continuous compliance throughout the process lifecycle. For example, during the modeling phase, formal methods can help in identifying and correcting inconsistencies in the process design. During the execution phase, runtime verification techniques can monitor the process to ensure that it adheres to the specified compliance requirements in real-time. This holistic approach not only enhances the reliability and trustworthiness of the processes but also facilitates agile and adaptive process management, enabling organizations to respond effectively to changing regulatory environments and business needs.

# 5 Large Language Models in BPM

## 5.1 Application and Evaluation of LLMs

### 5.1.1 Case Study Approaches in Enterprise Settings
Case study approaches in enterprise settings are pivotal for understanding the practical implications and real-world applicability of Business Process Management (BPM) methodologies and tools [26]. These approaches typically involve an in-depth, contextual analysis of specific business processes within an organization, allowing researchers to capture the nuances and complexities that are often overlooked in more generalized studies. By focusing on detailed, real-world scenarios, case studies provide a rich, qualitative understanding of how BPM practices are implemented and the challenges that arise during their deployment.

In enterprise settings, case studies often serve multiple purposes, including the evaluation of BPM tools and techniques, the identification of best practices, and the exploration of organizational change and process improvement. For instance, a case study might examine the adoption of a new BPM software in a manufacturing company, assessing the impact on process efficiency, employee satisfaction, and overall business performance [27]. These studies often involve a combination of qualitative data, such as interviews and observations, and quantitative data, such as process metrics and performance indicators, to provide a comprehensive analysis.

Moreover, case studies in enterprise settings are instrumental in validating theoretical models and frameworks by grounding them in practical scenarios. They can highlight the limitations of existing BPM approaches and suggest areas for improvement [7]. For example, a case study might reveal that while a particular BPM tool is effective in optimizing control-flow perspectives, it may fall short in managing data and resource perspectives, leading to inefficiencies in the overall process. By providing such insights, case studies contribute to the iterative development and refinement of BPM practices, ensuring they remain relevant and effective in diverse organizational contexts [7].

### 5.1.2 Benchmarking LLM Performance on BPM Tasks
Benchmarking the performance of Large Language Models (LLMs) on Business Process Management (BPM) tasks is crucial for understanding their capabilities and limitations in this domain [28]. To this end, a comprehensive benchmark has been developed, comprising a diverse set of 20 business processes, each paired with a ground-truth model. This benchmark serves dual purposes: it acts as a standardized testing dataset to evaluate the ability of LLMs to reason about BPM tasks and as a training dataset to fine-tune LLMs for better performance on these tasks [29]. The benchmark includes a variety of BPM tasks, such as process information extraction, process model generation, and compliance checking, ensuring a holistic assessment of LLM capabilities [29].

The evaluation of LLMs on this benchmark involves both open-source and commercial models, allowing for a comparative analysis of their performance [30]. Key metrics include accuracy, efficiency, and the ability to handle complex process structures and temporal constraints. Initial results highlight significant variations in performance across different models and tasks, emphasizing the need for task-specific optimization. For instance, while some LLMs excel in process information extraction, others may struggle with generating accurate process models or conducting compliance checks [31]. These findings underscore the importance of selecting the right model and fine-tuning it for specific BPM tasks [7].

To further enhance the performance of LLMs in BPM tasks, the benchmark also explores self-improvement techniques such as self-evaluation, input optimization, and output optimization [28]. Self-evaluation involves the LLM assessing its own generated outputs against the ground truth, identifying errors, and learning from them. Input optimization focuses on refining the prompts and input data to better guide the LLM, while output optimization involves post-processing the generated outputs to improve their quality and alignment with BPM standards. These strategies aim to make LLMs more autonomous and reliable in BPM tasks, ultimately contributing to more efficient and accurate process management solutions [32].

### 5.1.3 Comparative Analysis of Open-Source and Proprietary LLMs
In the comparative analysis of open-source and proprietary Large Language Models (LLMs), several key dimensions emerge as critical for evaluating their suitability for business process management (BPM) tasks [30]. These dimensions include model performance, cost, flexibility, and ethical considerations. Open-source LLMs, such as those from the Hugging Face ecosystem, offer the advantage of transparency and community-driven development, which can lead to rapid improvements and adaptations. However, they may lag behind proprietary models in terms of raw performance metrics, such as accuracy and speed, due to the typically larger training datasets and computational resources available to commercial entities. Proprietary LLMs, like those from Google and Anthropic, benefit from extensive proprietary data and fine-tuning, which can result in superior performance on specialized tasks, including those relevant to BPM [30].

When assessing the practical implications of using open-source versus proprietary LLMs, cost and flexibility play significant roles. Open-source models are generally more cost-effective, as they do not require licensing fees and can be deployed with minimal overhead. This makes them particularly appealing for smaller organizations or those with budget constraints. On the other hand, proprietary models often come with robust support, regular updates, and integrated tools, which can streamline deployment and maintenance. However, these benefits come at a higher cost, which may be prohibitive for some organizations. Additionally, the flexibility of open-source models allows for greater customization and adaptation to specific organizational needs, whereas proprietary models may have more restrictive usage policies that limit such modifications.

Ethical considerations also factor into the decision-making process. Open-source models promote transparency and accountability, as the underlying algorithms and training data are accessible for scrutiny. This transparency can help build trust and ensure that the models are aligned with organizational values and regulatory requirements. In contrast, proprietary models may raise concerns about data privacy and security, as the inner workings of the models are not publicly disclosed. Furthermore, the reliance on proprietary models can create vendor lock-in, making it difficult for organizations to switch providers or integrate multiple models. Overall, the choice between open-source and proprietary LLMs for BPM tasks depends on a careful balance of these factors, tailored to the specific needs and constraints of the organization [30].

## 5.2 Process Modeling and Automation

### 5.2.1 Converting Unstructured SOPs to Structured Formats
Converting unstructured Standard Operating Procedures (SOPs) to structured formats is a critical step in enhancing their usability and integration into automated systems [33]. This process involves breaking down lengthy, unstructured text into discrete, manageable components and capturing the dependencies between these elements. By doing so, the structured representation not only simplifies the understanding of the SOPs but also makes them more amenable to automatic processing, thereby improving workflow efficiency [33]. The transformation process typically consists of two main stages: information extraction and process modeling. The first stage involves extracting relevant information from the unstructured text, such as activities, conditions, and control flow elements. This information is then used in the second stage to construct a process model in a target modeling language, such as BPMN or Petri nets [31].

One of the key advantages of this two-step procedure is that the quality of the extracted information can be evaluated using established metrics from the information extraction domain, such as precision, recall, and F1-score. This allows for a more rigorous assessment of the initial phase, ensuring that the subsequent process modeling is based on accurate and reliable data. Furthermore, the extracted information can be transformed into multiple target process modeling languages, providing flexibility and adaptability to different organizational needs. This approach also facilitates the integration of the extracted information with existing process models, enabling incremental updates and refinements without the need for a complete rework of the entire process.

Another significant benefit of converting unstructured SOPs to structured formats is the enhancement of their comprehensibility and usability. By creating logical interpretations of the procedures, the structured format makes it easier for stakeholders to understand and follow the steps involved. This is particularly important in complex environments where clear communication and adherence to procedures are crucial. Moreover, the structured format enables better alignment with regulatory requirements and industry standards, reducing the risk of non-compliance. Additionally, the structured representation can be easily integrated with AI-driven solutions, such as process mining and robotic process automation (RPA), further automating and optimizing business processes [8]. Overall, the conversion of unstructured SOPs to structured formats represents a strategic investment in process improvement and operational efficiency.

### 5.2.2 Automating Identification of Unnecessary Steps
Automating the identification of unnecessary steps in business processes is a critical aspect of process optimization, leveraging the capabilities of Large Language Models (LLMs) to enhance efficiency and accuracy [3]. This section explores the technical underpinnings and practical applications of using LLMs for this purpose [8]. The approach involves a two-phase methodology: first, the LLM parses and understands the textual description of the business process, breaking it down into a structured representation of activities and their dependencies. This phase utilizes advanced natural language processing techniques to accurately capture the semantics and context of each step, ensuring a comprehensive understanding of the process flow.

In the second phase, the LLM performs a value-added analysis to identify and flag unnecessary steps. This analysis is based on a set of predefined criteria, such as the contribution of each step to the overall process goal, the presence of redundant activities, and the potential for streamlining. The LLM's ability to reason about the process context and dependencies allows it to detect steps that do not add value or can be optimized. For instance, the model can identify repetitive tasks, unnecessary approvals, or steps that can be automated, thereby reducing manual intervention and improving process efficiency. This phase also incorporates a feedback loop, where the identified steps are presented to domain experts for validation and refinement, ensuring that the automated analysis aligns with practical insights and organizational goals.

To validate the effectiveness of this approach, we conducted a series of experiments using a diverse set of business processes from various industries, including manufacturing, finance, and healthcare. Each process was annotated with detailed activity-to-step breakdowns and step-level value-added analysis, providing a rich dataset for training and evaluating the LLM. The results demonstrated that the LLM could accurately identify unnecessary steps with a high degree of precision and recall, significantly outperforming traditional manual methods in terms of both speed and accuracy. This automated approach not only reduces the time and cost associated with process optimization but also enhances the consistency and objectivity of the analysis, making it a valuable tool for continuous process improvement.

### 5.2.3 Generating Process Models from Textual Descriptions
The generation of process models from textual descriptions is a critical area in business process management (BPM) that leverages advanced natural language processing (NLP) techniques to automate the conversion of unstructured text into structured process models [31]. This section delves into the methodologies and frameworks that enable the extraction of process-related information from textual sources, such as business documents, standard operating procedures (SOPs), and employee notes [4]. The primary challenge in this domain is the accurate identification and interpretation of entities and relations within the text, which are essential for constructing a valid and meaningful process model. Large Language Models (LLMs) have emerged as a powerful tool for this task, offering advanced capabilities in understanding complex textual descriptions and generating structured outputs [28].

LLMs, such as GPT-4 and Gemini, are trained on vast datasets and can process and interpret natural language with high accuracy. These models are particularly well-suited for tasks that require the extraction of business process information from unstructured text. The process involves several key steps: first, the text is pre-processed to remove noise and irrelevant information. Then, the LLM is prompted with carefully crafted instructions to identify and extract process entities, such as activities, roles, and constraints [31]. The extracted information is then used to construct a process model, often in a standardized format like BPMN or Petri nets [31]. The framework also includes mechanisms for error handling and iterative refinement, allowing for the correction of any inaccuracies in the generated model. This iterative process is crucial for ensuring the fidelity and usability of the final process model.

To further enhance the accuracy and reliability of the generated process models, recent research has focused on integrating LLMs with domain-specific knowledge and formal logic [10]. For example, temporal logic, such as Linear Temporal Logic on finite traces (LTLf), is used to formalize compliance patterns and ensure that the generated models adhere to business rules and constraints [34]. Additionally, the use of gold-standard datasets, such as the Process Extraction Task (PET) dataset, has facilitated the development and evaluation of these models. The dataset provides a benchmark for assessing the performance of different approaches, enabling researchers to compare and refine their methods. Overall, the combination of LLMs, formal logic, and benchmark datasets represents a significant advancement in the field of process model generation from textual descriptions [10].

## 5.3 Enhancing BPM Tasks with LLMs

### 5.3.1 Post-Merger Integration Planning
Post-merger integration (PMI) planning is a critical phase in the M&A lifecycle, significantly influencing the success or failure of the merger [35]. Effective PMI planning requires a systematic approach to manage the complexities and interdependencies of integrating two distinct organizational structures, cultures, and processes [35]. The primary challenge lies in sequencing the numerous integration activities, which often exhibit intricate dependencies that constrain viable implementation sequences. These dependencies can range from legal and regulatory requirements to operational and technological alignment, making it imperative to adopt a structured framework that can navigate these complexities.

One of the key aspects of PMI planning is the identification and prioritization of integration activities [35]. This involves a thorough assessment of the synergies between the merging entities, including cost savings, revenue enhancements, and operational efficiencies. However, the identification of these synergies is not straightforward, as it requires a deep understanding of both organizations' business models, market positions, and strategic goals. Moreover, resource constraints and stakeholder considerations further complicate the planning landscape, necessitating a balanced approach that aligns with the overall strategic vision of the merged entity. Practitioners often face the challenge of becoming anchored to familiar integration paths, which can limit the exploration of innovative and potentially more effective integration strategies.

To address these challenges, advanced planning tools and methodologies are increasingly being employed. These tools leverage data analytics, simulation models, and decision-support systems to provide a more systematic and data-driven approach to PMI planning. For instance, simulation models can help visualize and test different integration scenarios, allowing managers to identify potential bottlenecks and optimize the integration timeline. Additionally, decision-support systems can facilitate the alignment of integration activities with strategic objectives, ensuring that the integration process is not only efficient but also aligned with the long-term goals of the merged entity. Overall, a well-planned and executed PMI strategy is essential for realizing the full potential of the merger and achieving the desired outcomes.

### 5.3.2 Reasoning About Causally-Augmented Business Processes
Reasoning about causally-augmented business processes (BPCs) involves the ability to infer and model the causal relationships and dependencies that exist between activities within a business process [29]. Unlike traditional process models, which primarily focus on the sequence and structure of activities, BPCs explicitly incorporate causal links that explain why certain activities must precede others or why certain conditions must be met before an activity can occur. This causal reasoning is crucial for understanding the underlying logic and constraints of business processes, which can significantly enhance process optimization, compliance, and robustness.

In the context of BPCs, causal reasoning can be applied to various tasks, such as identifying bottlenecks, predicting process outcomes, and ensuring compliance with regulatory requirements. For instance, in a supply chain process, understanding the causal relationship between inventory levels and order fulfillment can help in optimizing stock management and reducing lead times. Similarly, in a healthcare setting, the causal relationship between patient diagnosis and treatment protocols can be critical for ensuring patient safety and effective care. Large Language Models (LLMs) have shown promise in this area by their ability to parse and understand unstructured textual descriptions of processes, extract causal relationships, and reason about the implications of these relationships on process behavior [8].

The integration of LLMs into the reasoning about BPCs presents both opportunities and challenges [29]. On one hand, LLMs can automate the extraction of causal relationships from textual descriptions, reducing the manual effort required for process modeling and making it more accessible to non-experts [29]. On the other hand, ensuring the accuracy and reliability of the causal inferences made by LLMs remains a significant challenge, particularly in complex and dynamic business environments. Future research should focus on developing methods to validate the causal relationships identified by LLMs and on integrating these models with existing process modeling tools to create a more comprehensive and robust approach to business process management [3].

### 5.3.3 Enhancing Comprehension of Process Models
Enhancing the comprehension of process models involves developing methodologies that can accurately interpret and transform unstructured textual descriptions into structured process models [31]. Traditional approaches to process modeling, such as those based on BPMN (Business Process Model and Notation) and Petri nets, require significant manual effort and domain expertise [28]. These models are often complex and difficult to maintain, especially when business processes evolve over time [21]. The recent advancements in Large Language Models (LLMs) offer a promising solution to these challenges by automating the extraction of process-related information from textual sources [36]. LLMs can be trained to identify key entities, relationships, and dependencies within process descriptions, thereby facilitating the generation of formal process models [10].

One of the primary challenges in enhancing process model comprehension is the accurate extraction of process-relevant information from unstructured text. This task involves identifying activities, events, and control flow elements, as well as the relationships between them. Recent initiatives, such as the Process Extraction Task (PET) dataset, provide a gold-standard resource for developing and evaluating systems that can perform this extraction. However, the complexity and ambiguity of natural language descriptions make this task particularly challenging. LLMs, with their ability to understand context and infer relationships, can significantly improve the accuracy of information extraction. By leveraging pre-trained LLMs and fine-tuning them on domain-specific datasets, researchers have demonstrated promising results in automating the transformation of textual descriptions into structured process models [31].

To further enhance the comprehension of process models, it is crucial to develop algorithms that can effectively transform the extracted information into actionable process models. This involves not only identifying the necessary elements but also ensuring that the resulting model is logically sound and adheres to the principles of good process design. One approach is to use a proof-of-concept algorithm that integrates the extracted information into a process model, ensuring that the model is both comprehensive and accurate [31]. Additionally, the use of formalisms such as Linear Temporal Logic (LTL) can help in verifying the correctness and consistency of the generated models [34]. By combining the strengths of LLMs in text understanding with formal verification techniques, it is possible to create robust and reliable process models that can be used for various BPM tasks, including process analysis, optimization, and compliance checking [32].

# 6 Future Directions


The current landscape of Business Process Management (BPM) research and practice, while rich and diverse, still harbors several limitations and gaps. One of the primary limitations is the lack of standardized methodologies for integrating emerging technologies such as artificial intelligence (AI), machine learning (ML), and blockchain into BPM frameworks. Despite the promising potential of these technologies, their adoption and integration remain fragmented and often lack a systematic approach. Additionally, there is a need for more robust empirical validation of theoretical models, particularly in real-world settings. Many existing studies are based on simulations or controlled environments, which may not fully capture the complexities and dynamics of actual business processes. Another significant gap is the limited focus on the human aspects of BPM, including organizational culture, employee engagement, and change management. While technological advancements are crucial, the success of BPM initiatives often hinges on the human element, which is frequently overlooked in current research.

To address these limitations, several directions for future research are proposed. First, there is a need to develop and refine methodologies for the seamless integration of AI and ML into BPM systems. This includes the creation of standardized frameworks that guide the implementation of these technologies, ensuring that they enhance process efficiency and effectiveness while maintaining transparency and accountability. Research could also explore the use of AI to automate the identification and correction of process inefficiencies, as well as the prediction of potential issues before they occur. Second, a more comprehensive empirical validation of BPM models in real-world settings is essential. This could be achieved through longitudinal studies that track the performance of BPM initiatives over time, as well as case studies that provide in-depth insights into the practical challenges and successes of these initiatives. Third, future research should place greater emphasis on the human aspects of BPM. This includes the development of tools and strategies for effective change management, employee training, and the fostering of a culture that supports continuous process improvement. Additionally, research could explore the role of BPM in enhancing employee engagement and satisfaction, which can lead to better organizational outcomes.

The potential impact of the proposed future work is significant. By developing standardized methodologies for integrating AI and ML into BPM, organizations can harness the full potential of these technologies to optimize their processes, reduce costs, and improve service quality. This can lead to greater operational efficiency and a competitive advantage in the market. Empirical validation of BPM models in real-world settings will provide more reliable and actionable insights, guiding organizations in the adoption and implementation of BPM practices. This can help ensure that BPM initiatives are not only theoretically sound but also practically effective. Finally, a greater focus on the human aspects of BPM can lead to more successful and sustainable organizational change. By addressing the cultural and human factors that influence BPM, organizations can create a more engaged and productive workforce, ultimately driving better business performance and outcomes.

# 7 Conclusion



This survey paper has provided a comprehensive overview of the current state and evolution of Business Process Management Systems (BPMS) within organizational contexts. The paper explored various methodologies and frameworks that support the development and implementation of BPMS, including empirical and qualitative research methods, quantitative approaches, and the use of large language models (LLMs). Key findings include the significant role of comprehensive overviews and systematic reviews in advancing BPM research, the importance of visual aggregation frameworks in consolidating and presenting complex data, and the effectiveness of mixed-methods approaches in providing a more nuanced understanding of complex issues in BPM. The paper also highlighted the practical implications and real-world applications of BPMS, emphasizing the importance of interdisciplinary collaboration and empirical validation in advancing the field. Additionally, the integration of digital technologies, such as artificial intelligence, machine learning, and blockchain, has been shown to enhance the operational efficiency and strategic alignment of organizations, facilitating better compliance and performance.

The significance of this survey lies in its multifaceted contributions to the field of BPM. First, it provides a structured and comprehensive overview of the current state of BPMS, synthesizing insights from a wide range of research and practice. This synthesis not only highlights the latest advancements and best practices but also identifies key gaps and areas for future research. Second, the paper underscores the practical implications of BPMS, demonstrating how these systems can be leveraged to enhance organizational performance and compliance. By providing real-world examples and case studies, the paper illustrates the tangible benefits of BPMS in various industries and organizational settings. Third, the paper highlights the potential of emerging technologies, particularly LLMs, in transforming BPM and driving innovation in organizational processes. The integration of these technologies offers advanced capabilities in process modeling, automation, and comprehension, paving the way for more sophisticated and data-driven approaches to process management.

In conclusion, this survey paper calls for continued innovation and the integration of emerging technologies to address the evolving challenges and opportunities in the field of BPM. Researchers and practitioners are encouraged to explore the identified gaps and areas for future research, focusing on the development of more robust and adaptable BPMS. The practical implications of this research emphasize the need for organizations to invest in BPMS and related technologies to enhance their operational efficiency, strategic alignment, and regulatory compliance. By embracing these advancements, organizations can position themselves for sustainable success in a rapidly changing business environment.

# References
[1] Unraveling the Never-Ending Story of Lifecycles and Vitalizing Processes  
[2] Recent Advances in Data-Driven Business Process Management  
[3] Automated Business Process Analysis  An LLM-Based Approach to Value  Assessment  
[4] Assisted Data Annotation for Business Process Information Extraction  from Textual Documents  
[5] Web-based Interactive Narratives to Present Business Processes Models  
[6] Machine learning in business process management  A systematic literature  review  
[7] A Systematic Review of Business Process Improvement  Achievements and  Potentials in Combining Conce  
[8] Large Language Models can accomplish Business Process Management Tasks  
[9] Blockchains for Business Process Management - Challenges and  Opportunities  
[10] Process Modeling With Large Language Models  
[11] SOPA  A Framework for Sustainability-Oriented Process Analysis and  Re-design in Business Process Ma  
[12] Digital Twins of Business Processes  A Research Manifesto  
[13] Instability of the Environment as a Necessary Condition for Optimal  Control of an Economic Object  
[14] Employees Adoption of E-Procurement System  An Empirical Study  
[15] Event-based Failure Prediction in Distributed Business Processes  
[16] Reviewing Uses of Regulatory Compliance Monitoring  
[17] BPCMont  Business Process Change Management Ontology  
[18] An overview of process model quality literature - The Comprehensive  Process Model Quality Framework  
[19] A Survey on Conceptual model of Enterprise ontology  
[20] Extending Business Process Management for Regulatory Transparency  
[21] Interpreted Execution of Business Process Models on Blockchain  
[22] Process Channels  A New Layer for Process Enactment Based on Blockchain  State Channels  
[23] Incentive Alignment of Business Processes  a game theoretic approach  
[24] A Hybrid BPMN-DMN Framework for Secure Inter-organizational Processes  and Decisions Collaboration o  
[25] Machine Understandable Policies and GDPR Compliance Checking  
[26] Towards Nudging in BPM  A Human-Centric Approach for Sustainable  Business Processes  
[27] Sustainability Analysis Patterns for Process Mining and Process  Modelling Approaches  
[28] Evaluating Large Language Models on Business Process Modeling   Framework, Benchmark, and Self-Impro  
[29] Towards a Benchmark for Causal Business Process Reasoning with LLMs  
[30] Towards a Benchmark for Large Language Models for Business Process  Management Tasks  
[31] A Universal Prompting Strategy for Extracting Process Model Information  from Natural Language Text  
[32] LLM4PM  A case study on using Large Language Models for Process Modeling  in Enterprise Organization  
[33] Generating Structured Plan Representation of Procedures with LLMs  
[34] Verifying Compliance in Process Choreographies  Foundations, Algorithms,  and Implementation  
[35] Enhancing Post-Merger Integration Planning through AI-Assisted  Dependency Analysis and Path Generat  
[36] Revolutionizing Process Mining  A Novel Architecture for ChatGPT  Integration and Enhanced User Expe  