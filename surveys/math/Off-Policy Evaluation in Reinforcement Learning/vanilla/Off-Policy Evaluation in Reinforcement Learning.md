# Literature Survey: Off-Policy Evaluation in Reinforcement Learning

## Introduction
Off-Policy Evaluation (OPE) is a critical area of reinforcement learning (RL) that focuses on estimating the performance of a target policy using data collected under a different behavior policy. This capability is essential for practical applications where direct experimentation with the target policy may be costly, risky, or infeasible. OPE methods are widely used in domains such as healthcare, robotics, and recommendation systems, where offline evaluation can significantly reduce the need for online trials.

This survey provides an overview of the key concepts, methodologies, and challenges in off-policy evaluation. It begins with foundational definitions and then delves into major categories of OPE techniques, followed by a discussion of recent advances and open problems.

## Background and Definitions
Reinforcement learning involves an agent interacting with an environment to maximize cumulative rewards. The goal of OPE is to estimate the value of a target policy $\pi_e$ based on trajectories generated by a behavior policy $\pi_b$. The value function of a policy $\pi$, denoted $V^{\pi}(s)$, represents the expected return starting from state $s$ under $\pi$:
$$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s\right],
$$
where $r_t$ is the reward at time $t$, and $\gamma \in [0, 1]$ is the discount factor.

Key challenges in OPE include high variance in importance sampling-based methods, model bias in fitted approaches, and the need for robustness when behavior and target policies diverge significantly.

## Categories of Off-Policy Evaluation Methods
OPE methods can broadly be categorized into three main groups: Importance Sampling (IS), Model-Based Approaches, and Hybrid Methods.

### Importance Sampling (IS)
Importance Sampling estimates the value of the target policy by reweighting the observed rewards according to the ratio of probabilities between the target and behavior policies. The basic IS estimator is given by:
$$
\hat{V}_{\text{IS}} = \frac{1}{N} \sum_{i=1}^N \rho_i \sum_{t=0}^{T} \gamma^t r_t,
$$
where $\rho_i = \prod_{t=0}^{T} \frac{\pi_e(a_t|s_t)}{\pi_b(a_t|s_t)}$ is the importance weight for trajectory $i$.

#### Variants of IS
- **Per-Decision Importance Sampling (PDIS):** Reduces variance by applying importance weights only to individual decisions rather than entire trajectories.
- **Stationary Importance Sampling:** Assumes stationarity in the environment and simplifies the weighting process.
- **Truncated Importance Sampling:** Limits the magnitude of importance weights to control variance.

![](placeholder_for_IS_diagram)

### Model-Based Approaches
Model-based methods involve learning a transition model $P(s'|s, a)$ and a reward model $R(s, a)$ from the logged data. These models are then used to simulate the target policy's performance. A common approach is Fitted Q-Evaluation (FQE):
$$
Q^{\pi_e}(s, a) \approx \mathbb{E}_{s' \sim P}[r + \gamma Q^{\pi_e}(s', \pi_e(s'))].
$$

| Method | Advantages | Disadvantages |
|--------|------------|---------------|
| FQE    | Low variance | Requires accurate modeling |
| MBPO   | Flexible    | Computationally expensive |

### Hybrid Methods
Hybrid methods combine the strengths of IS and model-based approaches. Doubly Robust (DR) estimators, for example, use both importance weights and learned models to provide unbiased estimates while reducing variance:
$$
\hat{V}_{\text{DR}} = \frac{1}{N} \sum_{i=1}^N \left(\sum_{t=0}^T \gamma^t \left(r_t + \gamma Q(s_{t+1}, \pi_e(s_{t+1})) - Q(s_t, a_t)\right) \rho_t + Q(s_0, a_0)\right).
$$

## Recent Advances
Recent research in OPE has focused on addressing limitations of traditional methods. Key developments include:

- **Minimax Estimators:** Formulating OPE as a minimax optimization problem to handle distributional shifts.
- **Causal Inference Techniques:** Leveraging tools from causal inference to improve robustness in scenarios with confounding variables.
- **Deep Learning Integration:** Using neural networks to approximate complex value functions and transition models.

## Challenges and Open Problems
Despite progress, several challenges remain in OPE:

1. **High-Dimensional State Spaces:** Handling environments with large or continuous state-action spaces.
2. **Distribution Shifts:** Dealing with significant differences between behavior and target policies.
3. **Data Efficiency:** Improving the ability to make accurate evaluations with limited data.
4. **Sequential Decision-Making:** Extending OPE to multi-step decision processes with long horizons.

## Conclusion
Off-Policy Evaluation is a vibrant and evolving field with significant implications for real-world RL applications. While classical methods like Importance Sampling and Model-Based Approaches have laid the groundwork, recent innovations in hybrid techniques and deep learning integration offer promising directions for future research. Addressing the challenges of high-dimensional spaces, distribution shifts, and data efficiency will be crucial for advancing the practical applicability of OPE.
