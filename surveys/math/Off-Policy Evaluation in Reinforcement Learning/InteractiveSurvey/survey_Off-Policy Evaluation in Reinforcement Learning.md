# A Survey of Off-Policy Evaluation in Reinforcement Learning

# 1 Abstract


Reinforcement learning (RL) has emerged as a powerful paradigm for decision-making in complex, dynamic environments, with applications ranging from autonomous systems to personalized healthcare. This survey paper focuses on the state-of-the-art techniques and methodologies in off-policy evaluation (OPE) within RL, aiming to provide a comprehensive overview of recent advancements and the key challenges in this field. The paper highlights the development of advanced estimation techniques, such as doubly robust (DR) estimators and marginalized inverse propensity scoring (MIPS), and the integration of causal inference and uncertainty quantification. It also explores the use of novel importance sampling techniques and state abstraction methods to manage large state spaces and reduce variance. The contributions of this survey include a structured and accessible resource for researchers and practitioners, a detailed examination of the theoretical foundations of OPE, and a roadmap for future research directions. The paper concludes by emphasizing the need for further research in areas such as model-based approaches, practical applicability, and the integration of OPE with other machine learning paradigms.

# 2 Introduction
Reinforcement learning (RL) has emerged as a powerful paradigm for decision-making in complex, dynamic environments [1]. From autonomous systems to personalized healthcare, RL algorithms are increasingly being applied to solve a wide range of real-world problems. One of the key challenges in RL is off-policy evaluation (OPE), which involves assessing the performance of a target policy using data collected by a different behavior policy [2]. OPE is crucial for practical applications, as it allows for the evaluation and optimization of policies without the need for additional data collection, thereby reducing the cost and risk associated with deploying new policies in real-world settings. However, the accurate and reliable estimation of policy performance remains a significant challenge, particularly in scenarios with high-dimensional state and action spaces, limited data, and complex environmental dynamics.

This survey paper focuses on the state-of-the-art techniques and methodologies in off-policy evaluation (OPE) within the realm of reinforcement learning [3]. The paper aims to provide a comprehensive overview of the recent advancements in OPE, highlighting the key challenges and solutions in this rapidly evolving field [4]. By synthesizing insights from a wide range of research contributions, this survey seeks to offer a structured and accessible resource for researchers and practitioners interested in the theoretical and practical aspects of OPE [4].

The paper begins by exploring advanced estimation techniques for OPE, including the development of doubly robust (DR) estimators, marginalized inverse propensity scoring (MIPS), and methods for addressing missing not at random (MNAR) data. These techniques are essential for balancing the bias-variance trade-off and ensuring the reliability of OPE in various applications. The discussion then delves into the integration of causal inference and uncertainty quantification, which are critical for handling unmeasured confounding and providing robust estimates of policy performance. The transition from explainability to interpretability in OPE is also examined, emphasizing the importance of transparent and understandable models in high-stakes domains such as healthcare and finance.

The survey further explores the development of novel importance sampling techniques and clustering methods for state abstraction, which are crucial for managing the complexity of large state spaces and reducing the variance of OPE estimators. Advanced estimators and algorithms, such as those leveraging deep learning and transformer architectures, are also discussed, highlighting their potential to enhance the efficiency and accuracy of OPE in complex environments. The paper then shifts to innovative off-policy RL algorithms, including self-play and preference learning methods, which are designed to improve the adaptability and robustness of learning processes [5].

The theoretical foundations of OPE are also thoroughly examined, with a focus on state abstraction, exploration in latent Markov decision processes (POMDPs), and the sample complexity of OPE algorithms [3]. These theoretical insights provide a deeper understanding of the underlying mechanisms and limitations of OPE, guiding the development of more effective and efficient evaluation methods [4]. The paper concludes by discussing the future directions and open challenges in OPE, emphasizing the need for further research in areas such as model-based approaches, practical applicability, and the integration of OPE with other machine learning paradigms.

The contributions of this survey paper are multifaceted. Firstly, it provides a comprehensive and up-to-date overview of the key techniques and methodologies in off-policy evaluation, synthesizing insights from a wide range of research contributions [6]. Secondly, it highlights the critical challenges and open problems in OPE, offering a roadmap for future research. Finally, it serves as a valuable resource for both researchers and practitioners, facilitating the application of OPE in various domains and driving the advancement of reinforcement learning technologies [2].

# 3 Advanced Estimation Techniques for OPE

## 3.1 Inverse Propensity Scoring and Variants

### 3.1.1 Balancing Bias and Variance in Estimation
Balancing bias and variance is a fundamental challenge in the development of estimators, particularly in the context of off-policy evaluation (OPE) in reinforcement learning (RL) [2]. Traditional methods such as Importance Sampling (IS) offer unbiased estimates but suffer from high variance, especially when the overlap between the behavior and target policies is poor [7]. This high variance can lead to unreliable estimates, making it difficult to draw meaningful conclusions about the performance of the target policy. To address this issue, researchers have explored various techniques to reduce variance while maintaining or improving the bias properties of the estimators.

One prominent approach is the Doubly Robust (DR) estimator, which combines the strengths of IS and Direct Method (DM) estimators. DR estimators use a reward model to predict the expected reward for each action, thereby reducing the reliance on importance weights and potentially lowering variance. However, the accuracy of the reward model is crucial; if the model is misspecified, the DR estimator can still exhibit high variance. Recent advancements have focused on refining the DR estimator by incorporating techniques such as weight clipping and variance regularization. These modifications aim to strike a better balance between bias and variance, making the DR estimator more robust in practical settings.

Another line of research involves the development of marginalized IS ratios for stationary state distributions, which leverage the recursive property of stationary distributions to formulate optimization tasks [8]. This approach results in a backward Bellman-based update, where the value at the next step depends on the current step’s value. By integrating this recursive structure, the estimators can achieve a more stable and accurate approximation of the state distribution ratio. Additionally, the use of least squares regression for learning the state distribution ratio offers a computationally efficient and effective method, particularly in the context of linear function approximation [8]. These techniques collectively contribute to a more nuanced understanding of the bias-variance trade-off, enabling the design of estimators that are both theoretically sound and practically useful.

### 3.1.2 Addressing Missing Not at Random Data
Addressing Missing Not at Random (MNAR) data is a critical challenge in the context of off-policy evaluation (OPE) and reinforcement learning (RL), as it can introduce significant biases in the estimation of policy performance. Traditional methods, such as Inverse Propensity Score (IPS) weighting, are designed under the assumption that data are Missing at Random (MAR), which often does not hold in real-world scenarios. When rewards or actions are missing not at random, the standard IPS estimator can suffer from high variance and bias, leading to unreliable policy evaluations. To tackle this issue, recent research has focused on developing methods that can account for the MNAR nature of the data.

One approach to addressing MNAR data involves the use of doubly robust (DR) estimators, which combine the strengths of IPS and direct method (DM) estimators. DR estimators leverage a reward model to predict missing rewards and use IPS to correct for the distributional mismatch between the behavior and target policies. However, in the presence of MNAR data, the reward model itself can be biased, leading to suboptimal performance of DR estimators. To mitigate this, researchers have proposed incorporating additional information, such as historical data or auxiliary variables, to improve the accuracy of the reward model. For instance, counterfactual annotations can be used to provide more accurate predictions of missing rewards, thereby reducing the bias in the DR estimator [9].

Another promising direction is the development of specialized estimators that explicitly model the MNAR mechanism. These methods often involve the use of latent variable models or graphical models to capture the dependencies between the missingness and the observed data. For example, the Marginalized Inverse Propensity Score (MIPS) estimator pools information across action embeddings to account for the MNAR nature of the data, thereby reducing the variance and bias of the estimator [10]. Additionally, recent work has explored the use of contextual bandit algorithms that can adaptively learn the optimal treatment allocation in the presence of MNAR data, providing a more robust and efficient approach to policy evaluation [9]. These methods not only improve the accuracy of the estimators but also enhance their computational efficiency, making them suitable for large-scale applications.

### 3.1.3 Enhancing Estimation with Human Feedback
Enhancing Estimation with Human Feedback involves leveraging human judgments to improve the accuracy and reliability of off-policy evaluation (OPE) methods [11]. In the context of reinforcement learning and recommendation systems, human feedback can provide valuable insights that are difficult to capture through automated methods alone. For instance, in recommendation systems, human feedback can help correct biases in logged data, such as position bias, where items at the top of a list are more likely to be clicked regardless of their quality. By incorporating human feedback, OPE methods can more accurately estimate the performance of a new policy without requiring additional data collection.

To integrate human feedback effectively, several approaches have been developed. One such approach is the Doubly Robust (DR) estimator, which combines the strengths of Inverse Propensity Score (IPS) weighting and direct method (DM) estimation [10]. The DR estimator can leverage human feedback to reduce the variance of IPS while maintaining low bias. For example, in the context of large action spaces, the Marginalized Inverse Propensity Score (MIPS) estimator has been extended to incorporate reward observation IPS (ROIPS), which uses human feedback to model the probabilities of reward observations [11]. This approach not only addresses the issue of low-probability actions but also enhances the robustness of the estimator by accounting for the variability in human preferences.

Moreover, the integration of human feedback can be particularly beneficial in settings where the data is biased or noisy. For instance, in the evaluation of language models, human feedback can help refine the alignment of the model with human preferences, thereby improving the accuracy of OPE. The use of human feedback in this context often involves re-ranking lists of responses generated by the model, which can then be used to estimate the performance of a new policy. This approach not only enhances the reliability of the OPE but also provides a more nuanced understanding of the model's performance across different user segments.

## 3.2 Causal and Uncertainty Considerations

### 3.2.1 Integrating Causal Inference
Integrating causal inference into reinforcement learning (RL) has emerged as a critical approach to address the challenges posed by unmeasured confounding in off-policy evaluation (OPE) [6]. Traditional RL methods often assume that the environment is fully observable and that the data are collected under a well-defined policy. However, in many real-world applications, such as healthcare and recommendation systems, unobserved confounders can significantly bias the estimates of policy performance. Causal inference techniques, particularly those focused on latent confounders, offer a promising solution by explicitly accounting for these hidden variables [6].

One notable approach is the direct estimation of latent confounders, as proposed by Wang and Blei (2019) [6]. This method leverages a generative model to infer the unobserved confounders from the observed data, thereby allowing for more accurate policy evaluation. However, this approach requires the unmeasured confounders to be a deterministic function of the actions, which may not always hold in practice. To overcome this limitation, recent works have introduced methods that do not rely on the Markov assumption or external proxy variables. These methods often use advanced statistical techniques, such as instrumental variables and mediators, to handle latent confounders more flexibly.

Another line of research focuses on adapting causal inference techniques to the specific requirements of RL. For example, the use of doubly robust (DR) estimators, which combine the strengths of importance sampling (IS) and direct method (DM) estimators, has shown promise in reducing the variance of OPE estimates while maintaining low bias. DR estimators achieve this by leveraging a learned reward model to correct for the bias introduced by IS, making them particularly suitable for settings with large action spaces and complex state dynamics. By integrating these causal inference methods into RL, researchers aim to develop more robust and reliable algorithms for policy evaluation and optimization in the presence of unmeasured confounders.

### 3.2.2 Quantifying Uncertainty in Policy Evaluation
Quantifying uncertainty in policy evaluation is a critical aspect of off-policy evaluation (OPE), particularly in high-stakes domains where the deployment of suboptimal policies can have severe consequences [2]. Traditional OPE methods, such as importance sampling (IS) and doubly robust (DR) estimators, provide point estimates of the expected cumulative return of a target policy [1]. However, these methods often fail to capture the full extent of uncertainty surrounding these estimates, which is essential for making informed decisions. The challenge lies in accurately quantifying the variability in the estimated policy value, especially when the behavior and target policies exhibit significant distributional mismatches.

One approach to addressing this challenge is through the use of confidence intervals, which provide a range within which the true policy value is likely to fall. Recent advances in this area have focused on developing non-asymptotic bounds for OPE estimators, which are particularly useful in settings with limited data or when the overlap between the behavior and target policies is poor [2]. Techniques such as shrinkage of importance weights, local stabilization, and adaptive weighting have been proposed to reduce the variance of IS-based estimators while maintaining their unbiasedness. These methods aim to balance the trade-off between bias and variance, thereby providing more reliable uncertainty estimates.

Another promising direction is the integration of Bayesian methods into OPE, which naturally incorporate uncertainty through posterior distributions over the policy value. Bayesian OPE methods can provide probabilistic guarantees on the estimated policy performance, making them suitable for risk-sensitive applications. Additionally, model-based approaches, which leverage learned models of the environment to simulate the target policy, offer another avenue for quantifying uncertainty. These models can be used to generate synthetic trajectories, allowing for the estimation of the distribution of the policy value under different scenarios. By combining these techniques, researchers can develop more robust and reliable methods for quantifying uncertainty in policy evaluation, ultimately enhancing the safety and effectiveness of policy deployment.

### 3.2.3 Transitioning from Explainability to Interpretability
The transition from explainability to interpretability in reinforcement learning (RL) and off-policy evaluation (OPE) represents a significant shift in the focus of algorithmic development and evaluation [2]. Explainability, while crucial for understanding the decision-making processes of RL agents, often provides post-hoc explanations that are not always aligned with human cognitive models. In contrast, interpretability aims to ensure that the underlying mechanisms of these algorithms are understandable and transparent to human stakeholders, thereby enhancing trust and facilitating more effective human-AI collaboration. This shift is particularly important in high-stakes domains such as healthcare, where the ability to understand and trust the decisions made by AI systems is paramount.

Interpretable models in RL and OPE are designed to provide insights into the key factors and decision points that influence the performance of a policy. For instance, in healthcare applications, interpretable models can highlight specific patient features or treatment sequences that are critical for the success of a treatment plan. This level of transparency allows domain experts to validate the model's decisions and identify potential biases or errors. Moreover, interpretability can help in the identification of subgroups within the population that may benefit differently from a given policy, enabling more personalized and equitable decision-making [7]. By focusing on interpretable concepts, researchers can also better understand the causal relationships within the data, which is essential for developing robust and reliable policies.

To achieve interpretability, recent approaches have integrated human-readable concepts and domain knowledge into the model training process. For example, in the context of OPE, methods have been developed to incorporate counterfactual annotations from domain experts, which can provide valuable insights into the potential outcomes of alternative actions. These annotations can be used to refine the model's understanding of the environment and improve its predictive accuracy. Additionally, the use of interpretable models can facilitate the development of more effective intervention strategies, as stakeholders can directly modify or remove high-variance concepts to assess their impact on the policy evaluation. This approach not only enhances the reliability of OPE but also supports the broader goal of creating AI systems that are transparent, trustworthy, and aligned with human values.

## 3.3 Advanced Estimators and Algorithms

### 3.3.1 Doubly Robust Estimation in Complex Environments
Doubly Robust (DR) estimation methods have emerged as a powerful tool for off-policy evaluation (OPE) in complex environments, combining the strengths of model-based and importance sampling (IS) approaches [12]. In these environments, the direct method (DM) often suffers from model misspecification and sparse reward signals, while IS methods can exhibit high variance, particularly when the behavior policy and the target policy differ significantly. DR estimators mitigate these issues by leveraging a reward model to reduce variance while maintaining the unbiasedness of IS, thereby providing a more robust and accurate evaluation of the target policy.

In complex environments, the DR estimator is particularly valuable due to its ability to handle high-dimensional state and action spaces, where accurate modeling of the environment dynamics is challenging. By integrating a learned reward function with importance sampling, DR estimators can effectively reduce the variance of the importance weights, which is crucial for stable and reliable policy evaluation [13]. This is achieved through the use of a correction term that accounts for the difference between the estimated and actual rewards, ensuring that the estimator remains unbiased even when the reward model is not perfectly accurate. This combination of low bias and reduced variance makes DR estimators particularly suitable for applications in healthcare, finance, and other domains where accurate policy evaluation is critical.

Moreover, recent advancements in DR estimation have focused on extending these methods to handle unmeasured confounding and model misspecification, which are common in real-world settings. For instance, the two-way deconfounder algorithm constructs a neural tensor network to jointly learn unmeasured confounders and system dynamics, thereby improving the accuracy of the DR estimator. This approach not only enhances the robustness of the estimator but also broadens its applicability to scenarios where traditional methods may fail due to the presence of hidden variables. Additionally, the development of finite-sample analysis for DR estimators has provided a deeper understanding of their performance in practical settings, highlighting the importance of careful tuning and validation to ensure optimal performance.

### 3.3.2 Clustering and State Abstraction
Clustering and state abstraction techniques play a crucial role in managing the complexity of large state spaces in reinforcement learning (RL). These methods aim to reduce the dimensionality of the state space by grouping similar states into abstract states, thereby simplifying the learning process and improving the efficiency of policy evaluation and optimization. State abstraction can be achieved through various clustering algorithms, such as k-means, hierarchical clustering, or more advanced methods like spectral clustering, which leverage the structure of the state space to identify meaningful clusters.

In the context of off-policy evaluation (OPE), state abstraction is particularly important for addressing the curse of dimensionality and the sparsity of data in large state spaces [14]. By clustering similar states, the abstracted state space can be treated as a smaller, more manageable problem, allowing for the application of tabular methods or simpler function approximators. This abstraction not only reduces the variance of importance sampling (IS) estimators but also helps in mitigating the impact of model misspecification. The abstracted states can be used to construct a discrete abstract reward process (ARP), where the rewards and transition probabilities are aggregated over the states within each cluster, leading to a more stable and accurate estimation of the policy value [2].

Moreover, clustering and state abstraction can be integrated into the optimization process to improve the performance of RL algorithms. For instance, the abstracted state space can be used to guide the exploration of the agent, ensuring that it focuses on the most relevant regions of the state space. Additionally, the use of state abstraction can facilitate the transfer of knowledge across different tasks or environments, as the abstract states may capture generalizable features that are useful in multiple settings. This transferability is particularly valuable in multi-task learning and lifelong learning scenarios, where the agent needs to adapt to new tasks with limited data.

### 3.3.3 Novel Importance Sampling Techniques
Novel importance sampling techniques have emerged as a critical area of research to address the limitations of traditional methods, particularly in terms of variance reduction and computational efficiency. One notable advancement is the introduction of the average state distribution correction estimation (AVG-DICE), which leverages Monte Carlo expansion to compute the average of all discounted importance sampling ratio products for a given state in the dataset [8]. Unlike conventional approaches that rely on recursive properties, AVG-DICE provides a consistent estimation of the density ratio between the discounted target and the undiscounted behavior stationary state distributions, thereby offering a more robust solution to the curse of the horizon.

Another significant development is the adaptation of importance sampling to handle unmeasured confounders without relying on the Markov assumption or external proxy variables. This approach, which operates under the "memoryless unmeasured confounding" assumption, ensures that the observed data still satisfies the Markov property despite the presence of latent confounders. By avoiding the need for deterministic functions of actions, this method broadens the applicability of importance sampling to a wider range of real-world scenarios, where such assumptions may not hold.

Furthermore, recent work has focused on integrating importance sampling with model-based approaches to achieve a balance between variance reduction and consistency. For instance, the combination of importance sampling with model learning allows for the construction of small, interpretable mental models that can mitigate the exponential variance growth associated with long horizons. This hybrid approach not only enhances the accuracy of off-policy evaluation but also provides a more intuitive understanding of the underlying dynamics, making it particularly useful in complex, high-dimensional environments.

# 4 Innovative Off-Policy RL Algorithms

## 4.1 Self-Play and Preference Learning

### 4.1.1 Self-Augmented Preference Optimization
Self-Augmented Preference Optimization (SAPO) represents a significant advancement in the realm of preference-based reinforcement learning (PRL), addressing the limitations of traditional approaches that rely heavily on static, pre-collected preference datasets [5]. SAPO introduces a dynamic mechanism for generating and refining preference data through self-augmentation, thereby enhancing the adaptability and robustness of the learning process. Unlike conventional methods that require extensive manual labeling of preferences, SAPO leverages the model's own predictions to generate synthetic preference pairs, which are then used to refine the policy. This self-augmentation process not only reduces the dependency on external data but also ensures that the policy remains aligned with the evolving preferences of the environment.

The core of SAPO lies in its ability to dynamically adjust the quality and diversity of the generated preference data. By incorporating mechanisms such as uncertainty quantification and active learning, SAPO can identify and prioritize the generation of preference pairs that are most informative for policy improvement. This is particularly beneficial in environments with sparse rewards or complex state spaces, where the availability of high-quality preference data is crucial. The adaptive nature of SAPO allows it to continuously improve the policy by focusing on areas of the state space that are most relevant to the task at hand, thereby enhancing both the sample efficiency and the overall performance of the learning algorithm.

Furthermore, SAPO integrates advanced techniques from deep learning and reinforcement learning to ensure that the generated preference data is both diverse and representative. The use of generative models, such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs), enables the creation of synthetic preference pairs that capture the nuances of the environment. Additionally, the incorporation of off-policy learning methods, such as Importance Sampling (IS) or Retrace, allows SAPO to effectively utilize historical data, even when the policy is updated frequently. This combination of self-augmentation and off-policy learning makes SAPO a powerful tool for tackling a wide range of PRL tasks, from natural language processing to robotics, where the ability to adapt to changing preferences is essential.

### 4.1.2 Balancing Sequence Novelty and Robustness
Balancing sequence novelty and robustness is a critical challenge in the application of reinforcement learning (RL) to sequence generation tasks. Novelty ensures that the generated sequences are diverse and innovative, which is essential for exploring the action space and discovering optimal policies. However, excessive novelty can lead to sequences that are unrealistic or suboptimal, undermining the robustness of the model. This section explores the methodologies and frameworks that aim to strike a balance between these two competing objectives.

One approach to balancing novelty and robustness involves the use of generative flow networks (GFlowNets), which are designed to generate sequences by sampling from a probabilistic model. GFlowNets can be trained to optimize a reward function that encourages both novelty and adherence to known patterns or constraints. By introducing a conservativeness parameter δ, the GFlowNet can be fine-tuned to control the degree of exploration versus exploitation. This parameter allows the model to inject controlled amounts of noise, such as by masking tokens in high-score sequences, thereby promoting the generation of novel sequences while maintaining a level of robustness against proxy misspecification.

Another method that addresses this balance is the Self-Play Fine-Tuning (SPIN) approach, which leverages self-play to generate training data. SPIN uses the model itself to create rejected inputs, which are then used to refine the policy. This self-generated data helps the model to explore a wider range of sequences, enhancing novelty, while the iterative refinement process ensures that the generated sequences remain grounded in the context of the task. Additionally, the use of a replay buffer with a curriculum learning principle allows the model to gradually increase the complexity of the sequences it generates, further balancing the need for innovation with the requirement for robust performance [5].

### 4.1.3 Continuation-Based Preference Data Synthesis
Continuation-Based Preference Data Synthesis (InCo-DPO) represents a significant advancement in the integration of on-policy and off-policy data for reinforcement learning (RL) applications [15]. Unlike traditional methods that rely solely on one type of data, InCo-DPO dynamically adjusts the balance between distribution shift and data quality, thereby enhancing the robustness and adaptability of the learning process. This method leverages the strengths of both on-policy and off-policy data: on-policy data ensures high-quality, relevant feedback, while off-policy data provides a broader exploration of the state-action space, which is crucial for learning in complex environments [16].

InCo-DPO operates by synthesizing preference data through a continuation-based approach, which allows for the incremental generation of new training data [15]. This is achieved by querying a policy to generate new sequences, which are then evaluated and used to update the dataset [17]. The key innovation lies in the adaptive adjustment of the trade-off between distribution shift and reward, which is managed through a carefully designed mechanism that monitors the quality of the generated data. This dynamic adjustment ensures that the learning process remains stable and effective, even when the distribution of the training data shifts over time.

Empirical evaluations on a variety of benchmark tasks have demonstrated the superior performance of InCo-DPO compared to both pure on-policy and off-policy methods. The method's ability to dynamically balance data quality and distribution shift results in more accurate and reliable policy evaluations, making it particularly suitable for applications where real-time feedback is limited or costly. Furthermore, the flexibility of InCo-DPO in handling different types of data and policies makes it a versatile tool for a wide range of RL tasks, from robotics to personalized medicine.

## 4.2 Hybrid and Structured Approaches

### 4.2.1 Structure-Guided Unified Dual On-Off Policy DRL
In the realm of Deep Reinforcement Learning (DRL), the integration of on-policy and off-policy methods has emerged as a promising direction to enhance both the stability and efficiency of learning processes [18]. Structure-Guided Unified Dual On-Off Policy DRL (SUDO-DRL) represents a novel approach that leverages the strengths of both paradigms to address the challenges of complex decision-making tasks [19]. By combining the stability of on-policy training, which ensures that the policy is updated based on actions directly influenced by the current policy, with the efficiency of off-policy training, which allows for the utilization of previously collected data, SUDO-DRL aims to achieve a balanced and robust learning framework [19].

At the core of SUDO-DRL is the concept of structure-guided learning, which involves the identification and exploitation of structural insights within the task environment. These insights are used to guide the dual on-off policy updates, ensuring that the learning process remains focused and efficient [16]. Specifically, the on-policy component of SUDO-DRL is responsible for fine-tuning the policy based on immediate feedback, while the off-policy component leverages a replay buffer to stabilize learning and improve sample efficiency. The replay buffer stores a diverse set of experiences, which are sampled and used to update the policy in a manner that minimizes the impact of distribution shifts and enhances the generalization capabilities of the model [5].

To further enhance the effectiveness of SUDO-DRL, the algorithm incorporates an Exponential Moving Average (EMA) model of the current policy, which helps to smooth out the updates and reduce the volatility associated with on-policy learning. Additionally, the use of a first-in-first-out (FIFO) replay buffer ensures that the off-policy updates are based on a balanced and representative set of past experiences. This hybrid approach not only improves the stability and convergence of the learning process but also enables SUDO-DRL to adapt quickly to changes in the environment, making it particularly suitable for dynamic and complex tasks such as robotic manipulation and autonomous navigation.

### 4.2.2 Transformer-Based Policy Networks for Clinical Applications
In the realm of clinical applications, Transformer-based policy networks have emerged as a powerful tool for enhancing decision-making processes, particularly in personalized treatment recommendations [20]. These networks leverage the Transformer architecture, originally designed for natural language processing tasks, to model the intricate and long-term dependencies in patient data. By capturing the full context of a patient’s clinical history, these networks can provide more informed and accurate treatment recommendations. The Transformer-based policy network, MeDT, exemplifies this approach by integrating a patient’s historical data to recommend medication dosages, thereby addressing the sparse reward issue often encountered in clinical settings [20].

The key innovation in MeDT lies in its ability to incorporate clinician-specified short-term target improvements in patient stability, which serves as a critical guide for treatment decisions. This feature is particularly valuable in scenarios where immediate feedback is limited, and the long-term outcomes of treatment decisions are crucial. By aligning with clinical goals, MeDT not only enhances the interpretability of its recommendations but also increases the likelihood of clinical adoption. The network’s performance is further validated through various off-policy evaluation (OPE) methods, such as Fitted Q-Evaluation (FQE), Weighted Doubly Robust (WDR), and Weighted Importance Sampling (WIS), demonstrating its superiority or competitiveness with existing offline RL baselines.

To ensure robust and reliable decision-making, MeDT employs a transformer-based state predictor that models the evolution of a patient’s clinical state in response to treatment [20]. This autoregressive inference capability allows the network to generate treatment sequences that are both clinically meaningful and aligned with the patient’s condition. The integration of this state predictor with the policy network creates a cohesive framework that not only improves the accuracy of treatment recommendations but also enhances the overall stability and reliability of the decision-making process, making it a promising tool for clinical applications.

### 4.2.3 Efficient Goal-Reaching with Prior Observations
In the context of efficient goal-reaching with prior observations, the primary challenge lies in leveraging historical data to guide the learning of optimal policies, particularly in environments with sparse rewards and long horizons. Traditional reinforcement learning (RL) methods often struggle with these conditions due to the difficulty in propagating rewards back to earlier actions, leading to inefficient exploration and suboptimal policies [1]. To address this, recent approaches have focused on integrating prior observations into the learning process, enabling the agent to make more informed decisions and improve its ability to reach distant goals.

One such method, Efficient Goal-Reaching with Prior Observations (EGR-PO), employs a hierarchical learning framework that combines a high-level policy for generating subgoals and a low-level policy for executing actions [21]. The high-level policy is trained using a diffusion model to generate reasonable subgoals that act as waypoints towards the final goal [21]. These subgoals are designed to provide intermediate rewards, which help mitigate the guidance decay and prediction inaccuracy that can occur when relying solely on the final goal for reinforcement. The state-goal value function, learned from prior observations, plays a crucial role in this process by providing exploration rewards that encourage the agent to efficiently achieve these subgoals [21].

During the pre-training phase, the state-goal value function is trained to estimate the expected future rewards from various states and subgoals, leveraging the rich information contained in prior observations. This pre-training step is essential for initializing the value function with a robust understanding of the environment, which is then fine-tuned during the online learning phase. The integration of prior observations into the learning process not only enhances the efficiency of goal-reaching but also improves the overall sample efficiency of the RL algorithm, making it more suitable for real-world applications where data collection is often limited or costly.

## 4.3 Advanced Q-Learning and Actor-Critic Methods

### 4.3.1 Decomposing Q-Functions for Long-Horizon Tasks
Decomposing Q-functions is a critical technique for addressing the challenges associated with long-horizon tasks in reinforcement learning (RL). Traditional Q-learning algorithms often struggle with the bias-variance trade-off and the difficulty of learning across various time scales, especially in environments with extended planning horizons [22]. By decomposing the Q-function into multiple components, each responsible for different temporal scales, the algorithm can more effectively balance short-term and long-term rewards. This decomposition allows the agent to focus on immediate rewards while still considering the long-term impact of its actions, thereby improving overall performance and stability.

One effective approach to decomposing Q-functions is through the use of hierarchical reinforcement learning (HRL). In HRL, the task is broken down into a series of subtasks, each with its own Q-function. The high-level policy selects subgoals, which are then executed by low-level policies. This hierarchical structure not only simplifies the learning problem but also enables the agent to learn more abstract representations of the task. For instance, the high-level policy can generate subgoals that act as waypoints towards the final goal, while the low-level policies handle the execution of actions to reach these subgoals. This division of labor reduces the complexity of the learning problem and helps mitigate issues such as guidance decay and prediction inaccuracy, which are common in long-horizon tasks.

Another technique for decomposing Q-functions involves the use of multi-scale Q-learning, where the Q-function is partitioned into delta components over various time scales. This method, known as TD(∆), extends traditional Q-learning by learning action-values for different discount factors [22]. By doing so, the algorithm can capture both short-term and long-term rewards more effectively. The delta components allow the Q-function to adapt to different temporal dynamics, making it more versatile and capable of handling a wide range of tasks. This approach has been shown to enhance the stability and efficiency of Q-learning, particularly in environments with complex reward structures and long planning horizons [22].

### 4.3.2 Value-Improved Actor Critic Framework
The Value-Improved Actor Critic (VIAC) framework represents a significant advancement in the realm of reinforcement learning by addressing the inherent limitations of traditional actor-critic methods, particularly in the context of continuous action spaces [23]. Unlike conventional methods that tightly couple the acting policy with the evaluated policy, VIAC decouples these components, allowing for independent optimization and evaluation. This separation enables the critic to evaluate more aggressive and potentially superior policies without the constraints imposed by the current acting policy, thereby facilitating faster convergence and more robust policy improvements. The framework introduces a value improvement step, where the critic evaluates an independently-improved policy, which can be more greedy and less constrained by the current policy's parameterization.

In practice, the VIAC framework leverages deep neural networks to parameterize both the actor and the critic, ensuring that the learning process remains stable and scalable. The actor is updated slowly, following a conservative approach that ensures gradual improvements and prevents catastrophic divergence. Meanwhile, the critic can aggressively evaluate and suggest improvements, leading to a dynamic interplay between exploration and exploitation. This dual-speed learning mechanism is particularly beneficial in complex environments where rapid policy adjustments can lead to instability. By decoupling the evaluation and acting policies, VIAC also mitigates the risk of overfitting to the current policy, a common issue in on-policy methods, thus enhancing the overall robustness and generalization of the learned policies.

Moreover, the VIAC framework extends the applicability of off-policy algorithms to continuous action spaces, which have traditionally been challenging due to the reliance on on-policy methods for their stability and reliability. The ability to incorporate off-policy data and updates significantly improves the sample efficiency of the learning process, making it more practical for real-world applications with limited data availability [19]. This extension is achieved through careful design of the loss functions and update rules, which balance the need for exploration with the requirement for stable and reliable policy improvements. The VIAC framework thus represents a promising direction for advancing reinforcement learning in both theoretical and practical contexts, paving the way for more efficient and effective learning algorithms in complex and dynamic environments.

### 4.3.3 Robust Off-Policy Actor-Critic Algorithms
Robust off-policy actor-critic algorithms are designed to enhance the stability and performance of reinforcement learning (RL) in environments where the policy updates are driven by data collected from a different behavior policy [24]. These algorithms leverage the off-policy nature to improve sample efficiency and robustness, particularly in continuous action spaces [24]. Traditional off-policy methods, such as Soft Actor-Critic (SAC), rely on temporal difference (TD) errors to update the critic, which can lead to high variance and instability [18]. To address these issues, recent advancements have focused on developing algorithms that can handle the distributional shift between the behavior and target policies more effectively.

One notable approach is the introduction of f-divergence constrained methods, which ensure that the updates to the policy remain within a certain divergence bound from the behavior policy. This constraint helps in maintaining the stability of the learning process and prevents the policy from diverging too far from the data distribution, thereby reducing the risk of overfitting and poor generalization. Additionally, these methods often incorporate techniques like weighted importance sampling (WIS) to correct for the distributional shift, providing a more accurate estimation of the policy's performance.

Another significant development is the use of transformer architectures in off-policy actor-critic algorithms. These models, such as the Transformer-based Off-Policy ERL (TOP-ERL), leverage the transformer's ability to handle long sequences and capture complex dependencies, making them well-suited for tasks with long horizons. By predicting the value of action sequences, the critic can provide more informative feedback to the actor, leading to more effective policy updates. This approach not only enhances the robustness of the algorithm but also improves its ability to handle high-dimensional and continuous action spaces, making it a promising direction for future research in robust off-policy RL [24].

# 5 Theoretical Foundations of Off-Policy Evaluation

## 5.1 State Abstraction and Compression

### 5.1.1 Iterative State Abstraction Procedures
Iterative state abstraction procedures are essential for managing the complexity of large state spaces in non-Markov environments, particularly in the context of offline policy evaluation (OPE) [14]. These procedures progressively compress the state space while maintaining the necessary information to accurately evaluate policies. The core idea is to iteratively project the original high-dimensional state space into a sequence of smaller, more manageable spaces, each of which preserves the Markov property. This iterative compression not only reduces the computational burden but also mitigates the curse of dimensionality, making OPE feasible in complex environments.

The process begins by identifying a set of features that capture the essential dynamics of the environment. These features are used to define an initial abstraction of the state space. Subsequently, the algorithm iterates through a series of steps, each refining the abstraction to better align with the underlying Markovian structure. During each iteration, the algorithm evaluates the quality of the current abstraction by assessing its ability to predict future observations and rewards accurately. If the abstraction fails to meet a predefined threshold, it is refined further by incorporating additional features or adjusting the existing ones. This iterative refinement continues until the abstraction is sufficiently accurate or no further improvement can be achieved.

Empirical evaluations have demonstrated the effectiveness of iterative state abstraction procedures in enhancing OPE [14]. Multi-iteration abstractions, which involve multiple rounds of refinement, generally outperform single-iteration abstractions by providing a more accurate and compact representation of the state space. This improved representation leads to better policy evaluation performance, as it reduces the variance and bias in the estimation of policy values. Moreover, the proposed iterative procedures have shown superior performance compared to existing state abstraction methods, highlighting their potential for practical applications in large-scale and complex environments.

### 5.1.2 Markov State Abstractions for Large State Spaces
Markov State Abstractions for Large State Spaces involve techniques designed to manage the complexity of state representations in environments where the state space is vast or even infinite. In such settings, traditional methods that rely on explicit state enumeration become infeasible due to the exponential growth in the number of states. State abstraction methods aim to reduce this complexity by grouping similar states into abstract states, thereby simplifying the state space while preserving the essential dynamics of the environment. This abstraction process is crucial for efficient learning and decision-making, especially in partially observable Markov decision processes (POMDPs) where the true state is not directly observable, and the agent must infer the state from a sequence of observations and actions [3].

One of the key challenges in designing effective state abstractions for large state spaces is ensuring that the abstract states maintain the Markov property [14]. This means that the transition probabilities and rewards within the abstract states should be independent of the history, relying solely on the current abstract state and action. Achieving this requires careful consideration of the features used to define the abstract states. Techniques such as spectral clustering, which leverages the eigenstructure of the transition matrix, can be particularly effective in identifying meaningful state groupings that preserve the Markov property. Additionally, function approximation methods, such as neural networks, can be employed to learn the mapping from raw observations to abstract states, allowing for the handling of high-dimensional and continuous observation spaces.

Another important aspect of Markov state abstractions is the trade-off between the level of abstraction and the accuracy of the resulting model. While higher levels of abstraction can significantly reduce computational complexity, they may also lead to a loss of information that is critical for optimal decision-making. Therefore, the design of state abstraction algorithms must balance these competing objectives [14]. Adaptive methods that dynamically adjust the level of abstraction based on the observed data can help mitigate this issue. Furthermore, theoretical guarantees on the performance of the abstracted models, such as bounds on the sub-optimality gap, are essential for understanding the limitations and applicability of these techniques in practical scenarios.

### 5.1.3 Primal-Dual Spectral Representation
In the context of constrained Markov Decision Processes (CMDPs), the primal-dual spectral representation offers a powerful framework for learning near-optimal policies with function approximation. This representation leverages the spectral decomposition of the state-action transition operator, which allows both the Q-function and the stationary distribution correction ratio to be linearly represented in the primal and dual feature spaces, respectively [12]. By doing so, it addresses the challenge of high-dimensional state spaces and complex transition dynamics, making the optimization problem more tractable.

The key insight of the primal-dual spectral representation is that it transforms the non-linear and potentially intractable optimization problem into a linear one, which can be efficiently solved using primal-dual algorithms. Specifically, the spectral decomposition of the transition operator provides a set of basis functions that span the space of value functions and stationary distributions. This linear representation facilitates the design of algorithms that can handle large state spaces and function approximation, such as SpectralDICE. SpectralDICE is an off-policy algorithm that uses this spectral representation to learn policies with small constraint violations, even in the presence of model misspecification.

Moreover, the primal-dual spectral representation enhances the sample efficiency and computational efficiency of the learning process. By leveraging the linear structure induced by the spectral decomposition, the algorithm can converge faster and with fewer samples compared to traditional methods. Theoretical guarantees are provided to ensure that the learned policies achieve near-optimal performance with respect to both the reward and the constraints. Empirical evaluations on various reinforcement learning benchmarks further validate the effectiveness of the spectral representation in improving the performance and stability of the learning algorithms.

## 5.2 Exploration and Learning in Latent MDPs

### 5.2.1 Coverage Coefficient and Moment-Matching
The concept of the coverage coefficient is pivotal in addressing the challenges of off-policy evaluation (OPE) in partially observable Markov decision processes (POMDPs) with large observation spaces [3]. This coefficient serves as a measure of how well the data collected under the behavior policy can support the evaluation of a target policy. Specifically, the coverage coefficient quantifies the extent to which the empirical distribution of observed trajectories under the behavior policy overlaps with the distribution of trajectories that the target policy would generate. In the context of POMDPs, this overlap is crucial because the partial observability introduces additional complexity in estimating the true state of the environment.

To formalize the coverage coefficient, we consider a sampled trajectory \( T = (s_t, a_t, r_t)_{t \in [25]} \) and an event \( E \) of visiting a sequence of state-action tuples within an episode [26]. The coverage coefficient is defined as the probability of the event \( E \) occurring under the behavior policy, given the latent context \( m \) [26]. This coefficient is particularly important because it directly influences the variance of the OPE estimators. When the coverage coefficient is low, the variance of the estimators tends to be high, leading to less reliable policy evaluations. Conversely, a higher coverage coefficient implies better overlap between the behavior and target policies, resulting in more accurate and stable OPE.

In the context of moment-matching, the coverage coefficient is further utilized to ensure that the moments of the empirical distribution of the behavior policy match those of the target policy. This is achieved by constructing a set of moment conditions that the empirical data must satisfy. By matching these moments, the OPE estimators can more accurately approximate the true value function of the target policy, even in the presence of partial observability. The moment-matching approach leverages the coverage coefficient to balance the trade-off between bias and variance, ultimately leading to more robust and reliable OPE in complex environments. This method is particularly effective in reducing the mean squared error (MSE) of the estimators, as it ensures that the empirical distribution closely aligns with the target distribution, thereby mitigating the effects of distributional shift.

### 5.2.2 Efficient Exploration in Latent Contexts
Efficient exploration in latent contexts is a critical challenge in reinforcement learning (RL), particularly in environments modeled as Partially Observable Markov Decision Processes (POMDPs) [26]. In such settings, the agent must navigate through a large observation space where the underlying state is not directly observable. The key to effective exploration lies in leveraging the structure of the latent state space, which can be inferred from the observation-action sequences. Zhang & Jiang (2024) propose that future observation-action sequences can probabilistically decode the latent state, a condition that can be strengthened to require only the immediate observation, aligning with the "revealing" assumption common in online RL for POMDPs [3]. This assumption simplifies the exploration problem by reducing the dependency on the entire history, making it more tractable for practical applications.

To address the exploration challenge, a line of research has focused on developing algorithms that can efficiently explore the latent contexts. One prominent approach is the use of the local-access model, where the RL algorithm can restart the environment from any previously visited states. This model is particularly useful in scenarios where a simulator is available, as it allows the agent to explore different parts of the state space more effectively. By providing the ability to revisit specific states, the local-access model enhances both sample and computational efficiency, especially in settings where the state transitions are complex and the latent state dynamics vary significantly. This approach has been shown to be more general than the generative model, which allows visitation to arbitrary states but does not provide the same level of control over the exploration process.

Another important aspect of efficient exploration in latent contexts is the development of algorithms that can adapt to the specific characteristics of the environment. For instance, the Empirical Decision with Divergence (EDD) algorithm, introduced in this paper, achieves an instance-dependent upper bound and a fast convergence rate, making it suitable for a wide range of POMDPs. The EDD algorithm leverages the structure of the latent state space to optimize exploration, thereby improving the agent's ability to learn optimal policies. Additionally, the use of spectral structures and representation learning techniques has gained traction in recent years, offering new ways to uncover the underlying latent variables and enhance exploration efficiency. These methods not only help in reducing the dimensionality of the state space but also improve the stability and performance of the learning process, making them valuable tools for addressing the challenges of efficient exploration in latent contexts.

### 5.2.3 Theoretical Guarantees for Sample Efficiency
Theoretical guarantees for sample efficiency in offline policy evaluation (OPE) are crucial for ensuring that the estimators used to evaluate target policies are reliable and efficient [1]. In the context of Markov Decision Processes (MDPs), the bounded state-action density ratio between the target policy \(\pi_e\) and the behavior policy \(\pi_b\) is a well-established condition that ensures polynomial sample complexity [3]. This condition implies that the behavior policy must adequately cover the state-action pairs visited by the target policy, thereby ensuring that the estimator can accurately approximate the target policy's performance using the available data. However, the complexity of achieving these guarantees increases significantly when the target policy is history-dependent, as the state-action density ratio alone is insufficient to ensure sample efficiency [3].

In the case of history-dependent target policies, the theoretical landscape becomes more nuanced. Our analysis reveals that achieving polynomial sample complexity in this setting depends on specific relaxations or assumptions. For instance, when the behavior policy is memoryless or when the environment exhibits single-step revealing properties, a Maximum Likelihood Estimation (MLE)-based model-based algorithm can achieve the desired sample complexity guarantees. These findings highlight the importance of the structure of the behavior policy and the environment in determining the feasibility of efficient OPE. In contrast, model-free algorithms, which do not rely on explicit models of the environment, are shown to suffer from hardness in these settings, indicating a fundamental limitation in their applicability to history-dependent policies [3].

To further solidify these theoretical findings, we introduce a hardness measure, the Offline Evaluation Complexity (OEC), which provides a lower bound for the sample complexity of offline decision-making tasks [27]. This measure helps in understanding the inherent difficulty of evaluating history-dependent policies and serves as a benchmark for comparing the efficiency of different algorithms. Our results show that the MLE-based model-based algorithm achieves sample complexity bounds that are polynomial in the relevant parameters, aligning well with the OEC lower bound. This alignment underscores the optimality of the MLE-based approach in these specific settings and provides a strong theoretical foundation for its use in practical applications.

## 5.3 Off-Policy Evaluation in POMDPs

### 5.3.1 Model-Free and Model-Based Approaches
In the realm of offline reinforcement learning (RL), the distinction between model-free and model-based approaches is fundamental, particularly in the context of offline policy evaluation (OPE) [13]. Model-free algorithms, as exemplified in Uehara et al. (2023a) and Zhang & Jiang (2024), operate by querying the target policy, πe, solely on the histories present in the offline dataset. This approach is advantageous in its simplicity and computational efficiency, as it avoids the need to estimate the environment's dynamics. However, this simplicity comes at a cost, as model-free methods are inherently limited in their ability to handle general history-dependent target policies [3]. Even under coverage assumptions, which ensure that the offline data is sufficiently rich, model-free algorithms struggle to provide reliable estimates for such policies.

In contrast, model-based approaches leverage an estimated model of the environment to simulate the behavior of the target policy. This allows them to extrapolate beyond the observed data, making them more robust to the complexities of history-dependent policies. For instance, under relaxations such as single-step revealing or memoryless behavior policies (πb), a simple Maximum Likelihood Estimation (MLE)-based model-based algorithm can achieve the desired guarantees. These findings highlight a formal separation between model-free and model-based OPE, particularly in partially observable Markov decision processes (POMDPs) [3]. The model-based approach, while more computationally intensive, offers a polynomial sample-complexity bound for evaluating history-dependent target policies, a significant advancement in the field [3].

The implications of these findings are profound, as they suggest that for tasks requiring the evaluation of complex, history-dependent policies, model-based methods are essential. This is particularly relevant in safety-critical applications where deterministic policies are often required, such as in healthcare, robotics, and industrial systems. The ability to accurately evaluate such policies using offline data is crucial, as it allows for the development and testing of new policies without the need for extensive and potentially risky real-world interactions [7]. The theoretical guarantees provided by model-based approaches, especially in terms of sample complexity and robustness, make them a preferred choice in these scenarios, despite the increased computational overhead.

### 5.3.2 Sample Complexity and Coverage Assumptions
Sample complexity and coverage assumptions play a crucial role in the efficiency and reliability of off-policy evaluation (OPE) in partially observable Markov decision processes (POMDPs). Recent advancements have introduced novel coverage assumptions, such as belief and outcome coverage, which are essential for achieving polynomial sample complexity in OPE for POMDPs with large observation spaces [3]. Belief coverage ensures that the behavior policy sufficiently explores the belief space, while outcome coverage guarantees that the outcomes of actions are adequately represented in the data. These assumptions are particularly important because they help mitigate the challenges posed by the partial observability and the large state-action space, which can otherwise lead to exponential sample complexity.

The relationship between sample complexity and coverage assumptions is further nuanced by the specific settings in which OPE is applied. For instance, in settings with history-dependent target policies, achieving polynomial sample complexity is contingent on the interplay between the coverage assumptions and the structure of the problem [3]. Zhang and Jiang (2024) demonstrate that under belief and outcome coverage, their model-free algorithm can achieve polynomial sample complexity. However, the feasibility of this achievement varies across different settings, such as those defined by single-step versus multi-step revealing, and whether the setting is model-free or model-based. Table 1 provides a summary of these settings, indicating that while some configurations allow for polynomial sample complexity, others are information-theoretically hard, highlighting the importance of carefully selecting the appropriate coverage assumptions.

Moreover, the effectiveness of these coverage assumptions is also influenced by the presence of strong assumptions, such as uniform policy coverage, which are often necessary to ensure convergence rates in certain problems, including supervised learning and Markovian sequential problems. Despite these strong assumptions, recent research has shown that more relaxed coverage conditions can still lead to efficient OPE in specific scenarios, such as linear MDPs and tabular MDPs [26]. However, the generalization of these results to more complex environments, particularly those with function approximation, remains an open challenge. The ongoing research in this area aims to develop more robust and flexible coverage assumptions that can be applied across a broader range of POMDP settings, thereby enhancing the practical applicability of OPE methods [3].

### 5.3.3 Theoretical Proofs and Complexity Bounds
Theoretical proofs and complexity bounds are crucial for understanding the feasibility and limitations of various algorithms in the context of offline reinforcement learning (RL). In particular, the complexity of achieving polynomial bounds in terms of the horizon \( H \), the logarithm of the inverse of the failure probability \( \log(|M|/\delta) \), the desired accuracy \( \epsilon \), and the constants \( C_A \), \( C_{\Box} \), and \( C_H \) (where \( C_{\Box} \) is either \( C_O \) or \( C_F \) depending on the revealing assumption) have been extensively studied. These bounds are essential for ensuring that algorithms can efficiently estimate the performance of a target policy using data collected by a behavior policy.

In the context of model-free (MF) and model-based (MB) settings, the complexity bounds vary significantly. For instance, in the model-free setting, achieving polynomial complexity is generally easier when the behavior policy provides sufficient coverage over the state-action space. This is often formalized through the bounded state-action density ratio condition, which ensures that the target policy's actions are adequately represented in the behavior policy's data. However, as the problem becomes more complex, such as when moving from single-step to multi-step revealing or from model-free to model-based settings, the complexity increases, and achieving polynomial bounds becomes more challenging. The bottom-right corner of Table 1, representing the most complex MB setting, remains an open problem, with a conjecture suggesting that it may be intractable due to the increased difficulty in modeling and estimating the environment dynamics accurately.

To further understand the theoretical underpinnings, several hardness measures have been introduced, such as the Offline Evaluation Complexity (OEC), which provides a lower bound for offline decision-making tasks [27]. These measures help in comparing the theoretical guarantees of different algorithms and highlight the gaps between the best-known upper bounds and the fundamental limits. For instance, while some algorithms achieve polynomial sample complexity under specific assumptions, others struggle to maintain efficiency in more general settings. The development of instance-dependent bounds, particularly in special cases like linear and tabular MDPs, has shown promise, but extending these results to more general settings remains an active area of research. Theoretical proofs in this domain often involve intricate analyses of the bias-variance trade-offs, the impact of function approximation, and the role of exploration in offline RL.

# 6 Future Directions


Despite the significant advancements in off-policy evaluation (OPE) within reinforcement learning, several limitations and gaps remain. Current methods often struggle with high variance and bias, particularly in scenarios with high-dimensional state and action spaces and limited data. The presence of unmeasured confounders and missing not at random (MNAR) data further complicates the estimation of policy performance. Additionally, the integration of causal inference and uncertainty quantification remains a challenge, as these techniques are often computationally intensive and require strong assumptions. The interpretability of OPE methods is another critical issue, as many existing techniques are opaque and difficult to understand, which can hinder their adoption in high-stakes domains such as healthcare and finance. Moreover, the practical applicability of OPE methods is often limited by the need for extensive data and the lack of robust theoretical guarantees, especially in partially observable Markov decision processes (POMDPs) and environments with long horizons.

To address these limitations, several directions for future research are proposed. First, the development of more robust and efficient variance reduction techniques is essential. This could involve the creation of novel importance sampling methods that are less sensitive to distributional mismatches between the behavior and target policies. Additionally, the integration of advanced machine learning techniques, such as deep learning and transformers, could enhance the accuracy and efficiency of OPE estimators, particularly in high-dimensional and complex environments. Another promising direction is the development of methods that can effectively handle unmeasured confounders and MNAR data. This could be achieved through the use of latent variable models, graphical models, and advanced causal inference techniques that can account for hidden variables and missing data mechanisms.

Furthermore, the integration of causal inference and uncertainty quantification into OPE is crucial for improving the reliability and robustness of policy evaluations. This could involve the development of doubly robust estimators that incorporate causal models and uncertainty estimates, providing more accurate and reliable policy performance assessments. Additionally, the transition from explainability to interpretability in OPE methods is a critical area of research. Future work should focus on developing models that are not only accurate but also transparent and understandable, making them more suitable for high-stakes applications. This could involve the use of human-readable concepts, domain knowledge, and interactive tools that allow domain experts to validate and interpret the model's decisions.

The potential impact of these proposed future research directions is substantial. More robust and efficient OPE methods could significantly enhance the practical applicability of reinforcement learning in real-world scenarios, particularly in domains where accurate and reliable policy evaluation is critical. For example, in healthcare, improved OPE methods could lead to better treatment recommendations and personalized care, ultimately improving patient outcomes. In finance, enhanced OPE techniques could enable more effective risk management and portfolio optimization, leading to better financial decisions. Additionally, the development of interpretable OPE methods could increase trust and adoption in these domains, as stakeholders would have a clearer understanding of the decision-making processes and the underlying mechanisms of the algorithms. Overall, these advancements have the potential to drive significant progress in the field of reinforcement learning and contribute to the broader goal of creating more intelligent and trustworthy AI systems.

# 7 Conclusion



This survey has comprehensively reviewed the state-of-the-art techniques and methodologies in off-policy evaluation (OPE) within the realm of reinforcement learning (RL). We have explored advanced estimation techniques, including doubly robust (DR) estimators, marginalized inverse propensity scoring (MIPS), and methods for addressing missing not at random (MNAR) data. These techniques are essential for balancing the bias-variance trade-off and ensuring the reliability of OPE in various applications. The integration of causal inference and uncertainty quantification was also highlighted, emphasizing their critical role in handling unmeasured confounding and providing robust estimates of policy performance. The survey further discussed the development of novel importance sampling techniques and clustering methods for state abstraction, which are crucial for managing the complexity of large state spaces and reducing the variance of OPE estimators. Advanced estimators and algorithms, such as those leveraging deep learning and transformer architectures, were also examined, showcasing their potential to enhance the efficiency and accuracy of OPE in complex environments. Additionally, the survey covered innovative off-policy RL algorithms, including self-play and preference learning methods, which are designed to improve the adaptability and robustness of learning processes.

The significance of this survey lies in its comprehensive and structured overview of the recent advancements in OPE, which is a critical component of RL for practical applications. By synthesizing insights from a wide range of research contributions, this survey provides a valuable resource for researchers and practitioners. It highlights the key challenges and solutions in OPE, offering a roadmap for future research and development. The survey's focus on both theoretical foundations and practical applications ensures that it is relevant to a broad audience, from academics to industry professionals. The detailed examination of advanced techniques and methodologies not only deepens the understanding of OPE but also identifies areas where further research is needed, such as model-based approaches, practical applicability, and the integration of OPE with other machine learning paradigms.

In conclusion, the field of off-policy evaluation in reinforcement learning is rapidly evolving, with significant progress being made in addressing the challenges of accurate and reliable policy evaluation. This survey has aimed to provide a clear and accessible overview of the current state of the art, highlighting the importance of OPE in advancing RL technologies. We call upon the research community to continue exploring the open challenges and future directions identified in this survey, such as the development of more efficient and robust OPE methods, the integration of OPE with other ML paradigms, and the practical applicability of OPE in real-world scenarios. By addressing these challenges, we can further enhance the reliability and effectiveness of reinforcement learning systems, paving the way for broader and more impactful applications in various domains.

# References
[1] OPERA  Automatic Offline Policy Evaluation with Re-weighted Aggregates  of Multiple Estimators  
[2] Abstract Reward Processes  Leveraging State Abstraction for Consistent  Off-Policy Evaluation  
[3] Statistical Tractability of Off-policy Evaluation of History-dependent  Policies in POMDPs  
[4] Off-policy Evaluation for Payments at Adyen  
[5] Self-Augmented Preference Optimization  Off-Policy Paradigms for  Language Model Alignment  
[6] Two-way Deconfounder for Off-policy Evaluation in Causal Reinforcement  Learning  
[7] Concept-driven Off Policy Evaluation  
[8] AVG-DICE  Stationary Distribution Correction by Regression  
[9] CANDOR  Counterfactual ANnotated DOubly Robust Off-Policy Evaluation  
[10] Clustering Context in Off-Policy Evaluation  
[11] Off-Policy Evaluation from Logged Human Feedback  
[12] Primal-Dual Spectral Representation for Off-policy Evaluation  
[13] Kernel Metric Learning for In-Sample Off-Policy Evaluation of  Deterministic RL Policies  
[14] Off-policy Evaluation with Deeply-abstracted States  
[15] InCo-DPO  Balancing Distribution Shift and Data Quality for Enhanced  Preference Optimization  
[16] Doubly Robust Monte Carlo Tree Search  
[17] Improved Off-policy Reinforcement Learning in Biological Sequence Design  
[18] TOP-ERL  Transformer-based Off-Policy Episodic Reinforcement Learning  
[19] Goal-oriented Transmission Scheduling  Structure-guided DRL with a  Unified Dual On-policy and Off-p  
[20] Empowering Clinicians with Medical Decision Transformers  A Framework  for Sepsis Treatment  
[21] Goal-Reaching Policy Learning from Non-Expert Observations via Effective  Subgoal Guidance  
[22] Time-Scale Separation in Q-Learning  Extending TD($ triangle$) for  Action-Value Function Decomposit  
[23] Value Improved Actor Critic Algorithms  
[24] Robust off-policy Reinforcement Learning via Soft Constrained Adversary  
[25] Unknown PDF  
[26] RL in Latent MDPs is Tractable  Online Guarantees via Off-Policy  Evaluation  
[27] A Fast Convergence Theory for Offline Decision Making  