# 5/1/2025, 6:06:46 PM_Off-Policy Evaluation in Reinforcement Learning  

# 0. Off-Policy Evaluation in Reinforcement Learning  

# 1. Introduction  

Reinforcement Learning (RL) is a learning paradigm focused on creating autonomous agents that learn to maximize a cumulative reward signal by interacting with an external environment [5]. This approach is highly versatile and finds application across numerous domains, including gaming, robotics, autonomous driving, and healthcare [5]. Driven by recent advancements in deep learning and computational power, RL has achieved remarkable successes in complex tasks, such as those demonstrated by systems like AlphaGo/AlphaZero [5]. These achievements have significantly spurred interest in applying RL techniques to real-world challenges [5].‚Äã  

A critical challenge arises when transitioning from simulated or limited environments to real-world deployment: directly implementing and evaluating new strategies can be prohibitively costly, risky, or even ethically questionable, particularly in sensitive areas like healthcare or autonomous systems [5,22]. While constructing environment simulators is a common practice for evaluation, building high-fidelity simulators capable of accurately predicting a strategy's real-world performance can often be as difficult as, if not more difficult than, developing the strategy itself [5]. This scenario underscores a fundamental dilemma: evaluating the true quality of a policy typically necessitates its deployment, yet reliable evaluation is crucial before deployment due to associated risks [5].‚Äã  

This challenge highlights the indispensable role of Off-Policy Evaluation (OPE) [12,15]. OPE addresses the problem of assessing the performance of a targetpolicy using data collected by a diferentpolicy, known as the behaviorpolicy [7,12,15]. This is especially relevant in settings like Offline Reinforcement Learning (Offline RL), also known as Batch RL, where learning must occur solely from pre-collected static datasets without any online interaction [3,7]. OPE allows for the assessment of potential alternative strategies using historical interaction data, thereby enabling evaluation without direct, potentially risky, real-world experimentation [22]. The surge of research in both statistics and computer science dedicated to developing various OPE methods attests to its fundamental importance in RL [15].‚Äã  

Despite its critical importance, OPE presents significant challenges. A primary difficulty stems from the distribution shift between the data generated by the behavior policy and the distribution induced by the target policy, particularly when the target policy deviates significantly from the behavior policy [14,15,22]. This discrepancy can lead to biased or high-variance estimates of the target policy's value [1,4,6]. Further complexities arise in scenarios involving large or continuous state and action spaces, where accurately modeling state visitation distributions or estimating policy probabilities becomes challenging [9,15]. The issue of high variance is particularly detrimental in iterative policy learning methods, where OPE errors can amplify, leading to performance degradation [1]. Furthermore, many existing OPE methods, while providing point estimates, may lack rigorous statistical interpretations and guarantees, which is essential for applications requiring high precision [7]. Challenges also exist in specific application contexts, such as off-policy learning from bandit feedback [6,13] or evaluating complex policies like those for slate recommendations [10].  

This survey provides a focused review of off-policy evaluation in reinforcement learning [15]. We aim to discuss the theoretical foundations and statistical properties of OPE methods, examine state-of-the-art approaches, and explore related active research areas [15]. The survey covers key topics including various OPE estimator classes (e.g., importance sampling, model-based methods, and their combinations), techniques for variance reduction, and methods addressing the challenges of large state spaces and distribution shift. We also touch upon related problems like off-policy policy optimization where OPE plays a crucial role [1]. Our objective is to synthesize the diverse landscape of OPE research, providing researchers and practitioners with a structured understanding of the field's progress, challenges, and future directions.‚Äã  

# 2. Background and Preliminaries  

Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make sequential decisions by interacting with an environment to maximize a cumulative reward signal [26]. The fundamental framework for modeling such sequential decision-making problems is the Markov Decision Process (MDP) [8,11,26,27].  

<html><body><table><tr><td>Term</td><td>Description</td><td>Relevance to OPE</td></tr><tr><td>MDP</td><td>Markov Decision Process: (S,A,p,P,R, Œ≥)</td><td>Formal framework for sequential decision-making; defines the environment.</td></tr><tr><td>Policy ($\pi$)</td><td>A distribution over actions given a state, œÄ(a| s) .</td><td>The object of evaluation ( Ttarget Ôºâor data generationÔºà œÄbehavior).</td></tr><tr><td>Value Function</td><td>Expected cumulative reward when following a policy( VœÄ(s),QœÄ (s,a)).</td><td>The quantity OPE methods aim to estimate for the target policy.</td></tr><tr><td>On-Policy RL</td><td>Learning/evaluation using data generated by the same policy being improved.</td><td>Contrast to OPE; requires online interaction with the learning policy.</td></tr><tr><td>Off-Policy RL</td><td>Learning/evaluation using data generatedby a different policy.</td><td>OPE is a core component; enables learning from pre- collected datasets.</td></tr><tr><td>OPE</td><td>Off-Policy Evaluation: Estimating VœÄ or QœÄ using data from œÄb¬∑</td><td>The central problem addressed by the survey.</td></tr></table></body></html>  

An MDP is formally defined by the tuple  

[1]. Here, $\backslash ( \backslash \mathsf { m a t h c a l \{ S \} \backslash } )$ represents the set of possible states the agent can be in, and $\langle \langle \mathsf { m a t h c a l } \{ \mathsf { A } \} \backslash )$ is the set of actions the agent can take [8,19,26]. The symbol \(\rho\) denotes the initial state distribution, and $\backslash ( \mathsf { P } ( \mathsf { s } ^ { \prime } \backslash \mathsf { m i d } \mathsf { s } , \mathsf { a } ) \backslash )$ represents the transition probability function, specifying the probability of transitioning to state $\big \langle \big ( \mathsf { s } ^ { \prime } \big \rangle \big )$ from state $\mathsf { \backslash } ( \mathsf { s } \backslash )$ after taking action \ (a\) [1]. The reward function $\big \langle \big | \big | \mathsf { R } \big ( \mathsf { s } , \mathsf { a } , \mathsf { s } ^ { \prime } \big ) \big \rangle \big |$ provides a scalar reward signal after taking action $\left\backslash \left( \mathsf { a } \right\backslash \right)$ in state $\mathsf { \backslash } ( \mathsf { s } \backslash )$ and transitioning to state $\left. \left( \mathsf { s } ^ { \prime } \right. \right)$ [8,26]. The discount factor \(\gamma\) weighs future rewards against immediate rewards [1,8]. Data in RL is typically structured as state-action-reward sequences collected over time [5].  

Within the MDP framework, the agent's behavior is governed by a policy, denoted by \(\pi\). A policy $\langle \langle \mathsf { p i } ( \mathsf { a } \lfloor \mathsf { m i d } \mathsf { s } ) \rangle )$ is a distribution over actions given a state, specifying the probability of taking action $\left\backslash \left( \mathsf { a } \right\backslash \right)$ in state $ ( \mathsf { s } \backslash ) $ . Policies can be deterministic, where $\mathsf { \backslash } ( \mathsf { \backslash p i } ( \mathsf { s } ) \backslash )$ returns a single action, or stochastic, where $\langle \lvert \mathsf { p i } ( \mathsf { a } \backslash \mathsf { m i d } \mathsf { s } ) \backslash \rangle$ is a probability distribution [2]. The objective in RL is often to find an optimal policy that maximizes the expected cumulative reward, known as the return. The return $\mathsf { \backslash } ( \mathsf { G \_ t } \backslash )$ at time $\backslash ( { \mathsf { t } } \backslash )$ is defined as the discounted sum of future rewards:‚Äã   
\‚Äã   
[22].‚Äã   
The performance of a policy is quantified by value functions [8,19,23,26]. The state-value function $\mathsf { \backslash } ( \mathsf { V } ^ { \wedge } \backslash \mathsf { p i } ( \mathsf { s } ) \backslash )$ represents the expected return when starting in state $\mathsf { \backslash } ( \mathsf { s } \backslash )$ and following policy $\mathsf { \backslash } ( \mathsf { \backslash p i } )$ :   
\‚Äã   
\]‚Äã   
[19,22]. The state-action value function, or Q-value function, $\backslash ( \mathsf { Q } ^ { \wedge } \backslash \mathsf { p i } ( \mathsf { s } , \mathsf { a } ) \backslash )$ represents the expected return when starting in state $\mathsf { \backslash } ( \mathsf { s } \backslash )$ , taking action $\left\backslash \left( \mathsf { a } \right\backslash \right)$ , and subsequently following policy $\mathsf { \backslash } ( \mathsf { \backslash p i } )$ :‚Äã   
\‚Äã  

[1,19]. Value functions are central to solving RL problems, enabling policy evaluation and improvement [19]. Policy iteratio methods, based on value functions, are a common approach to finding optimal policies [4].  

A key relationship in RL is captured by the Bellman equation, which expresses the value of a state or state-action pair in   
terms of the values of successor states or state-action pairs [11,12,26,27]. For the state-value function, the Bellman equation   
is given by   
$\backslash ,$   
\]‚Äã   
and similarly, for the Q-value function,   
\.‚Äã   
\]‚Äã  

The Bellman equation is crucial for developing dynamic programming and temporal difference learning algorithms and is particularly relevant for model-based methods [11,12] and the Bellman operator used in value iteration [3].  

![](images/3abf57022dc3a5ae3612de741446e4d18ee4753df0218f7ecdc58324c92d81d6.jpg)  

RL algorithms can be broadly categorized into on-policy and off-policy methods [2,8,19,23,26]. In on-policy methods, the policy being evaluated or improved is the same policy used to generate the data [2,8,23]. This means that when the policy is updated, new data must be collected according to the modified policy [2,26]. Examples include SARSA [8]. Conversely, offpolicy methods evaluate or improve a target policy \(\pi\) using data generated by a different behavior policy \(\mu\) [2,8,19,23]. This allows off-policy algorithms to learn from data collected by any policy, including previous versions of the target policy or a fixed exploratory policy, and potentially reuse data more effectively, often stored in an experience replay buffer [2]. This flexibility is a significant opportunity, enabling learning from logged data or from a behavior policy designed purely for exploration [26]. Offline RL, for instance, trains the agent using a pre-collected dataset \(\mathcal{D}\) generated by a behavior policy [1,3], relying heavily on off-policy techniques [3].‚Äã  

A fundamental challenge in RL is the exploration‚Äìexploitation dilemma: the agent must explore the environment to discover better actions while exploiting its current knowledge to maximize rewards [19]. Off-policy methods naturally address this by allowing the behavior policy to be more exploratory (e.g., using an \(\epsilon\)-greedy strategy where non-greedy actions are chosen with probability \(\epsilon\)) while the target policy can be more greedy or deterministic, focusing on optimizing performance based on the collected data [2,19].  

Off-Policy Evaluation (OPE) is the problem of estimating the value or performance of a target policy \(\pi\) using data collected by a behavior policy $\backslash ( \backslash \mathsf { m u } \backslash \mathsf { n e q } \backslash \mathsf { p i } \backslash )$ [6,13,22]. This is crucial for evaluating new policies offline before deployment, selecting the best policy from a set of candidates, or for stable policy improvement in off-policy control. The goal is often to estimate the value function $\backslash ( { \mathsf { V } } ^ { \wedge } \backslash { \mathsf { p i } } ( { \mathsf { s } } ) \backslash ) { \mathsf { o r } } \backslash ( { \mathsf { Q } } ^ { \wedge } \backslash { \mathsf { p i } } ( { \mathsf { s } } , { \mathsf { a } } ) \backslash )$ using the collected episodes or state-action-reward tuples [22,27]. Performance metrics for OPE estimators include Mean Squared Error (MSE) and bias [6,27].  

mportance Sampling (IS) is a fundamental statistical technique widely used for OPE, particularly in Monte Carlo methods [11,22]. The core idea is to re-weight the returns observed under the behavior policy $\sf { \backslash ( \backslash m u \backslash ) }$ by the ratio of the probability of trajectories occurring under the target policy $\mathsf { \backslash } ( \mathsf { \backslash p i } )$ to their probability under the behavior policy $\left\backslash \left( \mathsf { m u } \right\backslash \right)$ . This ratio is known as the importance weight. The standard IS estimator for the value of a state $\mathsf { \backslash } ( \mathsf { s } \backslash )$ under policy $\mathsf { \backslash } ( \mathsf { \backslash p i } )$ , using trajectories starting from $\mathsf { \backslash } ( \mathsf { s } \backslash )$ generated by $\left. \left. \mathsf { m } \mathsf { u } \right. \right.$ , involves averaging the product of the importance weight and the return for each trajectory. The importance weight for a trajectory $\backslash ( ( \mathsf { s } _ { - } \mathsf { t } , \mathsf { a } _ { - } \mathsf { t } , \mathsf { s } _ { - } \{ t + 1 \} , \mathsf { \backslash } \mathsf { d o t s } ) \backslash )$ from time \(t\) is given by‚Äã \‚Äã  

assuming a finite horizon $\left. { \left( { \mathsf { T } } \right\backslash } \right)$ . A key limitation of standard Importance Sampling in OPE is its high variance, which increases significantly with the horizon length and the divergence between the behavior and target policies [11]. This high variance can render IS estimates unreliable in practice, necessitating the development of more stable and efficient OPE methods.  

# 3. Traditional Off-Policy Evaluation Methods  

Off-Policy Evaluation (OPE) is a fundamental challenge in reinforcement learning, aiming to estimate the performance of a target policy using data collected from a different behavior policy [19,22]. Traditional OPE methods address the distribution mismatch between the observed data and the target policy in various ways.  

![](images/62f522d72edfa37bfce722db6e2e4d0bb11d73b98e3421925e98f15b59693e69.jpg)  

These methods can be broadly categorized into distinct classes based on their underlying principles: Importance Sampling (IS) based methods, Model-based methods, Direct Methods (DM), and Hybrid Methods (HM) which combine elements of IS and DM. Additionally, other approaches rooted in Temporal Difference (TD) learning, such as variants like Tree Backup, contribute to this landscape [7,11,12,25,26].‚Äã  

Importance Sampling (IS) methods tackle the distribution shift by re-weighting observed rewards or returns using the ratio of the target policy's probability to the behavior policy's probability for the observed actions [4,5,10,19,22]. While conceptually simple and unbiased in certain forms like Inverse Propensity Scoring (IPS), IS methods, particularly in their basic forms, are highly susceptible to large variance, especially with long horizons or significant policy differences. Techniques like Weighted Importance Sampling (WIS), Per-Decision IS (PDIS), Per-Decision WIS (PDWIS), and clipping are employed to mitigate this variance, often introducing a bias-variance trade-off [5,6,11,12,25].  

Model-based OPE methods operate by first learning a model of the environment dynamics (transitions and rewards) from the off-policy data [11,12,26]. Once learned, this model is then used to evaluate the target policy, typically by solving or simulating the Bellman equations. A key advantage of model-based approaches is their potential for lower variance compared to IS methods as they leverage information across multiple transitions. However, their performance is highly dependent on the accuracy of the learned model; errors or misspecification in the model can introduce significant bias into the value estimate [11,12,14].  

Direct Methods (DM), in contrast, focus on directly estimating the value function $( \backslash ( \mathsf { V } ^ { \wedge } \backslash \mathsf { p i } \backslash ) \mathsf { o r } \backslash ( \mathsf { Q } ^ { \wedge } \backslash \mathsf { p i } \backslash ) )$ of the target policy using the offline data [7,11,12]. These methods often employ function approximation techniques, frequently leveraging deep learning, alongside temporal difference (TD) learning principles [1,2,25,26]. Examples include methods like Fitted QEvaluation (FQE). Similar to model-based methods, DM often exhibit lower variance than IS, as they do not rely on importance weights. However, they are prone to bias stemming from inaccuracies in the function approximation or the challenges of learning value functions from off-policy data, particularly under distribution shift [11,12].  

Hybrid Methods (HM) seek to combine the strengths of IS and DM, aiming to reduce both bias and variance simultaneously. These methods typically use a learned value function (from DM) as a control variate to reduce the variance of an IS estimator, or employ importance weighting to correct the bias of a DM estimator [7,11,25]. Doubly Robust (DR) estimators are a prominent example within this category, providing consistency if either the IS part (propensity scores) or the DM part (value function) is correctly specified [7,11].  

Beyond these primary categories, other traditional approaches draw upon foundational reinforcement learning algorithms. Temporal Difference (TD) learning methods like Q-learning and SARSA, although originally presented in different contexts, form a basis for off-policy value estimation by bootstrapping from sampled transitions [27]. Q-learning's inherent off-policy nature allows it to learn about the optimal policy directly, while SARSA requires modifications like importance sampling to evaluate a target policy different from the behavior policy [27]. Specific algorithms like Tree Backup can be viewed within this broader TD framework for OPE, offering different trade-offs in their updates and variance characteristics.  

<html><body><table><tr><td>Category</td><td>Core ldea</td><td>Handling Distribution Shift / Data</td><td>Bias</td><td>Variance</td></tr><tr><td>Importance Sampling (IS)</td><td>Re-weight data based on policy ratios.</td><td>Explicitly via importance weights</td><td>Low (IPS) / Higher (WIS)</td><td>High (especially long horizon)</td></tr><tr><td></td><td>environment model,evaluate policy on model.</td><td>Implicitly via learned dynamics</td><td>High (from model error)</td><td>Low (vs IS)</td></tr><tr><td>Direct Methods (DM)</td><td>Directly estimate value function.</td><td>Via function approximation learning</td><td>High (from approx error)</td><td>Low (vs IS)</td></tr></table></body></html>  

In summary, traditional OPE methods offer diverse strategies, each with its own strengths and weaknesses, primarily characterized by the bias-variance trade-off and their approach to handling the distribution shift inherent in off-policy data. The selection and application of these methods depend heavily on the specific OPE problem, the nature of the offline data, and the acceptable levels of bias and variance.  

# 3.1 Importance Sampling (IS) based Methods  

<html><body><table><tr><td>IS Variant</td><td>Core ldea</td><td>Bias (Finite Sample)</td><td>Variance (Finite Sample)</td><td>Horizon Sensitivity</td><td>Weight Calculation</td></tr><tr><td>IPS (Plain IS)</td><td>Re-weight total return by trajectory likelihood</td><td>Unbiased</td><td>High</td><td>High (grows exponentiall</td><td>Product of step-wise ratios (I pt</td></tr><tr><td>(Weighted IS)</td><td>average of IS estimates.</td><td>(consistent)</td><td>IPS)</td><td>exponentiall y)</td><td>step-wise ratios(IIpt ), then normalize</td></tr><tr><td>PDIS (Per- Decision IS)</td><td>Apply IS weighting at each step for value updates.</td><td>Consistent</td><td>Can still be high</td><td>Lower (vs IPS/WIS)</td><td>Product of step-wise ratios up to current step ( I pkÔºâ k=0</td></tr><tr><td>Decision WIS)</td><td>average of PDIS estimates at each step.</td><td>(consistent)</td><td>among variants</td><td>IPS/WIS)</td><td>PDIS weights, then normalize</td></tr></table></body></html>  

<html><body><table><tr><td>Clipping /</td><td>Cap</td><td>Biased</td><td>Lower (vs</td><td>Reduced</td><td>Modified</td></tr><tr><td rowspan="4">Truncation</td><td>importance</td><td></td><td>standard IS)</td><td>impact</td><td>step-wise</td></tr><tr><td>weights at a</td><td></td><td></td><td></td><td>ratios (</td></tr><tr><td>maximum</td><td></td><td></td><td></td><td>min(pt,C))</td></tr><tr><td>value.</td><td></td><td></td><td></td><td></td></tr></table></body></html>  

Importance Sampling (IS) is a fundamental technique for Off-Policy Evaluation (OPE) in Reinforcement Learning. The core challenge in OPE is the distribution mismatch between the data collected by a behavior policy and the performance we wish to evaluate for a different target policy [19,22]. IS addresses this mismatch by re-weighting the observed data based on the relative probabilities of trajectories or state-action pairs occurring under the target versus the behavior policy [5,19,22]. This re-weighting relies on propensityscores, which are the probabilities of taking observed actions given observed states under both policies.‚Äã  

The most basic IS estimator is the Inverse Propensity Scoring (IPS) estimator, also known as the plain IS estimator. IPS corrects the distribution discrepancy by weighting the rewards from the behavior policy data by the ratio of the target policy's probability to the behavior policy's probability for the observed actions [5,6,10,25]. For a given trajectory  

$$
\tau = ( S _ { 0 } , A _ { 0 } , R _ { 1 } , S _ { 1 } , A _ { 1 } , R _ { 2 } , \ldots , S _ { T - 1 } , A _ { T - 1 } , R _ { T } ) ,
$$  

the importance weight is the product of the policy probability ratios at each step:  

$$
\rho _ { T } ( \tau ) = \prod _ { t = 0 } ^ { T - 1 } \frac { \pi _ { \mathrm { t a r g e t } } ( A _ { t } \mid S _ { t } ) } { \pi _ { \mathrm { b e h a v i o r } } ( A _ { t } \mid S _ { t } ) } .
$$  

The IPS estimator for the total return of a trajectory is then given by:  

$$
\hat { V } _ { \mathrm { I P S } } ( \pi _ { \mathrm { t a r g e t } } ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } R _ { i , T } \rho _ { T } ( \tau _ { i } ) ,
$$  

where $N$ is the number of observed trajectories, $R _ { i , T }$ ‚Äã is the total return of the $\mathbf { \chi } _ { i }$ -th trajectory, and $\rho _ { T } ( \tau _ { i } )$ is its importance weight. IPS provides an unbiased estimate of the target policy's value and is consistent under mild assumptions [5,6,10,11,12,25]. However, its primary drawback is high variance, particularly in scenarios with long horizons or significant differences between the behavior and target policies, where the importance weights can become very large [5,6,11,12,22]. Small propensity scores under the behavior policy for actions preferred by the target policy exacerbate this variance issue [6].‚Äã  

To mitigate the high variance of IPS, Weighted Importance Sampling (WIS) was introduced as a variance reduction techniqu [5,11,12]. The WIS estimator is given by:  

$$
\hat { V } _ { \mathrm { W I S } } ( \pi _ { \mathrm { t a r g e t } } ) = \frac { \sum _ { i = 1 } ^ { N } R _ { i , T } \rho _ { T } ( \tau _ { i } ) } { \sum _ { i = 1 } ^ { N } \rho _ { T } ( \tau _ { i } ) } .
$$  

WIS is generally more data-efficient and exhibits lower variance than IPS [25]. However, unlike IPS, WIS is typically biased, especially with limited data, although it remains consistent [25]. This represents a classical bias-variance trade-off; WIS sacrifices some bias for a reduction in variance, leading to a more accurate estimate in practice, particularly with finite data [5,11,12,25].‚Äã  

For tasks with long horizons, trajectory-based IS methods like IPS and WIS suffer significantly from the product form of the importance weight, which grows exponentially with the trajectory length, leading to instability and high variance [25]. PerDecision Importance Sampling (PDIS) addresses this by applying importance weighting at each time step, considering the state-dependent probability ratio up to that point [4,25]. For an estimated value function at state $S _ { t }$ ‚Äã , the per-decision weight at time $t$ is  

$$
\rho _ { t } ( S _ { 0 } , { \cal A } _ { 0 } , \ldots , S _ { t } , { \cal A } _ { t } ) = \prod _ { k = 0 } ^ { t } \frac { \pi _ { \mathrm { t a r g e t } } ( A _ { k } \mid S _ { k } ) } { \pi _ { \mathrm { b e h a v i o r } } ( A _ { k } \mid S _ { k } ) } .
$$  

While the digest mentions PDIS and its consistency, it also notes that it can still exhibit large variance in finite sample cases, potentially leading to unstable learning [4]. PDIS and its weighted counterpart, Per-Decision Weighted Importance Sampling (PDWIS), are designed to mitigate the long-horizon problem of IPS and WIS [25]. Empirical studies suggest that PDWIS generally performs the best among the four variants (IPS, WIS, PDIS, PDWIS) in terms of balancing bias and variance for long horizons [25].‚Äã  

Variance reduction remains a critical challenge for IS-based methods. Beyond using weighted averages (as in WIS and PDWIS), other techniques involve modifying the importance weights themselves. Truncatedimportancesampling, also known as clipping, is a common strategy where importance weights are capped at a maximum value [11,12]. This explicitly trades off the unbiasedness of IPS for a reduction in variance, introducing bias but potentially yielding a more stable and useful estimate, especially when the distribution shift is large [5,11,12].  

In summary, IS-based methods provide a direct way to estimate the value of a target policy from off-policy data by reweighting observations according to their likelihood under the target policy. IPS is unbiased and consistent but suffers from high variance, particularly over long horizons or with significant policy differences. WIS reduces variance by using a weighted average, trading off unbiasedness for improved finite-sample performance. PDIS and PDWIS extend this concept to per-decision estimation, which is crucial for long-horizon tasks [25]. While PDIS is consistent, its finite-sample variance can still be substantial [4]. Techniques like clipping further manage the bias-variance trade-off to achieve more stable estimates in practice [11,12]. Despite these variance reduction efforts, the practical performance of IS variants can still be limited in complex scenarios [11,12].‚Äã  

# 3.2 Model-Based Methods  

![](images/01a4aef566b520d5d451b54e2d57bd53ae464877d111dbfa11e20c3433cc2612.jpg)  

Model-based methods represent an alternative approach to Off-Policy Evaluation (OPE), particularly relevant for infinite horizon Markov Decision Processes (MDPs) [11,12]. Unlike model-free methods, such as Monte Carlo approaches that learn directly from sampled experience without requiring prior knowledge of the environment's dynamics [23], model-based techniques explicitly learn a model of the environment from the available off-policy data [26]. This learned model typically encompasses the transition dynamics‚Äîspecifying the probability of moving to a new state given a state and action‚Äîand, potentially, the reward function.‚Äã  

The process of learning the environment model from off-policy data involves estimating the transition probabilities, denoted as $\mathsf { P } ( \mathsf { s } ^ { \prime } | \mathsf { s } , \mathsf { a } )$ , and the expected reward, ${ \sf R } ( \mathsf { s } , \mathsf { a } , \mathsf { s } ^ { \prime } )$ , based on observed transitions (s‚Çú, a‚Çú, r‚Çú‚Çä‚ÇÅ, s‚Çú‚Çä‚ÇÅ) collected under the behavior policy. Common techniques for this estimation include maximum likelihood estimation, which seeks the model parameters that maximize the probability of observing the collected data, as well as Bayesian methods that maintain a probability distribution over possible models.  

Once a model, represented as ùëÉÃÇand ùëÖ,ÃÇis learned, it is utilized to estimate the value function of the target policy œÄ. This is typically achieved by leveraging the Bellman equations with the learned model. For instance, the value function $\mathsf { V } ^ { \wedge } \pi ( \mathsf { s } )$ or the action-value function $Q ^ { \wedge } \pi ( \mathsf { s } , \mathsf { a } )$ can be computed either by iteratively solving the Bellman equations or by using simulation with the learned model, potentially combined with function approximation [12]. Specifically, the value function can be estimated using the Bellman expectation equation:‚Äã  

$$
V ^ { \pi } ( s ) = \sum _ { a } \pi ( a | s ) \sum _ { s ^ { \prime } } \hat { P } ( s ^ { \prime } | s , a ) \Bigl ( \hat { R } ( s , a , s ^ { \prime } ) + \gamma V ^ { \pi } ( s ^ { \prime } ) \Bigr )
$$  

where Œ≥ is the discount factor.  

A primary advantage of model-based estimators is their propensity for lower variance compared to Importance Sampling (IS) estimators [11,12]. This variance reduction occurs because they consolidate information from multiple transitions to build a global model, rather than relying solely on single-trajectory weightings. However, this benefit comes at the cost of potentially higher bias [11,12]. The main source of bias is errors in the learned model of the environment [11,12]. If the learned model (ùëÉ,ÃÇùëÖ)ÃÇdeviates significantly from the true environment dynamics (P, R), then the value estimate derived from this model will be biased.  

The impact of model accuracy on OPE performance is substantial. Model misspecification‚Äîwhere the true dynamics lie outside the hypothesis class of the learned model‚Äîor errors in parameter estimation can lead to significant bias in the value estimate. This challenge is particularly relevant when the off-policy data does not sufficiently cover the state-action space relevant to the target policy, or when there is a distribution shift [14]. Research efforts focus on developing robust model learning techniques to mitigate the effects of such errors and improve the reliability of model-based OPE under these conditions [14].  

In summary, model-based OPE methods estimate the target policy value by first learning an environmental model from offpolicy data and then using this model for evaluation. While these methods offer potential variance reduction compared to IS, their performance is highly sensitive to the accuracy of the learned model, with model errors potentially leading to bias.  

# 3.3 Direct Methods (DM)  

<html><body><table><tr><td>Aspect</td><td>Description</td></tr><tr><td>Core Goal</td><td>Directly estimate the value function ÔºàVœÄ, QœÄ )of the target policy.</td></tr><tr><td>Key Techniques</td><td>Function Approximation (Linear, Kernel, DNNs), Temporal Difference (TD) Learning principles applied to offline data.</td></tr><tr><td>Examples</td><td>Fitted Q-Evaluation (FQE), Fitted Value Iteration,QœÄ(Œª),DDPG/DQN-style Q- learning on offline data, Minimax estimators.</td></tr><tr><td>Pros</td><td>Potential for lower variance compared to IS methods. Can handle large state spaces with function approximation.</td></tr><tr><td>Cons</td><td>Prone to bias from function approximation errors and distribution shift.Can be unstable in offline settings.</td></tr></table></body></html>  

Direct Methods (DM) constitute a fundamental category of approaches for Off-Policy Evaluation (OPE) in reinforcement learning [7,11,12]. The core principle behind DM is to directly estimate the value function, such as the state value function \ $( \mathsf { V } ^ { \wedge } \mathsf { \backslash p i } ( \mathsf { s } ) \backslash )$ or the state-action value function $\backslash ( \mathsf { Q } ^ { \wedge } \backslash \mathsf { p i } ( \mathsf { s } , \mathsf { a } ) \backslash )$ , for the target policy $\mathsf { \backslash } ( \mathsf { \backslash p i } )$ using the offline dataset collected under a behavior policy $\mathsf { \backslash } ( \mathsf { \backslash p i \_ b \backslash } )$ [11,12]. Unlike Importance Sampling (IS) methods that re-weight returns based on the ratio of target and behavior policy probabilities, DM bypasses the need for explicit importance weights or dynamics models of the environment by learning a model of the value function from the observed transitions $\big \backslash \big ( \big ( \mathsf { s } , \mathsf { a } , \mathsf { r } , \mathsf { s } ^ { \prime } \big ) \big \backslash \big )$ in the offline data [11,12].  

Prominent examples of Direct Methods include Fitted Q-Evaluation (FQE) and $\backslash ( \mathsf { Q \_ { \mathsf { D } } i } ( \backslash \lfloor \mathsf { a m b d a } ) \backslash )$ [25]. These methods typically involve fitting a function approximator, often a neural network, to estimate the value function based on temporal difference (TD) learning principles applied to the offline data. Related techniques rooted in value function estimation, such as Fitted Value Iteration and Deep Q-Networks (DQN), can also be considered within the purview of DM, particularly when adapted for off-policy data [26]. Foundational temporal difference methods like TD(0), Sarsa, and Q-learning implicitly utilize the concept of directly learning value functions from experience [8], though their adaptation and analysis specifically for offline OPE are key aspects of DM. Furthermore, methods like minimax-style estimators have also been explored within the DM framework [25].  

Deep learning-based approaches such as Deep Deterministic Policy Gradient (DDPG), originally designed for online settings with continuous action spaces as an adaptation of Deep Q-learning, learn a Q-function to guide policy updates [2]. The objective function in such methods involves maximizing the Q-value under the current policy distribution, which relies on evaluating the Q-function using observed data. A DDPG-style approach utilizing target networks and TD learning has been employed for policy evaluation in an offline context, specifically for estimating the behavior policy's Q-value \ $\left( \backslash \mathsf { l e f t } ( \mathsf { Q } ^ { \wedge } \backslash \mathsf { b e t a } \backslash \mathsf { r i g h t } ) \backslash \right)$ to facilitate one-step policy improvement [1]. This illustrates how direct value function estimation using deep neural networks and TD methods forms the basis of many DM techniques for OPE.‚Äã  

A significant advantage of Direct Methods compared to IS methods is their potential for lower variance [11,12]. Since DM does not rely on potentially large and unstable importance weights, which are sensitive to the discrepancy between the behavior and target policies, the variance of the estimate tends to be more controlled. However, the primary disadvantage of DM is the potential for bias, particularly when using function approximation to estimate the value function or if an environment model is learned and is misspecified [11,12]. The accuracy of the value estimate heavily depends on the capacity and correctness of the chosen function approximator and the data distribution. This introduces a fundamental variance-bias trade-off when comparing DM and IS methods for OPE.  

Addressing the distribution shift between the offline data (generated by $\mathsf { \backslash ( | p i \_ b \backslash ) }$ ) and the target policy \(\pi\) is crucial for reliable OPE [11,12]. While the provided digests mention the need to analyze how direct methods handle this problem, potentially through regularization or constraint techniques [11,12], the specific mechanisms by which these techniques are integrated into DM are not detailed in the available information. However, Direct Methods that leverage deep neural networks, such as DQN or DDPG-style approaches, are inherently capable of handling large state and action spaces [2,26]. The use of flexible function approximators allows these methods to generalize across vast continuous or high-dimensional discrete state and action spaces, which is a prerequisite for applying OPE in complex real-world scenarios.‚Äã  

# 3.4 Tree Backup  

The exploration of specific details regarding the Tree Backup algorithm is not feasible based on the currently available input materials [26]. However, the application of Temporal Difference (TD) learning methods constitutes a significant approach within off-policy evaluation (OPE) in reinforcement learning [27]. These methods leverage sampled transitions collected under a behavior policy to estimate the value function or state‚Äìaction value function of a different target policy. This section focuses on how prominent TD control algorithms, namely Q-learning and SARSA, are conceptualized and utilized within the OPE framework [27].‚Äã  

TD learning methods employed for OPE operate by updating value estimates based on the discrepancy between the current estimate and a bootstrapped estimate derived from subsequent states and rewards observed in sampled transitions. For a given transition‚Äã   
\  

the core idea is to use this experience, generated by the behavior policy $\backslash ( { \mathsf { b } } ( { \mathsf { a } } \backslash { \mathsf { m i d } } { \mathsf { s } } ) \backslash )$ , to learn about the target policy \ $\mathrm { \langle \backslash p i ( a \backslash m i d s ) \backslash \rangle }$ .  

Q-learning, inherently an off-policy algorithm, is directly applicable to OPE. The standard Q-learning update for a state‚Äì   
action pair $\mathsf { \backslash } ( ( \mathsf { S \_ t } , \mathsf { A \_ t } ) \backslash )$ visited by the behavior policy is given by:   
\‚Äã  

Here, $\mathsf { \backslash } ( \mathsf { \backslash a l p h a } )$ is the learning rate and $\backslash ( \backslash \mathsf { g a m m a } \backslash )$ is the discount factor. Crucially, the update uses the maximum value over all possible actions in the next state $\backslash ( \mathsf { S \_ } \{ \mathsf { t } + \mathsf { 1 } \} \backslash )$ (i.e., $\backslash ( \backslash \mathsf { m a x \_ } \{ \mathsf { a } ^ { \prime } \} \mathsf { Q } ( \mathsf { S \_ } \{ t + 1 \} , \mathsf { a } ^ { \prime } ) \backslash ) \rceil$ , which corresponds to the greedy action under the learnedvalue function, representing the target policy's optimal action. This selection of the next action value based on the target policy's greedy choice, rather than the action actually taken by the behavior policy, makes Qlearning off-policy and allows it to converge to the optimal action‚Äìvalue function $\big \backslash ( \mathsf { Q } ^ { \wedge \star } ( \mathsf { s } , \mathsf { a } ) \backslash )$ regardless of the behavior policy, provided all state‚Äìaction pairs are sufficiently explored [27].  

SARSA (State‚ÄìAction‚ÄìReward‚ÄìState‚ÄìAction), in its original form, is an on-policy algorithm. Its update rule uses the action \   
$( \mathsf { A } _ { - } \{ \mathsf { t } + 1 \} \backslash )$ takenbythebehaviorpolicyin state $\backslash ( \mathsf { S \_ } \{ \mathsf { t } + \mathsf { 1 } \} \backslash )$ :‚Äã   
\‚Äã   
For SARSA to be used for OPE, modifications are necessary, typically involving importance sampling techniques to account   
for the difference between the behavior policy $\left\backslash ( \mathsf { b } \backslash ) \right.$ and the target policy $\mathsf { \backslash } ( \mathsf { \backslash p i \backslash } )$ . The update can be weighted by the   
importance sampling ratio   
\‚Äã   
A simple importance‚Äêweighted SARSA update might look like:   
\‚Äã  

However, applying importance sampling to the bootstrapping term $\backslash ( { \mathsf { Q } } ( { \mathsf { S } } _ { - } \{ { \mathsf { t } } + 1 \} , { \mathsf { A } } _ { - } \{ { \mathsf { t } } + 1 \} ) \backslash )$ also requires careful consideration, often leading to more complex updates or variants like SARSA\((\lambda)\) with importance weighting.  

Comparing their convergence properties in the OPE context, standard Q-learning directly estimates $\backslash ( \mathsf { Q } ^ { \wedge } ( s , a ) | ) .$ ,which representstheoptimalpolicy'svaluefunction.Itconvergesreliablyunderstandardconditions,providedsuficient exploration.StandardSARSA,withoutimportancesampling,estimatesthevaluefunctionofthebehaviorpolicy $| ( b | )$ .To estimatethevaluefunctionofatargetpolicy\(\pi\)diferentfrom $| ( b | ) \rrangle$ usingSARSA,importancesamplingistypicaly required,asmentioned.Thisintroducesvarianceintotheupdates,whichcanmakeconvergenceslowerorles stable comparedtostandard $Q$ -learning,especialywhenthetargetandbehaviorpoliciesdivergesignificantly. $Q$ -learning's relianceonthe $| ( | m a x | )$ operatormakesitnaturalyof-policyandgeneralymorerobusttothechoiceofbehaviorpolicyfo learning $| / Q ^ { \Lambda } \backslash )$ , whereas SARSA's direct use of the behavior policy's next action requires explicit correction mechanisms for OPE beyond simply learning $\left\backslash \left( \mathsf { Q } ^ { \wedge } \mathsf { b } \right\backslash \right)$ [27].  

In summary, while Tree Backup is a specific OPE algorithm, TD methods like Q-learning and SARSA offer foundational approaches to OPE by learning from sampled transitions. Q-learning's inherent off-policy nature makes it directly applicable for learning optimal values, while SARSA requires modifications, such as importance sampling, to effectively estimate values under a target policy different from the behavior policy. The choice between these methods or their variants for OPE depends on factors like the desired target (optimal vs. arbitrary target policy) and tolerance for variance [27].  

# 4. Hybrid and Doubly Robust (DR) Methods  

![](images/404b3635ff15ac6e1c901a653737c18ce91c5dd9006d61b3303f90f668596d04.jpg)  

Hybrid Methods (HM), particularly Doubly Robust (DR) estimators, represent a significant class of Off-Policy Evaluation (OPE) techniques designed to leverage the complementary strengths of both Importance Sampling (IS) and Direct Methods (DM) or model-based approaches [11,12]. By combining these methodologies, DR estimators aim to achieve a superior balance between bias and variance, addressing the high variance often associated with pure IS and the potential bias of pure modelbased methods due to model misspecification [11,12]. DR is recognized as one of the prominent methods utilized in OPE [7].  

The core principle behind DR estimators is their eponymous "double robustness" property. While the specific formula can vary depending on the estimated quantity (e.g., state value, action value, or trajectory return), a general structure involves a sum of a model-based term and an IS-weighted residual term. This construction leads to a desirable property: the estimator is statistically consistent if either the importance weights (derived from the behavior and target policies) are accurate, or the environment model (used for value function or transition estimates) is accurate. Furthermore, DR estimators can be shown to be unbiased if either the importance weights or the value function estimates are correct. This characteristic provides a  

safeguard against potential errors in one of the estimation components, contributing to the robustness of the method. The process typically involves estimating nuisance components, which can often be performed with theoretical guarantees [7].  

Empirical studies and theoretical analyses indicate that DR methods effectively reduce variance compared to pure IS estimators while mitigating bias compared to pure model-based or DM methods [11,12]. This makes them a compelling choice when both the policy ratio and the value function (or model) can be estimated.  

Within the framework of Hybrid Methods, various variants exist. For many Direct Method approaches (excluding the Infinite Horizon setting), three corresponding HM variants are commonly discussed: standard Doubly Robust (DR), Weighted Doubly Robust (WDR), and MAGIC (Model Aided, Goals-based Importance Sampling Corrected) [25]. Empirical comparisons across different Direct Methods suggest a general performance ranking among these variants, with MAGIC typically outperforming WDR, and WDR outperforming standard DR [25]. This ranking implies that further refinements and weighting schemes built upon the fundamental DR structure can lead to improved performance in practice.‚Äã  

Research in this area continues to evolve, with contributions exploring novel DR formulations and their applications, such as "From Importance Sampling to Doubly Robust Policy Gradient" and "Double Reinforcement Learning for Efficient and Robust Off-Policy Evaluation" [24]. These works underscore the ongoing effort to develop more efficient and robust OPE methods leveraging the DR principle.‚Äã  

# 5. Theoretical Properties of OPE Methods  

Analyzing the theoretical properties of Off-Policy Evaluation (OPE) methods is crucial for understanding their reliability and performance guarantees.  

<html><body><table><tr><td>Property</td><td>Description</td><td>Relevance to OPE</td></tr><tr><td>Consistency</td><td>Estimator converges to the true value as sample size approaches infinity.</td><td>Ensures reliability with large datasets; basic requirement for a good estimator.</td></tr><tr><td>Asymptotic Normality</td><td>Estimator error distribution approaches a normal distribution for large samples.</td><td>Allows construction of confidence intervals and hypothesis testing.</td></tr><tr><td>Finite-Sample Bounds</td><td>Guarantees on estimator performance (e.g., error magnitude) for a fixed, limited dataset size.</td><td>Crucial for practical applications where data is always finite.</td></tr><tr><td>Bias-Variance Trade-off</td><td>The relationship where reducing bias often increases variance,and vice- versa.</td><td>Central challenge in OPE; different methods make different trade-offs.</td></tr><tr><td>Error Propagation</td><td>How estimation errors (e.g., in Q-values or model) accumulate or amplify, especially in iterative methods.</td><td>Impacts stability and final performance; requires robust error control.</td></tr></table></body></html>  

Key properties include consistency, asymptotic normality, and finite-sample error bounds [15]. These theoretical foundations provide insights into the efficiency and limitations of various estimators. While traditional statistical viewpoints have contributed significantly, achieving novel theoretical results beyond the classic Importance Sampling (IS), Modelbased, and Doubly Robust (DR) methods has been noted as challenging [11]. There is a recognized need for a deeper statistical understanding and more rigorous theoretical guarantees for recent OPE algorithms [7].‚Äã  

The theoretical efficiency of different estimators is often assessed by their asymptotic variance or finite-sample error bounds [15]. The trade-offs between bias and variance are central to understanding these properties. For instance, the  

MLIPS estimator has been proven to be asymptotically unbiased and exhibits a smaller non-asymptotic mean squared error (MSE) compared to the standard Importance Sampling (IPS) estimator [6]. This demonstrates improved theoretical efficiency in terms of both asymptotic consistency and finite-sample performance for MLIPS relative to a baseline. Furthermore, studies have established the Fisher consistency of various OPE estimators when applied to abstract state spaces, contributing to the understanding of their asymptotic properties [9].  

Specific methods offer distinct theoretical guarantees. For example, an estimator for slate recommendation has theoretical conditions under which it is unbiased, alongside demonstrating exponential savings in data requirements relative to general unbiased estimators [10]. This illustrates how specific problem structures can allow for estimators with strong properties regarding bias and data efficiency. In the context of model-based approaches, theoretical analysis of novel loss functions has been provided, supporting their empirical improvements [14].‚Äã  

Finite-sample error bounds provide guarantees on estimator performance with limited data. For hierarchical off-policy learning from bandit feedback, novel Bayesian error bounds, specifically PAC bounds, have been developed for the HierOPO algorithm [13]. These bounds demonstrate improvement with more informative priors and effectively capture statistical gains derived from leveraging hierarchical model structures [13]. This highlights the dependence of bound tightness on factors like the underlying model structure and available prior information.  

A critical consideration for the practical applicability of methods, particularly iterative ones, is the propagation and potential amplification of errors. The iterative error exploitation issue can be theoretically modeled, where policy evaluation error acts as an "auxiliary reward" that can accumulate over iterations, potentially leading to overestimation of Q-values and suboptimal policies [1]. This phenomenon is particularly pronounced when errors across policies are highly correlated [1]. The Q-value estimation with error can be formulated as‚Äã  

# $\textcircled{8}$ Êó†ÊïàÂÖ¨Âºè  

where $\varepsilon _ { \beta }$ ‚Äã represents the error incurred during estimation, and $\widetilde { Q } _ { \beta } ^ { \pi } ( s , a )$ denotes the resulting error in the Q-value estimate [1]. This analysis underscores the importance of robust error control, even when methods offer theoretical guarantees like asymptotic unbiasedness.  

The assumptions required for these theoretical guarantees vary depending on the specific method. Common assumptions often relate to sufficient data coverage, stationarity of the environment or policies, and properties of function approximators if used. The tightness of error bounds depends on factors including the amount of data available (as seen with data savings analysis [10]), the dissimilarity between the behavior and target policies (often impacting variance, not explicitly detailed in provided digests), the complexity of the environment, and the structural assumptions made by the model (e.g., hierarchical structures [13]). Understanding these dependencies is vital for predicting performance in realworld scenarios.‚Äã  

# 6. Off-Policy Evaluation with Function Approximation  

Off-policy evaluation (OPE) in reinforcement learning faces significant challenges when applied to environments with large or continuous state and action spaces. In such scenarios, it becomes infeasible to represent value functions or policies using tabular methods, necessitating the use of function approximation [28]. Function approximation techniques employ parameterized functions, such as linear models, kernel methods, or neural networks, to generalize value or Q-function estimates from observed states and actions to unobserved ones. This approach is crucial for handling the high dimensionality inherent in complex tasks and for enabling OPE in infinite horizon Markov Decision Processes [11].  

Various function approximation methods have been explored for OPE and related off-policy learning tasks. These include linear function approximation using feature construction techniques like polynomials, Fourier basis, and coarse coding [26]. Kernel methods have also been employed, such as learning a kernel metric to capture similarity between state‚Äìaction pairs, enabling improved value function estimation within the offline dataset‚Äôs support, particularly useful for continuous state spaces [28]. Fitted methods, like Fitted Value Iteration or the related Fitted Q-Iteration (FQI), provide a framework for using function approximation in an iterative manner to estimate value or Q-functions from offline data [26]. FQI, for instance, involves repeatedly fitting a Q-function approximator to target values computed using the Bellman equation with data from the offline dataset.‚Äã  

Recent advancements in deep reinforcement learning have significantly influenced OPE through the widespread use of deep neural networks (DNNs) as powerful function approximators [24,26]. DNNs can learn complex, non‚Äêlinear mappings from high‚Äêdimensional state inputs (like images) to value or Q-function outputs. Deep off‚Äêpolicy algorithms like Deep Deterministic Policy Gradient (DDPG), Twin Delayed Deep Deterministic Policy Gradient (TD3), and Soft Actor‚ÄìCritic (SAC) utilize deep networks to approximate actor (policy) and critic (value/Q) functions [2]. While not solely OPE methods, the techniques developed within these algorithms for stable off‚Äêpolicy learning with DNNs are highly relevant to OPE. Using deep neural networks for OPE offers the advantage of handling complex, high‚Äêdimensional state spaces effectively and potentially learning rich representations automatically. However, this comes with significant challenges, primarily instability and overfitting to the finite offline dataset [11,12]. DNNs are prone to divergence or learning unstable value estimates, especially in off‚Äêpolicy settings where the data distribution differs from the evaluation policy‚Äôs distribution. Overfitting can lead to poor generalization performance when estimating the value of the evaluation policy.  

![](images/e2645d7f40da77d7130ba659dd1d4b3edfe93631099c96ba9e22251e9b67d429.jpg)  

Strategies to mitigate these challenges are crucial. Techniques commonly used in deep off‚Äêpolicy reinforcement learning, which are also applicable to OPE with DNNs, include:  

‚Ä¢ Replay Buffers: Storing and reusing experiences from the offline dataset helps decorrelate samples and stabilize training [2].‚Äã   
‚Ä¢ Target Networks: Using delayed, slowly updated copies of the Q-network (e.g., $\phi _ { t a r g }  \rho \phi _ { t a r g } + ( 1 - \rho ) \phi )$ provides stable targets for value updates, reducing oscillations caused by using the same network for both estimation and   
target computation [2]. Target policy networks can also be used [2].   
‚Ä¢ Double Learning: To combat maximization bias‚Äîwhich arises when estimating the value of the greedy action using the same noisy estimator that selected the action‚Äîdouble learning uses two independent value estimators. One estimator selects the action, and the other estimates its value, as seen in the calculation $Q _ { 2 } \left( \arg \operatorname* { m a x } _ { a } Q _ { 1 } ( a ) \right)$ [8]. Clipped Double-Q Learning in TD3 extends this by using two target Q-functions and taking the minimum of their   
estimates to further reduce overestimation: $y ( r , s ^ { \prime } , d ) = r + \gamma ( 1 - d ) \operatorname* { m i n } _ { i = 1 , 2 } Q _ { \phi _ { i , \mathrm { t a r g } } } \big ( s ^ { \prime } , a ^ { \prime } ( s ^ { \prime } ) \big )$ [2].‚Äã   
‚Ä¢ Delayed Updates: Updating the policy (when estimating the value of a policy, e.g., in actor‚Äìcritic settings relevant to policy evaluation) less frequently than the value function can improve stability [2].‚Äã Target Policy Smoothing: Adding noise to the target action in value updates can smooth the value landscape and improve robustness, as used in TD3: $a ^ { \prime } ( s ^ { \prime } ) = \mathrm { c l i p } \Big ( \mu _ { \theta , \mathrm { t a r g } } \big ( s ^ { \prime } \big ) + \mathrm { c l i p } ( \epsilon , - c , c ) , a _ { \mathrm { l o w } } , a _ { \mathrm { h i g h } } \Big ) , \quad \epsilon \sim N ( 0 , \sigma )$ [2].‚Äã   
Beyond directly approximating value functions, representation learning plays a significant role in improving OPE, especially   
in complex domains [9]. By learning low-dimensional state representations or abstractions that preserve information  

relevant for value prediction, the effective state space size can be reduced. This can significantly simplify the sample complexity required for accurate OPE [9]. Methods for learning state abstractions often involve projecting the original state space into a smaller, abstracted space. One approach involves an iterative procedure that learns this projection, potentially adapting state abstractions initially developed for policy learning to the context of OPE [9]. Different representation learning methods can impact the accuracy and efficiency of OPE by determining how effectively relevant information is encoded and irrelevant variations are discarded, thus influencing the generalization capability and stability of the subsequent value or Qfunction approximation. Robustness to hyperparameter choices and evaluation policies in OPE estimators, often relying on function approximation, remains an area of study, with methods like Interpretable Evaluation for Offline Evaluation (IEOE) proposed to assess estimator reliability [31].‚Äã  

# 7. Advanced and Specialized OPE Methods  

<html><body><table><tr><td>Method /Approach</td><td>Focus Area /Technique</td></tr><tr><td>State-based IS Weights</td><td>Reduce variance by weighting based on state probability.</td></tr><tr><td>State Abstraction</td><td>Learn low-dimensional representations to reduce complexity/variance.</td></tr><tr><td>Optimization View</td><td>Formulate OPE as an optimization problem (e.g., Regularized Lagrangian).</td></tr><tr><td>Minimax Approaches</td><td>Learn weights/Q-functions minimizing worst- case error.</td></tr><tr><td>Conditional Importance Sampling</td><td>Reduce variance over long horizons.</td></tr><tr><td>Multi-policy Data OPE</td><td>Leverage data from multiple behavior policies.</td></tr><tr><td>Safe OPE</td><td>Evaluate policies under safety constraints (e.g., via constrained RL).</td></tr><tr><td>Kernel Metric Learning</td><td>Tailored for deterministic policies, continuous spaces.</td></tr><tr><td>Adaptive Importance Sampling</td><td>Control consistency-stability trade-off.</td></tr><tr><td>MLIPS</td><td>Estimate surrogate policy for improved IPS MSE.</td></tr><tr><td>MGPolicy (Recommendations)</td><td>Use meta-graphs to address bias in specific domains.</td></tr><tr><td>HierOPO (Hierarchical)</td><td>Pessimistic approach for hierarchical bandit feedback.</td></tr><tr><td>SOPR</td><td>Simplify OPE to a supervised ranking problem.</td></tr><tr><td>One-step Methods (Offline RL)</td><td>Avoid iterative OPE complexity/errors in offline RL.</td></tr></table></body></html>  

Beyond fundamental importance sampling and model-based techniques, the field of off-policy evaluation (OPE) has seen significant advancements through specialized methods designed to address specific challenges such as variance, bias, efficiency, and safety. A key area of development involves computing importance weights based on states rather than entire trajectories, which can help mitigate the exponential increase in variance with trajectory length [5]. Relatedly, research  

explores learning state abstractions for OPE, including backward-model-irrelevance conditions derived from time-reversed Markov decision processes to achieve irrelevance in importance sampling ratios [9].  

A notable approach transforms the statistical OPE problem into an optimization problem. The method "Off-policy Evaluation via the Regularized Lagrangian" serves as a representative example of this strategy, yielding reliable confidence interval algorithms [11,12]. This transformation allows leveraging optimization techniques to enhance the robustness and provide theoretical guarantees for OPE estimates.‚Äã  

Minimax approaches represent another direction for achieving robust OPE. "Minimax Weight and Q-Function Learning for Off-Policy Evaluation" explores this concept, focusing on learning weights and/or Q-functions under a minimax objective to minimize worst-case estimation error [24]. A minimax approach to weight learning has also been presented, integrating this loss with improvements in off-policy optimization [14].  

A significant challenge in OPE is the "curse of horizon," where the variance of estimates grows exponentially with the time horizon, compromising reliability. Research addresses this by employing techniques such as conditional importance sampling, which aims to reduce variance compared to standard importance sampling methods over long horizons [24].  

The problem of OPE becomes more complex when data is collected from multiple different logging policies. Methods have been developed for optimal off-policy evaluation leveraging data from multiple logging policies, demonstrating how combining data from diverse sources can potentially improve the accuracy and robustness of value estimates [16].  

Ensuring safety in reinforcement learning is paramount, especially in critical applications, leading to the development of safe OPE methods. These methods are crucial for evaluating policies under constraints and providing guarantees on the performance or safety compliance of the target policy before deployment. Research in this area includes enhancing offpolicy constrained reinforcement learning through techniques like adaptive ensemble C estimation and trajectoryconstrained diffusion planning for offline safe reinforcement learning [18]. Furthermore, methods exploring safe reinforcement learning in constrained Markov Decision Processes are directly relevant to safe OPE [24], as are approaches using advantage-based intervention [16]. Off-policy conservative distributional reinforcement learning with safety constraints also falls under this category [18].‚Äã  

Other specialized OPE techniques cater to specific problem structures or objectives. For instance, a kernel metric learning approach has been tailored for deterministic policies and continuous state spaces, potentially outperforming traditional methods by capturing the underlying state-action space structure [28]. Adaptive importance sampling techniques have been introduced to control the trade-off between consistency and stability in value function approximation and policy evaluation, often involving automatic model selection for parameter tuning [4]. Maximum Likelihood Inverse Propensity Scoring (MLIPS) offers a novel approach by estimating a surrogate policy from logged data for use in inverse propensity weighting, aiming to reduce the mean squared error compared to standard IPS [6]. Specialized methods also exist for domains like recommendation systems, such as MGPolicy which uses meta-graphs and counterfactual risk minimization for off-policy learning [20]. For tasks with hierarchical structures and bandit feedback, specialized methods like HierOPO have been developed, employing pessimistic approaches to handle parameter uncertainty [13]. Furthermore, methods that simplify OPE to tasks like supervised off-policy ranking (SOPR) by learning a scoring function represent alternative advanced directions [29]. These advanced techniques demonstrate the field's evolution in developing tailored solutions for complex OPE scenarios, contrasting sometimes with simpler one-step approaches favored in certain offline RL methods over iterative policy regularization techniques like BEAR, BRAC, and CQL that aim to mitigate extrapolation errors [1]. Recent work continues to explore new combinations of OPE problems with traditional statistical approaches [7].‚Äã  

# 8. Addressing Key Challenges  

Off-Policy Evaluation (OPE) in Reinforcement Learning is fundamental for assessing the performance of a target policy using data collected under a different behavior policy [5]. However, this process is fraught with significant challenges and limitations that must be addressed for reliable application, particularly in complex or safety-critical domains [18].  

Key among these challenges is the inherent bias-variance trade-off [11,12]. Estimators based on Importance Sampling (IS) typically suffer from high variance, which can render off-policy methods impractical due to instability [4]. Conversely, modelbased approaches, while potentially reducing variance, are susceptible to significant bias arising from model misspecification [11,12,14]. Doubly Robust (DR) methods are designed to mitigate this trade-off by combining aspects of both IS and model-based estimation [11].  

A critical limitation, especially for trajectory-based IS methods, is the "curse of horizon" [5]. The variance of importance sampling estimates grows exponentially with the length of the evaluated trajectories, making reliable long-horizon OPE challenging [5,24]. This is compounded by sensitivity to the behavior policy [5]. Large discrepancies between the distribution of the behavior policy and the target policy, referred to as distribution shift, significantly impact OPE accuracy [20,22]. Distribution shift reduces the effective sample size and increases estimation variance [1], posing a key challenge, particularly in offline reinforcement learning settings where policies may select out-of-distribution (OOD) actions [3].  

Sufficient exploration of the state-action space is crucial for obtaining data that can adequately cover the support of potential target policies [5,19,21,23]. The lack of sufficient coverage under the behavior policy directly impacts the reliability of OPE, especially in the presence of distribution shift. This challenge is acutely felt in the "cold-start" problem, where limited or no data exists for the target policy [5]. Techniques like behavior cloning and transfer learning are often employed to address this, while hierarchical learning can mitigate cold starts by sharing information across related tasks [13].  

Further challenges arise in complex state and action spaces. OPE in high-dimensional state spaces requires sophisticated function approximation techniques [15,28]. Challenges include distribution shift and limited data coverage when using function approximation [28]. Approaches like using deeply-abstracted states aim to reduce sample complexity in large state spaces [9]. In settings with combinatorial action spaces, such as slate recommendation, OPE must handle the complexity while aiming to reduce bias compared to simpler parametric methods [10].‚Äã  

Model misspecification represents another significant hurdle [15]. When the learned model of the environment is inaccurate, OPE methods that rely on this model can produce biased estimates. Robust methods aim to make the OPE process less sensitive to such inaccuracies [14].‚Äã  

Overestimated Q-functions pose a particular problem, especially in offline learning where exploration is limited [27]. For actions not well-represented in the data, Q-function estimates can be overly optimistic, leading to the selection of actions with poor actual returns [3,27]. This is related to the maximization bias observed in methods like Q-learning [8]. Iterative error exploitation in Q-value estimation can propagate and amplify these errors [1]. Solutions include policy constraint methods (though potentially reducing efficiency) and conservative estimation techniques, applied in both model-based and model-free contexts [18,27]. These conservative methods also contribute to ensuring safety by mitigating risks associated with constraint violations [18].‚Äã  

Beyond theoretical difficulties, practical deployment introduces challenges like the sensitivity of OPE estimators to hyperparameter tuning [31]. Simplifying OPE, such as formulating it as a supervised ranking problem, can help mitigate complexities in real-world applications [29].  

Current OPE methods address these limitations through various techniques. Variance reduction is tackled by methods like MLIPS which uses a surrogate policy [6] and by DR estimators [11]. Regularization, robust methods [14], and function approximation [28] are employed to handle high-dimensional spaces, distribution shift, and model inaccuracies. Conservative approaches [18,27] and policy constraints [27] aim to mitigate Q-value overestimation and enhance safety. Methods like MGPolicy specifically address bias caused by policy discrepancies in domains like recommendations [20]. Some approaches minimize the need for OPE altogether by adopting one-step methods to avoid distribution shift and iterative error [1]. Despite these advancements, the interplay of these challenges necessitates continued research into more robust, efficient, and reliable OPE techniques [5].‚Äã  

# 9. Connections to Related Fields  

![](images/baf965b107ca448d06dd9cda23bcd7ad6aae651be6a76a6a33b501aaa8a80c14.jpg)  

Off-Policy Evaluation (OPE) and the broader field of Off-Policy Reinforcement Learning (RL) exhibit significant connections and distinctions with several related areas, including causal inference, imitation learning, supervised learning, and actorcritic methods. Understanding these relationships provides valuable context for the challenges and techniques employed in OPE and Offline RL [14].  

A key relationship exists between OPE and causal inference. Both fields grapple with estimating outcomes under hypothetical interventions. OPE is fundamentally about counterfactual reasoning, asking "what if" a different policy had been followed given the observed data generated by a behavior policy [5,8]. This aligns closely with causal inference techniques aimed at estimating the effect of different treatments or actions. Specifically, OPE methods often draw parallels to the estimation of the Average Treatment Effect (ATE) in causal inference literature, highlighting the shared statistical challenges in dealing with observational data and confounding factors [7].  

Another closely related field is imitation learning (IL). Both Offline RL and IL leverage static datasets for training, eschewing online environment interaction and exploration [3,26]. However, fundamental differences distinguish them. While IL typically aims to mimic a behavior policy, often assuming the data comes from an optimal expert, Offline RL seeks to learn an optimal or improved policy from potentially sub-optimal historical data [3,6]. This difference in objective is reflected in their methodologies: IL primarily operates within a supervised learning paradigm, whereas Offline RL algorithms are rooted in Off-Policy RL principles and typically involve optimizing variations of the Bellman equation or temporal difference (TD) errors to leverage reward signals [3]. Despite these distinctions, there are overlaps; for instance, behavior cloning‚Äîa core IL technique‚Äîis often considered a baseline or a component (e.g., as a policy improvement operator) in some Offline RL methods, and analyses may relate Offline RL algorithms to policy constraint or regularization methods like BCQ and BRAC which incorporate elements of behavior cloning [1,27]. Some work also explores Inverse Reinforcement Learning (IRL), which infers rewards from observed behavior, further linking the fields [26]. Specific applications, such as Domain Adaptive Imitation Learning, also highlight the ongoing research at the intersection of IL and related areas [24].‚Äã  

It is possible to approach Offline RL without explicitly relying on complex Off-Policy Evaluation for value estimation in certain algorithms. One such direction involves focusing on direct policy updates or simpler learning objectives that might implicitly handle or mitigate the off-policy challenges. For example, some methods analyze constrained and regularized policy updates within frameworks like one-step methods, drawing on techniques from imitation learning and policy constraint literature (e.g., relating to BCQ and BRAC) [1]. This approach may simplify the learning process by focusing on policy improvement operators that stay "close" to the data distribution or employ methods that are less sensitive to out-ofdistribution actions, potentially offering an alternative path compared to methods heavily reliant on accurate off-policy value function estimation, though specific advantages and disadvantages of this alternative are context-dependent and relate to theoretical guarantees and empirical performance trade-offs.‚Äã  

Off-Policy Actor-Critic methods form a significant class of algorithms in continuous control that inherently perform offpolicy learning [2,24]. Algorithms like Deep Deterministic Policy Gradient (DDPG), Twin Delayed Deep Deterministic Policy Gradient (TD3), and Soft Actor-Critic (SAC) operate off-policy by utilizing experience replay buffers to store and sample transitions generated by potentially older versions of the policy. In these methods, the critic network estimates the stateaction value function (Q-function). This Q-function is estimated using off-policy data through TD learning updates, where the target values are computed using the next state and the action selected by the current actor policy, even though the transition was generated by a different behavior policy stored in the buffer [2]. DDPG, for instance, can be viewed as an extension of Deep Q-Learning (DQL) to continuous action spaces [2]. TD3 and SAC introduce improvements over DDPG, such as clipped double Q-learning (TD3) or entropy regularization (SAC), to address issues like overestimation bias and improve exploration, all while maintaining an off-policy learning structure.‚Äã  

Beyond these major connections, OPE and Off-Policy RL also intersect with other research areas. The task of OPE can sometimes be reduced to supervised learning problems, such as Supervised Off-Policy Ranking (SOPR), creating direct links between RL and supervised learning techniques [27,29]. In specific application domains like recommendation systems, OPE research leverages techniques from areas such as meta-learning and counterfactual reasoning [20], and builds on methods from combinatorial bandits for complex action spaces like slate recommendations [10]. Furthermore, connections have been explored with metric learning [28], as well as with concepts in hierarchical reinforcement learning and multi-task learning when considering learning across related tasks or at different levels of abstraction [13]. These diverse connections underscore the interdisciplinary nature of OPE research within the broader machine learning landscape.  

# 10. Evaluation Metrics and Experimental Design  

Evaluating the performance of Off-Policy Evaluation (OPE) estimators is crucial for understanding their reliability and practical utility. This typically involves assessing how accurately an estimator predicts the value or performance of a target policy using data collected under a different behavior policy.  

Common metrics employed for this purpose include Mean Squared Error (MSE) and bias. MSE quantifies the average squared difference between the estimated value (ùëâ)ÃÇand the true value $( \boxtimes )$ ,providingameasureofoveral estimation acuracy,includingbothvarianceandbias: $\bigtriangledown \bigotimes \bigtriangledown \bigotimes = E .$ .Arelatedmetric,Ro tMeanSquaredEror(RMSE),issimplythe squarero tofMSEandisalsousedtoevaluateperformance,forinstance,inthecontextofbatchtrainingforTDmethods [8].Bias,ontheotherhand,measuresthesystematicdeviationoftheestimatorfromthetruevalue,definedasBias $= E -$ ùëâ. While MSE provides a combined measure of error, bias specifically captures whether an estimator systematically overestimates or underestimates the true value, independent of its variance. Understanding both is vital, as estimators may exhibit a trade-off between bias and variance; for example, a biased estimator might sometimes possess lower variance, leading to a smaller MSE in certain scenarios. Other metrics, such as regret, may also be relevant, particularly in contexts like hierarchical off-policy learning, where the focus might be on the cumulative performance difference compared to an optimal approach [13].  

Benchmark datasets play a significant role in standardizing the evaluation of OPE methods, allowing for comparable analysis across different approaches. Datasets like the D4RL benchmark provide collections of offline data from various control tasks, enabling researchers to evaluate OPE methods in a consistent environment [1]. Similarly, the Open Bandit Dataset offers a public resource for evaluating bandit-based OPE estimators on real-world interaction data [31]. The use of such publicly available benchmarks is highly recommended for ensuring reproducibility and facilitating direct comparisons between new methods and existing baselines.  

Beyond standardized benchmarks, empirical evaluation frequently involves the use of synthetic environments or simulations, as well as large-scale real-world datasets to demonstrate practical effectiveness and scalability. Simulations on environments like the swing-up inverted pendulum or mountain car are used to evaluate specific algorithm properties in controlled settings [4]. However, validating OPE estimators on real-world data, such as large-scale ad placement datasets, ecommerce platforms, or general real-world interaction logs, is critical for demonstrating their applicability to practical problems [6,10,20,31]. Recommendations for rigorous experimental design include not only evaluating accuracy metrics like MSE [6] but also assessing robustness, for example, using procedures like the "Interpretable Evaluation for Offline Evaluation (IEOE)" [31]. Comparing performance against state-of-the-art methods on relevant datasets is standard practice [13,20]. Furthermore, analysis may extend beyond just the estimator's accuracy to evaluate the final performance of a policy learned using the offline data [1] or analyzing training dynamics through curves showing performance over time steps or  

episodes [1,8]. Experimental evaluations across different studies often vary in scope, ranging from verifying specific estimator types like DR estimators in controlled settings [11] to extensive empirical evaluations on large real-world datasets [10,20] or standardized offline RL benchmarks comparing different algorithmic approaches [1]. While some studies focus purely on the OPE estimator's performance metrics, others integrate OPE within a larger offline RL framework and evaluate the performance of the resultant learned policy, highlighting the dual role of OPE in both evaluation and facilitating policy improvement.‚Äã  

# 11. Applications of Off-Policy Evaluation  

Off-Policy Evaluation (OPE) is a crucial methodology enabling the assessment of new policies using data collected under different, historical policies. This capability is particularly valuable in domains where online experimentation is costly, risky, or unethical.  

<html><body><table><tr><td>Application Domain</td><td>Specific Examples /Relevance</td></tr><tr><td>Recommender Systems</td><td>Evaluating recommendation strategies from user interaction logs; handling bandit feedback bias.</td></tr><tr><td>Advertising</td><td>Evaluating ad placement policies using impression/click data; maximizing revenue.</td></tr><tr><td>Healthcare</td><td>Evaluating treatment policies from patient data; precision medicine; adaptive learning platforms.</td></tr><tr><td>Robotics</td><td>Evaluating robot control policies from logged operations; autonomous vehicles.</td></tr><tr><td>Game Playing</td><td>Evaluating game Al strategies using gameplay data.</td></tr></table></body></html>  

OPE finds wide-ranging applications across both theoretical research and various industrial sectors [11], facilitating the deployment of more effective decision-making systems.  

A prominent area where OPE is extensively applied is in recommender systems. OPE allows for the evaluation of different recommendation strategies based on historical user interaction logs, such as clicks, purchases, or ratings [5,31]. This is vital for improving long-term user engagement and satisfaction by correcting biases inherent in logged bandit feedback [20]. For instance, OPE techniques are applied as subroutines in tasks like learning-to-rank for slate recommendations, achieving competitive performance [10]. Furthermore, OPE is applicable to complex recommender system models, including those leveraging graph neural networks (GNNs) [20], allowing researchers to evaluate these models' performance on tasks such as identifying users with similar preferences or items with similar affinities using existing datasets [13]. The ability to rigorously evaluate policies offline is essential before deploying potentially suboptimal or harmful recommendation policies in a live setting.‚Äã  

In the advertising industry, OPE plays a key role in evaluating ad placement strategies. Evaluating different advertising policies based on historical impression and click data is critical for maximizing revenue and user engagement without the need for extensive A/B testing, which can be expensive or disruptive. OPE methods, such as MLIPS, have been demonstrated to be effective when applied to large-scale ad placement datasets in real-world scenarios [5,6].  

Healthcare is another domain where OPE is highly relevant due to the high stakes and costs associated with online interactions and interventions [31]. Applications include precision medicine, where OPE can evaluate treatment policies based on historical patient data, and online learning platforms for medical professionals, such as midwives, to personalize content and adaptive learning journeys based on user logs [30,31]. Evaluating such policies using OPE before clinical deployment is paramount for patient safety and ethical considerations. While specific ethical guidelines and regulatory requirements for OPE in healthcare are complex and context-dependent, the fundamental ability of OPE to provide reliable policy evaluations using offline data directly supports responsible innovation by mitigating the risks of deploying unverified policies. The cost and risk associated with taking actions in healthcare make OPE particularly useful [22].  

Robotics also benefits significantly from OPE, particularly in scenarios where physical experimentation is expensive, timeconsuming, or potentially damaging. OPE allows for the evaluation of robot control policies using data collected from previous operations without requiring repeated real-world trials [22]. This is valuable in applications like autonomous vehicle control, such as safety-aware intelligent adaptive cruise control, and micro-robotics applications like parallel droplet control in biochips, both of which have seen RL applications potentially involving OPE [16]. The primary challenge in applying OPE to robotics lies in accurately modeling the complex dynamics and observational noise of physical systems. However, the opportunity lies in using OPE to accelerate policy development and validation by leveraging vast amounts of logged data, thereby reducing the need for costly and risky physical experiments.‚Äã  

While the subsection description highlights finance as an application area requiring consideration of high-frequency data and market volatility, the provided digests do not contain specific information detailing the application of OPE within the finance domain or the specific challenges encountered in this context.‚Äã  

Game playing represents another field where OPE is increasingly utilized, particularly in the context of developing intelligent agents and analyzing gameplay strategies [29]. RL applications in game AI, such as mastering complex games like DouDizhu, have been explored, potentially leveraging OPE techniques for evaluating strategies offline [16]. Algorithms like Monte Carlo methods, which are often fundamental to OPE techniques like Importance Sampling, are well-suited for analyzing strategies in various game types, including both complete and incomplete information games [23]. The application of OPE in game playing enables researchers and developers to evaluate the performance of different game AI policies using historical gameplay data, refining strategies before live deployment.‚Äã  

Beyond these core areas, OPE also finds utility in other domains. For example, OPE can be relevant in transportation logistics, such as evaluating policies for optimizing delivery routes based on historical delivery data [17,30]. Understanding user preferences and demand sensitivity in urban transport, as studied during the COVID-19 pandemic, could also potentially leverage OPE for evaluating policy interventions related to transportation management based on observational data [30]. The versatility of OPE in evaluating policies from offline data makes it applicable to a wide array of sequential decision-making problems encountered in the real world [29].  

# 12. Future Directions and Open Challenges  

Off-policy evaluation (OPE) remains a vibrant and challenging area of research in reinforcement learning, with numerous avenues for future exploration [3,5].  

<html><body><table><tr><td>Area</td><td>Description /Focus</td></tr><tr><td>Robustness & Efficiency</td><td>Improving robustness to hyperparameters, model inaccuracies; increasing sample efficiency.</td></tr><tr><td>Distribution Shift</td><td>Developing better techniques from domain adaptation/causal inference; using policy constraints, uncertainty.</td></tr><tr><td>Variance Reduction</td><td>Creating novel techniques beyond traditional IS/DR; leveraging problem structure.</td></tr><tr><td>Theoretical Understanding</td><td>Deeper statistical analysis of bias-variance; stronger theoretical guarantees.</td></tr><tr><td>Novel Methods</td><td>Exploring optimization-based approaches, simplified formulations (e.g., SOPR).</td></tr><tr><td>Integration</td><td>Combining OPE with Imitation Learning, Transfer Learning, etc.</td></tr><tr><td></td><td></td></tr></table></body></html>  

<html><body><table><tr><td>Complex Settings</td><td>Extending OPE to Multi-agent RL, Continuous Control, Constrained RL, Hierarchical RL.</td></tr><tr><td>Data-Driven RL</td><td>Shifting RL towards leveraging large, static datasets for off-policy optimization.</td></tr><tr><td>Practitioner Support</td><td>Developing methods to help select/tune OPE estimators; ensuring safety (e.g., IEOE).</td></tr></table></body></html>  

A primary focus is the development of methods that are both more robust and efficient, particularly addressing limitations of existing techniques [31]. A key challenge is improving the robustness of OPE estimators to factors such as hyperparameter selection and model inaccuracies [31]. Moreover, addressing the critical issue of distributional shift between the behavior and target policies is paramount [3,22]. Future work should explore techniques from domain adaptation or causal inference to mitigate these differences [22], alongside policy constraint methods and uncertainty estimation [3,27]. Conservative methods, both model-based and model-free, are also being explored to address issues like overestimated Q-functions in offline learning [27]. The surrogate policy technique has been identified as a promising direction, complementary to existing error reduction methods and capable of boosting the performance of widely used approaches [6]. Improving sample efficiency and robustness continues to be a crucial objective [18].‚Äã  

Significant technical challenges persist, including the effective handling of high-dimensional state and action spaces [5]. There is a pressing need for novel variance reduction techniques beyond the conventional methods [5]. Understanding the statistical nature of OPE, especially in general reinforcement learning scenarios, requires further research to better characterize the bias-variance trade-off inherent in most general algorithms [5]. Application-specific structures, such as reducing the number of effective actions, can offer avenues for decreasing variance [5].  

The field is witnessing a shift away from relying solely on traditional statistical viewpoints, which may be reaching their limits in yielding fundamentally new results beyond the classic Importance Sampling, Model-Based, and Doubly Robust methods [12]. Recent years have seen the emergence of new ideas and methods, indicating a move towards optimizationbased approaches and other innovative techniques [11,12]. Exploring more sophisticated policy evaluation techniques within frameworks like the one-step setting, such as Double Q-Learning or Q ensembles, presents a valuable research direction [1]. Furthermore, understanding the specific conditions under which methods like one-step techniques outperform iterative approaches is vital [1].  

Integrating OPE with other reinforcement learning and machine learning techniques is another promising area. This includes exploring combinations with imitation learning and transfer learning [3]. The simplification of OPE to techniques like SOPR by leveraging supervised learning methods suggests a potential path towards enhancing and simplifying OPE for practical applications [29]. Future research should also focus on extending OPE to more complex settings beyond standard formulations. This involves addressing challenges in multi-agent reinforcement learning and continuous control domains [3]. Applying OPE to a wider range of constrained reinforcement learning problems is also an important direction [18]. Exploring OPE in the context of complex hierarchical structures, including developing efficient algorithms for task parameter estimation, offers rich possibilities [13]. Ultimately, the goal is to transition reinforcement learning towards a data-driven learning paradigm, effectively leveraging large and diverse datasets for off-policy optimization, which involves optimizing strategies based on historical data collected by behavior policies [3,5]. Developing methods to help practitioners select and tune OPE estimators for specific applications, while ensuring the avoidance of unsafe choices, is crucial for the practical deployment of OPE techniques [31]. Specific techniques like Random Ensemble Mixture and Monte Carlo dropout are also being explored to address challenges in offline evaluation [27].‚Äã  

# 13. Conclusion  

Off-Policy Evaluation (OPE) stands as a fundamental and increasingly vital area within reinforcement learning, offering the critical ability to assess the performance of a target policy using data collected by a different behavior policy without requiring costly or risky online interactions [5,15,22]. This capability is particularly crucial in real-world applications like recommendation systems [6,10,20], autonomous driving, and robotics [3], where online experimentation is often impractical or unsafe [5,29]. The core challenge in OPE lies in addressing the distribution shift between the behavior and target policies, which can introduce significant bias and variance [3,22].‚Äã  

Historically, OPE research has focused on classical methods such as Importance Sampling (IS), Model-Based approaches, and Doubly Robust (DR) estimators [7,11,12]. Each class presents inherent strengths and weaknesses regarding the biasvariance trade-off [12]. IS methods, while conceptually simple, often suffer from high variance, particularly when the distribution shift is large [4]. Model-based methods rely on accurate environment models, and their performance can degrade significantly under model misspecification [14]. DR estimators combine elements of both IS and model-based approaches, aiming to provide robustness if either the model or the importance weights are accurately estimated [12].  

Recent advancements in OPE have sought to mitigate these limitations and extend applicability to more complex scenarios. To combat the instability of traditional IS, adaptive techniques have been proposed to manage the consistency-stability trade-off [4]. Novel approaches like MLIPS utilize surrogate policies to reduce the Mean Squared Error (MSE) compared to basic IPS [6]. For model-based methods, developing robust model learning techniques, such as those employing minimax loss functions, is crucial to handle distribution shift and model errors [14].  

Beyond refinements of classical methods, novel perspectives have emerged. Simple one-step approaches to offline RL have demonstrated strong performance by explicitly avoiding the complexities and potential iterative error exploitation inherent in multi-step or iterative OPE methods [1]. Domain-specific challenges have spurred innovative solutions, such as metagraph enhanced off-policy learning (MGPolicy) for recommendation systems to address bias issues [20], and practical estimators for slate recommendation exhibiting smaller bias and data efficiency benefits [10]. For large state spaces, state abstraction techniques offer a means to reduce sample complexity by identifying irrelevance conditions [9]. Furthermore, novel reformulations, such as simplifying OPE into a supervised ranking problem, offer alternative routes to tackle realworld application challenges [29]. The connection between OPE and causal inference also provides a rich statistical perspective for developing robust and theoretically grounded methods [7].‚Äã  

Despite significant progress, challenges remain, including the persistent issue of distributional shift, managing the biasvariance trade-off effectively across diverse settings, ensuring stability in the presence of large importance weights, and guaranteeing robustness under model misspecification or function approximation errors. The development of interpretable evaluation procedures, like IEOE, is essential for ensuring the safety and reliability of OPE estimators when deployed in practice [31].  

The current state of OPE research is dynamic, characterized by a surge in the development of diverse methods and a growing understanding of their statistical properties [15]. The field is actively exploring related directions, including optimization-based methods [11] and novel ways to leverage data. Looking forward, the potential for OPE is immense. The shift from traditional "action-learning" to leveraging large, static datasets through data-driven OPE methods represents a promising direction for tackling previously intractable real-world problems [3]. Future research will likely continue to focus on developing more robust, stable, and data-efficient estimators, particularly in complex, large-scale, and non-stationary environments. As OPE methods become more reliable and widely applicable, they will play an increasingly critical role in accelerating the development and deployment of advanced RL systems, fundamentally advancing the field of artificial intelligence.‚Äã  

# References  

[1] One-Step Offline RL: ÁÆÄÂçïËÉú‰∫éÂ§çÊùÇ https://blog.csdn.net/wxc971231/article/details/128742668   
[2] Off-Policy Actor-Critic Methods for Continuous Con https://blog.csdn.net/upr_rom/article/details/126878071   
[3] Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÂõæÈâ¥Ôºö‰ªéÂÖ•Èó®Âà∞‰ºòÂåñÁ≠ñÁï• https://hub.baai.ac.cn/view/18131   
[4] Adaptive Importance Sampling for Stable Off-Policy   
https://www.sciencedirect.com/science/article/abs/pii/S0893608009000045   
[5] Âº∫ÂåñÂ≠¶‰π†ÔºöÂºÇÁ≠ñÁï•ËØÑ‰º∞ÊñπÊ≥ïÂèäÂ∫îÁî®Â±ïÊúõ https://news.sciencenet.cn/htmlpaper/2019/5/20195291582620950362.shtm   
[6] MLIPS: Off-Policy Learning via Surrogate Policy fo https://www.cs.utexas.edu/\~ai-lab/pub-view.php?PubID $\mid =$ 127791   
[7] ÂåóÂ§ßÊï∞Â≠¶Âë®Â≠¶ÊúØÊä•ÂëäÔºöÂº∫ÂåñÂ≠¶‰π†Off-PolicyËØÑ‰º∞ÁöÑÁªüËÆ°ËßÜËßí https://mp.weixin.qq.com/s? _biz=MzUzMzg4MzgxMQ $\scriptstyle = =$ &mid=2247493808&idx $\mathop { : = }$ 1&sn=bb98d3e8cb14f358e92fab676bcc134b&chksm=fa9f967ccde81f6a1   
3e5ed9104194ce3b8a2cb9326c0a11f495cfe522e5255836be1c64f631e&scene=27   
[8] Âº∫ÂåñÂ≠¶‰π†Êó∂Â∫èÂ∑ÆÂàÜÊñπÊ≥ïËØ¶Ëß£ https://blog.csdn.net/sword_csdn/article/details/118144693   
[9] Â§çÊùÇÊï∞ÊçÆÁªüËÆ°Â≠¶‰π†Á†îËÆ®‰ºö (ÈùíÂπ¥ÁªüËÆ°Â≠¶ÂÆ∂Âçè‰ºö2025Âπ¥Âπ¥‰ºö) https://news.sohu.com/a/871615445_455817   
[10] Off-Policy Evaluation for Slate Recommendation https://www.microsoft.com/en-us/research/publication/off-policy  
evaluation-slate-recommendation/   
[11] Off-Policy Evaluation ÈóÆÈ¢ò‰∏éÊñπÊ≥ïÔºöÂåóÂ§ßÁéã‰∫å‰∏úÂçöÂ£´ËÆ∫Âùõ https://dics.pku.edu.cn/xzhd/tlb/129722.htm‚Äã   
[12] Off-Policy Evaluation: ÈóÆÈ¢ò‰∏éÊñπÊ≥ï https://www.math.pku.edu.cn/is/xshd/jz/129722.htm   
[13] Hierarchical Off-Policy Learning from Bandit Feedb http://proceedings.mlr.press/v202/hong23a.html   
[14] Minimax Loss for Robust Off-Policy Model Learning https://paperswithcode.com/paper/minimax-model-learning‚Äã   
[15] Off-Policy Evaluation in Reinforcement Learning: A http://www.nathankallus.com/   
[16] ICML 2021Ôºö163ÁØáÂº∫ÂåñÂ≠¶‰π†È¢ÜÂüüËÆ∫ÊñáÊï¥ÁêÜÊ±áÊÄª https://blog.csdn.net/deeprl/article/details/117678353   
[17] ÁªüËÆ°Â≠¶Â≠¶ÊúØÈÄüÈÄí 6.23 https://cloud.tencent.com/developer/article/1841549‚Äã   
[18] Êó∂Á©∫Êï∞ÊçÆÊåñÊéò„ÄÅÊó∂Èó¥Â∫èÂàóÂàÜÊûê‰∏éÊô∫ËÉΩÂÜ≥Á≠ñÊäÄÊúØÁ†îÁ©∂ÊàêÊûú http://insis.bjtu.edu.cn/kycg.html   
[19] On-Policy vs. Off-Policy Reinforcement Learning: A https://www.baeldung.com/cs/off-policy-vs-on-policy   
[20] SIGIR'22 Êé®ËçêÁ≥ªÁªüÂõæÁΩëÁªúËÆ∫ÊñáËß£ËØª https://cloud.tencent.com/developer/article/2064446‚Äã   
[21] ËíôÁâπÂç°ÁΩóÊéßÂà∂ÔºöÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•‰ºòÂåñ https://mp.weixin.qq.com/s?   
__biz=MzI1MjQ2OTQ3Ng==&mid=2247639289&idx $\mathrel { \mathop : }$ 1&sn=ff082a5004806c9cd37a7ef1b1a3ca3a&chksm $\mid =$ e89783756f7fa342e6   
09e652ca9fd75ff35e48d4ec235e790607fa90b87f3b7d650130cd9f90&scene=27   
[22] ËíôÁâπÂç°Ê¥õOff-PolicyÁ≠ñÁï•ËØÑ‰º∞ https://www.cnblogs.com/wanghongze95/p/13842447.html   
[23] Âº∫ÂåñÂ≠¶‰π†ÔºöËíôÁâπÂç°Ê¥õÊñπÊ≥ï https://cloud.tencent.com/developer/article/1127898   
[24] ICML 2020Ôºö126ÁØáÂº∫ÂåñÂ≠¶‰π†ËÆ∫ÊñáÊ±áÊÄª https://www.zhuanzhi.ai/document/fd590dccfcfbbe080bc1f42000959d1c   
[25] Âº∫ÂåñÂ≠¶‰π†Á¶ªÁ≠ñÁï•ËØÑ‰º∞ÊñπÊ≥ïÂÆûËØÅÁ†îÁ©∂ https://blog.csdn.net/qq_52797432/article/details/138478188   
[26] Âº∫ÂåñÂ≠¶‰π†ËØæÁ®ãÁ¨îËÆ∞ÔºöÁªÑÊàê„ÄÅÂàÜÁ±ª‰∏éÊñπÊ≥ïÊ¶ÇËßà https://blog.csdn.net/weixin_42394913/article/details/141372604‚Äã   
[27] RLChinaÂº∫ÂåñÂ≠¶‰π†Á¨îËÆ∞ÔºöÁêÜËÆ∫„ÄÅÁÆóÊ≥ï‰∏éÂ∫îÁî® https://blog.csdn.net/weixin_43869415/article/details/119725103‚Äã   
[28] Kee-Eung Kim's DBLP Publication List https://dblp.uni-trier.de/pid/35/6703.html‚Äã   
[29] Êô∫ËÉΩÊó†‰∫∫Á≥ªÁªüÂÖ≥ÈîÆÊäÄÊúØ‰∏éÂ∫îÁî®Á†îËÆ®‰ºöÈ¢ÑÂëä https://www.ee.tsinghua.edu.cn/info/1076/3807.htm   
[30] ÁªüËÆ°Â≠¶Â≠¶ÊúØÈÄüÈÄí[7.7]ËÆ∫ÊñáÈõÜËêÉ https://cloud.tencent.com/developer/article/1852701   
[31] ÁªüËÆ°Â≠¶Â≠¶ÊúØÈÄüÈÄí[9.1]ÔºöarXivÁ≤æÈÄâËÆ∫ÊñáÈÄüËßà https://cloud.tencent.com/developer/article/1878276‚Äã   
[32] Êó†Ê≥ïÁîüÊàêÊ†áÈ¢òÔºöÁº∫Â∞ëÊ≠£ÊñáÂÜÖÂÆπ https://dl.acm.org/doi/pdf/10.5555/3648699.3648983  