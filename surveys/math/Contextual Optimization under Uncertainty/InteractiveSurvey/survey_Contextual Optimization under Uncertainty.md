# A Survey of Contextual Optimization under Uncertainty

# 1 Abstract


The field of optimization under uncertainty has seen significant advancements, driven by the increasing complexity and dynamism of real-world systems. This survey paper provides a comprehensive overview of the state-of-the-art techniques and their practical implications in managing uncertainties in decision-making processes, focusing on advanced Monte Carlo techniques, reinforcement learning, and probabilistic forecasting. The main findings include the development of coupled learning frameworks for prediction and optimization, the integration of machine learning with traditional optimization methods, and the advancement of differentially private fair learning techniques. These contributions enhance the robustness and efficiency of optimization algorithms, enabling better risk management and adaptability. By synthesizing insights from various disciplines and applications, this survey aims to serve as a valuable resource for researchers, practitioners, and policymakers in the field of optimization under uncertainty.

# 2 Introduction
The field of optimization under uncertainty has seen significant advancements in recent years, driven by the increasing complexity and dynamism of real-world systems. Traditional optimization methods, which often assume deterministic environments, are increasingly inadequate for addressing the challenges posed by stochastic and uncertain conditions. The integration of advanced probabilistic models, machine learning techniques, and robust optimization frameworks has emerged as a promising approach to enhance decision-making in various domains, including engineering, finance, and logistics. These methods not only improve the accuracy and reliability of optimization solutions but also provide a more comprehensive understanding of the underlying uncertainties, enabling better risk management and adaptability.

This survey paper focuses on contextual optimization under uncertainty, a broad and interdisciplinary area that encompasses a wide range of methodologies and applications [1]. The primary objective is to provide a comprehensive overview of the state-of-the-art techniques and their practical implications in managing uncertainties in decision-making processes. The paper delves into advanced Monte Carlo techniques, reinforcement learning, and probabilistic forecasting, highlighting their roles in enhancing the robustness and efficiency of optimization algorithms. By integrating these approaches, the survey aims to offer a unified perspective on the challenges and opportunities in contextual optimization under uncertainty [1].

The content of this survey is structured to cover a broad spectrum of topics, each of which contributes to the overarching theme of contextual optimization under uncertainty [2]. The paper begins by exploring advanced Monte Carlo techniques for control and optimization, including learning-based approaches for application placement in Mobile Edge Computing (MEC) systems. These techniques leverage machine learning algorithms, such as reinforcement learning and Bayesian optimization, to dynamically adjust to varying conditions and improve resource utilization. The integration of these methods with traditional optimization frameworks, such as Markov Decision Processes (MDPs) and dynamic programming, further enhances their capabilities, ensuring that the system remains robust to unexpected changes [3].

The survey then delves into coupled learning frameworks for prediction and optimization, which represent a significant advancement in addressing the challenges posed by complex, dynamic environments. These frameworks integrate predictive models with prescriptive models, enabling more robust and adaptive decision-making processes. For instance, in control scenarios involving stochastic dynamical systems, the integration of neural feedback policies with stochastic optimization techniques allows for the generation of control sequences that are resilient to disturbances and uncertainties [4]. Additionally, the use of risk-sensitive optimization techniques ensures that the system can adapt to new scenarios while maintaining stability and reliability.

Another key aspect covered in the survey is the development of differentially private fair learning (DPFL) methods, which aim to develop machine learning algorithms that are both fair and privacy-preserving [5]. The core challenge in DPFL is to ensure that the learning process does not disproportionately disadvantage any particular demographic group while also protecting individual data points from being inferred by the model. The survey discusses the theoretical foundations and practical techniques used in DPFL, focusing on the integration of differential privacy (DP) with fairness metrics in the context of empirical risk minimization (ERM). Recent advances in DPFL have focused on developing more efficient and effective algorithms, balancing privacy, fairness, and utility.

The survey also examines the application of Monte Carlo and importance sampling techniques in various domains, such as trajectory optimization and control in autonomous systems, cost-optimal operation, and nonlinear control in high-dimensional state spaces. These techniques leverage the duality between control and inference, allowing for the formulation of optimal decision-making as a policy learning problem in a nonlinear and non-Gaussian state-space [4]. By avoiding Gaussian approximations and heuristic methods, these algorithms aim to provide a more direct and rigorous treatment of the problem, enhancing the robustness and practical applicability of the solutions.

Finally, the survey explores the integration of reinforcement learning and probabilistic forecasting in contextual optimization under uncertainty. This includes deep reinforcement learning for bounded rational agents, hierarchical policy blending for motion planning, and generative probabilistic forecasting with weak innovation. These methods not only enhance the performance of decision-making processes but also provide a more nuanced understanding of how bounded rationality and probabilistic models influence the outcomes [6]. The survey concludes by discussing the contributions of these methods to the broader field of optimization under uncertainty, highlighting their potential to address real-world challenges and improve the robustness and adaptability of decision-making systems.

The contributions of this survey paper are multifaceted. First, it provides a comprehensive and up-to-date overview of the state-of-the-art techniques in contextual optimization under uncertainty, synthesizing insights from various disciplines and applications [2]. Second, it identifies key challenges and open research questions, offering a roadmap for future research in this rapidly evolving field. Third, it highlights the practical implications of these techniques, demonstrating their potential to enhance decision-making in complex and dynamic environments. By bridging the gap between theoretical advancements and practical applications, this survey aims to serve as a valuable resource for researchers, practitioners, and policymakers.

# 3 Advanced Monte Carlo Techniques for Control and Optimization

## 3.1 Learning Based Approaches for Optimization

### 3.1.1 Machine Learning for Application Placement in MEC
Machine Learning (ML) has emerged as a powerful tool for optimizing application placement in Mobile Edge Computing (MEC) systems, addressing the dynamic and resource-constrained nature of these environments [7]. Traditional heuristic and optimization-based approaches often struggle to adapt to the rapid changes in user demands and network conditions, leading to suboptimal resource utilization and increased latency. ML techniques, particularly those rooted in data-driven stochastic optimization, offer a promising alternative by leveraging historical data to predict future states and make informed decisions. These methods can dynamically adjust to varying conditions, thereby improving the efficiency and responsiveness of MEC systems.

One of the key challenges in MEC application placement is balancing the trade-offs between computational load, network bandwidth, and energy consumption. ML algorithms, such as reinforcement learning (RL) and Bayesian optimization, have been successfully applied to this problem. RL, for instance, enables the system to learn optimal placement policies through trial-and-error interactions with the environment, continuously refining its decisions based on feedback. Bayesian optimization, on the other hand, provides a principled approach to exploring the search space efficiently, reducing the number of required evaluations and thus lowering computational overhead. These techniques are particularly effective in handling the uncertainty and variability inherent in MEC systems, where user mobility and application demands can fluctuate rapidly [7].

Moreover, the integration of ML with traditional optimization frameworks, such as Markov Decision Processes (MDPs) and dynamic programming, has further enhanced the capabilities of MEC application placement. By combining the strengths of both approaches, hybrid models can achieve better performance and scalability. For example, MDPs can be used to model the decision-making process, while ML algorithms can provide the necessary predictions and insights to guide the optimization. This integration not only improves the accuracy of the placement decisions but also ensures that the system remains robust to unexpected changes. Additionally, the use of risk-sensitive optimization techniques, which balance exploration and exploitation, ensures that the system can adapt to new scenarios while maintaining stability and reliability.

### 3.1.2 Coupled Learning Frameworks for Prediction and Optimization
Coupled learning frameworks for prediction and optimization represent a significant advancement in addressing the challenges posed by complex, dynamic environments. These frameworks integrate predictive models, which forecast future states or outcomes, with prescriptive models that optimize decisions based on these predictions. By merging these two components, coupled frameworks enable more robust and adaptive decision-making processes. For instance, in control scenarios involving stochastic dynamical systems, the integration of neural feedback policies with stochastic optimization techniques allows for the generation of control sequences that are resilient to disturbances and uncertainties [4]. This is particularly important in nonlinear, non-Gaussian, and bounded domains where traditional methods often fail to provide reliable solutions.

One of the key contributions of coupled learning frameworks is their ability to handle the initial state divergence issue, a common problem in methods like Robust Model-Predictive Path Integral Control (RMPPI) [8]. By incorporating a nominal state solution, these frameworks can maintain stability even when the system experiences catastrophic disturbances. Furthermore, the use of Tube Model-Predictive Path Integral Control (Tube-MPPI) enhances robustness by ensuring that the system remains within a predefined tube around the nominal trajectory, thereby providing a safety margin against unexpected perturbations [8]. This approach is particularly useful in real-world applications where safety and reliability are paramount, such as autonomous vehicle navigation and industrial automation.

Another significant aspect of coupled learning frameworks is their adaptability and scalability. These frameworks can be seamlessly integrated with advanced machine learning techniques, such as deep neural networks and reinforcement learning, to enhance their predictive accuracy and decision-making capabilities. For example, the Neurodynamical Computational Framework (NeuCF) combines Dynamical Neural Fields (DNF) with Stochastic Optimal Control (SOC) to generate adaptive reaching trajectories in neurally plausible ways [9]. This integration not only improves the efficiency of reaching tasks but also ensures that the system can dynamically adjust to multiple competing targets and uncertain states. Overall, coupled learning frameworks offer a promising direction for developing intelligent systems that can operate effectively in complex and dynamic environments.

### 3.1.3 Differentially Private Fair Learning
Differentially Private Fair Learning (DPFL) is an emerging field that aims to develop machine learning algorithms that are both fair and privacy-preserving [5]. The core challenge in DPFL is to ensure that the learning process does not disproportionately disadvantage any particular demographic group while also protecting individual data points from being inferred by the model. This section explores the theoretical foundations and practical techniques used in DPFL, focusing on the integration of differential privacy (DP) with fairness metrics in the context of empirical risk minimization (ERM).

In DPFL, the primary approach involves modifying the standard ERM framework to incorporate fairness constraints and differential privacy. One common method is to add a fairness regularizer to the loss function, which penalizes the model for fairness violations. The fairness metric used can vary, but a popular choice is the Exponential RÃ©nyi Mutual Information (ERMI), which measures the dependence between the model's predictions and sensitive attributes. To ensure differential privacy, noise is added to the gradients or the final model parameters. This noise is calibrated to provide a strong privacy guarantee, typically quantified by the parameters \(\epsilon\) and \(\delta\). The trade-off between privacy and utility is a critical consideration, as increasing the level of privacy often leads to a decrease in model performance.

Recent advances in DPFL have focused on developing more efficient and effective algorithms. For instance, Lowy et al. proposed a novel algorithmic framework that combines DP with fairness constraints in a regularized ERM problem. Their approach leverages a result from the same authors, which shows that the ERMI can be efficiently approximated using a stochastic gradient method. This allows for the simultaneous optimization of accuracy, fairness, and privacy. Additionally, the framework addresses the challenge of training fair models without direct access to sensitive attributes, a common requirement in many practical scenarios [5]. By carefully designing the noise addition process and the fairness regularizer, the proposed method achieves a balance between these competing objectives, making it a promising direction for future research in DPFL.

## 3.2 Monte Carlo and Importance Sampling Techniques

### 3.2.1 Trajectory Optimization and Control in Autonomous Systems
Trajectory optimization and control in autonomous systems is a critical area of research that addresses the challenges of generating efficient and safe paths for robots and vehicles in dynamic environments [10]. This section delves into the methodologies and algorithms that enable autonomous systems to navigate complex and uncertain terrains. The primary focus is on stochastic optimal control, which provides a robust framework for decision-making under uncertainty [4]. Stochastic optimal control methods can be broadly categorized into analytical and data-driven approaches [4]. Analytical methods rely on precise modeling of the system dynamics and constraints, often using local approximations to solve the optimization problem. These methods are well-suited for systems where the dynamics are well-understood and can be accurately modeled, such as in chemical plant control and some robotic applications.

Data-driven methods, on the other hand, leverage machine learning and statistical techniques to learn the optimal control policies from data. These methods are particularly useful in scenarios where the system dynamics are complex, nonlinear, or partially known. Recent advancements in Markov chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) techniques have significantly enhanced the ability to handle high-dimensional and stochastic systems. The integration of these methods within an inference framework allows for the derivation of optimal control policies that can adapt to changing conditions and uncertainties in real-time. For instance, the control-as-inference paradigm has been successfully applied to learn neural feedback policies in control scenarios, demonstrating the potential of these methods in generating robust and adaptive control strategies [4].

However, the performance of these control methods is often contingent on the quality of the importance sampling (IS) trajectory, which can be sensitive to disturbances and initial conditions [10]. To address this, techniques such as Tube Model-Predictive Path Integral Control (Tube-MPPI) and Robust Model-Predictive Path Integral Control (RMPPI) have been developed [8]. Tube-MPPI incorporates a nominal state solution that is resilient to disturbances, while RMPPI introduces a theoretical performance guarantee by bounding the free energy growth, which measures the system's proximity to task failure [8]. These advancements not only improve the robustness of the control policies but also provide a clearer understanding of the system's dynamic limits. Despite these improvements, challenges remain in integrating optimal control principles for generating efficient trajectories in the presence of multiple competing targets and uncertain states, highlighting the need for further research in this area [9].

### 3.2.2 Stochastic Optimal Control for Cost-Optimal Operation
Stochastic optimal control (SOC) is a powerful framework for managing operations under uncertainty, particularly in dynamic environments where future states are not known with certainty. In the context of cost-optimal operation, SOC methods aim to find control policies that minimize expected costs over a planning horizon, accounting for the probabilistic nature of system dynamics and disturbances. The key challenge in SOC is to balance exploration and exploitation, ensuring that the control policy is robust to uncertainties while also being efficient in terms of computational resources.

One prominent approach in SOC is the use of model predictive control (MPC) combined with stochastic optimization techniques. Methods such as Tube-MPPI and RMPPI have been developed to address the limitations of traditional MPC in handling severe disturbances and maintaining nominal state stability. Tube-MPPI introduces a nominal trajectory that is designed to be resilient to disturbances, while RMPPI incorporates robustness through information-theoretic principles, providing a bound on the free energy growth that measures the system's proximity to failure. These methods enhance the reliability of control policies by ensuring that the system remains within a safe operating region, even under uncertain conditions.

To further improve the efficiency and adaptability of SOC, recent research has explored the integration of machine learning techniques, particularly in the context of data-driven stochastic optimization. Neurodynamical Computational Frameworks (NeuCF) and other bio-inspired methods have been proposed to generate adaptive control policies that can handle multiple objectives and uncertain states in real-time [9]. These approaches leverage neural models to simulate motor and sensory feedback, enabling the system to learn and adapt its control policies based on environmental changes. By combining the strengths of analytical modeling and data-driven methods, these hybrid approaches offer a promising direction for achieving cost-optimal operation in complex and dynamic systems.

### 3.2.3 Nonlinear Monte Carlo Algorithms for High-Dimensional State Spaces
Nonlinear Monte Carlo algorithms represent a significant advancement in addressing the challenges posed by high-dimensional state spaces in stochastic nonlinear control. These algorithms leverage the duality between control and inference, allowing for the formulation of optimal decision-making as a policy learning problem in a nonlinear and non-Gaussian state-space [4]. By avoiding Gaussian approximations and heuristic methods, these algorithms aim to provide a more direct and rigorous treatment of the problem. The core idea is to use advanced Monte Carlo techniques, such as Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC), to sample from the posterior distribution of states and actions, thereby enabling the exploration of complex, high-dimensional spaces without incurring significant bias.

One of the key contributions of this approach is the derivation and empirical validation of bounds on free energy growth for algorithms like Reproducing Model Predictive Path Integral Control (RMPPI). This free energy bound serves as a measure of the system's dynamic limit, indicating how close the system is to task failure. By providing performance guarantees both in simulation and on real-world platforms, such as the AutoRally system, these algorithms demonstrate their robustness and practical applicability. However, the performance of these controllers is often contingent on the choice of the Importance Sampling (IS) trajectory, which can become invalid if the system state is perturbed significantly. To mitigate this issue, techniques like Tube Model-Predictive Path Integral Control (Tube-MPPI) have been developed, offering a more resilient approach by maintaining a tube around the nominal trajectory [8].

In high-dimensional state spaces, traditional methods such as belief propagation and iterated smoothing struggle, particularly in nonlinear, non-Gaussian, and bounded domains [4]. Nonlinear Monte Carlo algorithms overcome these limitations by dynamically adapting to the system's state and uncertainties. The Neurodynamical Computational Framework (NeuCF), for instance, integrates Dynamic Neural Fields (DNF) and Stochastic Optimal Control (SOC) to generate adaptive reaching trajectories, incorporating action selection, stopping, and switching mechanisms [9]. This framework is particularly useful in scenarios requiring real-time decision-making and efficient handling of multiple competing targets and uncertain states. Despite the computational complexity associated with training such models, the integration of advanced Monte Carlo techniques ensures that these algorithms remain feasible and effective for modern robotic and control applications.

## 3.3 Reinforcement Learning and Probabilistic Forecasting

### 3.3.1 Deep Reinforcement Learning for Bounded Rational Agents
Deep reinforcement learning (DRL) has emerged as a powerful framework for addressing complex decision-making problems, particularly in environments where agents must operate under uncertainty and limited information [11]. In the context of bounded rationality, DRL techniques are particularly relevant as they can model agents that make decisions based on a balance between the expected utility and the computational resources required to achieve it. By incorporating the principles of bounded rationality, DRL algorithms can simulate more realistic agent behaviors, accounting for the cognitive and computational constraints that are inherent in real-world decision-making processes.

The integration of DRL with models of bounded rationality involves the development of algorithms that can learn optimal policies while considering the limitations of the agent [12]. This is achieved by formulating the learning problem as a trade-off between the expected reward and the computational cost of decision-making. For instance, in stochastic control problems, the agent must navigate a complex state space where the optimal action is not always clear, and the environment may be partially observable. By using techniques such as Monte Carlo simulation and Markov chain Monte Carlo (MCMC), DRL algorithms can explore the state space more effectively, learning policies that are robust to uncertainty and efficient in terms of computational resources.

Moreover, the application of DRL to bounded rational agents often involves the use of advanced sampling methods, such as importance sampling, to efficiently approximate the optimal policy. These methods help in reducing the variance of the policy updates, leading to more stable and faster convergence. Additionally, the incorporation of neural networks allows for the representation of complex, nonlinear relationships between the state and action spaces, enabling the agent to adapt to dynamic environments and learn sophisticated strategies. The combination of these techniques not only enhances the performance of DRL algorithms but also provides a more nuanced understanding of how bounded rationality influences decision-making in complex systems.

### 3.3.2 Hierarchical Policy Blending for Motion Planning
Hierarchical Policy Blending for Motion Planning (HPBMP) integrates the strengths of both online motion planning and reactive motion generation through a structured, multi-level decision-making framework [13]. At its core, HPBMP leverages the concept of planning as inference, where the optimal policy is derived by treating the motion planning problem as a probabilistic inference task. This approach allows for the seamless integration of prior knowledge and real-time sensory data, enabling the robot to adapt its motion plans dynamically in response to environmental changes. The hierarchical structure of HPBMP is designed to handle the complexity of high-dimensional state spaces and the computational demands of real-time decision-making, making it particularly suitable for applications in dynamic and uncertain environments.

On the lower level of the hierarchy, Riemannian Motion Policies (RMPs) are employed to generate reactive, low-level control commands that ensure immediate responsiveness to environmental perturbations. RMPs operate at a high frequency, providing a robust and stable foundation for the higher-level planning algorithms. These policies are myopic, focusing on short-term goals and local constraints, which makes them computationally efficient and capable of handling rapid changes in the environment. The higher level of the hierarchy, on the other hand, utilizes sample-based online planning methods to generate long-term, globally optimal trajectories. These methods, such as Model Predictive Path Integral Control (MPPI), use Monte Carlo simulations to explore a wide range of possible future states and actions, thereby ensuring that the robot's motion plans are both feasible and optimal over a longer horizon [10].

The key innovation of HPBMP lies in the way it blends these two levels of control through a principled policy blending mechanism. By formulating the blending process as an inference problem, HPBMP can dynamically adjust the influence of each policy based on the current state of the environment and the robot's objectives. This adaptive blending ensures that the robot can seamlessly transition between reactive and planned behaviors, depending on the situation. Empirical evaluations have demonstrated that HPBMP achieves a balance between reactivity and optimality, enabling robots to perform complex tasks in dynamic environments with high efficiency and reliability. The hierarchical structure also facilitates modular design and implementation, making it easier to integrate new components and adapt to different application domains.

### 3.3.3 Generative Probabilistic Forecasting with Weak Innovation
Generative Probabilistic Forecasting with Weak Innovation (GPF-WI) represents a significant advancement in the field of time series forecasting, leveraging the concept of weak innovation to address the limitations of traditional generative models [14]. Unlike conventional approaches that rely on strong innovation representations, which assume the existence of a uniform i.i.d. innovation process, GPF-WI extends the class of time series for which such a representation is valid. By utilizing the weak innovation representation, GPF-WI can effectively handle a broader range of time series, including those with complex dependencies and non-stationary characteristics.

The core of GPF-WI lies in its ability to transform the problem of generating future time series samples into a probabilistic framework. Specifically, it employs a deep learning technique based on the Weak Innovation Autoencoder (WIAE), which maps the observed time series data into a latent space where the weak innovation representation holds [14]. This transformation enables the model to generate future samples according to the conditional probability distribution derived from past realizations. The WIAE framework ensures that the latent process forms Bayesian sufficient statistics, preserving the essential information for generative probabilistic forecasting without loss of optimality [14].

By adopting the weak innovation representation, GPF-WI not only broadens the applicability of generative models but also enhances their robustness and flexibility. This approach is particularly advantageous in scenarios where the underlying data-generating process is unknown or highly complex, as it does not require explicit knowledge of the transition kernel. Instead, GPF-WI relies on independent realizations of one-step transitions, making it a model-free method that can adapt to a wide variety of time series data. Empirical evaluations have demonstrated the effectiveness of GPF-WI in producing accurate and reliable forecasts, even in high-dimensional and noisy environments, thus opening new avenues for real-time and data-driven decision-making applications.

# 4 Contextual and Distributionally Robust Optimization

## 4.1 Risk-Averse Decision Making

### 4.1.1 Stochastic Programming with Nested Risk Measures
Stochastic programming with nested risk measures represents a sophisticated approach to handling uncertainties in decision-making processes, particularly in contexts where the outcomes of decisions are influenced by multiple layers of risk. This section delves into the intricacies of nested risk measures, focusing on their integration into stochastic programming frameworks. Nested risk measures are essential in capturing the hierarchical nature of risks, where the risk associated with a decision is itself subject to further risk. For instance, in financial portfolio optimization, the risk of a portfolio might be measured using Conditional Value-at-Risk (CVaR), which itself is a function of the underlying asset returns' distribution [15]. This nested structure allows for a more nuanced representation of risk, providing a richer and more accurate model of the decision-making environment.

The primary challenge in stochastic programming with nested risk measures lies in the computational complexity and the need for robust estimation techniques. Traditional methods often struggle to handle the nested nature of these risks, leading to suboptimal solutions or intractable formulations. To address these issues, recent advancements have introduced methods such as the L-shaped method and scenario decomposition techniques, which break down the problem into more manageable subproblems. These methods leverage the structure of the nested risk measures to iteratively refine the solution, ensuring that the risk at each level is accurately captured. Additionally, the use of machine learning techniques, particularly in the context of contextual stochastic optimization (CSO), has shown promise in dynamically predicting and managing these nested risks. By integrating predictive models with optimization algorithms, CSO frameworks can adapt to changing risk profiles, making them highly suitable for real-world applications where uncertainties are dynamic and multifaceted.

Another critical aspect of stochastic programming with nested risk measures is the theoretical foundation that supports the use of these measures. Theoretical developments, such as the application of Jensen's inequality and the derivation of asymptotic properties, provide a rigorous basis for the validity and reliability of nested risk measures in optimization models. These theoretical insights ensure that the solutions obtained are not only computationally feasible but also statistically sound. Furthermore, the introduction of decision-dependent noise and the use of ambiguity sets in robust optimization frameworks enhance the robustness of the solutions, making them resilient to model misspecification and data uncertainty. Overall, the integration of nested risk measures into stochastic programming represents a significant advancement in the field, offering a powerful tool for managing complex and dynamic risk environments.

### 4.1.2 Collective Wisdom Policy Averaging
Collective Wisdom Policy Averaging (CWPA) is a method that leverages the collective intelligence of multiple policies to enhance decision-making robustness and accuracy. The core idea is to aggregate the outputs of various policies, each of which may have been trained on different data subsets or with different initial conditions, to produce a more reliable and generalized decision rule. This approach is particularly useful in contexts where individual policies may suffer from overfitting or where the data distribution is highly variable. By combining the strengths of multiple policies, CWPA can mitigate the risks associated with relying on a single, potentially biased or suboptimal policy.

In CWPA, the aggregation mechanism can take several forms, such as simple averaging, weighted averaging, or more sophisticated ensemble methods. Simple averaging involves computing the mean of the outputs from all policies, which is straightforward and computationally efficient. Weighted averaging, on the other hand, assigns different weights to each policy based on their performance or reliability, allowing for a more nuanced combination that can better reflect the relative strengths of each policy. More advanced ensemble methods, such as stacking or boosting, can further enhance the robustness of the aggregated policy by learning how to optimally combine the outputs of individual policies. These methods often involve training a meta-policy that learns to weight or select the best policy for a given context, thereby improving the overall performance of the system.

The effectiveness of CWPA is underpinned by its ability to reduce variance and improve generalization. By averaging the outputs of multiple policies, the method can smooth out the idiosyncrasies and noise present in individual policies, leading to more stable and reliable decisions. This is particularly beneficial in high-stakes applications where the cost of making a suboptimal decision is high. Additionally, CWPA can help in handling distributional shifts and non-stationary environments, as the aggregated policy is less likely to be affected by changes in the data distribution compared to a single policy. Overall, CWPA provides a principled and flexible framework for enhancing decision-making in complex and uncertain environments.

### 4.1.3 Distributionally Adversarial Stochastic Gradient Descent
Distributionally Adversarial Stochastic Gradient Descent (DA-SGD) represents a significant advancement in the realm of stochastic optimization, particularly in contexts where distributional ambiguity poses substantial challenges. Unlike traditional Stochastic Gradient Descent (SGD) methods, which rely on a fixed, often assumed, distribution of data, DA-SGD incorporates an adversarial component to account for potential shifts in the data distribution. This approach is particularly useful in dynamic environments where the underlying data-generating process can evolve over time, such as in financial markets or e-commerce platforms. By perturbing the samples used in each gradient update, DA-SGD generates a more robust decision policy that is less sensitive to distributional changes.

The core mechanism of DA-SGD involves the iterative perturbation of the training data to simulate a worst-case distribution within a specified Wasserstein ball around the empirical distribution. This perturbation is achieved by adding controlled noise to the samples, effectively creating an augmented dataset that is more adversarial to the current decision policy. The gradient update step then uses this augmented dataset to adjust the decision variables, ensuring that the policy remains effective even under adverse conditions. This process not only enhances the robustness of the optimization but also provides valuable insights into the sensitivity of the decision policy to various covariates and uncertain parameters. The adversarial perturbations help in identifying the most critical sources of uncertainty, allowing decision-makers to focus on mitigating the risks associated with these factors.

Empirically, DA-SGD has demonstrated superior performance in handling distributional shifts compared to traditional SGD methods. The algorithm's ability to adapt to changing environments makes it particularly suitable for real-world applications where data distributions are inherently unstable. Furthermore, the computational efficiency of DA-SGD, achieved through the use of mini-batch gradient updates and incremental perturbations, ensures that it remains scalable and practical for large-scale optimization problems [1]. The integration of adversarial perturbations into the SGD framework not only improves the robustness of the solutions but also provides a principled way to quantify and manage the uncertainty inherent in the optimization process.

## 4.2 Distributional Constraint Learning and Robust Optimization

### 4.2.1 Neural Network Based Distributional Constraint Learning
Neural Network Based Distributional Constraint Learning (DCL) represents a significant advancement in the integration of machine learning and stochastic optimization, particularly in the context of distributionally robust optimization (DRO) [16]. This approach leverages the expressive power of neural networks to estimate conditional distributions of uncertain parameters, which are then embedded as constraints within a mixed-integer linear program (MILP) [16]. By doing so, DCL enables the optimization model to account for the statistical uncertainty inherent in the data, thereby enhancing the robustness and reliability of the decisions made. The neural network architecture used in DCL is designed to capture complex dependencies and nonlinearities in the data, making it particularly suitable for problems with high-dimensional and heterogeneous data sources.

The key innovation in DCL lies in its ability to generate scenario realizations from the estimated conditional distributions, which are then used to construct distributional constraints within the optimization model. This process involves training a neural network to predict the conditional distribution of the uncertain parameters given the available contextual information. The predicted distributions are then sampled to generate scenarios that are used to formulate the constraints. This approach not only maintains the predictive accuracy of the neural network but also ensures that the optimization model remains tractable and computationally efficient. The use of neural networks in this context allows for the incorporation of a wide range of distributional assumptions, from simple parametric forms to more complex nonparametric distributions, thus providing a flexible framework for handling various types of uncertainty.

Empirical studies have demonstrated the effectiveness of DCL in various applications, including e-commerce fulfillment, portfolio optimization, and risk management. In these applications, DCL has been shown to outperform traditional methods that rely on point predictions or fixed distributional assumptions. The ability of DCL to adapt to changing distributions and to incorporate new data in real-time makes it particularly valuable in dynamic environments where the underlying uncertainties are subject to frequent changes. Furthermore, the end-to-end nature of DCL, which integrates the learning and optimization phases, ensures that the model is optimized for the specific decision-making task at hand, leading to improved performance and more actionable insights.

### 4.2.2 Distributionally Robust Optimization with Side Information
Distributionally Robust Optimization (DRO) with side information represents a sophisticated extension of traditional DRO frameworks, where the inclusion of additional data (side information) enhances the decision-making process by reducing uncertainty [17]. In this context, side information, often in the form of covariates or features, is leveraged to refine the ambiguity set of possible distributions, leading to more informed and robust decisions. The primary challenge lies in effectively integrating this side information into the optimization model without compromising computational tractability. This is achieved through the development of specialized algorithms and methodologies that can dynamically adjust the ambiguity set based on the observed side information, thereby providing a more tailored and responsive approach to decision-making under uncertainty.

The integration of side information in DRO is particularly valuable in settings where the underlying distribution of uncertainties is not fully known or is subject to change over time. By incorporating side information, the optimization model can adapt to new data, improving its robustness and relevance. For instance, in financial portfolio optimization, market indicators such as economic indices or news sentiment can serve as side information, helping to better predict asset returns and manage risk. Similarly, in supply chain management, real-time data on consumer behavior and environmental conditions can be used to adjust inventory levels and logistics planning. The key to effective integration is the development of a contextual distribution oracle, which uses machine learning techniques to predict the distribution of uncertainties conditioned on the side information. This oracle is then embedded into the DRO framework, enabling the optimization model to make decisions that are robust to a range of plausible future scenarios.

Recent advances in this area have focused on developing efficient algorithms for solving DRO problems with side information, such as the Contextual Sample Average Approximation (C-SAA) and Contextual Robust Optimization (C-RO) methods. These methods leverage the structure of the side information to construct more accurate and computationally feasible ambiguity sets. For example, C-SAA uses historical data to calibrate the ambiguity set, while C-RO incorporates machine learning models to predict the distribution of uncertainties. Both approaches aim to balance the trade-off between robustness and computational efficiency, ensuring that the optimization model remains practical for real-world applications. Additionally, the use of statistical inference techniques, such as the construction of confidence intervals for the estimated distributions, further enhances the reliability and interpretability of the DRO solutions.

### 4.2.3 Decision-Dependent Stochastic Optimization
Decision-Dependent Stochastic Optimization (DDSO) represents a significant advancement in the field of stochastic optimization, particularly in scenarios where the decisions made by the decision-maker directly influence the underlying probability distributions of the uncertain parameters. Unlike traditional stochastic optimization models, which assume static and exogenously determined distributions, DDSO accounts for the dynamic nature of these distributions, thereby providing a more realistic and flexible framework for decision-making under uncertainty. This section delves into the theoretical foundations and practical implications of DDSO, highlighting its unique characteristics and the challenges it addresses.

In DDSO, the decision variables not only optimize the objective function but also alter the distribution of the random variables, leading to a closed-loop system where decisions and uncertainties are interdependent. This interdependence introduces a layer of complexity, as the optimization problem must now account for the feedback loop between decisions and the evolving distribution of uncertainties. To address this, DDSO employs advanced techniques such as dynamic programming, reinforcement learning, and adaptive sampling methods. These techniques enable the model to learn and adapt to the changing environment, making it particularly suitable for applications in dynamic and uncertain settings such as financial markets, supply chain management, and energy systems.

The practical implementation of DDSO involves several key components, including the formulation of the decision-dependent distribution, the development of efficient algorithms to solve the resulting optimization problems, and the validation of the model through empirical studies. One common approach is to use a Contextual Sample Average Approximation (CSAA) method, which combines historical data with machine learning techniques to predict the decision-dependent distributions. This approach not only enhances the accuracy of the predictions but also improves the robustness of the decisions. Additionally, the asymptotic properties of the estimators used in DDSO are crucial for ensuring the reliability and consistency of the solutions. Empirical evidence from various case studies demonstrates the effectiveness of DDSO in improving decision-making processes and outcomes in complex, real-world scenarios.

## 4.3 End-to-End Learning and Statistical Inference

### 4.3.1 End-to-End Learning for Conditional Robust Optimization
End-to-End Learning for Conditional Robust Optimization (CRO) represents a paradigm shift from traditional two-step methods, where prediction and optimization are decoupled. In this framework, the prediction of uncertain parameters and the subsequent optimization are integrated into a single, cohesive process. This integration is crucial because it allows the optimization to directly account for the prediction errors, thereby mitigating their impact on the final decision. By constructing contextual uncertainty sets that are tailored to the downstream optimization task, end-to-end learning ensures that the solutions are robust to the inherent uncertainties in the data [18]. This approach leverages advanced machine learning techniques, such as deep neural networks, to dynamically adjust the uncertainty sets based on the observed data, leading to more adaptive and reliable optimization outcomes.

The key innovation in end-to-end learning for CRO lies in the development of a joint loss function that simultaneously optimizes the conditional coverage of the uncertainty sets and the CRO objective [18]. This dual-objective formulation ensures that the learned uncertainty sets not only cover the true parameter values with high probability but also minimize the worst-case cost associated with the optimization problem. The joint loss function is designed to balance the trade-off between the accuracy of the uncertainty sets and the robustness of the optimization solution. Empirical evaluations on both synthetic and real-world datasets, such as portfolio optimization using US stock market data, have demonstrated that this approach outperforms traditional methods in terms of both risk exposure and conditional coverage. These results highlight the practical utility of end-to-end learning in handling complex, dynamic environments where uncertainties are prevalent.

To implement end-to-end learning for CRO, the framework employs a novel training algorithm that iteratively refines the uncertainty sets and the optimization solutions. The algorithm uses gradient-based methods to update the parameters of the machine learning model, ensuring that the uncertainty sets are continuously adapted to the evolving data. This iterative process is critical for capturing the non-stationary nature of real-world problems, where the distribution of uncertainties can change over time. Additionally, the framework provides theoretical guarantees on the convergence and optimality of the solutions, even in non-convex settings. These guarantees are essential for building trust in the robustness and reliability of the optimization outcomes, making end-to-end learning a promising direction for advancing the field of conditional robust optimization [18].

### 4.3.2 Doubly Robust Estimators for Counterfactual Regression
Doubly robust estimators for counterfactual regression are a critical component in addressing the challenges posed by observational data in decision-making processes [19]. These estimators combine the strengths of both outcome regression and inverse probability weighting (IPW) to provide consistent and efficient estimates of counterfactual outcomes. By leveraging the semiparametric theory, doubly robust estimators can achieve consistency even if one of the models (either the outcome regression or the propensity score model) is misspecified. This robustness is particularly valuable in high-stakes applications where data quality and model assumptions are often uncertain.

The key innovation in doubly robust estimators lies in their ability to correct for the biases introduced by missing counterfactual data. In the context of omnichannel multi-courier order fulfillment optimization, where only the delivery times of chosen fulfillment options are observed, these estimators can provide reliable estimates of the counterfactual delivery times for unchosen options. This is achieved by first estimating the propensity scores, which represent the probability of choosing a particular fulfillment option given the observed covariates, and then using these scores to weight the observed outcomes. The outcome regression model is then used to predict the counterfactual outcomes, and the weighted outcomes are combined to produce a doubly robust estimate. This approach not only enhances the accuracy of the predictions but also provides a principled way to quantify the uncertainty associated with these estimates.

Furthermore, the flexibility of doubly robust estimators allows them to accommodate a wide range of risk functions and constraints, making them suitable for various optimization tasks. For instance, in settings where fairness is a concern, these estimators can be adapted to ensure that the counterfactual predictions are equitable across different groups. Additionally, the use of local C2-smoothness regularity conditions ensures that the estimators remain well-behaved and computationally tractable, even in complex, high-dimensional settings. This combination of robustness, flexibility, and computational efficiency makes doubly robust estimators a powerful tool for counterfactual regression in observational data analysis [19].

### 4.3.3 Certainty Equivalence Control in Multi-Stage Convex Optimization
Certainty Equivalence Control (CEC) is a fundamental principle in the realm of multi-stage convex optimization, particularly when dealing with uncertainties that are dynamically evolving over time [20]. The CEC approach simplifies the optimization problem by replacing uncertain parameters with their nominal values, typically the expected values, and solving the resulting deterministic problem. This method is particularly useful in settings where the uncertainties are not fully known but can be estimated from historical data or through predictive models. By treating these estimates as if they were the true values, CEC facilitates the transformation of a complex stochastic optimization problem into a more tractable deterministic convex program [20].

The application of CEC in multi-stage convex optimization involves a two-step relaxation process [20]. In the first step, the uncertain parameters are replaced with their nominal values, effectively converting the stochastic program into a deterministic one. This step leverages the inherent convexity of the problem, ensuring that the relaxed problem remains convex and thus amenable to efficient solution methods. The second step involves solving this deterministic convex program to obtain an initial solution, which serves as a baseline for further refinement. Despite its simplicity, CEC can provide a good approximation of the optimal solution, especially when the uncertainties are relatively small or the system is robust to variations in the uncertain parameters.

However, the effectiveness of CEC in multi-stage convex optimization is contingent upon the accuracy of the nominal values used in the relaxation process. In practice, these values are often estimated from limited data, which can introduce significant errors if not properly managed. To mitigate this issue, advanced techniques such as re-solving and update policies are employed. These methods involve periodically re-estimating the uncertain parameters and re-solving the optimization problem to refine the solution. This iterative process helps to adapt the solution to changing conditions and improve its robustness over time. Additionally, the integration of machine learning techniques can enhance the accuracy of the nominal values, thereby improving the overall performance of the CEC approach in multi-stage convex optimization.

# 5 Stochastic and Robust Optimization in Engineering Systems

## 5.1 Robust Optimization in HVAC and Energy Systems

### 5.1.1 Distributionally Robust Optimization for HVAC Scheduling
Distributionally Robust Optimization (DRO) for HVAC scheduling addresses the inherent uncertainties in building environments, such as ambient temperature fluctuations, occupancy patterns, and equipment performance [21]. By incorporating the probability distribution uncertainty through a DRO formulation with the Wasserstein metric, the method enhances the robustness of the optimal solution [21]. The Wasserstein metric-based ambiguity set allows for the identification of the worst-case probability distribution within a predefined range, without making strong assumptions about the underlying distribution [22]. This approach is particularly advantageous in HVAC systems, where the dynamic nature of building environments can significantly impact energy consumption and comfort levels.

The DRO formulation for HVAC scheduling involves defining an ambiguity set of possible probability distributions for the uncertain parameters, such as ambient temperature and occupancy. The optimization problem is then formulated to minimize the worst-case expected cost over this ambiguity set. This ensures that the scheduling solution is not only optimal under a specific scenario but also robust against a range of plausible future conditions. The use of the Wasserstein metric in defining the ambiguity set provides a principled way to balance between robustness and conservatism, as it allows for a trade-off between the size of the ambiguity set and the level of protection against distributional shifts.

In practice, the DRO approach for HVAC scheduling can be implemented using a two-stage optimization framework. The first stage involves determining the optimal control actions, such as setting the HVAC system's setpoints, based on the current and forecasted conditions. The second stage involves adjusting these control actions in response to real-time feedback and updated forecasts. This adaptive approach ensures that the system remains robust to uncertainties while maintaining operational efficiency and comfort. The DRO formulation also facilitates the integration of renewable energy sources and demand response programs, further enhancing the system's flexibility and sustainability.

### 5.1.2 Stochastic Optimization for Variable Renewable Resource Generators
Stochastic optimization plays a pivotal role in managing the inherent uncertainties associated with Variable Renewable Resource Generators (VRRGs), such as wind farms and solar photovoltaic plants [23]. These generators are characterized by their reliance on natural resources, which are inherently unpredictable and subject to significant variability. The stochastic nature of VRRGs poses significant challenges for grid operators and energy market participants, who must commit to energy production levels well in advance, often a day before actual delivery. This commitment must be made under conditions of high uncertainty, where the exact amount of energy that will be generated is not known. To address this, stochastic optimization models are employed to incorporate probabilistic forecasts of renewable energy generation, thereby enabling more informed and robust decision-making [3].

One of the key approaches in stochastic optimization for VRRGs is the single-sample online approximation (SSOA) algorithm. SSOA provides a tractable approximation of multi-stage stochastic optimization problems by dynamically re-optimizing decisions based on a single sampled realization of uncertain parameters. This approach is particularly advantageous in online settings where decisions must be made in real-time, and the computational complexity of traditional multi-sample methods, such as sample average approximation, becomes prohibitive. SSOA is designed to balance computational efficiency with solution quality, making it a practical choice for dynamic environments where rapid re-optimization is necessary. The algorithm's theoretical guarantees in canonical online resource allocation problems further enhance its credibility and applicability in the context of VRRGs.

To further enhance the robustness of stochastic optimization models for VRRGs, the integration of energy storage systems (ESS) is often considered. ESS can store excess energy generated during periods of high production and release it during periods of low production, thereby smoothing out the variability in VRRG output. However, the high cost and limited capacity of ESS present significant challenges. Therefore, stochastic optimization models must carefully balance the benefits of ESS with their associated costs. Additionally, the use of advanced risk measures, such as Conditional Value-at-Risk (CVaR), can help in managing the tail risks associated with extreme events, ensuring that the optimization model is not only efficient but also resilient to rare but potentially catastrophic scenarios [15].

### 5.1.3 Robustness-Infidelity Measure for Quantum Controllers
In the realm of quantum control, the evaluation of both robustness and fidelity is paramount due to the inherent susceptibility of quantum systems to environmental noise and operational imperfections. The Robustness-Infidelity Measure (RIM) serves as a comprehensive metric to assess the performance of quantum controllers under varying degrees of uncertainty [24]. Specifically, RIMp, defined as the p-th root of the p-th raw moment of the infidelity distribution, provides a flexible and non-parametric means to quantify the deviation of the actual fidelity from the ideal fidelity of 1. This measure is particularly advantageous as it does not rely on specific assumptions about the underlying probability distribution of the fidelity, thereby offering a more universal and robust assessment tool.

The practical utility of RIM1, the average infidelity, is highlighted in various quantum control applications, where it effectively balances the trade-off between robustness and fidelity [24]. By focusing on the average performance across multiple realizations, RIM1 captures the overall reliability of the quantum controller, making it a suitable choice for scenarios where consistent performance is crucial. Moreover, the relationship between RIMp and the Wasserstein distance of order p further underscores its versatility, allowing for a nuanced evaluation of the controller's performance in terms of both central tendency and tail behavior. This dual perspective is essential for understanding how the controller behaves under both typical and extreme conditions, which is critical for the reliable operation of quantum technologies.

To enhance the applicability of RIM in practical quantum control scenarios, the measure is often integrated with advanced optimization algorithms designed to minimize the infidelity distribution [24]. These algorithms leverage the non-parametric nature of RIM to iteratively refine control strategies, ensuring that the quantum system remains robust against a wide range of perturbations. The use of RIM in conjunction with stochastic and adaptive control techniques has shown promising results, particularly in the context of quantum gate operations and state preparation, where high fidelity and robustness are indispensable. Overall, the adoption of RIM as a standard metric in quantum control research facilitates a more rigorous and systematic approach to developing and evaluating control strategies, ultimately contributing to the advancement of quantum technologies.

## 5.2 Stochastic Models for Transportation and Logistics

### 5.2.1 Two-Stage Stochastic Model for Road-Rail Intermodal Freight
In the realm of intermodal freight transportation, the integration of road and rail modes presents unique challenges and opportunities, especially under conditions of uncertainty [25]. A two-stage stochastic model is proposed to address the complexities of road-rail intermodal freight, where the first stage involves strategic decisions such as the allocation of freight to specific routes and the selection of intermodal hubs [25]. These decisions are made under uncertainty regarding future demand, travel times, and operational disruptions. The second stage, or recourse stage, allows for tactical adjustments once the actual values of the uncertain parameters are revealed. This stage focuses on optimizing the detailed routing and scheduling of individual shipments, taking into account the realized conditions and the initial strategic decisions.

The two-stage stochastic model is formulated as a mixed-integer linear program (MILP) that captures the intricate interactions between road and rail transportation modes. Key components of the model include constraints related to capacity limits, time windows, and service level agreements. The objective function aims to minimize total transportation costs, which encompass both fixed and variable costs associated with each mode of transport. To handle the computational complexity arising from the large number of scenarios, the model employs advanced decomposition techniques such as Benders decomposition or column generation. These methods enable the efficient solution of the stochastic program by breaking it down into smaller, more manageable subproblems.

Furthermore, the model incorporates a risk management component to account for the variability in travel times and demand. This is achieved through the inclusion of a Conditional Value-at-Risk (CVaR) term in the objective function, which helps to balance the trade-off between cost minimization and risk mitigation [25]. The CVaR term ensures that the solution is not only cost-effective but also robust against adverse events that could disrupt the intermodal network. Numerical experiments demonstrate the effectiveness of the two-stage stochastic model in providing reliable and efficient transportation plans, even under highly uncertain conditions. The results highlight the importance of considering both strategic and tactical decisions in a unified framework to achieve optimal performance in intermodal freight transportation.

### 5.2.2 Stochastic Optimal Control for Residential Heating Systems
Stochastic optimal control (SOC) for residential heating systems is a critical area of research aimed at optimizing energy consumption and enhancing system efficiency under uncertainty [26]. This section focuses on the application of SOC techniques to manage the inherent variability in residential heating systems, particularly those integrated with renewable energy sources and thermal storage. The primary objective is to minimize operational costs while maintaining thermal comfort and ensuring system reliability. The SOC framework is formulated to handle the stochastic nature of renewable energy generation, such as solar and wind, and the variability in demand due to occupancy and weather conditions.

The proposed model incorporates a two-stage stochastic optimization approach, where the first stage involves making decisions based on the current state of the system and the second stage allows for adjustments based on new information. This approach is particularly useful in residential heating systems, where decisions such as the activation of heating units and the allocation of energy resources must be made dynamically. The model includes state-dependent control constraints to ensure that the system operates within safe and efficient limits, such as maximum and minimum temperature thresholds and energy storage capacities. The Hamilton-Jacobi-Bellman (HJB) equation is derived to solve the stochastic optimal control problem, providing a theoretical foundation for the optimization of control policies.

To address the computational challenges associated with solving the HJB equation, a numerical scheme is developed, which includes discretization of the state space and the use of efficient algorithms to approximate the solution. The performance of the proposed SOC framework is evaluated through numerical simulations, which demonstrate its effectiveness in reducing energy costs and improving thermal comfort under various scenarios. The results highlight the importance of integrating real-time data and adaptive control strategies in residential heating systems to enhance their resilience and efficiency in the face of uncertainty.

### 5.2.3 Post-Disruption Management Framework for Engineering Systems
Post-Disruption Management (PODIM) frameworks are essential for maintaining the operational continuity and resilience of complex engineering systems, such as power distribution networks and supply chains, in the aftermath of disruptive events [27]. These frameworks are designed to address the immediate and long-term challenges that arise from disruptions, including equipment failures, supply chain interruptions, and environmental impacts. The core of the PODIM framework lies in its ability to integrate heterogeneous dispatchable resources, such as Distributed Energy Resources (DERs) and Renewable Capacities (RCs), into a unified optimization model. This integration is crucial for enhancing the system's ability to respond dynamically to changing conditions and to minimize the financial and operational impacts of disruptions.

The PODIM framework employs a two-stage stochastic optimization (SO) approach, which allows for the consideration of multiple potential future scenarios and their associated probabilities. The first stage involves making immediate decisions based on the current state of the system, while the second stage focuses on adaptive strategies that can be implemented as new information becomes available. This two-stage approach is particularly effective in managing uncertainties, such as fluctuations in demand, supply chain delays, and weather-related disruptions. By incorporating Conditional Value-at-Risk (CVaR) as a risk measure, the PODIM framework further enhances its robustness by ensuring that the system remains stable and reliable even under extreme conditions. The use of CVaR helps in balancing the trade-offs between risk and reward, thereby providing a more comprehensive view of the system's performance.

To implement the PODIM framework, a Mixed Integer Linear Programming (MILP) model is formulated, which can handle the complexity and scale of real-world engineering systems. The model is designed to optimize the dispatch of resources while considering operational constraints, such as capacity limits, ramping rates, and network topology. The computational efficiency of the MILP model is improved through the use of scenario reduction techniques, which help in managing the computational burden associated with solving large-scale optimization problems. The results from the PODIM framework provide actionable insights for decision-makers, enabling them to develop and execute effective post-disruption strategies that enhance the resilience and sustainability of engineering systems.

## 5.3 Forecasting and Optimization in Energy Management

### 5.3.1 Two-Timescale Decision-Hazard-Decision Formulation
The two-timescale decision-hazard-decision (DHD) formulation is a structured approach to decision-making under uncertainty, particularly suitable for problems with distinct planning and operational timescales. In this formulation, decisions are made at a coarser timescale (e.g., weekly or monthly), while the realization of random variables and subsequent recourse actions occur at a finer timescale (e.g., daily or hourly). This structure allows for a more nuanced and adaptive response to uncertainty, as it separates strategic planning from operational adjustments. The planning decisions are non-anticipative, meaning they are made without knowledge of future random events, while the recourse actions are responsive to the realized uncertainties.

The DHD formulation is particularly useful in contexts such as supply chain management, energy systems, and transportation networks, where long-term strategic decisions must be balanced against short-term operational flexibility. For instance, in a supply chain, a company might decide on production levels and inventory policies at the beginning of a month, but adjust its distribution and logistics plans daily based on actual demand and supply disruptions. The separation of timescales ensures that the strategic decisions are robust to a wide range of possible scenarios, while the operational decisions can be fine-tuned to the specific conditions that unfold.

The mathematical formulation of the DHD problem typically involves a two-stage stochastic optimization model, where the first stage captures the planning decisions and the second stage models the recourse actions [28]. The objective function often includes both the cost or benefit associated with the planning decisions and the expected cost of the recourse actions. The constraints ensure that the planning decisions are feasible and that the recourse actions can be implemented under all possible realizations of the random variables. This approach provides a structured framework for integrating uncertainty into the decision-making process, allowing for a balance between long-term planning and short-term adaptability.

### 5.3.2 Integration of Forecasting and Optimization in Energy Management
The integration of forecasting and optimization in energy management is a critical area that addresses the inherent uncertainties in energy systems, particularly those involving renewable energy sources and variable demand patterns [3]. This integration leverages advanced predictive models to forecast future conditions, such as weather patterns and energy demand, and incorporates these forecasts into optimization algorithms to make informed decisions. The primary goal is to enhance the efficiency and reliability of energy systems while minimizing operational costs and environmental impacts. For instance, in the context of data centers, Microsoft has developed a decision-support software that uses predictive analytics to optimize energy usage and reduce waste [29]. The software iteratively refines its optimization algorithms based on continuous feedback from data center managers, leading to more accurate and actionable recommendations [29].

One of the key challenges in this integration is the dynamic nature of the problem environment, which requires real-time adjustments to forecasts and optimization strategies. To address this, researchers have proposed the Single-Sample Online Approximation (SSOA) algorithm, which simplifies the complex multi-stage stochastic optimization problem by sampling a single realization of uncertain parameters and dynamically re-optimizing decisions. This approach is computationally efficient and provides a practical solution for real-world applications where rapid decision-making is essential. However, the accuracy and stability of the forecasts play a crucial role in the effectiveness of the optimization outcomes. Inaccurate forecasts can lead to suboptimal decisions, potentially resulting in increased operational costs and reduced system performance.

Moreover, the integration of forecasting and optimization must consider the trade-offs between different objectives, such as minimizing energy costs and ensuring reliable service. For example, in the context of electric vehicle (EV) charging, the optimization problem involves balancing the charging schedule to avoid peak demand periods and reduce grid stress, while also ensuring that vehicles are charged in time for their next use. This requires a sophisticated understanding of both the demand patterns and the availability of renewable energy sources. The adoption of two-stage stochastic optimization approaches, which account for uncertainties in both demand and supply, is particularly useful in such scenarios. These methods allow for the formulation of robust strategies that can adapt to changing conditions, thereby improving the overall resilience and efficiency of energy management systems.

### 5.3.3 Stochastic Mixed Integer Linear Programming for Water Distribution
Stochastic Mixed Integer Linear Programming (SMILP) plays a pivotal role in optimizing water distribution systems, particularly in addressing the inherent uncertainties associated with demand, supply, and infrastructure reliability. In the context of tanker-based water distribution, these uncertainties are compounded by factors such as traffic congestion, roadblocks, and varying demand patterns [30]. To effectively manage these uncertainties, a two-stage stochastic optimization framework is employed, where the first stage involves making strategic decisions based on probabilistic scenarios, and the second stage focuses on operational adjustments once the actual conditions are known. This approach ensures that the system remains robust and adaptable to unforeseen events, thereby enhancing the overall resilience and efficiency of the water distribution network.

The proposed SMILP model for water distribution integrates a hybrid scenario generation method to capture the stochastic nature of the problem. This method combines historical data with predictive analytics to generate a set of plausible scenarios that represent the range of possible future conditions. By incorporating these scenarios into the optimization model, the decision-making process can account for the variability in demand and supply, leading to more informed and robust solutions [16]. The model also includes binary decision variables to represent discrete actions, such as the scheduling of tanker movements and the allocation of resources, which are essential for the practical implementation of the optimization results. The use of binary variables, however, increases the computational complexity of the problem, necessitating the application of advanced solution techniques such as branch-and-bound or cutting-plane methods.

To address the computational challenges associated with solving large-scale SMILP problems, the paper introduces a single-sample online approximation (SSOA) algorithm. This algorithm approximates the multi-stage stochastic optimization problem by sampling a single realization of the uncertain parameters and re-optimizing the decisions dynamically as new information becomes available [31]. The SSOA algorithm is designed to be computationally efficient, making it suitable for real-time decision-making in dynamic environments. Through theoretical analysis and empirical validation, the SSOA algorithm demonstrates its effectiveness in providing near-optimal solutions while significantly reducing the computational burden compared to traditional multi-sample stochastic programming approaches. This makes the SSOA algorithm a promising tool for optimizing tanker-based water distribution systems, especially in scenarios where rapid and adaptive decision-making is critical.

# 6 Future Directions


The current survey on contextual optimization under uncertainty has identified several limitations and gaps that warrant further exploration. One of the primary limitations is the computational complexity associated with integrating advanced probabilistic models and machine learning techniques into optimization frameworks. While these methods enhance the robustness and adaptability of decision-making processes, they often come at the cost of increased computational requirements, which can be prohibitive in real-time applications. Additionally, the integration of fairness and privacy considerations in optimization models remains a significant challenge, particularly in data-driven approaches where sensitive information is involved. Existing methods often struggle to balance the trade-offs between fairness, privacy, and utility, leading to suboptimal solutions. Furthermore, the dynamic and non-stationary nature of many real-world systems poses challenges for maintaining the performance and reliability of optimization algorithms over time. Current methodologies often require frequent re-calibration and updates, which can be resource-intensive and time-consuming.

To address these limitations, several directions for future research are proposed. First, the development of more efficient and scalable algorithms is essential to reduce the computational burden of integrating advanced models into optimization frameworks. This can be achieved through the use of approximate methods, parallel computing, and hardware accelerators. Additionally, the exploration of hybrid approaches that combine the strengths of different optimization techniques, such as combining reinforcement learning with traditional stochastic optimization, could lead to more robust and efficient solutions. Second, the integration of fairness and privacy into optimization models requires further theoretical and practical advancements. Research should focus on developing novel algorithms that can simultaneously ensure fairness and privacy while maintaining high utility. This could involve the use of advanced differential privacy techniques and the development of new fairness metrics that are more suitable for dynamic and uncertain environments. Third, the adaptability of optimization models to changing conditions can be improved through the development of online and adaptive learning methods. These methods should be able to continuously update and refine the optimization policies based on real-time data, ensuring that the system remains robust and reliable over time.

The impact of the proposed future work is significant. By developing more efficient and scalable algorithms, researchers can extend the applicability of advanced optimization techniques to a broader range of real-world problems, particularly in resource-constrained environments. This can lead to more widespread adoption of these methods in industries such as healthcare, finance, and logistics, where the ability to make robust and adaptive decisions under uncertainty is crucial. The integration of fairness and privacy into optimization models can help address ethical and legal concerns, making these models more acceptable and trustworthy to stakeholders. This is particularly important in applications involving sensitive data, such as personal health information and financial records. Finally, improving the adaptability of optimization models can enhance the resilience and reliability of systems in dynamic and uncertain environments, leading to better performance and reduced operational costs. Overall, the proposed future work has the potential to significantly advance the field of contextual optimization under uncertainty, contributing to more effective and sustainable decision-making processes.

# 7 Conclusion



The survey on contextual optimization under uncertainty has provided a comprehensive overview of the state-of-the-art techniques and methodologies in this interdisciplinary field. Key findings include the integration of advanced Monte Carlo techniques, reinforcement learning, and probabilistic forecasting, which collectively enhance the robustness and efficiency of optimization algorithms. The survey has explored the application of these methods in various domains, such as Mobile Edge Computing (MEC), autonomous systems, and energy management. Notably, the development of coupled learning frameworks for prediction and optimization, differentially private fair learning (DPFL), and distributionally robust optimization (DRO) has been highlighted as significant advancements. These frameworks not only improve the accuracy and reliability of decision-making processes but also address critical challenges such as fairness, privacy, and adaptability to dynamic environments.

The significance of this survey lies in its contribution to the broader field of optimization under uncertainty. By synthesizing insights from multiple disciplines, the survey offers a unified perspective on the challenges and opportunities in contextual optimization. It identifies key research gaps and open questions, providing a roadmap for future research. The survey also emphasizes the practical implications of these techniques, demonstrating their potential to enhance decision-making in complex and dynamic systems. This is particularly important in real-world applications where uncertainties and risks are prevalent, such as in engineering, finance, and logistics. By bridging the gap between theoretical advancements and practical applications, the survey serves as a valuable resource for researchers, practitioners, and policymakers.

In conclusion, the field of contextual optimization under uncertainty is poised for continued growth and innovation. The integration of machine learning, probabilistic models, and robust optimization frameworks has opened new avenues for addressing the challenges posed by stochastic and uncertain conditions. Future research should focus on developing more efficient and scalable algorithms, enhancing the interpretability and explainability of these methods, and exploring their applications in emerging areas such as quantum computing and sustainable systems. We call upon the research community to build upon the foundational work presented in this survey, fostering collaboration and interdisciplinary approaches to drive the field forward. The ultimate goal is to create more resilient, adaptive, and intelligent systems that can thrive in an increasingly complex and dynamic world.

# References
[1] Achieving Robust Data-driven Contextual Decision Making in a Data  Augmentation Way  
[2] Risk-averse Decision Making with Contextual Information  Model, Sample  Average Approximation, and K  
[3] Stochastic Optimal Control of an Industrial Power-to-Heat System with  High-Temperature Heat Pump an  
[4] Risk-Sensitive Stochastic Optimal Control as Rao-Blackwellized Markovian  Score Climbing  
[5] Stochastic Differentially Private and Fair Learning  
[6] Stochastic games of parental vaccination decision making and bounded  rationality  
[7] A learning-based solution approach to the application placement problem  in mobile edge computing un  
[8] Robust Model Predictive Path Integral Control  Analysis and Performance  Guarantees  
[9] Adaptive Environment-Aware Robotic Arm Reaching Based on a Bio-Inspired  Neurodynamical Computationa  
[10] Recent Advances in Path Integral Control for Trajectory Optimization  An  Overview in Theoretical an  
[11] Nonlinear Monte Carlo methods with polynomial runtime for Bellman  equations of discrete time high-d  
[12] Learning from zero  how to make consumption-saving decisions in a  stochastic environment with an AI  
[13] Hierarchical Policy Blending as Inference for Reactive Robot Control  
[14] Generative Probabilistic Time Series Forecasting and Applications in  Grid Operations  
[15] Stochastic SketchRefine  Scaling In-Database Decision-Making under  Uncertainty to Millions of Tuple  
[16] A Neural Network-Based Distributional Constraint Learning Methodology  for Mixed-Integer Stochastic  
[17] Managing Distributional Ambiguity in Stochastic Optimization through a  Statistical Upper Bound Fram  
[18] End-to-end Conditional Robust Optimization  
[19] Semiparametric Counterfactual Regression  
[20] Certainty Equivalence Control-Based Heuristics in Multi-Stage Convex  Stochastic Optimization Proble  
[21] HVAC Scheduling under Data Uncertainties  A Distributionally Robust  Approach  
[22] Distributionally robust stochastic programs with side information based  on trimmings -- Extended ve  
[23] Two-Stage Stochastic Optimization Frameworks to Aid in Decision-Making  Under Uncertainty for Variab  
[24] Statistically Characterising Robustness and Fidelity of Quantum Controls  and Quantum Control Algori  
[25] A Two-Stage Stochastic Model for Road-Rail Intermodal Freight  Transportation Under Demand and Capac  
[26] Stochastic Optimal Control of Prosumers in a District Heating System  
[27] Risk-Averse Optimization for Resilience Enhancement of Complex  Engineering Systems under Uncertaint  
[28] A Two-Timescale Decision-Hazard-Decision Formulation for Storage Usage  Values Calculation  
[29] Online Rack Placement in Large-Scale Data Centers  
[30] An Operational Scheduling Framework for Tanker-based Water Distribution  System under Uncertainty  
[31] On designing a resilient green supply chain to mitigate ripple effect  a  two-stage stochastic optim  