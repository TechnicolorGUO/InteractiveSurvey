# A Survey of Contextual Optimization Methods for Decision-Making under Uncertainty  

Utsav Sadana Department of Computer Science and Operations Research Universite¬¥ de Montre¬¥al, Que¬¥bec, Canada utsav.sadana@umontreal.ca  

Abhilash Chenreddy GERAD & Department of Decision Sciences HEC Montre¬¥al, Que¬¥bec, Canada abhilash.chenreddy@hec.ca  

Erick Delage GERAD & Department of Decision Sciences HEC Montre¬¥al, Que¬¥bec, Canada erick.delage@hec.ca  

Alexandre Forel CIRRELT & SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematical and Industrial Engineering, Polytechnique Montre¬¥al, Que¬¥bec, Canada alexandre.forel@polymtl.ca  

Emma Frejinger CIRRELT & Department of Computer Science and Operations Research Universite¬¥ de Montre¬¥al, Que¬¥bec, Canada emma.frejinger@umontreal.ca  

Thibaut Vidal CIRRELT & SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematical and Industrial Engineering, Polytechnique Montre¬¥al, Que¬¥bec, Canada thibaut.vidal@polymtl.ca  

Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. This survey article unifies these models under the lens of contextual stochastic optimization, thus providing a general presentation of a large variety of problems. We identify three main frameworks for learning policies from data and present the existing models and methods under a uniform notation and terminology. Our objective with this survey is to both strengthen the general understanding of this active field of research and stimulate further theoretical and algorithmic advancements in integrating ML and stochastic programming.  

# Contents  

# 1 Introduction 3  

2 Contextual optimization: An overview 5   
2.1 Contextual problem and policy 5   
2.2 Mapping covariate to actions in a data-driven environment 6   
2.2.1 Decision rule optimization. 7   
2.2.2 Learning and optimization. 7   
2.3 Summary . 10  

# 3 Decision rule optimization 11  

3.1 Linear decision rules 12   
3.2 RKHS-based decision rules 12   
3.3 Non-linear decision rules 13   
3.4 Distributionally robust decision rules 14  

# 1 Sequential learning and optimization 15  

4.1 Learning conditional distributions 15   
4.1.1 Residual-based distribution. 16   
4.1.2 Weight-based distribution. 16   
4.1.3 Expected value-based models. 18   
4.2 Regularization and distributionally robust optimization . 18  

# 5 Integrated learning and optimization 19  

5.1 Models 20   
5.2 Training by unrolling 23   
5.3 Training using implicit differentiation 23   
5.4 Training using a surrogate differentiable loss function 26   
5.4.1 SPO+. 26   
5.4.2 Surrogate loss for a stochastic forest. 29   
5.4.3 Other surrogates. 29   
5.5 Training using a surrogate differentiable optimizer . 30   
5.5.1 Differentiable perturbed optimizer. 30   
5.5.2 Supervised learning. 32   
5.6 Applications 33   
6 Active research directions 34   
7 Conclusions 39   
A Theoretical guarantees 51   
B List of abbreviations 51  

# 1. Introduction  

This article surveys the literature on single and two-stage contextual optimization. In contextual optimization, a decision-maker faces a decision-making problem with uncertainty where the distribution of uncertain parameters that affect the objective and the constraints is unknown, although correlated side information (covariates or features) can be exploited. The usefulness of side information in inferring relevant statistics of uncertain parameters and, thereby, in decisionmaking is evident in many different fields. For example, weather and time of day can help resolve uncertainty about congestion on a road network and aid in finding the shortest path traversing a city. In portfolio optimization, stock returns may depend on historical prices and sentiments posted on Twitter (Xu and Cohen 2018). Harnessing this information can allow decision-makers to build a less risky portfolio. Similarly, a retailer facing uncertain demand for summer products can infer whether the demand will be low or high depending on the forecasted weather conditions (Martƒ±¬¥nez-de Albeniz and Belkaid 2021).  

In these applications, the decision-maker has access to historical data, that is, past values of the covariates (e.g., weather) and the corresponding uncertain parameter (e.g., congestion). Datadriven contextual optimization methods use this data to estimate the conditional distribution of the uncertain parameter (or a sufficient statistic) based on the covariate. Conversely, traditional stochastic optimization models ignore contextual information and use unconditional distributions of the uncertain parameters to make a decision (Birge and Louveaux 2011). Such a decision may be suboptimal (Ban and Rudin 2019) and, in some cases, even at the risk of being infeasible (Rahimian and Pagnoncelli 2022). The availability of data and huge computational power combined with advancements in machine learning (ML) and optimization techniques have resulted in a shift of paradigm to contextual optimization (MiÀási¬¥c and Perakis 2020).  

Making prescriptions using the side information requires a decision rule that maps the observed covariate to an action. We identify three different paradigms for learning this mapping.  

‚Ä¢ Decision rule optimization: This approach was introduced to the operation research community in Liyanage and Shanthikumar (2005) for data-driven optimization and popularized in Ban and Rudin (2019) for big data environments, although a similar idea was already common practice in reinforcement learning under the name of policy gradient methods (see  

Sutton et al. 1999, and literature that followed). It consists in employing a parameterized mapping as the decision rule and in identifying the parameter that achieves the best empirical performance based on the available data. The decision rule can be formed as a linear combination of functions of the covariates or even using a deep neural network (DNN). When the data available is limited, some form of regularization might also be needed.  

‚Ä¢ Sequential learning and optimization (SLO): Bertsimas and Kallus (2020) appears to be the first to have formalized this two-stage procedure (also referred to as predict/estimatethen-optimize or prescriptive optimization/stochastic programming) that first uses a trained model to predict a conditional distribution for the uncertain parameters given the covariates, and then solves an associated contextual stochastic optimization (CSO) problem to obtain the optimal action. This procedure can be robustified to reduce post-decision disappointment (Smith and Winkler 2006) caused by model overfitting or misspecification by using proper regularization at training time or by adapting the CSO problem formulation.  

‚Ä¢ Integrated learning and optimization (ILO): In the spirit of identifying the best decision rule, one might question in SLO the need for high precision predictors when one is instead mostly interested in the quality of the resulting prescribed action. This idea motivates an integrated version of learning and optimization that searches for the predictive model that guides the CSO problem toward the best-performing actions. The ILO paradigm appears as early as in Bengio (1997) and has seen a resurgence recently in active streams of literature under various names such as smart predict-then-optimize, decision-focused learning, and (task-based) end-to-end learning/forecasting/optimization.  

The outline of the survey goes as follows. Section 2 rigorously defines the three frameworks for identifying the best mapping from covariate to action based on data: decision rule optimization, SLO, and ILO. Section 3 reviews the literature on decision rule optimization with linear and nonlinear decision rules. Section 4 focuses on SLO, including the models that lead to robust decisions, and Section 5 describes the models based on the ILO framework and the algorithms used to train them. Because ILO is the more recent and less explored framework of the three identified, we provide a separate subsection of applications of ILO to diverse problems such as logistics and energy management. Section 6 provides an overview of active research directions being pursued both from a theoretical and applications perspective. Section 7 concludes our survey with a summary of our contributions.  

We note that there are other surveys and tutorials in the literature that are complementary to ours. MiÀási¬¥c and Perakis (2020) survey the applications of the SLO framework to problems in supply chain management, revenue management, and healthcare operations. Qi and Shen (2022) is a tutorial that mainly focuses on the application of ILO to expected value-based models with limited discussions on more general approaches. It summarizes the most popular methods and some of their theoretical guarantees. Kotary et al. (2021) provide a comprehensive survey of the literature proposing ML methods to accelerate the resolution of constrained optimization models (see also Bengio et al. 2021). It also reviews some of the earlier literature on ILO applied to what we define as ‚Äúexpected value-based models‚Äù, a subset of CSO problems (see Definition 1) where uncertainty can be completely described by a sufficient statistic before the optimization problem is solved. Mandi et al. (2023)1 include more recent expected value-based models and provide a comprehensive evaluation of their methods, complementing the toolbox of Tang and Khalil (2022) that provided an interface for solving expected value-based models.  

Our survey of ILO literature goes beyond the expected value-based models and reflects better the more modern literature by casting the contextual decision problem as a CSO problem and presenting a comprehensive overview of the current state of this rapidly progressing field of research. We establish links between approaches that minimize regret (Elmachtoub and Grigas 2022), (taskbased) end-to-end learning (Donti et al. 2017) and imitation-based models (Kong et al. 2022). Further, we create a taxonomy based on the training procedure for a general ILO framework encompassing recent theoretical and algorithmic progresses in designing differentiable surrogates and optimizers and improving training procedures based on unrolling and implicit differentiation.  

# 2. Contextual optimization: An overview  

The contextual optimization paradigm seeks a decision (i.e., an action) $z$ in a feasible set $\mathcal { Z } \subseteq$ $\mathbb { R } ^ { d _ { z } }$ that minimizes a cost function $c ( z , y )$ with uncertain parameters $\pmb { y } \in \mathcal { y } \subseteq \mathbb { R } ^ { d _ { \pmb { y } } }$ . The uncertain parameters are unknown when making the decision. However, a vector of relevant covariates $\pmb { x } \in \mathcal { X } \subseteq \mathbb { R } ^ { d _ { \pmb { x } } } ,$ , which is correlated with the uncertain parameters $_ { { \pmb y } , }$ is revealed before having to choose $z$ . The joint distribution of the covariates in $\mathcal { X }$ and uncertain parameters in $y$ is denoted by $\mathbb { P }$ .  

# 2.1. Contextual problem and policy  

In general, uncertainty can appear in the objectives and constraints of the problem. In the main sections of this paper, we focus on problems with uncertain objectives and consider that the decision-maker is risk-neutral. We broaden the discussion to risk-averse settings and uncertain constraints in Section 6.  

The CSO problem. Given a covariate described by a vector of covariates $\scriptstyle { \mathbf { { \vec { x } } } }$ and the joint distribution $\mathbb { P }$ of the covariates $\scriptstyle { \mathbf { { \vec { x } } } }$ and uncertain parameter $\pmb { y } ,$ , a risk-neutral decision-maker is interested in finding an optimal action $z ^ { \ast } ( x ) \in \mathcal { Z }$ that minimizes the expected costs conditioned on the covariate $\scriptstyle { \mathbf { { \vec { x } } } }$ . Formally, the optimal action is a solution to the CSO problem given by:  

$$
( \mathrm { C S O } ) \quad z ^ { * } ( \pmb { x } ) \in \underset { z \in \mathcal { Z } } { \arg \operatorname* { m i n } } \mathbb { E } _ { \mathbb { P } ( \pmb { y } | \pmb { x } ) } \left[ c \left( z , \pmb { y } \right) \right] ,
$$  

where $\mathbb { P } ( \pmb { y } | \pmb { x } )$ denotes the conditional distribution of $_ y$ given the covariate $\scriptstyle { \mathbf { { \vec { x } } } }$ and it is assumed that a minimizer exists. For instance, a minimizer exists when $\mathcal { Z }$ is compact, $\mathbb { P } ( y | \boldsymbol { x } )$ has bounded support and $c ( z , y )$ is continuous in $z$ almost surely (see Van Parys et al. 2021, for more details).  

Problem (1) can equivalently be written in a compact form using the expected cost operator $h ( \cdot , \cdot )$ that receives an action as a first argument and a distribution as a second argument:  

$$
z ^ { \ast } ( \pmb { x } ) \in \underset { z \in \mathcal { Z } } { \mathrm { a r g m i n } } h ( z , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) : = \mathbb { E } _ { \mathbb { P } ( \pmb { y } | \pmb { x } ) } \left[ c \left( \pmb { z } , \pmb { y } \right) \right] .
$$  

Optimal policy. In general, the decision-maker repeatedly solves CSO problems in many different contexts. Hence, the decision-maker is interested in finding the policy that provides the lowest long-term expected cost, that is:  

$$
\pi ^ { * } \in \underset { \pi \in \Pi } { \mathrm { a r g m i n } } \mathbb { E } _ { \mathbb { P } } \big [ c ( \pi ( \pmb { x } ) , \pmb { y } ) \big ] = \underset { \pi \in \Pi } { \mathrm { a r g m i n } } \mathbb { E } _ { \mathbb { P } } \big [ h ( \pi ( \pmb { x } ) , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) \big ] ,
$$  

where $\Pi : = \{ \pi : \mathcal { X } \xrightarrow { } \mathcal { Z } \}$ denotes the class of all feasible policies.  

Note that the optimal policy does not need to be obtained explicitly in a closed form. Indeed, based on the interchangeability property (see Theorem 14.60 of Rockafellar and Wets 2009), solving the CSO problem (1) for any covariate $\scriptstyle { \mathbf { { \vec { x } } } }$ naturally identifies an optimal policy:  

$$
\bar { \pi } ( \pmb { x } ) \in \underset { \pmb { z } \in \mathbb { Z } } { \mathrm { a r g m i n } } h ( \pmb { z } , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) \mathrm { a . s . } \Leftrightarrow \mathbb { E } _ { \mathbb { P } } \left[ h ( \bar { \pi } ( \pmb { x } ) , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) \right] = \underset { \pi \in \Pi } { \mathrm { m i n } } \mathbb { E } _ { \mathbb { P } } \left[ h ( \pi ( \pmb { x } ) , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) \right] ,
$$  

assuming that a minimizer of $h ( \boldsymbol { z } , \mathbb { P } ( \pmb { y } | \pmb { x } ) )$ exists almost surely. Thus, the two optimal policies $\pi ^ { * }$ and $z ^ { \ast } ( \cdot )$ coincide.  

# 2.2. Mapping covariate to actions in a data-driven environment  

Unfortunately, the joint distribution $\mathbb { P }$ is generally unknown. Instead, the decision-maker possesses historical data $\mathcal { D } _ { N } = \{ ( \boldsymbol { \mathbf { { x } } } _ { i } , \boldsymbol { \mathbf { { y } } } _ { i } ) \} _ { i = 1 } ^ { N }$ that is assumed to be made of independent and identically distributed realizations of $( { \pmb x } , { \pmb y } ) \in \mathcal { X } \times \mathcal { Y }$ . Using this data, the decision-maker aims to find a policy that approximates well the optimal policy given by (3). Many approaches have been proposed to find effective approximate policies. Most of them can be classified into the three main frameworks that we introduce below: (i) decision rule optimization, (ii) sequential learning and optimization, and (iii) integrated learning and optimization.  

2.2.1. Decision rule optimization. In this framework, the policy is assumed to belong to a hypothesis class $\Pi ^ { \theta } : = \{ \pi _ { \theta } \} _ { \theta \in \Theta } \subseteq \Pi$ that contains a family of parametric policies $\pi _ { \theta } : \mathcal { X }  \mathcal { Z }$ (e.g., linear functions or decision trees). The parameterized policy $\pi _ { \theta }$ maps directly any covariate $\scriptstyle { \mathbf { { \vec { x } } } }$ to an action $\pi _ { \pmb { \theta } } ( \pmb { x } )$ and will be referred to as a decision rule.  

Denote by $\hat { \mathbb { P } } _ { N }$ the empirical distribution of $( { \pmb x } , { \pmb y } )$ given historical data $\mathcal { D } _ { N }$ . One can identify the ‚Äúbest‚Äù parameterization of the policy in $\Pi ^ { \theta }$ by solving the following empirical risk minimization (ERM) problem:  

$$
\begin{array} { r } { ( \mathrm { E R M } ) \quad \pmb { \theta } ^ { * } \in \underset { \pmb { \theta } } { \operatorname { a r g m i n } } H ( \pi _ { \pmb { \theta } } , \hat { \mathbb { P } } _ { N } ) : = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } \left[ c ( \pi _ { \pmb { \theta } } ( \pmb { x } ) , \pmb { y } ) \right] . } \end{array}
$$  

In simple terms, Problem (4) finds the policy $\pi _ { \theta ^ { \ast } } \in \Pi ^ { \theta }$ that minimizes the expected costs over the training data. This decision pipeline is shown in Figure 1. Notice that there are two approximations of Problem (3) made by Problem (4). First, the policy is restricted to a hypothesis class that may not contain the true optimal policy. Second, the long-term expected costs are calculated over the empirical distribution $\hat { \mathbb { P } } _ { N }$ rather than the true distribution $\mathbb { P }$ . Furthermore, Problem (4) focuses its policy optimization efforts on the overall performance (averaged over different covariates) and disregards the question of making the policy achieve a good performance uniformly from one covariate to another.  

![](images/65aaada011f07cc3d1eecaa5a9726b912474226de8b3c9fa55b33b94abd413b5.jpg)  
Figure 1 Decision and training pipelines based on the decision rule paradigm: (left) the decision pipeline and (right) the training pipeline for a given training example $( \pmb { x } _ { i } , \pmb { y } _ { i } )$ .  

2.2.2. Learning and optimization. The second and third frameworks combine a predictive component and an optimization component. The predictive component is a general model $f _ { \theta , }$ , parameterized by $\pmb \theta$ , whose role is to provide the input of the optimization component. For any covariate $\scriptstyle { \mathbf { { x } } } ,$ the intermediate input $f _ { \theta } ( { \pmb x } )$ can be interpreted as a predicted distribution that approximates the true conditional distribution $\mathbb { P } ( y | \boldsymbol { x } )$ (or a sufficient statistic in the case of expected value-based models). The predictive component is typically learned from historical data.  

At decision time, a learning and optimization decision pipeline (see Figure 2) solves the CSO problem under $f _ { \theta } ( { \pmb x } )$ , namely:  

$$
z ^ { * } ( x , f _ { \theta } ) \in \mathop { \mathrm { a r g m i n } } _ { z \in \mathcal { Z } } h ( z , f _ { \theta } ( x ) ) .
$$  

The solution of Problem (5) minimizes the expected cost with respect to the predicted distribution $f _ { \theta } ( { \pmb x } )$ . Notice that the only approximation between Problem (5) and the true CSO problem in (2) lies in $f _ { \theta }$ being an approximation of $\mathbb { P } ( \pmb { y } | \pmb { x } )$ . Since the predicted distribution changes with the covariate $\scriptstyle x$ , this pipeline also provides a policy. In fact, if the predictive component were able to perfectly predict the true conditional distribution $\mathbb { P } ( \pmb { y } | \pmb { x } )$ for any $\scriptstyle { \mathbf { { x } } }$ , the pipeline would recover the optimal policy $\pi ^ { * }$ given in (3).  

<html><body><table><tr><td>Context</td><td>Prediction</td><td>Prediction</td><td>Optimization</td><td>Decision</td></tr><tr><td rowspan="2"></td><td rowspan="2">model</td><td>fe(x)</td><td>model</td><td rowspan="2"></td></tr><tr><td></td><td></td></tr></table></body></html>  

We now detail the second and third frameworks to address contextual optimization: SLO and ILO. They differ in the way the predictor $f _ { \theta } ( x )$ is trained using the historical data.  

Sequential learning and optimization. In this framework, the contextual predictor is obtained by minimizing an estimation error, $\rho ,$ between the conditional distribution given by $f _ { \theta } ( { \pmb x } )$ and the true conditional distribution of $_ y$ given $\scriptstyle { \mathbf { { \vec { x } } } }$ . Training the contextual parametric predictor usually implies solving the following estimation problem:  

$$
\operatorname* { m i n } _ { \pmb { \theta } } \rho \big ( f _ { \pmb { \theta } } , \hat { \mathbb { P } } _ { N } \big ) + \Omega ( \pmb { \theta } ) \mathrm { w i t h } \rho \big ( f _ { \pmb { \theta } } , \hat { \mathbb { P } } _ { N } \big ) : = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } \big [ \mathfrak { D } \big ( f _ { \pmb { \theta } } ( \pmb { x } ) , \pmb { y } \big ) \big ] ,
$$  

where $\mathfrak { D }$ is a divergence function, e.g., negative log-likelihood and the regularization term $\Omega ( \pmb \theta )$ controls the complexity of $f _ { \theta }$ . The conditional distribution can also take a non-parametric form, e.g. $k$ -nearest neighbor, where $\pmb \theta$ then captures hyper-parameters of the non-parametric model (such as the number of neighbors) selected through a form of cross-validation scheme. The SLO training pipeline is shown in Figure 3.  

DEFINITION 1 (EXPECTED VALUE-BASED MODELS). When the cost function $c ( \pmb { x } , \pmb { y } )$ of the decision model is linear in $_ { y , }$ , the problem of estimating a conditional distribution reduces to finding the expected value of the uncertain parameter given the covariates since $h ( \boldsymbol { z } , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) =$ $\mathbb { E } _ { \mathbb { P } ( \pmb { y } | \pmb { x } ) } [ c ( \pmb { z } , \pmb { y } ) ] = c ( \pmb { z } , \mathbb { E } _ { \mathbb { P } ( \pmb { y } | \pmb { x } ) } [ \pmb { y } ] )$ . Training the contextual predictor, therefore, reduces to a mean regression problem over a parameterized function $g _ { \pmb { \theta } } ( \pmb { x } )$ . Specifically,  

$$
\operatorname* { m i n } _ { \pmb { \theta } } \rho \big ( g _ { \pmb { \theta } } , \hat { \mathbb { P } } _ { N } \big ) + \Omega ( \pmb { \theta } ) \mathrm { w i t h } \rho \big ( g _ { \pmb { \theta } } , \hat { \mathbb { P } } _ { N } \big ) : = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } \left[ d \big ( g _ { \pmb { \theta } } ( \pmb { x } ) , \pmb { y } \big ) \right] ,
$$  

![](images/c98e01f2cef04c1e24739e84c25f94508ba23b75fd7c3d0e33c5bcc87480f1a1.jpg)  
Figure 3 SLO training pipeline for a given training example.  

for some distance metric $d ,$ usually the mean squared errors. While the mean squared error is known to asymptotically retrieve $g _ { \hat { \pmb \theta } } ( \pmb x ) = \mathbb { E } _ { \mathbb { P } ( \pmb y | \pmb x ) } [ \pmb y ]$ as $N  \infty$ under standard conditions, other distance metric or more general loss functions can also be used (Hastie et al. 2009). For any new covariate $\scriptstyle { \mathbf { { \vec { x } } } }$ , an action is obtained using:  

$$
z ^ { * } ( x , g _ { \theta } ) \in \underset { z \in \mathcal { Z } } { \mathrm { a r g m i n } } h ( z , \delta _ { g _ { \theta } ( x ) } ) = \underset { z \in \mathcal { Z } } { \mathrm { a r g m i n } } c ( z , g _ { \theta } ( x ) ) ,
$$  

where, with a slight abuse of notation, $z ^ { * }$ now takes an estimator of the mean of the conditional distribution as the second argument, and with $\delta _ { y }$ being the Dirac distribution putting all its mass at $_ y$ . In the remainder of this survey, we refer to these approaches as expected value-based models, while the more general models that prescribe using a conditional distribution estimator (i.e. $z ^ { * } ( x , f _ { \theta } ) )$ will be referred as a conditional distribution-based models when it is not clear from the context.  

Integrated learning and optimization. Sequential approaches ignore the mismatch between the prediction divergence $\mathfrak { D }$ and the cost function $c ( \pmb { x } , \pmb { y } )$ . Depending on the covariate, a small prediction error about $\mathbb { P } ( \pmb { y } | \pmb { x } )$ may have a large impact on the prescription. In integrated learning, the goal is to maximize the prescriptive performance of the induced policy. That is, we want to train the predictive component to minimize the task loss (i.e., the downstream costs incurred by the decision) as stated in (3). The prescriptive performance may guide the estimation procedure toward a solution with higher MSE (or any distance metric) that nevertheless produces a nearlyoptimal decision. This is illustrated in Figure 4.  

Finding the best parameterization of a contextual predictor that minimizes the downstream expected costs of the CSO solution can be formulated as the following problem:  

$$
\operatorname* { m i n } _ { \theta } H \big ( z ^ { * } \big ( \cdot , f _ { \theta } \big ) , \widehat { \mathbb { P } } _ { N } \big ) = \operatorname* { m i n } _ { \theta } \mathbb { E } _ { \widehat { \mathbb { P } } _ { N } } \big [ c \big ( z ^ { * } \big ( x , f _ { \theta } \big ) , y \big ) \big ] .
$$  

The objective function in (9) minimizes the expected cost of the policy over the empirical distribution. The policy induced by this training problem is thus optimal with respect to the predicted distribution and minimizes the average historical costs over the whole training data. Figure 5 describes how the downstream cost is propagated by the predictive model during training. This training procedure necessarily comes at the price of heavier computations because an optimization model needs to be solved for each data point, and differentiation needs to be applied through an argmin operation.  

![](images/1cbabb6f308604b94584c25595357451ae9eee932f3229201ec83946f4434a73.jpg)  
Figure 4 Predicting $g _ { \pmb { \theta } _ { A } } ( \pmb { x } )$ results in the optimal action ${ z ^ { * } } ( { \pmb x } , g _ { { \pmb \theta } _ { A } } ) = { z ^ { * } } ( { \pmb x } )$ whereas a small error resulting from predicting $g _ { \pmb { \theta } _ { B } } ( \pmb { x } )$ leads to a suboptimal action $z ^ { * } ( x , g _ { B _ { B } } )$ under $c ( \mathbf { x } , \mathbf { y } ) : = - \mathbf { y } ^ { \top } \mathbf { x } .$ , i.e., $\begin{array} { r } { h ( { \boldsymbol { z } } , \mathbb { P } ( { \pmb { y } } | { \pmb { x } } ) ) = - \mathbb { E } [ { \pmb { y } } | { \pmb { x } } ] ^ { \top } { \boldsymbol { z } } } \end{array}$ (adapted from Elmachtoub and Grigas 2022).  

![](images/4949005fed68dc0bc01e5f9eabe3e09558805f3e6a73c509ca09d562a2102bd6.jpg)  
Figure 5 ILO training pipeline for a given training example.  

# 2.3. Summary  

This section presented the main pipelines proposed in recent years to address contextual optimization. Although these pipelines all include a learning component, they differ significantly in their specific structures and training procedures. Overall, there are several design choices that the decision-maker should make when tackling contextual optimization: (i) the type of loss function used during training, which defines whether an approach belongs to the decision rule (using ERM), sequential (minimizing the estimation error), or integrated paradigm (minimizing the downstream cost of the policy), (ii) the class of the predictive model (e.g., linear, neural network, or random forest) and its hyperparameters. Each design choice has its own inductive bias and may imply specific methodological challenges, especially for ILO. In general, it is a priori unclear what combination of choices will lead to the best performance with limited data; therefore, pipelines have to be evaluated experimentally.  

In the following sections, we survey the recent literature corresponding to the three main frameworks for contextual optimization using the notation introduced so far, which is summarized in Table 1. A list of abbreviations used in this survey is given in Appendix B.  

Table 1 Notation: distributions, variables, and operators.   


<html><body><table><tr><td></td><td>Domain</td><td>Description</td></tr><tr><td>P</td><td></td><td>M(X √ó VÔºâ True (unknown) joint distribution of (x,y)</td></tr><tr><td>PN</td><td></td><td>M(X √ó VÔºâ Joint empirical distribution of (x,y)</td></tr><tr><td>Sy</td><td>M(V)</td><td>Dirac distribution that puts all of its weight on y</td></tr><tr><td></td><td>XCRd</td><td>Contextual information</td></tr><tr><td>y</td><td>YCRdy</td><td>Uncertain parameters</td></tr><tr><td></td><td>ZCRdz</td><td>A feasible action</td></tr><tr><td>0</td><td>Œò</td><td>Parameters of a prediction model</td></tr><tr><td></td><td>Œò</td><td>Optimal parameter value that minimizes the estimation error</td></tr><tr><td>c(z,y)</td><td>R</td><td>Cost of an action z under y</td></tr><tr><td>h(z,Qs)</td><td>R</td><td>Expected cost of an action z under Qg (a distribution over y)</td></tr><tr><td>H(œÄ,Q)</td><td>R</td><td>Expected cost of a policy œÄ under Q(a distribution over (x,y))</td></tr><tr><td>fe(x)</td><td>M(V)</td><td>Estimate of the conditional distribution of y given x</td></tr><tr><td>ge(x)</td><td>Rdy</td><td>Estimate of the conditional expectation of y given x</td></tr><tr><td>œÄ*Ôºà2Ôºâ</td><td>Z</td><td>Optimal solution of CSO under true conditional distribution P(y|x)</td></tr><tr><td>TŒ∏(x)</td><td>Z</td><td>Action prescribed by a policy parameterized by Œ∏ for context x</td></tr><tr><td>2*ÔºàxÔºâ</td><td>Z</td><td>Optimal solution to the CSO problem under the true conditional distribution P(ylx)</td></tr><tr><td>2*Ôºàac,fe)</td><td>Z</td><td>Optimal solution to the CSO problem under the conditional distribution fe(x)</td></tr><tr><td>2*Ôºàx,ge)</td><td>Z</td><td>Optimal solution to the CSO problem under the Dirac distribution Œ¥g@(ùë•)</td></tr><tr><td>p(fe,PN)</td><td>R</td><td>Expected prediction error for distribution model f based on empirical distribution PN</td></tr><tr><td>p(ge,PN)</td><td>R</td><td>Expected prediction error for point prediction model ge based on empirical distribution PN</td></tr></table></body></html>  

# 3. Decision rule optimization  

Decision rules obtained by solving the ERM in Problem (4) minimize the cost of a policy on the task, that is, the downstream optimization problem. Policy-based approaches are especially efficient computationally at decision time since it suffices to evaluate the estimated policy. No optimization problem needs to be solved once the policy is trained. We defined the decision rule approach as employing a parameterized mapping $\pi _ { \boldsymbol { \theta } } ( \pmb { x } )$ , e.g., linear policies (Ban and Rudin 2019) or a neural network (Oroojlooyjadid et al. 2020). Since policies obtained using neural networks lack interpretability, linear decision rules are widely used.  

# 3.1. Linear decision rules  

Ban and Rudin (2019) show that an approach based on the sample-average approximation (SAA) that disregards side information can lead to inconsistent decisions (i.e., asymptotically suboptimal) for a newsvendor problem. Using linear decision rules (LDRs), they study two variants of the newsvendor problem with and without regularization:  

$$
\operatorname* { m i n } _ { \substack { \tau : \exists \theta \in \mathbb { R } ^ { d _ { x } } , \pi ( x ) = \theta ^ { \top } x , \forall x } } H ( \pi , \hat { \mathbb { P } } _ { N } ) + \Omega ( \pi ) = \operatorname* { m i n } _ { \theta } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } u ( y _ { i } - \theta ^ { \top } x _ { i } ) ^ { + } + o ( \theta ^ { \top } x _ { i } - y _ { i } ) ^ { + } + \lambda \| \theta
$$  

where $u$ and $o$ denote the per unit backordering (underage) and holding (overage) costs. Ban and Rudin (2019) show that for a linear demand model, the generalization error for the ERM model scales as $\mathrm { O } ( d _ { x } / \sqrt { N } )$ when there is no regularization and as $\mathrm { O } ( d _ { x } / ( \sqrt { N } \lambda ) )$ with regularization. However, one needs to balance the trade-off between generalization error and bias due to regularization to get the optimal performance from using LDRs. Ban and Rudin (2019) consider unconstrained problems because it is difficult to ensure the feasibility of policies and maintain computational tractability using the ERM approach.  

Bertsimas and Kallus (2020) present a general theory for generalization bounds of decision rules based on Rademacher complexity that goes beyond LDR, although their main examples in this context pertain to LDR. Unfortunately, LDRs may not be asymptotically optimal in general. To generalize LDRs, one can consider decision rules that are linear in the transformations of the covariate vector (Ban and Rudin 2019). It is also possible to lift the covariate vector to a reproducing kernel Hilbert space (RKHS, Aronszajn 1950), as seen in the next section.  

# 3.2. RKHS-based decision rules  

To obtain decision rules that are more flexible than linear ones with respect to $\scriptstyle { \mathbf { { x } } } ,$ it is possible to lift the covariate vector to an RKHS in which LDRs might achieve better performance. Let $K : \mathcal { X } \times \mathcal { X }  \mathbb { R }$ be the symmetric positive definite kernel associated with the chosen RKHS, e.g., the Gaussian kernel $K ( \pmb { x } _ { 1 } , \pmb { x } _ { 2 } ) : = \exp ( - \| \pmb { x } _ { 1 } - \pmb { x } _ { 2 } \| ^ { 2 } / ( 2 \sigma ^ { 2 } ) )$ . Given $K _ { \cdot }$ , the RKHS $\mathcal { H } _ { K }$ is defined as the closure of a set of functions given below:  

$$
\Big \{ \varphi : \mathcal { X } \to \mathbb { R } | \exists L \in \mathbb { N } , \pmb { v } _ { 1 } , \pmb { v } _ { 2 } , \cdots , \pmb { v } _ { L } \in \mathcal { X } , \varphi ( \pmb { x } ) = \sum _ { l = 1 } ^ { L } a _ { l } K ( \pmb { v } _ { l } , \pmb { x } ) , \forall \pmb { x } \in \mathcal { X } \Big \} ,
$$  

with the inner product of $\begin{array} { r } { \varphi _ { 1 } ( { \pmb x } ) = \sum _ { i = 1 } ^ { L _ { 1 } } a _ { 1 } ^ { i } K ( { \pmb v } _ { 1 } ^ { i } , { \pmb x } ) } \end{array}$ and $\begin{array} { r } { \varphi _ { 2 } ( { \pmb x } ) = \sum _ { j = 1 } ^ { L _ { 2 } } a _ { 2 } ^ { j } K ( { \pmb v } _ { 2 } ^ { j } , { \pmb x } ) } \end{array}$ given by:  

$$
\langle \varphi _ { 1 } , \varphi _ { 2 } \rangle = \sum _ { i = 1 } ^ { L _ { 1 } } \sum _ { j = 1 } ^ { L _ { 2 } } a _ { 1 } ^ { i } a _ { 2 } ^ { j } K ( { \pmb v } _ { 1 } ^ { i } , { \pmb v } _ { 2 } ^ { j } ) .
$$  

Bertsimas and Koduri (2022) approximate the optimal policy with a linear policy in the RKHS, i.e. $\pi _ { \varphi } ( \pmb { x } ) : = \langle \varphi , K ( \pmb { x } , \cdot ) \rangle$ when $d _ { z } = 1$ , and show using the representer theorem (see Theorem 9 in Hofmann et al. 2008) that the solution of the following regularized problem:  

$$
\operatorname* { m i n } _ { \varphi \in \mathcal H _ { K } } H ( \pi _ { \varphi } , \hat { \mathbb P } _ { N } ) + \lambda \| \varphi \| _ { 2 } ^ { 2 } ,
$$  

takes the form $\begin{array} { r } { \pi ^ { * } ( \pmb { x } ) = \sum _ { i = 1 } ^ { N } K ( \pmb { x } _ { i } , \pmb { x } ) a _ { i } ^ { * } } \end{array}$ . Hence, this reduces the decision rule problem to:  

$$
\operatorname* { m i n } _ { a \in \mathbb { R } ^ { N } } H \left( \sum _ { i = 1 } ^ { N } K ( { \pmb x } _ { i } , \cdot ) a _ { i } , \hat { { \mathbb P } } _ { N } \right) + \lambda \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { N } K ( { \pmb x } _ { i } , { \pmb x } _ { j } ) a _ { i } a _ { j } .
$$  

This can be extended to $d _ { z } > 1$ by employing one RKHS for each $z _ { i }$ .  

This RKHS approach appeared earlier in Ban and Rudin (2019) and Bazier-Matte and Delage (2020) who respectively study the data-driven single item newsvendor and single risky asset portfolio problems and establish bounds on the out-of-sample performance. Bertsimas and Koduri (2022) show the asymptotic optimality of RKHS-based policies. Notz and Pibernik (2022) study a two-stage capacity planning problem with multivariate demand and vector-valued capacity decisions for which the underlying demand distribution is difficult to estimate in practice. Similar to Bazier-Matte and Delage (2020), the authors optimize over policies that are linear in the RKHS associated with the Gaussian kernel and identify generalization error bounds. For large dimensional problems, this kernel is shown to have a slow convergence rate, and as a result, the authors propose instead using a data-dependent random forest kernel.  

# 3.3. Non-linear decision rules  

Many non-linear decision rule approaches have been experimented with. Zhang and Gao (2017), Huber et al. (2019), and Oroojlooyjadid et al. (2020) study the value of training a DNN to learn the ordering policy of a newsvendor problem. It is well-known that neural networks enjoy the universal approximation property; that is, they can approximate any continuous function arbitrarily well (Cybenko 1989, Lu et al. 2017). For constrained problems, one can use softmax as the final layer to ensure that decisions lie in a simplex, e.g., in a portfolio optimization problem (Zhang et al. 2021). Yet, in general, the output of a neural network might not naturally land in the feasible space $\mathcal { Z }$ . To circumvent this issue, Chen et al. (2023) introduce an application-specific differentiable repair layer that projects the solution back to feasibility. Rychener et al. (2023) show that the decision rule obtained by using the stochastic gradient descent (SGD) method to train DNN-based policies approximately minimizes the Bayesian posterior loss.  

Exploiting the fact that the optimal solution of a newsvendor problem is a quantile of the demand distribution, Huber et al. (2019) further train an additive ensemble of decision trees using quantile regression to produce the ordering decision. They test these algorithms on a real-world dataset from a large German bakery chain. Bertsimas et al. (2019), Ciocan and MiÀási¬¥c (2022), and Keshavarz (2022) optimize decision tree-based decision rules to address the multi-item newsvendor, treatment planning, and optimal stopping problems, respectively. A tutorial on DNN-based decision rule optimization is given in Shlezinger et al. (2022).  

Zhang et al. (2023b) introduce piecewise-affine decision rules and provide non-asymptotic and asymptotic consistency results for unconstrained and constrained problems, respectively. The policy is learned through a stochastic majorization-minimization algorithm, and experiments on a constrained newsvendor problem show that piecewise-affine decision rules can outperform the RKHS-based policies.  

# 3.4. Distributionally robust decision rules  

Most of the literature on policy learning assumes a parametric form $\Pi ^ { \theta }$ for the policy. A notable exception is Zhang et al. (2023a), which studies a distributionally robust contextual newsvendor problem under the type-1 Wasserstein ambiguity set without assuming an explicit structure on the policy class. The type- $p$ Wasserstein distance (earth mover‚Äôs distance) between distributions $\mathbb { P } _ { 1 }$ and $\mathbb { P } _ { 2 }$ is given by:  

$$
W _ { p } ( \mathbb { P } _ { 1 } , \mathbb { P } _ { 2 } ) = \operatorname* { i n f } _ { \gamma \in \mathcal { M } ( \mathcal { Y } ^ { 2 } ) } \left( \int _ { \mathcal { Y } \times \mathcal { Y } } \lVert y _ { 1 } - y _ { 2 } \rVert ^ { p } \gamma ( d y _ { 1 } , d y _ { 2 } ) \right) ^ { \frac { 1 } { p } } ,
$$  

where $\gamma$ is a joint distribution of $y _ { 1 }$ and $y _ { 2 }$ with marginals $\mathbb { P } _ { 1 }$ and $\mathbb { P } _ { 2 }$ . The distributionally robust model in Zhang et al. (2023a) avoids the degeneracies of ERM for generic $\Pi$ by defining an optimal ‚ÄúShapley‚Äù extension to the scenario-based optimal policy. Mathematically,  

$$
\operatorname* { i n } _ { \mathbb { Q } \in \mathcal { M } ( \mathcal { X } \times \mathcal { Y } ) } \operatorname* { s u p } _ { \left\{ H ( \pi , \mathbb { Q } ) : \mathcal { W } ( \mathbb { Q } , \hat { \mathbb { P } } _ { N } ) \leq r \right\} } \equiv \operatorname* { m i n } _ { \pi : \hat { x } \to \mathcal { Z } } \operatorname* { s u p } _ { \mathbb { Q } \in \mathcal { M } ( \hat { x } \times \mathcal { Y } ) } \{ H ( \pi , \mathbb { Q } ) : \mathcal { W } ( \mathbb { Q } , \hat { \mathbb { P } } _ { N } ) \leq r \} ,
$$  

where $\hat { \mathcal { X } } : = \cup _ { i = 1 } ^ { N } \{ \pmb { x } _ { i } \}$ and $\mathcal { M } ( \hat { \mathcal { X } } \times \mathcal { Y } )$ is the set of all distribution supported on $\hat { \mathcal { X } } \times \mathcal { Y }$ .  

Prior to the work of Zhang et al. (2023a), many have considered distributionally robust versions of the decision rule optimization problem in the non-contextual setting (Yanƒ±koÀòglu et al. 2019) while Bertsimas et al. (2023) use LDRs to solve dynamic optimization problems with side information.  

Yang et al. (2023) point out that the perturbed distributions in the Wasserstein ambiguity set might have a different conditional information structure than the estimated conditional distribution. They introduce a distributionally robust optimization (DRO) problem with causal transport metric (Backhoff et al. 2017, Lassalle 2018) that places an additional causality constraint on the transport plan compared to the Wasserstein metric. Tractable reformulations of the DRO problem are given under LDRs as well as for one-dimensional convex cost functions.  

Table 2 Overview of contextual optimization papers in the SLO framework.   


<html><body><table><tr><td></td><td colspan="3">Method</td><td colspan="2">Regularization</td><td colspan="6">Learning model</td></tr><tr><td></td><td>rCso</td><td>WSAA</td><td>EVB</td><td>Reg. CSO</td><td>DRO</td><td>General</td><td>Linear</td><td>Kernel</td><td>kNN</td><td>DT</td><td>RF</td></tr><tr><td>Hannah et al. (2010)</td><td>√ó</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td>√ó</td><td>X</td></tr><tr><td>Ferreira et al. (2016)</td><td>X</td><td>X</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td></td><td>X</td></tr><tr><td>Ban et al. (2019)</td><td></td><td>√ó</td><td>X</td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>Chen and Paschalidis (2019)</td><td>X</td><td></td><td>X</td><td>X</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td>X</td></tr><tr><td>Bertsimas and Kallus (2020)</td><td>√ó</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td></td><td></td><td></td></tr><tr><td>Kannan et al. (2020)</td><td></td><td>√ó</td><td>X</td><td>X</td><td>X</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Kannan et al. (2021)</td><td></td><td>√ó</td><td>X</td><td>X</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Liu et al. (2021)</td><td>√ó</td><td>√ó</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td>X</td><td></td><td>X</td></tr><tr><td>Srivastava et al. (2021)</td><td>X</td><td></td><td>X</td><td></td><td>X</td><td>X</td><td>X</td><td>V</td><td>X</td><td>X</td><td>X</td></tr><tr><td>Wang et al. (2021)</td><td>√ó</td><td></td><td>X</td><td>X</td><td></td><td>X</td><td>X</td><td></td><td>X</td><td>X</td><td>X</td></tr><tr><td>Bertsimas and Van Parys (2022)</td><td></td><td></td><td>X</td><td>X</td><td></td><td>√ó</td><td>X</td><td>v</td><td></td><td>X</td><td>X</td></tr><tr><td>Deng and Sen (2022)</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td>v</td><td>v</td><td></td><td></td><td></td><td></td></tr><tr><td>Esteban-P√©rez and Morales (2022)</td><td>X</td><td></td><td>X</td><td>X</td><td></td><td>X</td><td>√ó</td><td></td><td></td><td>X</td><td>X</td></tr><tr><td>Kannan et al. (2022)</td><td></td><td>√ó</td><td>X</td><td>X</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Lin et al. (2022)</td><td>X</td><td></td><td>X</td><td></td><td>X</td><td>X</td><td>√ó</td><td>X</td><td></td><td></td><td></td></tr><tr><td>Nguyen et al. (2021)</td><td>X</td><td></td><td>X</td><td>X</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td>X</td></tr><tr><td>Notz and Pibernik (2022)</td><td>√ó</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td></td><td>X</td></tr><tr><td>Zhu et al. (2022)</td><td></td><td>√ó</td><td></td><td>X</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Perakis et al. (2023)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>X</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td></tr></table></body></html>

Note: we distinguish between regularized CSO models (Reg. CSO) and DRO-based regularization; an approach is classified as ‚ÄúGeneral‚Äù if its learning model s not restricted to specific classes.  

Rychener et al. (2023) present a Bayesian interpretation of decision rule optimization using SGD and show that their algorithm provides an unbiased estimate of the worst-case objective function of a DRO problem as long as a uniqueness condition is satisfied. The authors note that the Wasserstein ambiguity set violates this condition and thus use the Kullback-Leibler (KL) divergence (Kullback and Leibler 1951) to train the models.  

# 4. Sequential learning and optimization  

In reviewing contextual optimization approaches that are based on SLO, we distinguish two settings: (i) a more traditional setting where the conditional distribution is learned from data and used directly in the optimization model, and (ii) a setting that attempts to produce decisions that are robust to model misspecification. An overview of the methods presented in this section is given in Table 2.  

# 4.1. Learning conditional distributions  

Most of the recent literature has employed discrete models for $f _ { \theta } ( { \pmb x } )$ . This is first motivated from a computational perspective by the fact that the CSO Problem (5) is easier to solve in this setting. In fact, more often than not, the CSO under a continuous distribution needs to be first replaced by its SAA to be solved (Shapiro et al. 2014). From a statistical viewpoint, it can also be difficult to assess the probability of outcomes that are not present in the dataset, thus justifying fixing the support of $_ y$ to its observed values.  

4.1.1. Residual-based distribution. A first approach (found in Sen and Deng 2017, Kannan et al. 2020, Deng and Sen 2022) is to use the errors of a trained regression model (i.e., its residuals) to construct conditional distributions. Let $g _ { \hat { \pmb { \theta } } }$ be a regression model trained to predict the response $_ y$ from the covariate $\scriptstyle { \mathbf { { x } } } ,$ , thus minimizing an estimation error $\rho$ as in (7). The residual error of sample $i$ is given by $\pmb { \epsilon } _ { i } = \pmb { y } _ { i } - g _ { \hat { \pmb { \theta } } } ( \pmb { x } _ { i } )$ . The set of residuals measured on the historical data, $\{ \epsilon _ { i } \} _ { i = 1 } ^ { N } ,$ , is then used to form the conditional distribution $\begin{array} { r } { f _ { \theta } ( x ) = \mathbb { P } ^ { \mathrm { E R } } ( x ) : = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \delta _ { \mathrm { p r o j } _ { \mathcal { V } } ( \mathtt { g } _ { \hat { \theta } } ( x ) + \epsilon _ { \mathrm { i } } ) } , } \end{array}$ with $\mathrm { p r o j } _ { \boldsymbol { y } }$ denoting the projection on the support $y$ . The residual-based CSO (rCSO) problem is now given by:  

$$
( \pmb { \mathrm { r c s o } } ) \operatorname* { m i n } _ { z \in \mathcal { Z } } h ( z , \mathbb { P } ^ { \mathrm { E R } } ( \pmb { x } ) )
$$  

The advantage of residual-based methods is that they can be applied in conjunction with any trained regression model. Ban et al. (2019) and Deng and Sen (2022) build conditional distributions for two-stage and multi-stage CSO problems using the residuals obtained from parametric regression on the historical data.  

Notice that, in this approach, the historical data is used twice: to train the regression model $g _ { \theta } ,$ and to measure the residuals $\epsilon _ { i }$ . This can lead to an underestimation of the distribution of the residual error. To remove this bias, Kannan et al. (2020) propose a leave-one-out model (also known as jackknife). They measure the residuals as $\tilde { \mathbf { \epsilon } } _ { i } = \pmb { y } _ { i } - g _ { \hat { \pmb { \theta } } _ { - i } } ( \pmb { x } _ { i } ) ,$ where $\hat { \pmb { \theta } } _ { - i }$ is trained using all the historical data except the $i$ -th sample $( \pmb { x } _ { i } , \pmb { y } _ { i } )$ . This idea can also be applied to the heteroskedastic case studied in Kannan et al. (2021), where the following conditional distribution is obtained by first estimating the conditional covariance matrix $\hat { Q } ( { \pmb x } )$ (a positive definite matrix for almost every $\scriptstyle { \mathbf { { \vec { x } } } }$ ) and then forming the residuals $\hat { \mathbf { \epsilon } } _ { i } = [ \hat { Q } ( \mathbf { x } _ { i } ) ] ^ { - 1 } ( \pmb { y } _ { i } - g _ { \hat { \pmb { \theta } } } ( \mathbf { x } _ { i } ) )$ :  

$$
f _ { \theta } ( \pmb { x } ) : = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \delta _ { \mathrm { p r o j } \mathscr { y } ( \mathbf { g } _ { \hat { \theta } } ( \pmb { x } ) + \hat { \mathrm { Q } } ( \pmb { x } ) \hat { \mathbf { \epsilon } } _ { \mathrm { i } } ) } .
$$  

4.1.2. Weight-based distribution. A typical approach for formulating the CSO problem is to assign weights to the observations of the uncertain parameters in the historical data and solving the weighted SAA problem (wSAA) given by (Bertsimas and Kallus 2020):  

$$
( \boldsymbol { \mathrm { w S A A } } ) \quad \operatorname* { m i n } _ { z \in \mathcal { Z } } h \left( z , \sum _ { i = 1 } ^ { N } w _ { i } ( \pmb { x } ) \delta _ { \pmb { y } _ { i } } \right) .
$$  

In this case, the conditional distribution $\begin{array} { r } { f _ { \theta } ( \pmb { x } ) = \sum _ { i = 1 } ^ { N } w _ { i } ( \pmb { x } ) \delta _ { \pmb { y } _ { i } } } \end{array}$ is fully determined by the function used to assign a weight to the historical samples. Different approaches have been proposed to determine the sample weights with ML methods.  

Weights based on proximity. Sample weights can be assigned based on the distance between a covariate $\scriptstyle { \mathbf { { \vec { x } } } }$ and each historical sample $\pmb { x } _ { i }$ . For instance, a $k$ -nearest neighbor (kNN) estimation gives equal weight to the $k$ closest samples in the dataset and zero weight to all the other samples. That is, $w _ { i } ^ { \mathrm { k N N } } ( \pmb { x } ) : = ( 1 / k ) \mathbb { 1 } [ \pmb { x } _ { i } \in \mathcal { N } _ { k } ( \pmb { x } ) ] ,$ where $\mathcal { N } _ { k } ( \pmb { x } )$ denotes the set of $k$ nearest neighbors of $\scriptstyle { \mathbf { { \vec { x } } } }$ and $\mathbb { 1 } [ \cdot ]$ is the indicator function. Even though it may appear simple, this non-parametric approach benefits from asymptotic consistency guarantees on its prescriptive performance. Another method to determine sample weights is to use kernel density estimators (Hannah et al. 2010, Srivastava et al. 2021, Ban and Rudin 2019). The Nadaraya-Watson (NW) kernel estimator (Watson 1964, Nadaraya 1964) employs a weight function:  

$$
w _ { i } ^ { \mathrm { K D E } } ( \pmb { x } ) : = \frac { K \left( ( \pmb { x } - \pmb { x } _ { i } ) / \pmb { \theta } \right) } { \sum _ { j = 1 } ^ { N } K \left( ( \pmb { x } - \pmb { x } _ { j } ) / \pmb { \theta } \right) }
$$  

where $K$ is a kernel function and $\pmb \theta$ denotes its bandwidth parameter. Different kernel functions can be used, e.g., the Gaussian kernel defined as $K ( \Delta ) \propto \exp ( - \left\| \Delta \right\| ^ { 2 } )$ . Hannah et al. (2010) also use a Bayesian approach that exploits the Dirichlet process mixture to assign sample weights.  

Weights based on random forest. Weights can also be designed based on random forest regressors (Bertsimas and Kallus 2020). In its simplest setting, the weight function of a decision tree regressor is given by:  

$$
w _ { i } ^ { t } ( \pmb { x } ) : = \frac { \mathbb { 1 } \left[ \mathcal { R } _ { t } ( \pmb { x } ) = \mathcal { R } _ { t } ( \pmb { x } _ { i } ) \right] } { \sum _ { j = 1 } ^ { N } \mathbb { 1 } \left[ \mathcal { R } _ { t } ( \pmb { x } ) = \mathcal { R } _ { t } ( \pmb { x } _ { j } ) \right] }
$$  

where $\mathcal { R } _ { t } ( { \pmb x } )$ denotes the terminal node of tree $t$ that contains covariate $\scriptstyle { \mathbf { { \vec { x } } } }$ . Thus, a decision tree assigns equal weights to all the historical samples that end in the same leaf node as $\scriptstyle { \mathbf { { \boldsymbol { x } } } }$ . The random forest weight function generalizes this idea over many random decision trees. Its weight function is defined as:  

$$
w _ { i } ^ { \mathrm { R F } } ( { \pmb x } ) : = \frac { 1 } { T } \sum _ { t = 1 } ^ { T } w _ { i } ^ { t } ( { \pmb x } ) ,
$$  

where $\boldsymbol { w } _ { i } ^ { t }$ is the weight function of tree $t$ . Random forests are typically trained in order to perform an inference task, e.g. regression, or classification, but can also be used and interpreted as nonparametric conditional density estimators.  

Bertsimas and Kallus (2020) provide conditions for the asymptotic optimality (see Definition 2 in Appendix A) and consistency (see Definition 3 in Appendix A) of prescriptions obtained by solving Problem (11) with the weights functions given by kNN, NW kernel estimator, and local linear regression.  

4.1.3. Expected value-based models. As described in Definition 1, when the cost function is linear, the training pipeline of SLO reduces to conditional mean estimation. For instance, Ferreira et al. (2016) train regression trees to forecast daily expected sales for different product categories in an inventory and pricing problem for an online retailer. Alternatively, one may attempt to approximate the conditional density $f _ { \theta } ( { \pmb x } )$ using a point prediction $g _ { \pmb { \theta } } ( \pmb { x } )$ . For example, Liu et al. (2021) study a last-mile delivery problem, where customer orders are assigned to drivers, and replace the conditional distribution of the stochastic travel time with a point predictor (e.g. a linear regression or decision tree) that accounts for the number of stops, total distance of the trip, etc.  

# 4.2. Regularization and distributionally robust optimization  

While non-parametric conditional density estimation methods benefit from asymptotic consistency (Bertsimas and Kallus 2020, Notz and Pibernik 2022), they are known to produce overly optimistic policies when the size of the covariate vector is large (see discussions in Bertsimas and Van Parys 2022). To circumvent this issue, authors have proposed to either regularize the CSO problem (Srivastava et al. 2021, Lin et al. 2022) or to cast it as a DRO problem. In the latter case, one attempts to minimize the worst-case expected cost over the set of distributions $B _ { r } ( f _ { \theta } ( { \pmb x } ) )$ that lie at a distance $r$ from the estimated distribution $f _ { \theta } ( { \pmb x } )$ :  

$$
\operatorname* { m i n } _ { z \in \mathcal { Z } } \operatorname* { s u p } _ { \mathbb { Q } _ { y } \in B _ { r } ( f _ { \pmb { \theta } } ( \pmb { x } ) ) } h ( z , \mathbb { Q } _ { y } ) .
$$  

Bertsimas and Van Parys (2022) generate bootstrap data from the training set and use it as a proxy for the ‚Äúout-of-sample disappointment‚Äù of an action $z$ resulting from the out-of-sample cost exceeding the budget given by $\begin{array} { r } { \operatorname* { s u p } _ { \mathbb { Q } _ { y } \in \mathcal { B } _ { r } ( f _ { \pmb { \theta } } ( \pmb { x } ) ) } h \big ( z , \mathbb { Q } _ { y } \big ) } \end{array}$ . They show that for the NW kernel estimator and KNN estimator, the DRO, under a range of ambiguity sets, can be reformulated as a convex optimization problem. Using KL divergence to measure the distance between the probability distributions, they obtain guarantees (‚Äúbootstrap robustness‚Äù) with respect to the estimatethen-optimize model taking bootstrap data as a proxy for out-of-sample data. Taking the center of Wasserstein ambiguity set (see Kantorovich and Rubinshtein 1958) to be NW kernel estimator, Wang et al. (2021) show that the distributionally robust newsvendor and conditional value at risk (CVaR) portfolio optimization problems can be reformulated as convex programs. They provide conditions to obtain asymptotic convergence and out-of-sample guarantees on the solutions of the DRO model.  

Chen and Paschalidis (2019) study a distributionally robust kNN regression problem by combining point estimation of the outcome with a DRO model over a Wasserstein ambiguity set (Chen and Paschalidis 2018) and then using kNN to predict the outcome based on the weighted distance metric constructed from the estimates. Extending the methods developed in Nguyen et al. (2020), Nguyen et al. (2021) study a distributionally robust contextual portfolio allocation problem where worst-case conditional return-risk tradeoff is computed over an optimal transport ambiguity set consisting of perturbations of the joint distribution of covariates and returns. Their approach generalizes the mean-variance and mean-CVaR model, for which the distributionally robust models are shown to be equivalent to semi-definite or secondorder cone representable programs. Esteban-Pe¬¥rez and Morales (2022) solve a DRO problem with a novel ambiguity set that is based on trimming the empirical conditional distribution, i.e., reducing the weights over the support points. The authors show the link between trimming a distribution and partial mass transportation problem, and an interested reader can refer to Esteban-Pe¬¥rez and Morales (2023) for an application in the optimal power flow problem.  

A distributionally robust extension of the rCSO model is presented in Kannan et al. (2021) and Kannan et al. (2022). It hedges against all distributions that lie in the $r$ radius of the (Wasserstein) ambiguity ball centered at the estimated distribution $\mathbb { P } ^ { \mathrm { E R } } ( { \pmb x } )$ . Perakis et al. (2023) propose a DRO model to solve a two-stage multi-item joint production and pricing problem with a partitionedmoment-based ambiguity set constructed by clustering the residuals estimated from an additive demand model.  

Zhu et al. (2022) considers an expected value-based model and suggests an ambiguity set that is informed by the estimation metric used to train $g _ { \hat { \pmb { \theta } } }$ . Namely, they consider:  

$$
\operatorname* { m i n } _ { z \in \mathcal { Z } } \operatorname* { s u p } _ { \theta \in \mathcal { U } ( \hat { \theta } , r ) } c ( z , g _ { \theta } ( x ) ) ,
$$  

with  

$$
\mathcal { U } ( \hat { \pmb { \theta } } , r ) : = \{ \pmb { \theta } \in \pmb { \theta } | \rho ( g _ { \pmb { \theta } } , \hat { \mathbb { P } } _ { N } ) \leq \rho ( g _ { \hat { \pmb { \theta } } } , \hat { \mathbb { P } } _ { N } ) + r \} .
$$  

They show how finite-dimensional convex reformulations can be obtained when $g _ { \pmb \theta } ( \pmb x ) : = \pmb \theta ^ { T } \pmb x ,$ and promote the use of a ‚Äúrobustness optimization‚Äù form.  

# 5. Integrated learning and optimization  

As discussed previously, ILO is an end-to-end framework that includes three components in the training pipeline: (i) a prediction model that maps the covariate to a predicted distribution (or possibly a point prediction), (ii) an optimization model that takes as input a prediction and returns a decision, and (iii) a task-based loss function that captures the downstream optimization problem. The parameters of the prediction model are trained to maximize the prescriptive performance of the policy, i.e., it is trained on the task loss incurred by this induced policy rather than the estimation loss.  

Next, we discuss several methods for implementing the ILO approach. We start by describing the different models that are used in ILO (Section 5.1), and then we present the algorithms used to perform the training. We divide the algorithms into four categories. Namely, training using unrolling (Section 5.2), implicit differentiation (Section 5.3), a surrogate differentiable loss function (Section 5.4), and a differentiable optimizer (Section 5.5). An overview of the methods presented in this section is given in Table 3.  

# 5.1. Models  

Bengio (1997) appears to be the first to train a prediction model using a loss that is influenced by the performance of an action prescribed by a conditional expected value-based decision rule. This was done in the context of portfolio management, where an investment decision rule exploits a point prediction of asset returns. Effective wealth accumulation is used to steer the predictor toward predictions that lead to good investments. More recent works attempt to integrate a full optimization model, rather than a rule, into the training pipeline. Next, we summarize how ILO is applied to the two types of contextual optimization models and introduce two additional popular task models that have been considered under ILO, replacing the traditional expected cost task.  

Expected value-based model. To this date, most of the literature has considered performing ILO on an expected value-based optimization model. Namely, following the notation presented in Definition 1 (Section 2.2.2), this training pipeline is interested in the loss $\mathcal { L } ( \pmb { \theta } ) : = H ( z ^ { \ast } ( \cdot , g _ { \pmb { \theta } } ) , \hat { \mathbb { P } } _ { N } ) =$ $\mathbb { E } _ { \hat { \mathbb { P } } _ { N } } [ c ( z ^ { * } ( { \pmb x } , g _ { \pmb \theta } ) , { \pmb y } ) ]$ with $g _ { \pmb { \theta } } ( \pmb { x } )$ as a point predictor for $_ { y , }$ which we interpret as a prediction of $\mathbb { E } [ { \pmb y } | { \pmb x } ]$ . This already raises challenges related to the non-convexity of the integrated loss function $\mathcal { L } ( \pmb { \theta } )$ and its differentiation with respect to $\pmb \theta$ :  

$$
\begin{array} { l } { { \displaystyle \nabla _ { \theta } \mathcal { L } ( \theta ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \nabla _ { \theta } c ( z ^ { * } ( x _ { i } , g _ { \theta } ) , y _ { i } ) } \ ~ } \\ { { \displaystyle ~ = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { d _ { z } } \sum _ { k = 1 } ^ { d _ { y } } \frac { \partial c ( z ^ { * } ( x _ { i } , g _ { \theta } ) , y _ { i } ) } { \partial z _ { j } } \left. \frac { \partial z _ { j } ^ { * } ( x _ { i } , \hat { y } ) } { \partial \hat { y } _ { k } } \right. _ { \hat { y } = g _ { \theta } ( x _ { i } ) } \nabla _ { \theta } [ g _ { \theta } ( x _ { i } ) ] _ { k } } } \end{array}
$$  

with $\frac { \partial z _ { j } ^ { * } ( \pmb { x } _ { i } , \pmb { \hat { y } } ) } { \partial \hat { y } _ { k } }$ as the most problematic evaluation. For instance, when ${ z } ^ { \ast } ( { x } _ { i } , { g } _ { \theta } )$ is the solution of a linear program (LP), it is well known that its gradient is either null or non-existent as it jumps between extreme points of the feasible polyhedron as the objective is perturbed.  

Conditional distribution-based model. In the context of learning a conditional distribution model $f _ { \boldsymbol { \theta } } ( \pmb { x } )$ , Donti et al. (2017) appear to be the first to study the ILO problem. They model the distribution of the uncertain parameters using parametric distributions (exponential and normal). For the newsvendor problem, it is shown that the ILO model outperforms decision rule optimization with neural networks and SLO with maximum likelihood estimation (MLE) when there is model misspecification. Since then, it has become more common to formulate the CSO problem as a weighted SAA model (as discussed in Section 4.1.2). The prediction model then amounts to identifying a vector of weights to assign to each historical sample. This approach is taken by Kallus and Mao (2022), who train a random forest regressor in an integrated fashion to assign weights, and by Grigas et al. (2021), who show how to train general differentiable models to predict the probabilities of an uncertain parameter $\pmb { y }$ with finite support.  

Table 3 Overview of contextual optimization papers in the ILO framework.   


<html><body><table><tr><td></td><td colspan="4">Objective</td><td colspan="2">Feasible domain</td><td colspan="3">Training</td></tr><tr><td></td><td>LP</td><td>QP</td><td>Convex</td><td>Non convex</td><td>Integer</td><td>Uncertain Implicit</td><td>diff.</td><td>Surr. loss</td><td>Surr. optim.</td></tr><tr><td>Amos and Kolter (2017)</td><td>X</td><td></td><td></td><td>X</td><td>X</td><td></td><td></td><td>X</td><td>v</td></tr><tr><td>Donti et al. (2017)</td><td>X</td><td></td><td>v</td><td>X</td><td>X</td><td></td><td></td><td>X</td><td>X</td></tr><tr><td>Agrawal et al. (2019)</td><td>X</td><td></td><td></td><td>X</td><td>X</td><td></td><td></td><td>√ó</td><td></td></tr><tr><td>Vlastelica et al. (2019)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td>√ó</td><td></td><td>X</td></tr><tr><td>Wilder et al. (2019a)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td></td><td>√ó</td><td>X</td></tr><tr><td>Wilder et al. (2019b)</td><td>X</td><td>V</td><td></td><td>X</td><td></td><td>√ó</td><td>X</td><td></td><td>X</td></tr><tr><td>Berthet et al. (2020)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td>X</td><td>√ó</td><td></td></tr><tr><td>Elmachtoub et al. (2020)</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td></td><td></td><td></td><td>X</td></tr><tr><td>Ferber et al. (2020)</td><td></td><td></td><td></td><td>X</td><td>V</td><td>X</td><td></td><td>X</td><td>√ó</td></tr><tr><td>Mandi and Guns (2020)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td></td><td>√ó</td><td>X</td></tr><tr><td>Mandi et al. (2020)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td>X</td><td></td><td>X</td></tr><tr><td>Grigas et al. (2021)</td><td>X</td><td></td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td></td></tr><tr><td>Mulamba et al. (2021)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td>X</td><td>√ó</td><td></td></tr><tr><td>Chung et al. (2022)</td><td>X</td><td></td><td></td><td>X</td><td></td><td>X</td><td>X</td><td></td><td>√ó</td></tr><tr><td>Cristian et al. (2022)</td><td>X</td><td>X</td><td></td><td>X</td><td></td><td>√ó</td><td>X</td><td>X</td><td></td></tr><tr><td>Dalle et al. (2022)</td><td></td><td>√ó</td><td>X</td><td>X</td><td></td><td>√ó</td><td></td><td>√ó</td><td></td></tr><tr><td>Elmachtoub and Grigas (2022)</td><td></td><td>√ó</td><td></td><td>X</td><td></td><td>√ó</td><td>X</td><td></td><td>√ó</td></tr><tr><td>Jeong et al. (2022)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td>X</td><td></td><td>X</td></tr><tr><td>Kallus and Mao (2022)</td><td>X</td><td>X</td><td>v</td><td>X</td><td>X</td><td>√ó</td><td>X</td><td></td><td>X</td></tr><tr><td>Kong et al. (2022)</td><td>X</td><td></td><td></td><td></td><td></td><td>√ó</td><td>X</td><td>√ó</td><td></td></tr><tr><td>Lawless and Zhou (2022)</td><td></td><td>X</td><td></td><td>X</td><td></td><td>√ó</td><td>X</td><td></td><td>X</td></tr><tr><td>Loke et al. (2022)</td><td></td><td>√ó</td><td></td><td>X</td><td>X</td><td>√ó</td><td>X</td><td></td><td>X</td></tr><tr><td>Mandi et al. (2022)</td><td></td><td>X</td><td>X</td><td>X</td><td></td><td>√ó</td><td>X</td><td>√ó</td><td></td></tr><tr><td>Munoz et al. (2022)</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td></td><td>X</td></tr><tr><td>Shah et al. (2022)</td><td></td><td></td><td>v</td><td>v</td><td></td><td>√ó</td><td>√ó</td><td>√ó</td><td></td></tr><tr><td>Butler and Kwon (2023a)</td><td>X</td><td></td><td>X</td><td>X</td><td>X</td><td>√ó</td><td></td><td>√ó</td><td>X</td></tr><tr><td>Costa and Iyengar (2023)</td><td>X</td><td></td><td></td><td>X</td><td>X</td><td></td><td></td><td></td><td>X</td></tr><tr><td>Estes and Richard (2023)</td><td></td><td>X</td><td></td><td>X</td><td>X</td><td>√ó</td><td>√ó</td><td></td><td>X</td></tr><tr><td>Kotary et al. (2023)</td><td></td><td></td><td></td><td></td><td>X</td><td>X</td><td></td><td>X</td><td>√ó</td></tr><tr><td>McKenzie et al. (2023)</td><td></td><td>√ó</td><td>√ó</td><td>√ó</td><td>√ó</td><td>X</td><td></td><td>√ó</td><td>X</td></tr><tr><td>Sun et al. (2023a)</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td><td>√ó</td><td>√ó</td><td></td><td>X</td></tr><tr><td>Sun et al. (2023b)</td><td>X</td><td></td><td>X</td><td></td><td>√ó</td><td>X</td><td></td><td>√ó</td><td>X</td></tr></table></body></html>

Notes: an approach has a ‚ÄúConvex‚Äù objective if it can handle general convex objective functions that are not linear or quadratic such as convex piecewise-linear objective functions; an ‚ÄúUncertain‚Äù feasible domain denotes that some constraints are subject to uncertainty. Implicit diff., surr. loss and surr. optim. denote implicit differentiation, surrogate differentiable loss function and surrogate differentiable optimizer, respectively. This table covers papers that introduced an ILO framework to solve a class of CSO problems, papers focused on deriving theoretical guarantees and specific applications are therefore intentionally excluded. The tick marks reflect our understanding of the claimed scope of the contributions.  

Regret minimization task. A recent line of work has tackled the ILO problem from the point of view of regret. Indeed, in Elmachtoub and Grigas (2022), a contextual point predictor $g _ { \pmb { \theta } } ( \pmb { x } )$ is learned by minimizing the regret associated with implementing the prescribed action based on the mean estimator $g _ { \pmb { \theta } } ( \pmb { x } )$ instead of based on the realized parameters $\boldsymbol { y }$ (a.k.a. the optimal hindsight or wait-and-see decision). Specifically, the value of an expected value-based policy $\pi _ { \boldsymbol { \theta } } ( \boldsymbol { x } ) : =$ $z ^ { * } ( x , g _ { \boldsymbol { \theta } } )$ is measured as the expected regret defined as:  

$$
H _ { \mathrm { R e g r e t } } ( \pi _ { \boldsymbol { \theta } } , \mathbb { P } ) : = \mathbb { E } _ { \mathbb { P } } [ c ( \pi _ { \boldsymbol { \theta } } ( \boldsymbol { x } ) , \boldsymbol { y } ) - c ( z ^ { * } ( \boldsymbol { x } , \boldsymbol { y } ) , \boldsymbol { y } ) ] .
$$  

Minimizing the expected regret returns the same optimal parameter vector $\pmb \theta$ as the ILO problem (9). This is due to the fact that:  

$$
H _ { \mathrm { R e g r e t } } ( \pi , \hat { \mathbb { P } } _ { N } ) = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } [ c ( \pi ( x ) , y ) - c ( z ^ { * } ( x , y ) , y ) ] = H ( \pi , \hat { \mathbb { P } } _ { N } ) - \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } [ c ( z ^ { * } ( x , y ) , y ) ] .
$$  

Hence, both $H _ { \mathrm { R e g r e t } } ( \pi , \hat { \mathbb { P } } _ { N } )$ and $H ( \pi , \hat { \mathbb { P } } _ { N } )$ have the same set of minimizers.  

Optimal action imitation task. ILO has some connections to inverse optimization, i.e., the problem of learning the parameters of an optimization model given data about its optimal solution (see Sun et al. 2023a, where both problems are addressed using the same method). Indeed, one can replace the original objective of ILO with an objective that seeks to produce a $z ^ { * } ( x , f _ { \theta } )$ that is as close as possible to the optimal hindsight action and, therefore, closer to the regret objective. Specifically, to learn a policy that ‚Äúimitates‚Äù the optimal hindsight action, one can first augment the data set with $\pmb { z } _ { i } ^ { * } : = \pmb { z } ^ { * } ( \pmb { x } _ { i } , \pmb { y } _ { i } )$ to get $\{ ( \pmb { x } _ { i } , \pmb { y } _ { i } , \pmb { z } _ { i } ^ { * } ) \} _ { i = 1 } ^ { N }$ . Thereafter, a prediction model $f _ { \theta } ( { \pmb x } )$ is learned in a way that the action $z ^ { * } ( x _ { i } , f _ { \pmb { \theta } } )$ is as close as possible to $\boldsymbol { z } _ { i } ^ { * }$ for all samples in the training set (Kong et al. 2022):  

$$
\begin{array} { r } { H _ { \mathrm { I m i t a t i o n } } ( \pi , \hat { \mathbb { P } } _ { N } ^ { \prime } ) : = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } ^ { \prime } } [ d ( \pi ( \boldsymbol { x } ) , z ^ { * } ) ] = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } [ d ( \pi ( \boldsymbol { x } ) , z ^ { * } ( \boldsymbol { x } , y ) ) ] } \end{array}
$$  

with $\hat { \mathbb { P } } _ { N } ^ { \prime }$ as the empirical distribution on the lifted tuple $( { \pmb x } , { \pmb y } , z ^ { * } ( { \pmb x } , { \pmb y } ) )$ based on the augmented data set and a distance function $d ( z , z ^ { * } )$ . We note that there is no reason to believe that the best imitator under a general distance function, e.g., $\| z - z ^ { * } \| _ { 2 } ,$ , performs well under our original metric $H ( \pi , \hat { \mathbb { P } } _ { N } )$ . One exception is for $d ( z , z ^ { * } ) : = c ( z , \pmb { y } ) - c ( z ^ { * } , \pmb { y } ) ,$ , where we allow the distance to also depend on $_ { y , }$ , for which we recover the regret minimization approach, and therefore the same solution as with $H ( \pi , \hat { \mathbb { P } } _ { N } )$ . Readers that have an interest in general inverse optimization methods should consult Chan et al. (2021) for an extensive recent review of the field.  

# 5.2. Training by unrolling  

An approach to obtain the Jacobian matrix $\frac { \partial z ^ { * } ( \pmb { x } , \hat { \pmb { y } } ) } { \partial \hat { \pmb { y } } }$ is unrolling (Domke 2012), which involves approximating the optimization problem with an iterative solver (e.g., first-order gradient-based method). Each operation is stored on the computational graph, which then allows, in principle, for computing gradients through classical back-propagation methods. Unfortunately, this approach requires extensive amounts of memory. Besides this, the large size of the computational graph exacerbates the vanishing and exploding gradient problems typically associated with training neural networks (Monga et al. 2021).  

# 5.3. Training using implicit differentiation  

Implicit differentiation allows for a memory-efficient backpropagation as opposed to unrolling (we refer to Bai et al. 2019, for discussion on training constant memory implicit models using a fixed-point ‚Äì FP ‚Äì equation and feedforward networks of infinite depths). Amos and Kolter (2017) appear to be the first to have employed implicit differentiation methods to train an ILO model, which they refer to as OptNet. They consider expected value-based optimization models that take the form of constrained quadratic programs (QP) with equality and inequality constraints. They show how the implicit function theorem (IFT ‚Äì Halkin 1974) can be used to differentiate $z ^ { * } ( x , g _ { \boldsymbol { \theta } } )$ with respect to $\pmb \theta$ using the Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions that are satisfied at optimality. Further, they provide a custom solver based on a primal-dual interior method to simultaneously solve multiple QPs on GPUs in batch form, permitting 100-times speedups compared to Gurobi and CPLEX. This approach is extended to conditional stochastic and strongly convex optimization models in Donti et al. (2017). They use sequential quadratic programming (SQP) to obtain quadratic approximations of the objective functions of the convex program at each iteration until convergence to the solution and then differentiate the last iteration of SQP to obtain the Jacobian. For a broader view of implicit differentiation, we refer to the surveys by Duvenaud et al. (2020) and Blondel et al. (2022).  

To solve large-scale QPs with linear equality and box inequality constraints, Butler and Kwon (2023a) use the ADMM algorithm to decouple the differentiation procedure for primal and dual variables, thereby decomposing the large problem into smaller subproblems. Their procedure relies on implicit differentiation of the FP equations of the alternating direction method of multipliers (ADMM) algorithm (ADMM-FP). They show that unrolling the iterations of the ADMM algorithm on the computational graph (Sun et al. 2016, Xie et al. 2019) results in higher computation time than ADMM-FP. Their empirical results on a portfolio optimization problem with 254 assets suggest that computational time can be reduced by a factor of almost five by using ADMM-FP compared to OptNet, mostly due to the use of the ADMM algorithm in the forward pass. Note that the experiments in Butler and Kwon (2023a) were conducted on a CPU.  

To extend OptNet to a broader class of problems, Agrawal et al. (2019) introduce cvxpylayers that relies on converting disciplined convex programs in the domain-specific language used by CVXPY into conic programs. They implicitly differentiate the residual map of the homogeneous self-dual embedding associated with the conic program.  

McKenzie et al. (2023) note that using KKT conditions for constrained optimization problems with DNN-based policies is computationally costly as ‚Äúcvxpylayers struggles with solving problems containing more than 100 variables‚Äù (see also Butler and Kwon 2023a). An alternative is to use projected gradient descent (PGD) where DNN-based policies are updated using an iterative solver and projected onto the constraint set $\mathcal { Z }$ at each iteration and the associated FP system (Donti et al. 2021, Chen et al. 2021, Blondel et al. 2022) is used to obtain the Jacobian.  

Since a closed-form solution for the projection onto $\mathcal { Z }$ is unavailable in many cases, the projection step may be costly, and in some cases, PGD may not even converge to a feasible point (Rychener et al. 2023). To avoid computing the projection in the forward pass, McKenzie et al. (2023) solve the expected value-based CSO problem using Davis-Yin operator splitting (Davis and Yin 2017) while the backward pass uses the Jacobian-free backpropagation (Fung et al. 2022) in which the Jacobian matrix is replaced with an identity matrix (see also Sahoo et al. 2023, where a similar approach is used for expected value-based models).  

To mitigate the issues with unrolling, Kotary et al. (2023) propose FP folding (fold-opt) that allows analytically differentiating the FP system of general iterative solvers, e.g., ADMM, SQP, and PGD. By unfolding (i.e., partial unrolling), some of the steps of unrolling are grouped in analytically differentiable update function $\mathcal { T } : \mathbb { R } ^ { d _ { \pmb { y } } }  \mathbb { R } ^ { d _ { \pmb { y } } }$ :  

$$
z _ { k + 1 } ( \pmb { x } , \hat { \pmb { y } } ) = T ( z _ { k } ( \pmb { x } , \hat { \pmb { y } } ) , \hat { \pmb { y } } ) .
$$  

Realizing that $z ^ { * } ( \pmb { x } , \hat { \pmb y } )$ is the FP of the above system, they use the IFT to obtain a linear system (a differential FP condition) that can be solved to obtain the Jacobian. This effectively decouples the forward and backward pass enabling the use of black box solvers like Gurobi for the forward pass while cvxpylayers is restricted to operator splitting solvers like ADMM. An added benefit of using fold-opt is that it can solve non-convex problems. In the case of portfolio optimization, the authors note that the superior performance of their model with respect to cvxpylayers can be explained by the precise calculations made in the forward pass by Gurobi.  

While speedups can be obtained for sparse problems, Sun et al. (2023b) remark that the complexity associated with differentiating the KKT conditions is cubic in the total number of decision variables and constraints in general. They propose an alternating differentiation framework (called Alt-Diff) to solve parameterized convex optimization problems with polyhedral constraints using ADMM that decouples the objective and constraints. This procedure results in a smaller Jacobian matrix when there are many constraints since the gradient computations for primal, dual, and slack variables are done alternatingly. The gradients are shown to converge to those obtained by differentiating the KKT conditions. The authors employ truncation of iterations to compensate for the slow convergence of ADMM when compared to interior-point methods and provide theoretical upper bounds on the error in the resulting gradients. Alt-Diff is shown to achieve the same accuracy with truncation and lower computational time when compared to cvxpylayers for an energy generation scheduling problem.  

Motivated by OptNet, several extensions have been proposed to solve linear and combinatorial problems. Wilder et al. (2019a) solve LP-representable combinatorial optimization problems and LP relaxations of combinatorial problems during the training phase. Their model, referred to as QPTL (Quadratic Programming Task Loss), adds a quadratic penalty term to the objective function of the linear problem. This has two advantages: it recovers a differentiable linear-quadratic program, and the added term acts as a regularizer, which might avoid overfitting. To solve a general mixed-integer LP (MILP), Ferber et al. (2020) develop a cutting plane method MIPaal, which adds a given number of cutting planes in the form of constraints $S z \leq s$ to the LP relaxation of the MILP. Instead of adding a quadratic term, Mandi and Guns (2020) propose IntOpt based on the interior point method to solve LPs that adds a log barrier term to the objective function and differentiates the homogeneous self-dual formulation of the LP. Their experimental analyses show that this approach performs better on energy cost-aware scheduling problems than QPTL using the data from Ifrim et al. (2012).  

Costa and Iyengar (2023) introduce an ILO framework with the weighted average of Sharpe ratio and MSE loss as a task loss and replace the optimization problem with a surrogate DRO problem. By using convex duality, they reformulate the minimax problem as a minimization problem and learn the parameters (e.g., size of ambiguity set) using implicit differentiation instead of cross-validation (CV). More specifically, the DRO model uses a deviation risk measure (e.g., variance) to control variability in the portfolio returns associated with the prediction errors $\epsilon _ { i } =$ ${ \pmb y } _ { i } - g _ { \theta } ( { \pmb x } _ { i } )$ :  

$$
\underset { \boldsymbol { z } } { \mathrm { a r g m i n } } \underset { \mathbb { Q } \in \mathcal { B } _ { r } ^ { \phi } ( \mathbb { P } _ { N } ) } { \mathrm { m a x } } \mathbb { E } _ { \mathbb { Q } } \left[ \left( \boldsymbol { \epsilon } ^ { \top } \boldsymbol { z } - \mathbb { E } _ { \mathbb { Q } } [ \boldsymbol { \epsilon } ^ { \top } \boldsymbol { z } ] \right) ^ { 2 } \right] ,
$$  

where the distribution of errors lies in $\phi$ -divergence (e.g., Hellinger distance) based ambiguity set $\mathcal { B } _ { r } ^ { \phi } ( \hat { \mathbb { P } } _ { N } ) = \{ \mathbb { Q } : \mathbb { E } _ { \hat { \mathbb { P } } } [ \phi ( \mathbb { Q } / \hat { \mathbb { P } } ) ] \leq r \}$ centered at $\begin{array} { r } { \hat { \mathbb { P } } _ { N } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \delta _ { \epsilon _ { i } } } \end{array}$ .  

For convex problems, the optimality conditions are given by KKT conditions, which can be represented as $F ( \pmb \theta , z ) = 0$ where $F : \mathbb { R } ^ { d _ { x } } \times \mathbb { R } ^ { d _ { z } }  \mathbb { R } ^ { m } ,$ , where $m$ is proportional to the number of constraints that define $\mathcal { Z }$ . From the classical IFT (Dontchev et al. 2009), we know that if $F$ is continuously differentiable and the Jacobian matrix with respect to $z$ , denoted by $\nabla _ { z } F ( \pmb \theta , z ( \pmb \theta ) ) .$ , is non-singular at the point $( { \bar { \pmb { \theta } } } , { \bar { \pmb { z } } } )$ , then there exists a neighborhood around $\bar { \pmb { \theta } }$ for which the gradient of the optimal solution with respect to the parameters is given by:  

$$
\frac { \partial z ^ { * } ( \pmb { \theta } ) } { \partial \pmb { \theta } } = - ( \nabla _ { z } F ( \pmb { \theta } , z ( \pmb { \theta } ) ) ) ^ { - 1 } \nabla _ { \pmb { \theta } } F ( \pmb { \theta } , z ( \pmb { \theta } ) ) .
$$  

When the Jacobian matrix $\nabla _ { z } F ( \pmb \theta , z ( \pmb \theta ) )$ is singular, classical IFT cannot be applied. This occurs in linear programs and can also arise in smooth QPs as shown in Bolte et al. (2021). Bolte et al. (2021) obtain a generalization of IFT to non-smooth functions using conservative Jacobians that generalize Clarke Jacobians (Clarke 1990) for locally Lipschitz function $F$ . They also derive conservative Jacobians for conic optimization layers (Agrawal et al. 2019).  

Further, Bolte et al. (2021) illustrate using cvxpylayers that in a bilevel program which is a composition of a quadratic function with the solution map of a linear program, gradient descent does not converge but gets stuck in a ‚Äúlimit cycle of non-critical points‚Äù even though invertibility condition does not hold only on a set of measure 0 (defined by a line) where the solution map moves from extreme point to another. As this example illustrates, the convergence of gradient methods based on IFT can be impacted by the non-invertibility of the Jacobian matrix and nonsmoothness which is difficult to verify a priori. As a result, research efforts have been directed toward designing surrogate loss functions and perturbation-based models for CSO problems that could circumvent the need to use the IFT.  

# 5.4. Training using a surrogate differentiable loss function  

As discussed in Section 5.1, minimizing directly the task loss in (9) or the regret in (13) is computationally difficult in most cases. For instance, the loss may be piecewise-constant as a function of the parameters of a prediction model and, thus, may have no informative gradient. To address this issue, several surrogate loss functions with good properties, e.g., differentiability and convexity, have been proposed to train ILO models.  

5.4.1. $\mathtt { S P O + }$ . In CSO problems, Elmachtoub and Grigas (2022) first tackle the potential nonuniqueness of $z ^ { * } ( x , g _ { \boldsymbol { \theta } } )$ by introducing a Smart ‚ÄúPredict, then Optimize‚Äù (SPO) model where the decision-maker chooses to minimize the empirical average of the regret under the worst-case optimal solution as defined below:  

$$
\begin{array} { r l } & { \underset { \pmb { \theta } } { \operatorname* { m i n } } \underset { \pi } { \operatorname* { m a x } } H _ { \mathrm { R e g r e t } } ( \pi , \hat { \mathbb { P } } _ { N } ) , } \\ & { \quad \mathrm { s . t . } ~ \pi ( \pmb { x } ) \in \underset { \pmb { z } \in \mathcal { Z } } { \operatorname { a r g m i n } } c ( \boldsymbol { z } , g _ { \boldsymbol { \theta } } ( \pmb { x } ) ) , \forall \pmb { x } . } \end{array}
$$  

In the expected value-based model, they show that the SPO objective reduces to training the prediction model according to the ERM problem:  

$$
\pmb { \theta } ^ { \star } \in \underset { \pmb { \theta } } { \mathrm { a r g m i n } } \rho _ { \mathbf { S P 0 } } ( g _ { \pmb { \theta } } , \hat { \mathbb { P } } _ { N } ) : = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } \left[ \ell _ { \mathbf { S P 0 } } ( g _ { \pmb { \theta } } ( \pmb { x } ) , \pmb { y } ) \right] ,
$$  

with:  

$$
\ell _ { \mathtt { S P O } } ( \hat { y } , y ) : = \operatorname* { s u p } _ { \bar { z } \in \mathrm { a r g m i n } _ { z \in \mathcal { Z } } ^ { c ( z , \hat { y } ) } } c ( \bar { z } , y ) - c ( z ^ { * } ( x , y ) , y ) .
$$  

Since the SPO loss function is nonconvex and discontinuous in $\hat { \pmb y }$ (Ho-Nguyen and Kƒ±lƒ±n¬∏c-Karzan 2022, Lemma 1), Elmachtoub and Grigas (2022) focus on the linear objective $c ( \boldsymbol { z } , \boldsymbol { y } ) : = \boldsymbol { y } ^ { T } \boldsymbol { z }$ and replace the SPO loss with a convex upper bound called $\tt s e o +$ :  

$$
\ell _ { \mathtt { S P } 0 + } ( \hat { y } , y ) : = \operatorname* { s u p } _ { z \in \mathcal { Z } } ( y - 2 \hat { y } ) ^ { T } z + 2 \hat { y } ^ { T } z ^ { * } ( x , y ) - y ^ { T } z ^ { * } ( x , y ) ,
$$  

which has a closed-form expression for its subgradient  

$$
2 \left( z ^ { * } ( \pmb { x } , \pmb { y } ) - z ^ { * } ( \pmb { x } , 2 \hat { \pmb { y } } - \pmb { y } ) \right) \in \nabla _ { \hat { \pmb { y } } } \ell _ { \pmb { \mathsf { S P 0 + } } } ( \hat { \pmb { y } } , \pmb { y } ) .
$$  

Loke et al. (2022) propose a decision-driven regularization model (DDR) that combines prediction accuracy and decision quality in a single optimization problem with loss function as follows:  

$$
\ell _ { \mathtt { D D R } } ( \hat { \pmb y } , \pmb y ) = d ( \hat { \pmb y } , \pmb y ) - \lambda \operatorname* { m i n } _ { z \in \mathcal { Z } } \{ \mu \pmb { y } ^ { \top } \pmb z + ( 1 - \mu ) \hat { \pmb y } ^ { \top } \pmb z \} ,
$$  

and $\mathtt { s p o + }$ being a special case with $\mu = - 1$ , $\lambda = 1$ , and $\begin{array} { r } { d ( \hat { \pmb y } , \pmb y ) = 2 \hat { \pmb y } ^ { \top } \boldsymbol z ^ { * } ( \pmb x , \pmb y ) - \pmb y ^ { T } \boldsymbol z ^ { * } ( \pmb x , \pmb y ) . } \end{array}$  

SPO+ for combinatorial problems. Evaluating the gradient of $\tt s e o +$ loss in (16) requires solving the optimization problem (8) to obtain $z ^ { * } ( { \pmb x } , 2 \hat { { \pmb y } } - { \pmb y } )$ for each data point. This can be computationally demanding when the optimization model in (8) is an NP-hard problem. Mandi et al. (2020) propose a SPO-relax approach that computes the gradient of $\mathtt { s p o + }$ loss by solving instead a continuous relaxation when (8) is a MILP. They also suggest speeding up the resolution using a warm-start for learning with a pre-trained model that uses MSE as the loss function. Another way proposed to speed up the computation is warm-starting the solver. For example, $z ^ { * } ( \pmb { x } , \pmb { y } )$ can be used as a starting point for MILP solvers or to cut away a large part of the feasible space. Mandi et al. (2020) show that for weighted and unweighted knapsack problems as well as energycost aware scheduling problems (CSPLib, Problem 059, Simonis et al. 2014), SPO-relax results in faster convergence and similar performance compared to $\mathtt { s p o + }$ loss. Also, SPO-relax provides low regret solutions and faster convergence compared to QPTL in the aforementioned three problems, except in the weighted knapsack problem with low capacity.  

With a focus on exact solution approaches, Jeong et al. (2022) study the problem of minimizing the regret in (13) assuming a linear prediction model $g _ { \pmb { \theta } } ( \pmb { x } ) = \pmb { \theta } \pmb { x }$ with $\pmb { \theta } \in \mathbb { R } ^ { d \pmb { z } \times d \pmb { x } }$ . Under the assumption that $z ^ { * } ( x , g _ { \boldsymbol { \theta } } )$ is unique for all $\pmb \theta$ and $\scriptstyle { \mathbf { { x } } } ,$ the authors reformulate the bilevel SPO problem as a single-level MILP using symbolic variable elimination. They show that their model can achieve up to two orders of magnitude improvement in expected regret compared to $\mathtt { s p o + }$ on the training set. Mu Àúnoz et al. (2022) applies a similar idea of representing the set of optimal solutions with a MILP. They rely on the KKT conditions of the problem defining $z ^ { * } ( x , g _ { \boldsymbol { \theta } } )$ to transform the bilevel integrated problem into a single-level MILP. Finally, Estes and Richard (2023) use the SPO loss function to solve a two-stage LP with right-hand side uncertainty. They propose a lexicographical ordering rule to select the minimal solution when there are multiple optima and approximate the resulting piecewise-linear loss function, lex-SPO, by a convex surrogate to find the point predictor.  

SPO Trees. Elmachtoub et al. (2020) propose a model (SPOT) to construct decision trees that segment the covariates based on the SPO loss function while retaining the interpretability in the endto-end learning framework. Their model outperforms classification and regression trees (CART) in the numerical experiments on a news recommendation problem using a real-world dataset and on the shortest path problem with synthetic data (also used in Elmachtoub and Grigas (2022)).  

Guarantees. Elmachtoub and Grigas (2022) show that under certain conditions, the minimizers of the SPO loss, $\mathtt { S P O + }$ loss and MSE loss are almost always equal to $\mathbb { E } _ { \mathbb { P } ( { \pmb y } | { \pmb x } ) } [ { \pmb y } ]$ given that $\mathbb { E } _ { \mathbb { P } ( \pmb { y } | \pmb { x } ) } [ \pmb { y } ] \in \mathcal { H }$ . Thus, $\mathtt { S P O + }$ is Fisher consistent (see Definition 4 in Appendix A) with respect to the SPO loss. This means that minimizing the surrogate loss also minimizes the true loss function. Ho-Nguyen and Kƒ±lƒ±n¬∏c-Karzan (2022) show that for some examples of a multiclass classification problem, $\mathtt { s p o + }$ is Fisher inconsistent, while MSE loss is consistent. However, complete knowledge of the distribution is a limitation in practice where the decision-maker has access to only the samples from the distribution. As a result, Ho-Nguyen and Kƒ±lƒ±n¬∏c-Karzan (2022) and Liu and Grigas (2021) provide calibration bounds that hold for a class of distributions $\mathcal { D }$ on $\mathcal { X } \times \mathcal { Y }$ and ensure that a lower excess risk of predictor for MSE and $\mathtt { S P O + } ,$ , respectively, translates to lower excess SPO risk (see Definition 5 in Appendix A).  

In many ML applications, one seeks to derive finite-sample guarantees, which are given in the form of a generalization bound, i.e., an upper bound on the difference between the true risk of a loss function and its empirical risk estimate for a given sample size $N$ . A generalization bound for the SPO loss function is given in El Balghiti et al. (2022) (extension of El Balghiti et al. 2019) based on Rademacher complexity (see Definition 6 in Appendix A) of the SPO loss composed with the prediction functions $g _ { \pmb { \theta } } \in \mathcal { H }$ . More specifically, the bound achieved in El Balghiti et al. (2019) is $\mathrm { O } \left( { \sqrt { \frac { \log ( N ) } { N } } } \right) ,$ , and tighter bounds with respect to action and feature dimension are obtained using SPO function‚Äôs structure and if $\mathcal { Z }$ satisfies a ‚Äústrength‚Äù property. Hu et al. (2022) show that for linear CSO problems, the generalization bound for MSE loss and SPO loss is $\mathrm { O } \left( { \sqrt { \frac { 1 } { N } } } \right)$ while faster convergence rates for the SLO model compared to ILO model are obtained under certain low-noise assumptions. Elmachtoub et al. (2023) show that for non-linear optimization problems, SLO models stochastically dominate ILO in terms of their asymptotic optimality gaps when the hypothesis class covers the true distribution. When the model is misspecified, they show that ILO outperforms SLO asymptotically in a general nonlinear setting.  

5.4.2. Surrogate loss for a stochastic forest. Kallus and Mao (2022) propose an algorithm called StochOptForest, which generalizes the random-forest based local parameter estimation procedure in Athey et al. (2019). A second-order perturbation analysis of stochastic optimization problems allows them to scale to larger CSO problems since they can avoid solving an optimization problem at each candidate split. The policies obtained using their model are shown to be asymptotically consistent, and the benefit of ILO is illustrated by comparing their approach to the random forests of Bertsimas and Kallus (2020) on a set of problems with synthetic and real-world data.  

5.4.3. Other surrogates. Wilder et al. (2019b) introduce ClusterNet to solve hard combinatorial graph optimization problems by learning incomplete graphs. The model combines graph convolution networks to embed the graphs in a continuous space and uses a soft version of k-means clustering to obtain a differential proxy for the combinatorial problems, e.g., community detection and facility location. Numerical experiments on a synthetic data set show that ClusterNet outperforms the two-stage SLO approach of first learning the graph and then optimizing, as well as other baselines used in community detection and facility location.  

Focusing on combinatorial problems, Vlastelica et al. (2019) propose a differentiable black-box (DBB) approach to tackle the issue that the Jacobian of $z ^ { * } ( x , g _ { \boldsymbol { \theta } } )$ is zero almost everywhere by approximating the true loss function using an interpolation controlled in a way that balances between ‚Äúinformativeness of the gradient‚Äù and ‚Äúfaithfulness to the original function‚Äù. Algorithmically, this is done by perturbing the prediction $g _ { \pmb { \theta } } ( \pmb { x } )$ in the direction $\nabla _ { z } c ( z ^ { * } ( \pmb { x } , g _ { \pmb { \theta } } ) , \pmb { y } )$ and obtaining a gradient of the surrogate loss based on the effect of this perturbation on the resulting perturbed action.  

Chung et al. (2022) introduce a computationally tractable ILO model to solve non-linear CSO problems. Using the first-order Taylor expansion of the task loss around the prediction, they introduce a reweighted MSE loss function where weights are determined by taking the gradient of task loss with respect to the prediction. To solve a large-scale multi-facility inventory allocation problem with few samples for each facility, they use a single random forest that can predict the demand across facilities and products. Assuming that each tree in the random forest provides an independent and identically distributed realization of the uncertain parameter, they obtain the conditional distribution of uncertain parameter, $\begin{array} { r } { f _ { \pmb { \theta } _ { 0 } } = \frac { 1 } { T } \sum _ { t = 1 } ^ { T } \delta _ { \hat { \pmb { y } } ^ { t } } , } \end{array}$ , where $\hat { \boldsymbol y } ^ { t }$ is the prediction of tree $t$ . For each feature $\pmb { x } _ { i }$ and conditional distribution $f _ { \pmb { \theta } _ { 0 } }$ , they obtain an optimal allocation, $z _ { i } ^ { j } =$ $z ^ { * } ( x , f _ { \theta _ { 0 } } )$ for facility $j$ that minimizes the average unmet demand. In the last step, they retrain the random forest to minimize the reweighted MSE loss function:  

$$
\underset { \theta } { \operatorname { a r g m i n } } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } \sum _ { t = 1 } ^ { T } \mathbb { 1 } \left[ \hat { y } _ { i } ^ { t , j } \geq s _ { i } ^ { j } + z _ { i } ^ { j } \right] | f _ { \theta } ( \pmb { x } _ { i } ) - \hat { y } _ { i } ^ { t , j } | ,
$$  

where $M$ is the number of facilities, $\hat { y } _ { i } ^ { t , j }$ and $s _ { i } ^ { j }$ denote the demand and inventory levels, respectively, at facility $j$ . The above model (17) solves the optimization problem once during training, and is shown to be scalable for a medical allocation problem in Sierra Leone when compared to Kallus and Mao (2022) where splitting of the feature space is done based on the task loss.  

Lawless and Zhou (2022) introduce a loss function similar to Chung et al. (2022) that weighs the prediction error with a regret term as follows:  

$$
d ( g _ { \theta } ( x ) , y ) = [ c ( z ^ { \ast } ( x , g _ { \theta } ) , y ) - c ( z ^ { \ast } ( x , y ) , y ) ] ( y - g _ { \theta } ( x ) ) ^ { 2 } .
$$  

Learning optimal $\pmb \theta$ from the above formulation involves an argmin differentiation. So, the authors provide a two-step polynomial time algorithm to approximately solve the above problem. It first computes a pilot estimator $g _ { \pmb { \theta } _ { 0 } }$ by solving (7) with $d ( g _ { \theta } ( x ) , y ) = ( g _ { \theta } ( x ) - y ) ^ { 2 }$ and then solving (7) with the distance function in (18) where $c ( z ^ { * } ( x , g _ { \boldsymbol { \theta } } ) , \mathbf { y } )$ is substituted with $c ( z ^ { * } ( \pmb { x } , g _ { \pmb { \theta } _ { 0 } } ) , \pmb { y } )$ . The authors show that their simple algorithm performs comparably to $\mathtt { S P O + }$ .  

We conclude this subsection on surrogate loss functions by mentioning the efforts in Sun et al. (2023a) to learn a cost point estimator (in an expected value-based model) to imitate the hindsight optimal solution. This is done by designing a surrogate loss function that penalizes how much the optimal basis optimality conditions are violated. They derive generalization error bounds for this new loss function and employ them to provide a bound on the sub-optimality of the minimal $\pmb \theta$ .  

# 5.5. Training using a surrogate differentiable optimizer  

5.5.1. Differentiable perturbed optimizer. One way of obtaining a differentiable optimizer is to apply a stochastic perturbation to the parameters predicted by the ML model. Taking the case of expected value-based models as an example, the key idea is that although the gradient of the solution of the contextual problem with respect to the predicted parameters $\hat { \pmb { y } } : = g _ { \pmb { \theta } } ( \pmb { x } )$ is zero almost everywhere, if we perturb the predictor using a noise with differentiable density, then the expectation of the solution of the perturbed contextual problem,  

$$
\bar { z } ^ { \varepsilon } ( x , g _ { \theta } ) = \mathbb { E } _ { \Psi } \big [ \tilde { z } ^ { \varepsilon } ( x , g _ { \theta } , \Psi ) \big ] \ \mathrm { w i t h } \ \tilde { z } ^ { \varepsilon } ( x , g _ { \theta } , \Psi ) : = \underset { z \in \mathcal { Z } } { \mathrm { a r g m i n } } c ( z , g _ { \theta } ( x ) + \varepsilon \Psi ) ,
$$  

where $\varepsilon > 0$ controls the amount of perturbation, and more generally of the expected cost of the associated random policy $\mathbb { E } _ { \Psi } [ H ( \tilde { z } ^ { \varepsilon } ( \cdot , g _ { \theta } , \Psi ) , \hat { \mathbb { P } } _ { N } ) ]$ can be shown to be smooth and differentiable. This idea is proposed and exploited in Berthet et al. (2020), which focus on a bi-linear cost $c ( \boldsymbol { z } , \boldsymbol { y } ) : = \boldsymbol { y } ^ { T } \boldsymbol { z }$ thus simplifying $\mathbb { E } _ { \Psi } [ H ( \tilde { z } ^ { \varepsilon } ( \cdot , g _ { \theta } , \Psi ) , \hat { \mathbb { P } } _ { N } ) ] = H ( \bar { z } ^ { \varepsilon } ( \cdot , g _ { \theta } ) , \hat { \mathbb { P } } _ { N } )$ . Further, they show that when an imitation ILO model is used with a special form of Bregman divergence to capture the difference between $z ^ { * } ( \pmb { x } , \pmb { y } )$ and $\tilde { z } ^ { \varepsilon } ( \boldsymbol x , \hat { \boldsymbol y } , \Psi ) .$ , the gradient of $H _ { \mathrm { I m i t a t i o n } } ( \tilde { z } ^ { \varepsilon } ( \cdot , g _ { \theta } , \Psi ) , \hat { \mathbb { P } } _ { N } ^ { \prime } )$ can be computed directly without needing to determine the Jacobian of $\bar { z } ^ { \varepsilon } ( x , g _ { \theta } )$ (Blondel et al. 2020):  

$$
H _ { \mathrm { I m i t a t i o n } } ( \tilde { z } ^ { \varepsilon } ( \cdot , g _ { \theta } , \Psi ) , \hat { \mathbb { P } } _ { N } ^ { \prime } ) : = \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } [ \ell _ { \mathtt { P F X L } } ( g _ { \theta } ( x ) , y ) ]
$$  

where $\ell _ { \tt P E Y L }$ is a perturbed Fenchel-Young loss (PFYL) given by:  

$$
\ell _ { \mathtt { P F T L } } ( \hat { y } , y ) : = \hat { y } ^ { T } z ^ { * } ( x , y ) - \mathbb { E } _ { \Psi } [ ( \hat { y } + \varepsilon \Psi ) ^ { T } \tilde { z } ^ { \varepsilon } ( x , \hat { y } , \Psi ) ] + \varepsilon \Omega _ { \mathtt { P F T L } } ( z ^ { * } ( x , y ) ) ,
$$  

and $\Omega _ { \tt P F Y L } ( z )$ is the Fenchel dual of $\begin{array} { r } { F ( \pmb { y } ) : = - \mathbb { E } _ { \Psi } [ ( \pmb { y } + \Psi ) ^ { T } \tilde { z } ^ { \varepsilon } ( \pmb { x } , \pmb { y } , \Psi ) ] } \end{array}$ . The gradient of the FenchelYoung loss with respect to the model prediction is given by:  

$$
\nabla _ { \hat { \pmb { y } } } \ell _ { \mathtt { P F T L } } ( \hat { \pmb { y } } , \pmb { y } ) = z ^ { * } ( \pmb { x } , \pmb { y } ) - \bar { z } ^ { \varepsilon } ( \pmb { x } , \hat { \pmb { y } } ) .
$$  

Evaluating this gradient can be done through Monte Carlo evaluations by sampling perturbations and solving the corresponding perturbed problems.  

Dalle et al. (2022) introduce a multiplicative perturbation with the advantage that it preserves the sign of $g _ { \theta } ( \pmb x )$ without adding any bias:  

$$
\tilde { z } ^ { \varepsilon } ( x , g _ { \theta } , \Psi ) : = \underset { z \in \mathcal { Z } } { \mathrm { a r g m i n } } c ( z , g _ { \theta } ( x ) \odot \exp ( \varepsilon \Psi - \varepsilon ^ { 2 } / 2 ) ) ,
$$  

where $\odot$ is the Hadamard dot-product and the exponential is taken elementwise. Dalle et al. (2022) and Sun et al. (2023c) also show that there is a one-to-one equivalence between the perturbed optimizer approach and using a regularized randomized version of the CSO problem for combinatorial problems with linear objective functions. Finally, Dalle et al. (2022) show an intimate connection between the perturbed minimizer approach proposed by Berthet et al. (2020) and surrogate loss functions approaches such as $\mathtt { s p o + }$ by casting them as special cases of a more general surrogate loss formulation.  

Mulamba et al. (2021) and Kong et al. (2022) consider an ‚Äúenergy-based‚Äù perturbed optimizer defined by its density of the form:  

$$
\widetilde { z } ^ { \varepsilon } ( \mathbf { \boldsymbol { x } } , f _ { \theta } ) \sim \frac { \exp ( - h ( z , f _ { \theta } ( \mathbf { \boldsymbol { x } } ) ) / \varepsilon ) } { \int \exp ( - h ( z ^ { \prime } , f _ { \theta } ( \mathbf { \boldsymbol { x } } ) ) / \varepsilon ) d z ^ { \prime } } ,
$$  

with $\varepsilon = 1$ , in the context of an imitation ILO problem. This general form of perturbed optimizer captures a varying amount of perturbation through $\varepsilon _ { \scriptscriptstyle  { - } }$ , with $\tilde { \boldsymbol { z } } ^ { \varepsilon } ( \boldsymbol { x } , f _ { \theta } )$ converging in distribution to $z ^ { * } ( x , f _ { \theta } )$ as $\varepsilon$ goes to zero. They employ the negative log-likelihood to measure the divergence between $\tilde { \pmb { z } } ^ { \varepsilon } ( \pmb { x } , f _ { \pmb { \theta } } )$ and the hindsight optimal solution $z ^ { * } ( \pmb { x } , \pmb { y } )$ . Given the difficulties associated with calculating the partition function in the denominator of (19), Mulamba et al. (2021) devise a surrogate loss function based on noise-contrastive estimation, which replaces likelihood with relative likelihood when compared to a set of sampled suboptimal solutions. This scheme is shown to improve the performance over $\mathtt { s p o + }$ and DBB in terms of expected regret performance for linear combinatorial CSO.  

Based on the noise contrastive estimation approach of Mulamba et al. (2021), Mandi et al. (2022) note that ILO for combinatorial problems can be viewed as a learning-to-rank problem. They propose surrogate loss functions, with closed-form expressions for gradients, that are used to train to rank feasible points in terms of performance on the downstream optimization problem. Unlike Mulamba et al. (2021), Kong et al. (2022) tackles the partition function challenge by employing a self-normalized importance sampler that provides a discrete approximation. To avoid overfitting, the authors also introduce a regularization that penalizes the KL divergence between the perturbed optimizer distribution and a subjective posterior distribution over perturbed optimal hindsight actions $\mathbb { P } ( \tilde { \pmb { z } } ^ { \varepsilon } ( \pmb { x } , \pmb { y } ) | \pmb { y } )$ :  

$$
\begin{array} { r l } & { I _ { \mathrm { I m i t a t i o n } } ( \tilde { z } ^ { \varepsilon } ( \cdot , f _ { \theta } ) , \hat { \mathbb { P } } _ { N } ^ { \prime } ) : = } \\ & { \qquad - \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } [ \log ( \mathbb { P } ( \tilde { z } ^ { \varepsilon } ( x , f _ { \theta } ) = z ^ { * } ( x , y ) ) | x , y ) ] + \lambda \mathbb { E } _ { \hat { \mathbb { P } } _ { N } } [ \mathrm { K L } ( \mathbb { P } ( \tilde { z } ^ { \varepsilon } ( x , y ) | y ) | | \tilde { z } ^ { \varepsilon } ( x , y ) ) ] } \end{array}
$$  

The authors show that their model outperforms ILO trained using SQP and cvxpylayers in terms of computational time and gives lower task loss than sequential models trained using MLE and policy learning with neural networks.  

5.5.2. Supervised learning. Grigas et al. (2021) solve a CSO problem with a convex and nonnegative decision regularizer $\Omega ( z )$ assuming that the uncertain parameter $\pmb { y }$ has discrete support. Their model, called ICEO- $\therefore \lambda ,$ is thus trained by solving:  

$$
\begin{array} { r l } { ( { \tt I C E O - } \lambda ) \qquad } & { \underset { \theta } { \mathrm { m i n } } \quad H ( z _ { \lambda } ^ { * } ( \cdot , f _ { \theta } ) , \hat { \mathbb { P } } _ { N } ) + \lambda { \mathbb E } _ { \hat { \mathbb { P } } _ { N } } [ \Omega ( z _ { \lambda } ^ { * } ( x , f _ { \theta } ) ) ] } \\ & { \mathrm { s . t . } \quad z _ { \lambda } ^ { * } ( x , f _ { \theta } ) = \underset { z } { \mathrm { a r g m i n } } c ( z , f _ { \theta } ( x ) ) + \lambda \Omega ( z ) , \forall x . } \end{array}
$$  

The regularization ensures uniqueness and Lipschitz property of $z _ { \lambda } ^ { * } ( x , f _ { \theta } )$ with respect to $f _ { \theta }$ and leads to finite-sample guarantees. To circumvent the challenge associated with nondifferentiability of $z _ { \lambda } ^ { * } ( x , f _ { \theta } )$ with respect to $\pmb \theta$ , they replace $z _ { \lambda } ^ { * } ( x , f _ { \theta } )$ with a smooth approximation $\tilde { z } _ { \lambda } ( x , f _ { \theta } )$ that is learned using a random data set $( p _ { i } , z _ { i } )$ generated by sampling ${ \pmb p } _ { i }$ from the probability simplex over the discrete support and then finding the optimal solution $z _ { i }$ . They show asymptotic optimality and consistency of their solutions when the hypothesis class is wellspecified. They compare their approach to other ILO pipelines and to the SLO approach that estimates the conditional distribution using cross-entropy.  

Cristian et al. (2022) introduce the ProjectNet model to solve uncertain constrained linear programs in an end-to-end framework by training an optimal policy network, which employs a differentiable approximation of the step of projection to feasibility.  

Another approach, related to Berthet et al. (2020), that generalizes beyond LPs is given in Shah et al. (2022) that constructs locally optimized decision losses (LODL) with supervised learning to directly evaluate the performance of the predictors on the downstream optimization task. To learn a convex LODL for each data point, this approach first generates labels in the neighborhood of label $\pmb { y } _ { i }$ in the training set, e.g., by adding Gaussian noise, and then chooses the parameter that minimizes the MSE between LODL and the downstream task loss. The LODL is used in place of the task-specific surrogate optimization layers and outperforms SLO on three resource allocation problems (linear top-1 item selection problem, web advertising, and portfolio optimization). The numerical experiments indicate that handcrafted surrogate functions only perform better for the web advertising problem.  

# 5.6. Applications  

In this subsection, we discuss the applications of the ILO framework to a wide range of real-world problems.  

Tian et al. (2023a) and Tian et al. (2023b) use SPOT and noise-contrastive estimation method (Mulamba et al. 2021), respectively, to solve the maritime transportation problem. A comprehensive tutorial on prescriptive analytics methods for logistics is given in Tian et al. (2023c). SPO has been used in solving last-mile delivery (Chu et al. 2023) and ship inspection problems (Yan et al. 2020, 2021, 2023). Demirovi¬¥c et al. (2019) and Demirovi¬¥c et al. (2020) minimize the same expected regret as SPO for specific applications related to ranking optimization and dynamic programming problems, respectively.  

Perrault et al. (2020) solve a Stackelberg security game with the ILO framework by learning the attack probability distribution over a discrete set of targets to maximize a surrogate for the defender‚Äôs expected utility. They show that their model results in higher expected utility for the defender on synthetic and human subjects data than the sequential models that learn the attack probability by minimizing the cross entropy loss. Wang et al. (2020) replace the large-scale optimization problem with a low dimensional surrogate by reparameterizing the feasible space of decisions. They observe significant performance improvements for non-convex problems compared to the strongly convex case.  

Stratigakos et al. (2022) solve an integrated forecasting and optimization model for trading in renewable energy that trains an ensemble of prescriptive trees by randomly splitting the feature space $\chi$ based on the task-specific cost function. Sang et al. (2022) introduce an ILO framework for electricity price prediction for energy storage system arbitrage. They present a hybrid loss function to measure prediction and decision errors and a hybrid stochastic gradient descent learning method. Sang et al. (2023) solve a voltage regulation problem using a similar hybrid loss function, and backpropagation is done by implicitly differentiating the optimality conditions of a secondorder cone program.  

Liu et al. (2023b) use a DNN to model the routing behavior of users in a transportation network and learn the parameters by minimizing the mismatch between the flow prescribed by the variational inequality and the observed flow. The backward pass is obtained by applying the IFT to the variational inequality. Wahdany et al. (2023) propose an integrated model for wind-power forecasting that learns the parameters of a neural network to optimize the energy system costs under the system constraints. Vohra et al. (2023) apply similar ideas to develop end-to-end renewable energy generation forecasts, using multiple contextual sources such as satellite images and meteorological time series.  

Butler and Kwon (2023b) solves the contextual mean-variance portfolio (MVP) optimization problem by learning the parameters of the linear prediction model using the ILO framework. The covariance matrix is estimated using the exponentially weighted moving average model. They provide analytical solutions to unconstrained and equality-constrained MVP optimization problems and show that they outperform SLO models based on ordinary least squares regression. These analytical solutions lead to lower variance when compared with the exact solutions of the corresponding inequality-constrained MVP optimization problem.  

# 6. Active research directions  

We now summarize active and future research directions for further work in contextual optimization.  

Uncertainty in constraints. Most studies on contextual optimization assume that there is no uncertainty in the constraints. If constraints are also uncertain, the SAA solutions that ignore the covariates information might not be feasible (Rahimian and Pagnoncelli 2022). Bertsimas and Kallus (2020) have highlighted the challenges in using ERM in a constrained CSO problem. Rahimian and Pagnoncelli (2022) solve a conditional chance-constrained program that ensures with a high probability that the solution remains feasible under the conditional distribution given the realized covariates. Although they do not focus on contextual optimization, interesting links can be found with the literature on constraint learning (Fajemisin et al. 2023) and inverse optimization (Chan et al. 2021).  

Risk aversion. There has been a growing interest in studying contextual optimization in the risk-averse setting. Specifically, one can consider replacing the risk-neutral expectation from (1) with a risk measure such as value-at-risk. By doing so, one would expect, with a high probability, that a decision-maker‚Äôs loss is lower than a particular threshold. One can easily represent such a risk measure using an uncertainty set which represents the set of all possible outcomes that may occur in the future. The resulting uncertainty set should be carefully chosen. It should capture the most relevant scenarios to balance the trade-off between avoiding risks and obtaining returns. The recently proposed Conditional Robust Optimization (CRO) paradigm by Chenreddy et al. (2022) (see also Ohmori 2021, Sun et al. 2023b, PerÀásak and Anjos 2023) consists in learning a conditional set $\mathcal { U } ( \pmb { x } )$ to solve the following problem:  

$$
\operatorname* { m i n } _ { z \in \mathcal { Z } } \operatorname* { m a x } _ { \pmb { y } \in \mathcal { U } ( \pmb { x } ) } c \big ( \pmb { z } , \pmb { y } \big ) ,
$$  

where $\mathcal { U } ( \pmb { x } )$ is an uncertainty set designed to contain with high probability the realization of $\pmb { y }$ conditionally on observing $\scriptstyle { \mathbf { { \vec { x } } } }$ . Their approach solves the CRO problem sequentially where $\mathcal { U } ( \pmb { x } )$ is learned first and is subsequently used to solve the downstream RO problem. A challenging problem is to learn the uncertainty set to minimize the downstream cost function.  

Toolboxes and benchmarking. Several toolboxes and packages have been proposed recently to train decision pipelines. Agrawal et al. (2019) provide the cvxpylayers library, which includes a subclass of convex optimization problems as differentiable layers in auto-differentiation libraries in PyTorch, TensorFlow, and JAX. Other libraries for differentiating non-linear optimization problems for end-to-end learning include higher (Grefenstette et al. 2019), JAXopt (Blondel et al. 2022), TorchOpt (Ren et al. 2022), and Theseus (Pineda et al. 2022). Tang and Khalil (2022) introduce an open-source software package called PyEPO (Pytorch-based End-to-End Predict-then-Optimize) implemented in Python for ILO of problems that are linear in uncertain parameters. They implement various existing methods, such as $\mathtt { S P O + }$ , DBB, and PFYL. They also include new benchmarks and comprehensive experiments highlighting the advantages of integrated learning. Dalle et al. (2022) provide similar tools for combinatorial problems in Julia.  

Comparisons of existing approaches in fixed simulation settings are scarce, especially with realworld data. Buttler et al. (2022) provide a meta-analysis of selected methods on an unconstrained newsvendor problem on four data sets from the retail and food sectors. They highlight that there is no single method that clearly outperforms all the others on the four data sets. Mandi et al. (2023) carried out a comprehensive benchmarking of ILO frameworks tailored for expected value-based models on seven distinct problems using public datasets.  

Endogenous uncertainty. While there has been some progress in studying problems where the decision affects the uncertain parameters (Basciftci et al. 2021, Liu et al. 2022), the literature on decision-dependent uncertainty with covariates is sparse (Bertsimas and Kallus 2020, Bertsimas and Koduri 2022). An example could be a facility location problem where demand changes once a facility is located in a region or a price-setting newsvendor problem whose demand depends on the price (Liu and Zhang 2023). In these problems, the causal relationship between demand and prices is unknown. These examples offer interesting parallels with the research on heterogeneous treatment effects such as Wager and Athey (2018), which introduce causal forests for estimating treatment effects and provide asymptotic consistency results. Alley et al. (2023) study a price-setting problem and provide a new loss function to isolate the causal effects of price on demand from the conditional effects due to other covariates.  

Data privacy. Another issue is that the data might come from multiple sources and contain sensitive private information, so it cannot be directly provided in its original form to the system operator. Differential privacy techniques (see, e.g., Abadi et al. 2016) can be used to obfuscate data but may impact predictive and prescriptive performance. Mieth et al. (2023) determine the data quality after obfuscation in an optimal power flow problem with a Wasserstein ambiguity set and use a DRO model to determine the data value for decision-making.  

Interpretability $\boldsymbol { \varepsilon }$ explainability. Decision pipelines must be trusted to be implemented. This is evident from the European Union legislation ‚ÄúGeneral Data Protection Regulation‚Äù that requires entities using automated systems to provide ‚Äúmeaningful information about the logic involved‚Äù in making decisions, known popularly as the ‚Äúright to explanation‚Äù (Doshi-Velez and Kim 2017, Kaminski 2019). For instance, a citizen has the right to ask a bank for an explanation in the case of loan denial. While interpretability has received much attention in predictive ML applications (Rudin 2019), it remains largely unexplored in a contextual optimization, i.e., prescriptive context. Interpretability requires transparent decision pipelines that are intelligible to users, e.g., built over simple models such as decision trees or rule lists. In contrast, explainability may be achieved with an additional algorithm on top of a black box or complex model. Feature importance has been analyzed in a prescriptive context by Serrano et al. (2022). They introduce an integrated approach that solves a bilevel program with an integer master problem optimizing (cross-)validation accuracy. To achieve explainability, Forel et al. (2023) adapt the concept of counterfactual explanations to explain a given data-driven decision through differences of context that make this decision optimal, or better suited than a given expert decision. Having identified these differences, it becomes possible to correct or complete the contextual information, if necessary, or otherwise to give explanative elements supporting different decisions. Another research direction could be to train tree-based models (such as optimal classification trees) to approximate the policy of a complex learning-and-optimization pipeline. This has interesting connections with model distillation, i.e., the idea in the ML community of approximating a large model by a smaller one, and the work of Bertsimas and Stellato (2021), which learns the mapping from the problem parameters to optimal decisions through interpretable models.  

Fairness. Applying decisions based on contextual information can raise fairness issues when the context is made of protected attributes. This has been studied especially in pricing problems, to ensure that different customers or groups of customers are proposed prices that don‚Äôt differ significantly(Cohen et al. 2021, 2022).  

Finite sample guarantees for ILO. In Grigas et al. (2021), the authors derive finite-sample guarantees for ILO under the assumption of discrete support for the uncertain parameter. An open problem is to derive generalization bounds on the performance of ILO models for non-linear problems where the uncertain parameters have continuous support.  

Correcting for in-sample bias of data-driven optimization. When devising an optimal policy based on a finite number of samples, it is desired that low in-sample risk translates to low out-of-sample risk. However, decision rule optimization in (4) or learning and optimization model in (5) are known to produce optimistically biased estimates of the true expected cost of the prescribed policy (Ban and Rudin 2019, Costa and Iyengar 2023, Gupta et al. 2022). While one can replace this estimation with an unbiased one if data was reserved for this purpose, this is usually considered a wasteful use of data given that it could instead have been used to obtain a better-performing policy. Recent research has identified ways of circumventing this issue by estimating and correcting for the in-sample bias in contextual (Gupta et al. 2022) and non-contextual (Ito et al. 2018, Gupta and Rusmevichientong 2021, Iyengar et al. 2023) stochastic optimization problems under the assumption that errors in the estimation of uncertain parameters are normally distributed. In addition to correcting the bias using Stein‚Äôs lemma, Gupta and Rusmevichientong (2021) show that certain ‚ÄúBayes-inspired‚Äù and regularized policies achieve the same performance as optimal in-sample policy in small-data large-scale regimes.  

A promising future research direction could be to build a general framework to learn the insample policies that directly minimize the debiased objective functions. In this regard, one might find inspiration from the work of Gupta and Rusmevichientong (2021) addressing a similar issue in the non-contextual setting.  

Multi-agent decision-making. A multi-agent perspective becomes necessary in transportation and operations management problems, where different agents have access to different sources of information (i.e. covariates). In this regard, some recent work by Heaton et al. (2022) identifies the Nash equilibrium of contextual games using implicit differentiation of variational inequalities and jacobian-free backpropagation.  

Costly label acquisition. In many applications, it is costly to gather observations of uncertain vectors and covariate pairs. For instance, in personalized pricing, surveys can be sent to customers to obtain information on the sensitivity of purchasing an item with respect to its price. However, creating, sending, and collecting the surveys may have a cost. Liu et al. (2023a) develop an active learning approach to obtain labels to solve the SPO problem, while the more general case of developing active learning methods for non-linear contextual optimization is an interesting future direction. Besbes et al. (2023) provide theoretical results on the trade-off between the quality and quantity of data in a newsvendor problem, thus guiding decision-makers on how to invest in data acquisition strategies.  

Multi-stage contextual optimization. Most works on contextual optimization focus on single and two-stage problems. Ban et al. (2019) and Rios et al. (2015) use the residuals of the regression model to build multi-stage scenario trees and solve multi-stage CSO problems. Bertsimas et al. (2023) generalize the weighted SAA model for multi-stage problems. Qi et al. (2023) propose an end-to-end learning framework to solve a real-world multistage inventory replenishment problem.  

An active area of research is sequential decision-making with uncertainty. Inverse reinforcement learning ( $\mathrm { N g }$ et al. 2000) focuses on learning rewards that are consistent with observed trajectories. In the econometrics literature on dynamic discrete choice modeling the focus lies more broadly on estimating structural parameters of Markov decision processes (MDPs) (Rust 1988, Aguirregabiria and Mira 2010) including rewards, transition functions and discount factors. On both topics, estimates are typically obtained through MLE employing a soft version of the Bellman operator (e.g., Rust 1987, Ziebart et al. 2008).  

In the context of model-based reinforcement learning, so-called decision awareness (i.e. explicitly training components of a reinforcement learning system to help the agent improve the total amount of collected reward, Nikishin et al. 2022b) is receiving increasing attention (e.g., Joseph et al. 2013, Farahmand et al. 2017, Farahmand 2018, Grimm et al. 2020). For example, Nikishin et al. (2022a) introduce an approach that combines learning and planning to optimize expected returns for both tabular and non-tabular MDPs. They employ the soft version of the Bellman operator for efficient parameter learning using the IFT and show that their state-action value function has a lower approximation error than that of MLE in tabular MDPs.  

Another interesting research direction is to challenge the assumption that the joint distribution of the covariate and uncertain parameters is stationary. Neghab et al. (2022) study a newsvendor model with a hidden Markov model underlying the distribution of the covariates and demand.  

Finally, an area that requires attention is the deployment of models for real-world applications by tackling computational hurdles associated with decision-aware learning in MDPs, such as large state-action pairs and high-dimensional policy spaces (Wang et al. 2023). An example is a service call scheduling problem that is formulated as a restless multi-armed bandit (RMAB) problem in Mate et al. (2022) to improve maternal and child health in a non-profit organization. They model each beneficiary as an arm, apply a clustering method to learn the dynamics, and then use the Whittle Index policy to solve the RMAB. Wang et al. (2023) use decision-focused learning to solve RMAB, where the computational difficulty in differentiating the Whittle index policy of selecting the top-k arms, is mitigated by making a soft-top-k selection of arms which is an optimal transport problem (Xie et al. 2020).  

# 7. Conclusions  

In this survey, we summarize advancements in contextual optimization methods developed to solve decision-making problems under uncertainty. A salient feature of the problems studied is that a covariate that is correlated with the uncertain parameter is revealed to the decision-maker before a decision is made. Therefore, historical data on both covariates and corresponding uncertain parameter values can be used to prescribe decisions based on the covariate information. We showed that contextual optimization literature can be categorized into three data-driven frameworks for learning policies, namely, decision rule optimization, sequential learning and optimization, and integrated learning and optimization. While decision rule optimization explicitly parameterizes the policy as a function of the covariates, learning and optimization require estimating the conditional distribution (or a sufficient statistic in the case of expected value-based models) of the uncertain parameter given the covariates in a sequential or integrated manner. By providing a parametric description of these three data-driven frameworks, we have introduced a uniform notation and terminology for analyzing different methods. In particular, we have elaborately described integrated learning and optimization models using both their training pipeline and modeling choice, further emphasizing that these two aspects are intertwined. Furthermore, we have provided a list of active fields of research in CSO problems.  

# Acknowledgments  

The first author‚Äôs research is supported by the Group for Research in Decision Analysis (GERAD) postdoctoral fellowship and Fonds de Recherche du Que¬¥bec‚ÄìNature et Technologies (FRQNT) postdoctoral research scholarship [Grant 301065]. Erick Delage was partially supported by the Canadian Natural Sciences and Engineering Research Council [Grant RGPIN-2022-05261] and by the Canada Research Chair program [950-230057]. Emma Frejinger was partially supported by the Canada Research Chair program [950-232244]. Finally, the authors also acknowledge the support of IVADO and the Canada First Research Excellence Fund (Apoge¬¥e/CFREF).  

References   
Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, Zhang L (2016) Deep learning with differential privacy. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308‚Äì318, CCS ‚Äô16 (NY, USA: Association for Computing Machinery).   
Agrawal A, Amos B, Barratt S, Boyd S, Diamond S, Kolter JZ (2019) Differentiable convex optimization layers. Advances in Neural Information Processing Systems, volume 32 (Curran Associates, Inc.).   
Aguirregabiria V, Mira P (2010) Dynamic discrete choice structural models: A survey. Journal of Econometrics 156(1):38‚Äì67.   
Alley M, Biggs M, Hariss R, Herrmann C, Li ML, Perakis G (2023) Pricing for heterogeneous products: Analytics for ticket reselling. Manufacturing & Service Operations Management 25(2):409‚Äì426.   
Amos B, Kolter JZ (2017) OptNet: Differentiable optimization as a layer in neural networks. International Conference on Machine Learning, volume 70, 136‚Äì145 (PMLR).   
Aronszajn N (1950) Theory of reproducing kernels. Transactions of the American mathematical society 68(3):337‚Äì404.   
Athey S, Tibshirani J, Wager S (2019) Generalized random forests. The Annals of Statistics 47(2):1148 ‚Äì 1178.   
Backhoff J, Beiglbock M, Lin Y, Zalashko A (2017) Causal transport in discrete time and applications. SIAM Journal on Optimization 27(4):2528‚Äì2562.   
Bai S, Kolter JZ, Koltun V (2019) Deep equilibrium models. Advances in Neural Information Processing Systems, volume 32 (Curran Associates, Inc.).   
Ban GY, Gallien J, Mersereau AJ (2019) Dynamic procurement of new products with covariate information: The residual tree method. Manufacturing & Service Operations Management 21(4):798‚Äì815.   
Ban GY, Rudin C (2019) The Big Data Newsvendor: Practical Insights from Machine Learning. Operations Research 67(1):90‚Äì108.   
Basciftci B, Ahmed S, Shen S (2021) Distributionally robust facility location problem under decisiondependent stochastic demand. European Journal of Operational Research 292(2):548‚Äì561.   
Bazier-Matte T, Delage E (2020) Generalization bounds for regularized portfolio selection with market side information. INFOR: Information Systems and Operational Research 58(2):374‚Äì401.   
Bengio Y (1997) Using a financial training criterion rather than a prediction criterion. International Journal of Neural Systems 8(4):433‚Äì443.   
Bengio Y, Lodi A, Prouvost A (2021) Machine learning for combinatorial optimization: A methodological tour d‚Äôhorizon. European Journal of Operational Research 290(2):405‚Äì421.   
Berthet Q, Blondel M, Teboul O, Cuturi M, Vert JP, Bach F (2020) Learning with differentiable perturbed optimizers. Advances in Neural Information Processing Systems, volume 33, 9508‚Äì9519 (Curran Associates, Inc.).   
Bertsimas D, Dunn J, Mundru N (2019) Optimal prescriptive trees. INFORMS Journal on Optimization 1(2):164‚Äì183.   
Bertsimas D, Kallus N (2020) From predictive to prescriptive analytics. Management Science 66(3):1025‚Äì1044.   
Bertsimas D, Koduri N (2022) Data-driven optimization: A reproducing kernel hilbert space approach. Operations Research 70(1):454‚Äì471.   
Bertsimas D, McCord C, Sturt B (2023) Dynamic optimization with side information. European Journal of Operational Research 304(2):634‚Äì651.   
Bertsimas D, Stellato B (2021) The voice of optimization. Machine Learning 110(2):249‚Äì277.   
Bertsimas D, Van Parys B (2022) Bootstrap robust prescriptive analytics. Mathematical Programming 195(1- 2):39‚Äì78.   
Besbes O, Ma W, Mouchtaki O (2023) Quality vs. quantity of data in contextual decision-making: Exact analysis under newsvendor loss. arXiv preprint arXiv:2302.08424 .   
Birge JR, Louveaux F (2011) Introduction to Stochastic Programming (New York, NY: Springer New York).   
Blondel M, Berthet Q, Cuturi M, Frostig R, Hoyer S, Llinares-Lo¬¥pez F, Pedregosa F, Vert JP (2022) Efficient and modular implicit differentiation. Advances in Neural Information Processing Systems, volume 35, 5230‚Äì5242 (Curran Associates, Inc.).   
Blondel M, Martins AF, Niculae V (2020) Learning with Fenchel-Young losses. Journal of Machine Learning Research 21(1):1314‚Äì1382.   
Bolte J, Le T, Pauwels E, Silveti-Falls T (2021) Nonsmooth implicit differentiation for machine-learning and optimization. Advances in Neural Information Processing Systems 34:13537‚Äì13549.   
Butler A, Kwon RH (2023a) Efficient differentiable quadratic programming layers: an ADMM approach. Computational Optimization and Applications 84(2):449‚Äì476.   
Butler A, Kwon RH (2023b) Integrating prediction in mean-variance portfolio optimization. Quantitative Finance 23(3):429‚Äì452.   
Buttler S, Philippi A, Stein N, Pibernik R (2022) A meta analysis of data-driven newsvendor approaches. ICLR 2022 Workshop on Setting up ML Evaluation Standards to Accelerate Progress.   
Chan TC, Mahmood R, Zhu IY (2021) Inverse optimization: Theory and applications. arXiv preprint arXiv:2109.03920 .   
Chen B, Donti PL, Baker K, Kolter JZ, Berg¬¥es M (2021) Enforcing policy feasibility constraints through differentiable projection for energy optimization. Proceedings of the Twelfth ACM International Conference on Future Energy Systems, 199‚Äì210 (Virtual Event Italy: ACM).   
Chen R, Paschalidis I (2019) Selecting optimal decisions via distributionally robust nearest-neighbor regression. Advances in Neural Information Processing Systems, volume 32 (Curran Associates, Inc.).   
Chen R, Paschalidis IC (2018) A robust learning approach for regression models based on distributionally robust optimization. Journal of Machine Learning Research 19(13):1‚Äì48.   
Chen W, Tanneau M, Van Hentenryck P (2023) End-to-end feasible optimization proxies for large-scale economic dispatch. arXiv preprint arXiv:2304.11726 .   
Chenreddy AR, Bandi N, Delage E (2022) Data-driven conditional robust optimization. Advances in Neural Information Processing Systems, volume 35, 9525‚Äì9537 (Curran Associates, Inc.).   
Chu H, Zhang W, Bai P, Chen Y (2023) Data-driven optimization for last-mile delivery. Complex & Intelligent Systems 9(9):2271‚Äì2284.   
Chung TH, Rostami V, Bastani H, Bastani O (2022) Decision-aware learning for optimizing health supply chains. arXiv preprint arXiv:2211.08507 .   
Ciocan DF, MisÀáic¬¥ VV (2022) Interpretable optimal stopping. Management Science 68(3):1616‚Äì1638.   
Clarke FH (1990) Optimization and nonsmooth analysis (SIAM).   
Cohen MC, Elmachtoub AN, Lei X (2022) Price discrimination with fairness constraints. Management Science 68(12):8536‚Äì8552.   
Cohen MC, Miao S, Wang Y (2021) Dynamic pricing with fairness constraints. Available at https://dx.doi.org/10.2139/ssrn.3930622 .   
Costa G, Iyengar GN (2023) Distributionally robust end-to-end portfolio construction. Quantitative Finance 23(10):1465‚Äì1482.   
Cristian R, Harsha P, Perakis G, Quanz BL, Spantidakis I (2022) End-to-end learning via constraint-enforcing approximators for linear programs with applications to supply chains. AI for Decision Optimization Workshop of the AAAI Conference on Artificial Intelligence .   
Cybenko G (1989) Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems 2(4):303‚Äì314.   
Dalle G, Baty L, Bouvier L, Parmentier A (2022) Learning with combinatorial optimization layers: a probabilistic approach. arXiv preprint arXiv:2207.13513 .   
Davis D, Yin W (2017) A three-operator splitting scheme and its optimization applications. Set-Valued and Variational Analysis 25(4):829‚Äì858.   
Demirovic¬¥ E, J Stuckey P, Bailey J, Chan J, Leckie C, Ramamohanarao K, Guns T, Kraus S (2019) Predict+ optimise with ranking objectives: Exhaustively learning linear functions. Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 1078‚Äì1085 (International Joint Conferences on Artificial Intelligence Organization).   
Demirovic¬¥ E, Stuckey PJ, Guns T, Bailey J, Leckie C, Ramamohanarao K, Chan J (2020) Dynamic programming for predict $^ +$ optimise. Proceedings of the AAAI Conference on Artificial Intelligence 34(0202):1444‚Äì1451.   
Deng Y, Sen S (2022) Predictive stochastic programming. Computational Management Science 19(1):65‚Äì98.   
Domke J (2012) Generic methods for optimization-based modeling. International Conference on Artificial Intelligence and Statistics, 318‚Äì326 (PMLR).   
Dontchev AL, Rockafellar RT, Rockafellar RT (2009) Implicit functions and solution mappings: A view from variational analysis, volume 616 (Springer).   
Donti P, Amos B, Kolter JZ (2017) Task-based end-to-end model learning in stochastic optimization. Advances in Neural Information Processing Systems, volume 30 (Curran Associates, Inc.).   
Donti PL, Roderick M, Fazlyab M, Kolter JZ (2021) Enforcing robust control guarantees within neural network policies. International Conference on Learning Representations.   
Doshi-Velez F, Kim B (2017) Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608 .   
Duvenaud D, Kolter JZ, Johnson M (2020) Deep implicit layers tutorial - neural ODEs, deep equilibrium models, and beyond. Neural Information Processing Systems Tutorial .   
El Balghiti O, Elmachtoub AN, Grigas P, Tewari A (2019) Generalization bounds in the predict-thenoptimize framework. Advances in Neural Information Processing Systems, volume 32 (Curran Associates, Inc.).   
El Balghiti O, Elmachtoub AN, Grigas P, Tewari A (2022) Generalization bounds in the predict-thenoptimize framework. Mathematics of Operations Research Forthcoming.   
Elmachtoub AN, Grigas P (2022) Smart ‚ÄúPredict, then Optimize‚Äù. Management Science 68(1):9‚Äì26.   
Elmachtoub AN, Lam H, Zhang H, Zhao Y (2023) Estimate-then-optimize versus integrated-estimationoptimization: A stochastic dominance perspective. arXiv preprint arXiv:2304.06833 .   
Elmachtoub AN, Liang JCN, McNellis R (2020) Decision trees for decision-making under the predict-thenoptimize framework. International Conference on Machine Learning, 2858‚Äì2867 (PMLR).   
Esteban-P¬¥erez A, Morales JM (2022) Distributionally robust stochastic programs with side information based on trimmings. Mathematical Programming 195(1):1069‚Äì1105.   
Esteban-P¬¥erez A, Morales JM (2023) Distributionally robust optimal power flow with contextual information. European Journal of Operational Research 306(3):1047‚Äì1058.   
Estes AS, Richard JPP (2023) Smart predict-then-optimize for two-stage linear programs with side information. INFORMS Journal on Optimization Forthcoming.   
Fajemisin AO, Maragno D, den Hertog D (2023) Optimization with constraint learning: a framework and survey. European Journal of Operational Research .   
Farahmand AM (2018) Iterative value-aware model learning. Bengio S, Wallach H, Larochelle H, Grauman K, Cesa-Bianchi N, Garnett R, eds., Advances in Neural Information Processing Systems, volume 31.   
Farahmand AM, Barreto A, Nikovski D (2017) Value-Aware Loss Function for Model-based Reinforcement Learning. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54, 1486‚Äì1494.   
Ferber A, Wilder B, Dilkina B, Tambe M (2020) MIPaaL: Mixed Integer Program as a Layer. Proceedings of the AAAI Conference on Artificial Intelligence 34(02):1504‚Äì1511.   
Ferreira KJ, Lee BH, Simchi-Levi D (2016) Analytics for an online retailer: Demand forecasting and price optimization. Manufacturing & Service Operations Management 18(1):69‚Äì88.   
Forel A, Parmentier A, Vidal T (2023) Explainable data-driven optimization: From context to decision and back again. arXiv preprint arXiv:2301.10074 .   
Fung SW, Heaton H, Li $\scriptstyle \mathrm { Q } ,$ Mckenzie D, Osher S, Yin W (2022) JFB: Jacobian-free backpropagation for implicit networks. Proceedings of the AAAI Conference on Artificial Intelligence 36(66):6648‚Äì6656.   
Grefenstette E, Amos B, Yarats D, Htut PM, Molchanov A, Meier F, Kiela D, Cho K, Chintala S (2019) Generalized inner loop meta-learning. arXiv preprint arXiv:1910.01727 .   
Grigas P, Qi M, Shen M (2021) Integrated conditional estimation-optimization. arXiv preprint arXiv:2110.12351 .   
Grimm C, Barreto A, Singh S, Silver D (2020) The value equivalence principle for model-based reinforcement learning. Advances in Neural Information Processing Systems, volume 33.   
Gupta V, Huang M, Rusmevichientong P (2022) Debiasing in-sample policy performance for small-data, large-scale optimization. Operations Research .   
Gupta V, Rusmevichientong P (2021) Small-data, large-scale linear optimization with uncertain objectives. Management Science 67(1):220‚Äì241.   
Halkin H (1974) Implicit functions and optimization problems without continuous differentiability of the data. SIAM Journal on Control 12(2):229‚Äì236.   
Hannah L, Powell W, Blei D (2010) Nonparametric density estimation for stochastic optimization with an observable state variable. Advances in Neural Information Processing Systems, volume 23 (Curran Associates, Inc.).   
Hastie T, Tibshirani R, Friedman JH (2009) The Elements of Statistical Learning: Data Mining, Inference, and Prediction, volume 2 (Springer).   
Heaton H, McKenzie D, Li Q, Fung SW, Osher S, Yin W (2022) Learn to predict equilibria via fixed point networks. arXiv:2106.00906 .   
Ho-Nguyen N, Kƒ±lƒ±nc¬∏-Karzan F (2022) Risk guarantees for end-to-end prediction and optimization processes. Management Science 68(12):8680‚Äì8698.   
Hofmann T, Scho¬®lkopf B, Smola A (2008) Kernel methods in machine learning. Annals of Statistics 36(3):1171‚Äì1220.   
Hu Y, Kallus N, Mao X (2022) Fast rates for contextual linear optimization. Management Science 68(6):4236‚Äì4245.   
Huber J, M ¬®uller S, Fleischmann M, Stuckenschmidt H (2019) A data-driven newsvendor problem: From data to decision. European Journal of Operational Research 278(3):904‚Äì915.   
Ifrim G, O‚ÄôSullivan B, Simonis H (2012) Properties of energy-price forecasts for scheduling. International Conference on Principles and Practice of Constraint Programming (Springer).   
Ito S, Yabe A, Fujimaki R (2018) Unbiased objective estimation in predictive optimization. Proceedings of the 35th International Conference on Machine Learning, 2176‚Äì2185 (PMLR).   
Iyengar G, Lam H, Wang T (2023) Optimizer‚Äôs information criterion: Dissecting and correcting bias in datadriven optimization. arXiv preprint arXiv:2306.10081 .   
Jeong J, Jaggi P, Butler A, Sanner S (2022) An exact symbolic reduction of linear smart Predict+Optimize to mixed integer linear programming. International Conference on Machine Learning, volume 162, 10053‚Äì 10067 (PMLR).   
Joseph J, Geramifard A, Roberts JW, How JP, Roy N (2013) Reinforcement learning with misspecified model classes. IEEE International Conference on Robotics and Automation, 939‚Äì946.   
Kallus N, Mao X (2022) Stochastic optimization forests. Management Science 69(4):1975‚Äì1994.   
Kaminski ME (2019) The right to explanation, explained. Berkeley Technology Law Journal 34(1):189‚Äì218.   
Kannan R, Bayraksan G, Luedtke J (2021) Heteroscedasticity-aware residuals-based contextual stochastic optimization. arXiv preprint arXiv:2101.03139 .   
Kannan R, Bayraksan G, Luedtke JR (2020) Residuals-based distributionally robust optimization with covariate information. arXiv preprint arXiv:2012.01088 .   
Kannan R, Bayraksan G, Luedtke JR (2022) Data-driven sample average approximation with covariate information. arXiv preprint arXiv:2207.13554 .   
Kantorovich LV, Rubinshtein GS (1958) On a space of totally additive functions. Vestnik Leningradskogo Universiteta 13(7):52‚Äì59.   
Keshavarz P (2022) Interpretable Contextual Newsvendor Models: A Tree-Based Method to Solving Data-Driven Newsvendor Problems. Master‚Äôs thesis, University of Ottawa.   
Kong L, Cui J, Zhuang Y, Feng R, Prakash BA, Zhang C (2022) End-to-end stochastic optimization with energy-based model. Advances in Neural Information Processing Systems, volume 35, 11341‚Äì11354 (Curran Associates, Inc.).   
Kotary J, Dinh MH, Fioretto F (2023) Folded optimization for end-to-end model-based learning. International Joint Conference on Artificial Intelligence.   
Kotary J, Fioretto F, Van Hentenryck P, Wilder B (2021) End-to-end constrained optimization learning: A survey. Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 4475‚Äì4482 (International Joint Conferences on Artificial Intelligence Organization).   
Kullback S, Leibler RA (1951) On information and sufficiency. The Annals of Mathematical Statistics 22(1):79‚Äì 86.   
Lassalle R (2018) Causal transport plans and their Monge‚ÄìKantorovich problems. Stochastic Analysis and Applications 36(3):452‚Äì484.   
Lawless C, Zhou A (2022) A note on task-aware loss via reweighing prediction loss by decision-regret. arXiv preprint arXiv:2211.05116 .   
Lin S, Chen Y, Li Y, Shen ZJM (2022) Data-driven newsvendor problems regularized by a profit risk constraint. Production and Operations Management 31(4):1630‚Äì1644.   
Liu H, Grigas P (2021) Risk bounds and calibration for a smart predict-then-optimize method. Advances in Neural Information Processing Systems, volume 34, 22083‚Äì22094 (Curran Associates, Inc.).   
Liu J, Li G, Sen S (2022) Coupled learning enabled stochastic programming with endogenous uncertainty. Mathematics of Operations Research 47(2):1681‚Äì1705.   
Liu M, Grigas P, Liu H, Shen ZJM (2023a) Active learning in the predict-then-optimize framework: A margin-based approach. arXiv preprint arXiv:2305.06584 .   
Liu S, He L, Max Shen ZJ (2021) On-time last-mile delivery: Order assignment with travel-time predictors. Management Science 67(7):4095‚Äì4119.   
Liu W, Zhang Z (2023) Solving data-driven newsvendor pricing problems with decision-dependent effect. arXiv preprint arXiv:2304.13924 .   
Liu Z, Yin Y, Bai F, Grimm DK (2023b) End-to-end learning of user equilibrium with implicit neural networks. Transportation Research Part C: Emerging Technologies 150:104085.   
Liyanage LH, Shanthikumar JG (2005) A practical inventory control policy using operational statistics. Operations Research Letters 33(4):341‚Äì348.   
Loke GG, Tang Q, Xiao Y (2022) Decision-driven regularization: A blended model for predict-then-optimize. Available at https://dx.doi.org/10.2139/ssrn.3623006 .   
Lu Z, Pu H, Wang F, Hu Z, Wang L (2017) The expressive power of neural networks: A view from the width. Advances in Neural Information Processing Systems (Curran Associates, Inc.).   
Mandi J, Bucarey V, Tchomba MMK, Guns T (2022) Decision-focused learning: Through the lens of learning to rank. International Conference on Machine Learning, 14935‚Äì14947 (PMLR).   
Mandi J, Guns T (2020) Interior point solving for LP-based prediction $\cdot +$ optimisation. Advances in Neural Information Processing Systems, volume 33, 7272‚Äì7282 (Curran Associates, Inc.).   
Mandi J, Kotary J, Berden S, Mulamba M, Bucarey V, Guns T, Fioretto F (2023) Decision-focused learning: Foundations, state of the art, benchmark and future opportunities. arXiv preprint arXiv:2307.13565 .   
Mandi J, Stuckey PJ, Guns T, et al. (2020) Smart predict-and-optimize for hard combinatorial optimization problems. Proceedings of the AAAI Conference on Artificial Intelligence 34(02):1603‚Äì1610.   
Mart¬¥ƒ±nez-de Albeniz V, Belkaid A (2021) Here comes the sun: Fashion goods retailing under weather fluctuations. European Journal of Operational Research 294(3):820‚Äì830.   
Mate A, Madaan L, Taneja A, Madhiwalla N, Verma S, Singh G, Hegde A, Varakantham P, Tambe M (2022) Field study in deploying restless multi-armed bandits: Assisting non-profits in improving maternal and child health. Proceedings of the AAAI Conference on Artificial Intelligence 36(1111):12017‚Äì12025.   
McKenzie D, Fung SW, Heaton H (2023) Faster predict-and-optimize with three-operator splitting. arXiv preprint arXiv:2301.13395 .   
Mieth R, Morales JM, Poor HV (2023) Data valuation from data-driven optimization. arXiv preprint arXiv:2305.01775 .   
MisÀáic¬¥ VV, Perakis G (2020) Data analytics in operations management: A review. Manufacturing & Service Operations Management 22(1):158‚Äì169.   
Monga V, Li Y, Eldar YC (2021) Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing. IEEE Signal Processing Magazine 38(2):18‚Äì44.   
Mulamba M, Mandi J, Diligenti M, Lombardi M, Lopez VB, Guns T (2021) Contrastive losses and solution caching for predict-and-optimize. 30th International Joint Conference on Artificial Intelligence (IJCAI-21): IJCAI-21, 2833‚Äì2840 (International Joint Conferences on Artificial Intelligence).   
MuÀúnoz MA, Pineda S, Morales JM (2022) A bilevel framework for decision-making under uncertainty with contextual information. Omega 108:102575.   
Nadaraya E (1964) On estimating regression. Theory of Probability & its Applications 9(1):141‚Äì142.   
Neghab DP, Khayyati S, Karaesmen F (2022) An integrated data-driven method using deep learning for a newsvendor problem with unobservable features. European Journal of Operational Research 302(2):482‚Äì 496.   
Ng AY, Russell S, et al. (2000) Algorithms for inverse reinforcement learning. Icml, volume 1, 2.   
Nguyen VA, Zhang F, Blanchet J, Delage E, Ye Y (2020) Distributionally robust local non-parametric conditional estimation. Advances in Neural Information Processing Systems, volume 33, 15232‚Äì15242 (Curran Associates, Inc.).   
Nguyen VA, Zhang F, Blanchet J, Delage E, Ye Y (2021) Robustifying conditional portfolio decisions via optimal transport. arXiv preprint arXiv:2103.16451 .   
Nikishin E, Abachi R, Agarwal R, Bacon PL (2022a) Control-oriented model-based reinforcement learning with implicit differentiation. Proceedings of the AAAI Conference on Artificial Intelligence 36(7):7886‚Äì7894.   
Nikishin E, D‚ÄôOro P, Precup D, Barreto A, massoud Farahmand A, Bacon PL, Hall G (2022b) Decision awareness in reinforcement learning. Workshop abstract. International Conference on Machine Learning.   
Notz PM, Pibernik R (2022) Prescriptive analytics for flexible capacity management. Management Science 68(3):1756‚Äì1775.   
Ohmori S (2021) A predictive prescription using minimum volume k-nearest neighbor enclosing ellipsoid and robust optimization. Mathematics 9(2):119.   
Oroojlooyjadid A, Snyder LV, Taka¬¥cÀá M (2020) Applying deep learning to the newsvendor problem. IISE Transactions 52(4):444‚Äì463.   
Perakis G, Sim M, Tang Q, Xiong P (2023) Robust pricing and production with information partitioning and adaptation. Management Science 69(3):1398‚Äì1419.   
Perrault A, Wilder B, Ewing E, Mate A, Dilkina B, Tambe M (2020) End-to-End Game-Focused Learning of Adversary Behavior in Security Games. Proceedings of the AAAI Conference on Artificial Intelligence 34(02):1378‚Äì1386.   
PersÀáak E, Anjos MF (2023) Contextual robust optimisation with uncertainty quantification. Proceedings of the 20th International Conference on the Integrationg of Constraint Programming, Artificial Intelligence, and Operations Research, 124‚Äì132 (Springer).   
Pineda L, Fan T, Monge M, Venkataraman S, Sodhi P, Chen RT, Ortiz J, DeTone D, Wang A, Anderson S, et al. (2022) Theseus: A library for differentiable nonlinear optimization. Advances in Neural Information Processing Systems, volume 35, 3801‚Äì3818 (Curran Associates, Inc.).   
Qi M, Shen ZJ (2022) Integrating prediction/estimation and optimization with applications in operations management. Tutorials in Operations Research: Emerging and Impactful Topics in Operations, 36‚Äì58 (INFORMS).   
Qi M, Shi Y, Qi Y, Ma C, Yuan R, Wu D, Shen ZJM (2023) A practical end-to-end inventory management model with deep learning. Management Science 69(2):759‚Äì773.   
Rahimian H, Pagnoncelli B (2022) Data-driven approximation of contextual chance-constrained stochastic programs. Available at https://optimization-online.org/?p=20569 .   
Ren J, Feng X, Liu B, Pan X, Fu Y, Mai L, Yang Y (2022) Torchopt: An efficient library for differentiable optimization. arXiv preprint arXiv:2211.06934 .   
Rios I, Wets RJ, Woodruff DL (2015) Multi-period forecasting and scenario generation with limited data. Computational Management Science 12(2):267‚Äì295.   
Rockafellar RT, Wets RJB (2009) Variational Analysis (Berlin: Springer).   
Rudin C (2019) Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence 1(5):206‚Äì215.   
Rust J (1987) Optimal replacement of gmc bus engines: An empirical model of harold zurcher. Econometrica: Journal of the Econometric Society 999‚Äì1033.   
Rust J (1988) Maximum likelihood estimation of discrete control processes. SIAM Journal on Control and Optimization 26(5):1006‚Äì1024.   
Rychener Y, Kuhn D, Sutter T (2023) End-to-end learning for stochastic optimization: A bayesian perspective. International Conference on Machine Learning.   
Sahoo SS, Paulus A, Vlastelica M, Musil V, Kuleshov V, Martius G (2023) Backpropagation through combinatorial algorithms: Identity with projection works. International Conference on Learning Representations.   
Sang L, Xu Y, Long H, Hu Q, Sun H (2022) Electricity price prediction for energy storage system arbitrage: A decision-focused approach. IEEE Transactions on Smart Grid 13(4):2822‚Äì2832.   
Sang L, Xu Y, Long H, Wu W (2023) Safety-aware semi-end-to-end coordinated decision model for voltage regulation in active distribution network. IEEE Transactions on Smart Grid 14(3):1814‚Äì1826.   
Sen S, Deng Y (2017) Learning enabled optimization: Towards a fusion of statistical learning and stochastic programming. Available at https://optimization-online.org/?p=14456 .   
Serrano B, Minner S, Schiffer M, Vidal T (2022) Bilevel optimization for feature selection in the data-driven newsvendor problem. arXiv preprint arXiv:2209.05093 .   
Shah S, Wang K, Wilder B, Perrault A, Tambe M (2022) Decision-focused learning without decision-making: Learning locally optimized decision losses. Advances in Neural Information Processing Systems, volume 35, 1320‚Äì1332 (Curran Associates, Inc.).   
Shapiro A, Dentcheva D, Ruszczy ¬¥nski A (2014) Lectures on Stochastic Programming: Modeling and Theory (Philadelphia: SIAM), second edition.   
Shlezinger N, Eldar YC, Boyd SP (2022) Model-based deep learning: On the intersection of deep learning and optimization. IEEE Access 10:115384‚Äì115398.   
Simonis H, O‚ÄôSullivan B, Mehta D, Hurley B, Cauwer MD (2014) CSPLib problem 059: Energy-cost aware scheduling. http://www.csplib.org/Problems/prob059, accessed on June 14, 2023.   
Smith JE, Winkler RL (2006) The optimizer‚Äôs curse: Skepticism and postdecision surprise in decision analysis. Management Science 52(3):311‚Äì322.   
Srivastava PR, Wang Y, Hanasusanto GA, Ho CP (2021) On data-driven prescriptive analytics with side information: A regularized Nadaraya-Watson approach. arXiv preprint arXiv:2110.04855 .   
Stratigakos A, Camal S, Michiorri A, Kariniotakis G (2022) Prescriptive trees for integrated forecasting and optimization applied in trading of renewable energy. IEEE Transactions on Power Systems 37(6):4696‚Äì4708.   
Sun C, Liu S, Li X (2023a) Maximum optimality margin: A unified approach for contextual linear programming and inverse linear programming. arXiv preprint arXiv:2301.11260 .   
Sun H, Shi Y, Wang J, Tuan HD, Poor HV, Tao D (2023b) Alternating differentiation for optimization layers. International Conference on Learning Representations.   
Sun J, Li H, Xu Z, et al. (2016) Deep ADMM-Net for compressive sensing MRI. Advances in Neural Information Processing Systems, volume 29 (Curran Associates, Inc.).   
Sun X, Leung CH, Li Y, Wu Q (2023c) A unified perspective on regularization and perturbation in differentiable subset selection. International Conference on Artificial Intelligence and Statistics, 4629‚Äì4642 (PMLR).   
Sutton RS, McAllester D, Singh S, Mansour Y (1999) Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems, volume 12 (MIT Press).   
Tang B, Khalil EB (2022) PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming. arXiv preprint arXiv:2206.14234 . 49   
Tian X, Yan R, Liu Y, Wang S (2023a) A smart predict-then-optimize method for targeted and cost-effective maritime transportation. Transportation Research Part B: Methodological 172:32‚Äì52.   
Tian X, Yan R, Wang S, Laporte G (2023b) Prescriptive analytics for a maritime routing problem. Ocean $\mathcal { E }$ Coastal Management 242:106695.   
Tian X, Yan R, Wang S, Liu Y, Zhen L (2023c) Tutorial on prescriptive analytics for logistics: What to predict and how to predict. Electronic Research Archive 31(4):2265‚Äì2285.   
Van Parys BP, Esfahani PM, Kuhn D (2021) From data to decisions: Distributionally robust optimization is optimal. Management Science 67(6):3387‚Äì3402.   
Vlastelica M, Paulus A, Musil V, Martius G, Rolinek M (2019) Differentiation of blackbox combinatorial solvers. International Conference on Learning Representations.   
Vohra R, Rajaei A, Cremer JL (2023) End-to-end learning with multiple modalities for system-optimised renewables nowcasting. arXiv preprint arXiv:2304.07151 .   
Wager S, Athey S (2018) Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association 113(523):1228‚Äì1242.   
Wahdany D, Schmitt C, Cremer JL (2023) More than accuracy: end-to-end wind power forecasting that optimises the energy system. Electric Power Systems Research 221:109384.   
Wang K, Verma S, Mate A, Shah S, Taneja A, Madhiwalla N, Hegde A, Tambe M (2023) Scalable decisionfocused learning in restless multi-armed bandits with application to maternal and child health. arXiv preprint arXiv:2202.00916 .   
Wang K, Wilder B, Perrault A, Tambe M (2020) Automatically learning compact quality-aware surrogates for optimization problems. Advances in Neural Information Processing Systems, volume 33, 9586‚Äì9596 (Curran Associates, Inc.).   
Wang T, Chen N, Wang C (2021) Distributionally robust prescriptive analytics with Wasserstein distance. arXiv preprint arXiv:2106.05724 .   
Watson G (1964) Smooth regression analysis. Sankhy¬Øa: The Indian Journal of Statistics, Series A 26(4):359‚Äì372.   
Wilder B, Dilkina B, Tambe M (2019a) Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization. Proceedings of the AAAI Conference on Artificial Intelligence 33(01):1658‚Äì1665.   
Wilder B, Ewing E, Dilkina B, Tambe M (2019b) End to end learning and optimization on graphs. Advances in Neural Information Processing Systems, volume 32 (Curran Associates, Inc.).   
Xie X, Wu J, Liu G, Zhong Z, Lin Z (2019) Differentiable linearized ADMM. International Conference on Machine Learning, 6902‚Äì6911 (PMLR).   
Xie Y, Dai H, Chen M, Dai B, Zhao T, Zha H, Wei W, Pfister T (2020) Differentiable top-k with optimal transport. Advances in Neural Information Processing Systems, volume 33, 20520‚Äì20531 (Curran Associates, Inc.).   
Xu Y, Cohen SB (2018) Stock movement prediction from tweets and historical prices. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1970‚Äì1979 (Association for Computational Linguistics).   
Yan R, Wang S, Cao J, Sun D (2021) Shipping domain knowledge informed prediction and optimization in port state control. Transportation Research Part B: Methodological 149:52‚Äì78.   
Yan R, Wang S, Fagerholt K (2020) A semi-‚Äúsmart predict then optimize‚Äù (semi-SPO) method for efficient ship inspection. Transportation Research Part B: Methodological 142:100‚Äì125.   
Yan R, Wang S, Zhen L (2023) An extended smart ‚Äúpredict, and optimize‚Äù (spo) framework based on similar sets for ship inspection planning. Transportation Research Part E: Logistics and Transportation Review 173:103109.   
Yang J, Zhang L, Chen N, Gao R, Hu M (2023) Decision-making with side information: A causal transport robust approach. Available at https://optimization-online.org/?p=20639 .   
Yanƒ±koÀòglu IÀô, Gorissen BL, den Hertog D (2019) A survey of adjustable robust optimization. European Journal of Operational Research 277(3):799‚Äì813.   
Zhang C, Zhang Z, Cucuringu M, Zohren S (2021) A universal end-to-end approach to portfolio optimization via deep learning. arXiv preprint arXiv:2111.09170 .   
Zhang L, Yang J, Gao R (2023a) Optimal robust policy for feature-based newsvendor. Management Science Forthcoming.   
Zhang Y, Gao J (2017) Assessing the performance of deep learning algorithms for newsvendor problem. Liu D, Xie S, Li Y, Zhao D, El-Alfy ESM, eds., Neural Information Processing, 912‚Äì921 (Cham: Springer International Publishing).   
Zhang Y, Liu J, Zhao X (2023b) Data-driven piecewise affine decision rules for stochastic programming with covariate information. arXiv preprint arXiv:2304.13646 .   
Zhu T, Xie J, Sim M (2022) Joint estimation and robustness optimization. Management Science 68(3):1659‚Äì1677.   
Ziebart BD, Maas AL, Bagnell JA, Dey AK (2008) Maximum entropy inverse reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence 8:1433‚Äì1438.  

# Appendix A: Theoretical guarantees  

In this section, we provide some definitions and theoretical results for completeness.  

DEFINITION 2. (Bertsimas and Kallus 2020) We say that a policy $\pi ^ { N } ( { \pmb x } )$ obtained using $N$ samples is asymptotically optimal if, with probability 1, we have that for $\mathbb { P } ( { \pmb x } )$ -almost-everywhere $\pmb { x } \in \mathcal { X }$ :  

$$
\operatorname* { l i m } _ { N  \infty } h ( \pi ^ { N } ( \pmb { x } ) , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) = h ( \pi ^ { * } ( \pmb { x } ) , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) .
$$  

DEFINITION 3. (Bertsimas and Kallus 2020) We say that a policy $\pi ^ { N } ( { \pmb x } )$ obtained using $N$ samples is consistent if, with probability 1, we have that for $\mathbb { P } ( { \pmb x } )$ -almost-everywhere $\pmb { x } \in \mathcal { X }$ :  

$$
\| \pi ^ { N } ( \pmb { x } ) - \mathcal { Z } ^ { * } ( \pmb { x } ) \| = 0 \mathrm { w h e r e } \| \pi ^ { N } ( \pmb { x } ) - \mathcal { Z } ^ { * } ( \pmb { x } ) \| = \operatorname* { i n f } _ { \pmb { z } \in \mathcal { Z } ^ { * } ( \pmb { x } ) } \| \pi ^ { N } ( \pmb { x } ) - z \| ,
$$  

and  

$$
\mathcal { Z } ^ { * } ( \pmb { x } ) = \left. z \middle | z \in \underset { z ^ { \prime } \in \mathcal { Z } } { \mathrm { a r g m i n } } \mathbb { E } _ { \mathbb { P } } \left[ h ( z ^ { \prime } , \mathbb { P } ( \pmb { y } | \pmb { x } ) ) \right] . \right.
$$  

The above conditions imply that, as the number of samples tends to infinity, the performance of the policy under almost all covariates matches the optimal conditional cost.  

DEFINITION 4. A surrogate loss function $\ell$ is Fisher consistent with respect to the SPO loss if the following condition holds for all $\pmb { x } \in \mathcal { X }$ :  

$$
\underset { \theta } { \operatorname { a r g m i n } } \mathbb { E } _ { \mathbb { P } ( y | \mathbf { x } ) } [ \ell ( g _ { \theta } ( \pmb { x } ) , \pmb { y } ) ] \subseteq \underset { \theta } { \operatorname { a r g m i n } } \mathbb { E } _ { \mathbb { P } ( \pmb { y } | \mathbf { x } ) } [ \ell _ { \mathtt { S P } 0 } ( g _ { \theta } ( \pmb { x } ) , \pmb { y } ) ] .
$$  

The Fisher consistency condition defined above requires complete knowledge of the joint distribution $\mathbb { P }$ . Instead, an interesting issue is to determine if the surrogate loss $\ell$ is calibrated with respect to SPO, that is, whether low surrogate excess risk translates to small excess true risk.  

DEFINITION 5. (Ho-Nguyen and Kƒ±lƒ±nc¬∏-Karzan 2022) A loss function $\ell$ is uniformly calibrated with respect to SPO for a class of distributions $\mathcal { D }$ on $\mathcal { X } \times \mathcal { Y }$ if for $\epsilon > 0$ , there exists a function $\Delta _ { \ell } ( \cdot ) : \mathbb { R } ^ { + }  \mathbb { R } ^ { + }$ such that for all $x \in \mathcal { X }$ :  

$$
\begin{array} { r l } & { \mathbb { E } _ { \mathbb { P } ( y \mid x ) } [ \ell ( g _ { \theta } ( x ) , y ) ] - \underset { \theta ^ { \prime } } { \operatorname* { i n f } } \mathbb { E } _ { \mathbb { P } ( y \mid x ) } [ \ell ( g _ { \theta ^ { \prime } } ( x ) , y ) ] < \Delta _ { \ell } ( \epsilon ) } \\ & { \quad \quad \Rightarrow \mathbb { E } _ { \mathbb { P } ( y \mid x ) } [ \ell _ { \mathtt { S P } 0 } ( g _ { \theta } ( x ) , y ) ] - \underset { \theta ^ { \prime } } { \operatorname* { i n f } } \mathbb { E } _ { \mathbb { P } ( y \mid x ) } [ \ell _ { \mathtt { S P } 0 } ( g _ { \theta ^ { \prime } } ( x ) , y ) ] < \epsilon . } \end{array}
$$  

Ho-Nguyen and Kƒ±lƒ±nc¬∏-Karzan (2022) introduce a ‚Äúcalibration function‚Äù with which it is simpler to verify the uniform calibration of MSE loss and show that the calibration function is ${ \mathrm { O } } ( \epsilon ^ { 2 } )$ . Liu and Grigas (2021) assumed that the conditional distribution of $_ y$ given $\scriptstyle { \mathbf { { \vec { x } } } }$ is bounded from below by the density of a normal distribution and obtain a ${ \mathrm { O } } ( \epsilon ^ { 2 } )$ calibration function for polyhedral $\mathcal { Z }$ and ${ \mathrm { O } } ( \epsilon )$ when $\mathcal { Z }$ is a level set of a strongly convex and smooth function.  

DEFINITION 6. The empirical Rademacher complexity of a hypothesis class $\mathcal { H }$ under the loss function $\ell$ is given by:  

$$
\mathbb { E } _ { \sigma } \left[ \operatorname* { s u p } _ { g \in \mathcal { H } } \frac { 1 } { N } \Bigg | \sum _ { i = 1 } ^ { N } \sigma _ { i } \ell ( g ( \pmb { x } _ { i } ) , \pmb { y } _ { i } ) \Bigg | \right] ,
$$  

where $\sigma _ { 1 } , \sigma _ { 2 } , \cdots , \sigma _ { N }$ are independent and identically distributed Rademacher random variables, i.e., $\mathbb { P } ( \sigma _ { i } =$ $\begin{array} { r } { 1 ) = \mathbb { P } ( \sigma _ { i } = - 1 ) = \frac { 1 } { 2 } , \forall i \in \{ 1 , 2 , \cdots , N \} . } \end{array}$  

# Appendix B: List of abbreviations  

In Table B, we provide the list of abbreviations used in this survey.  

Table 4 List of abbreviations   


<html><body><table><tr><td>Abbreviation Description</td><td></td></tr><tr><td>ADMM</td><td>alternating direction method of multipliers</td></tr><tr><td>CSO</td><td>contextual stochastic optimization</td></tr><tr><td>CVaR</td><td>conditional value at risk</td></tr><tr><td>DRO</td><td>distributionally robust optimization</td></tr><tr><td>DNN</td><td>deep neural network</td></tr><tr><td>ERM</td><td>empirical risk minimization</td></tr><tr><td>FP</td><td>fixed point</td></tr><tr><td>ILO</td><td>integrated learning and optimization</td></tr><tr><td>IFT</td><td>implicit function theorem</td></tr><tr><td>kNN</td><td>k-nearest neighbor</td></tr><tr><td>KKT</td><td>Karush-Kuhn-Tucker</td></tr><tr><td>KL</td><td>Kullback-Leibler</td></tr><tr><td>LDR</td><td>linear decision rule</td></tr><tr><td>LP</td><td>linear program</td></tr><tr><td>MDP</td><td>Markov decision process</td></tr><tr><td>ML</td><td>machine learning</td></tr><tr><td>MLE</td><td>maximum likelihood estimation</td></tr><tr><td>MILP</td><td>mixed-integer linear program</td></tr><tr><td>NW</td><td>Nadaraya-Watson</td></tr><tr><td>RKHS</td><td>reproducing kernel Hilbert space</td></tr><tr><td>SAA</td><td>sample average approximation</td></tr><tr><td>SLO</td><td>sequential learning and optimization</td></tr><tr><td>QP</td><td>quadratic program</td></tr></table></body></html>  