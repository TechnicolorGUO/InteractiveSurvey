# A Survey of Integrated Sensing and Communication for 5G-A and 6G

# 1 Abstract


The rapid advancement of wireless communication technologies has driven the evolution of mobile networks from 1G to 5G, opening new frontiers in integrated services such as sensing and communication (ISAC). This survey paper focuses on the research and development of ISAC systems for 5G-Advanced (5G-A) and 6G networks, aiming to provide a comprehensive overview of recent advancements, challenges, and future directions in the design and optimization of ISAC systems. The paper delves into various aspects, including channel capacity and beamforming optimization, advanced antenna and waveform design, system integration and performance evaluation, and the application of artificial intelligence (AI) and machine learning (ML) techniques. Key findings include the development of efficient beamforming techniques, the design of dual-band reconfigurable antenna arrays, and the integration of AI for enhanced network performance. The survey also highlights the contributions of this work, which include a structured analysis of the state-of-the-art in ISAC systems, identification of key challenges, and a roadmap for future research. By synthesizing existing literature and presenting a detailed overview, this survey aims to serve as a valuable resource for researchers and practitioners in the field.

# 2 Introduction
The rapid advancement of wireless communication technologies has been a driving force behind the evolution of mobile networks, from the initial 1G systems to the current 5G networks. These advancements have not only increased data rates and reduced latency but have also opened new frontiers in integrated services, such as sensing and communication. The integration of sensing and communication (ISAC) represents a significant step forward in this evolution, aiming to merge the functionalities of communication and radar systems into a unified framework [1]. This integration is particularly crucial for the development of 5G-Advanced (5G-A) and 6G networks, where the coexistence and mutual enhancement of these functionalities are essential for achieving the desired performance and reliability.

This survey paper focuses on the research and development of ISAC systems for 5G-A and 6G networks [2]. The primary objective is to provide a comprehensive overview of the recent advancements, challenges, and future directions in the design and optimization of ISAC systems [3]. The paper delves into various aspects, including channel capacity and beamforming optimization, advanced antenna and waveform design, system integration and performance evaluation, and the application of artificial intelligence (AI) and machine learning (ML) techniques. By synthesizing the existing literature and presenting a structured analysis, this survey aims to serve as a valuable resource for researchers and practitioners in the field.

The paper begins by exploring the optimization and design of ISAC systems, with a focus on channel capacity and beamforming optimization. This section discusses the derivation of closed-form capacity formulas and the concept of ergodic saturation, which are crucial for understanding the fundamental limits of communication systems [4]. It also examines efficient beamforming techniques and computational load reduction methods, which are essential for managing the complexity associated with large-scale antenna arrays. Additionally, the paper delves into multi-objective optimization for peak-to-average power ratio (PAPR) and ambiguity functions, highlighting the trade-offs between communication and sensing performance.

The discussion then shifts to advanced antenna and waveform design, where the paper examines dual-band reconfigurable antenna arrays and hybrid beamforming techniques [5]. These technologies are vital for achieving the high data rates and wide coverage required for next-generation wireless systems. The paper also explores adaptive waveform and sequence generation networks (AWSGNet) and the application of Riemannian geometry for unimodular waveform optimization. These topics are essential for addressing the challenges of high-frequency bands and dynamic channel conditions.

System integration and performance evaluation are subsequently covered, with a focus on OFDM bistatic micro-Doppler models for multi-propeller drones, 6DMA technology, and field measurements and validation of ISAC channel models [6]. These sections provide insights into the practical implementation of ISAC systems, emphasizing the importance of accurate modeling and real-world validation [2]. The paper also discusses the role of AI and ML in enhancing network performance, including reinforcement learning for network optimization, deep learning for channel and data prediction, and AI-driven system design and evaluation.

Finally, the paper highlights the contributions of this survey, which include a comprehensive review of the state-of-the-art in ISAC systems, identification of key challenges and open research questions, and a roadmap for future research directions [2]. By providing a detailed and structured overview, this survey aims to facilitate further advancements in the field and inspire innovative solutions for the development of 5G-A and 6G networks [7].

# 3 Optimization and Design of ISAC Systems

## 3.1 Channel Capacity and Beamforming Optimization

### 3.1.1 Closed-Form Capacity Formulas and Ergodic Saturation
Closed-form capacity formulas play a pivotal role in understanding the fundamental limits of communication systems, particularly in scenarios involving multiple antennas and complex propagation environments. In the context of bistatic setups, these formulas provide a means to quantify the achievable data rates under various channel conditions. The derivation of such formulas often relies on the assumption of ergodicity, which simplifies the analysis by averaging over the channel realizations. For confined bistatic setups, the challenge lies in accurately modeling the channel statistics, especially when the transmitter and receiver arrays operate in different normalized angular frequency (NAF) domains. The SARA (Sampling and Reconstruction Algorithm) approach, originally developed for monostatic configurations, has been adapted to address this issue by decomposing the azimuth operations into separate tasks for the transmitter and receiver arrays [8]. This decomposition facilitates the optimal angular sampling required to capture the channel characteristics effectively.

The concept of ergodic saturation is closely tied to the behavior of the system as the number of antennas increases. In array-to-array systems, the ergodic capacity can reach a saturation point, beyond which additional antennas do not significantly enhance the capacity. This phenomenon is particularly relevant in scenarios where the total array power is limited. The saturation point can be derived analytically, providing insights into the optimal configuration of the antenna arrays [4]. For instance, Zhu et al. demonstrated that the ergodic capacity saturation occurs when the total array power reaches a certain threshold, which is influenced by the channel conditions and the array geometry [4]. In multi-user systems, the power limit is typically imposed on each individual user, leading to a different saturation behavior compared to array-to-array systems. Understanding this behavior is crucial for designing efficient beamforming schemes that can operate near the capacity limits.

To address the computational complexity associated with large-scale antenna arrays, several methods have been proposed to simplify the beamforming calculations [4]. One such method involves substituting the large-scale inner product operations with more manageable formulas, thereby reducing the computational load. This approach is particularly useful in scenarios where real-time processing is required. Numerical simulations have been used to validate these analytical findings, confirming the accuracy of the derived closed-form expressions. These results not only provide a theoretical foundation for the design of bistatic communication systems but also offer practical guidelines for optimizing the system performance in real-world deployments. The integration of these advanced techniques into the system design can lead to significant improvements in both capacity and reliability, making them essential tools for the development of next-generation communication networks.

### 3.1.2 Efficient Beamforming Techniques and Computational Load Reduction
Efficient beamforming techniques are essential for mitigating the computational load and hardware complexity in advanced wireless systems, particularly in the context of integrated sensing and communication (ISAC) and millimeter-wave (mmWave) communications [5]. Traditional beamforming methods, such as maximum-ratio transmission (MRT) and zero-forcing (ZF), are computationally intensive and may not be suitable for large-scale antenna arrays and high-frequency bands [4]. To address these challenges, recent research has focused on developing low-complexity algorithms that can achieve comparable performance while reducing computational requirements. One such approach is the use of suboptimal beamforming techniques that leverage structured matrices and fast algorithms, such as the fast Fourier transform (FFT), to reduce the complexity of the beamforming process.

In the context of ISAC, the integration of communication and sensing functions imposes additional constraints on the beamforming design, such as the need to simultaneously optimize for both communication and sensing performance [3]. This dual objective can be addressed through joint communication and sensing (JCS) beamforming, which involves the design of a unified waveform and beamforming strategy. For instance, the use of orthogonal time frequency space (OTFS) modulation in ISAC systems has been shown to offer robust performance in delay-Doppler channels, making it a promising candidate for sub-THz band communications. OTFS can effectively handle the high Doppler spread and phase noise common in these environments, thereby reducing the computational load associated with channel estimation and equalization.

Moreover, the deployment of intelligent reflecting surfaces (IRS) and reconfigurable intelligent surfaces (RIS) in ISAC systems has introduced new opportunities for efficient beamforming [9]. These surfaces can dynamically adjust their reflection coefficients to enhance the desired signal and suppress interference, thereby improving the overall system performance. To achieve this, advanced optimization techniques, such as alternating direction method of multipliers (ADMM) and successive convex approximation (SCA), have been proposed to design the IRS phase shifts and beamforming vectors. These methods can significantly reduce the computational complexity by breaking down the problem into smaller, more manageable subproblems, making them suitable for real-time implementation in large-scale ISAC systems.

### 3.1.3 Multi-Objective Optimization for PAPR and Ambiguity Functions
In the context of integrated sensing and communication (ISAC) systems, the optimization of waveforms to achieve a balance between peak-to-average power ratio (PAPR) and ambiguity functions (AFs) is a critical challenge. The PAPR is a key parameter that affects the efficiency and reliability of the communication system, while the ambiguity function is crucial for radar performance, particularly in terms of target detection and resolution. The inherent trade-offs between these two metrics necessitate a multi-objective optimization approach that can effectively manage the conflicting requirements of communication and sensing.

To address this, recent studies have explored various optimization techniques, including the use of fractional delay and fractional Doppler shift constraints to formulate a multi-objective problem [10]. The goal is to design waveforms that minimize the PAPR while simultaneously optimizing the ambiguity function for both auto-correlation (APSL) and cross-correlation (CPSL) properties. This optimization problem is inherently non-convex and complex, requiring advanced algorithms to find a feasible solution. One promising approach is the development of an adaptive waveform and sequence generation network (AWSGNet), which leverages machine learning to jointly optimize waveform parameters and sequences.

The AWSGNet framework is designed to convert the multi-objective optimization problem into a more tractable form, enabling the system to dynamically adapt to varying channel conditions and target environments [10]. To handle the non-convex constraints, a successive convex approximation method is employed, which iteratively refines the solution to approach the optimal trade-off between PAPR and AFs. This approach not only improves the performance of the ISAC system but also reduces the computational complexity, making it suitable for real-time applications. The effectiveness of this method is validated through extensive simulations, demonstrating its superiority over traditional optimization techniques in terms of both PAPR reduction and improved radar performance.

## 3.2 Advanced Antenna and Waveform Design

### 3.2.1 Dual-Band Reconfigurable Antenna Arrays and Hybrid Beamforming
Dual-Band Reconfigurable Antenna Arrays (DBRAAs) represent a significant advancement in antenna technology, enabling simultaneous operation in both sub-6 GHz and mmWave bands [5]. The integration of these bands within a single array is crucial for achieving the high data rates and wide coverage required for next-generation wireless systems. The DBRAA design leverages reconfigurable antennas, which can dynamically adjust their characteristics to optimize performance across different frequency bands. This adaptability is essential for mitigating the trade-offs between the high bandwidth and short-range of mmWave bands and the lower bandwidth but extended coverage of sub-6 GHz bands.

Hybrid beamforming (HBF) is a key technique employed in DBRAAs to balance the benefits of analog and digital beamforming. In HBF, a subset of the total number of antennas is connected to RF chains through phase shifters, allowing for the formation of multiple beams with reduced hardware complexity and power consumption. The reconfigurable nature of the DBRAA, combined with HBF, enables the system to dynamically allocate resources and adapt to varying channel conditions. This is particularly important in mmWave bands, where the high path loss and sensitivity to blockage necessitate agile beamforming solutions.

To further enhance the flexibility and efficiency of DBRAAs, a reconfigurable hybrid beamforming (RHB) structure has been proposed [5]. In this structure, the RF chains are connected to the antennas via phase shifters and a reconfigurable switch, allowing for real-time adjustments to the beamforming configuration. This approach not only reduces the number of required RF chains but also enables the system to switch between different beamforming modes, such as narrow-beam high-gain configurations for long-range communication and wide-beam low-gain configurations for short-range sensing. The RHB structure is particularly advantageous in integrated sensing and communication (ISAC) systems, where the same hardware must support both communication and radar functionalities.

### 3.2.2 Adaptive Waveform and Sequence Generation Networks
Adaptive Waveform and Sequence Generation Networks (AWSGNet) have emerged as a critical component in the design of integrated sensing and communication (ISAC) systems, particularly in addressing the challenges of high-frequency bands such as millimeter waves and terahertz. These networks are designed to optimize the tradeoff between peak-to-average power ratio (PAPR), aperiodic peak sidelobe level (APSL), and cyclic peak sidelobe level (CPSL) under fractional delay and fractional Doppler shift constraints. By leveraging advanced optimization techniques, AWSGNet can dynamically adjust waveform parameters and sequences to enhance both communication and sensing performance. This is particularly important in environments with high mobility and dynamic channel conditions, where traditional fixed waveforms may fail to provide the required robustness and efficiency.

The design of AWSGNet involves formulating a multi-objective optimization problem that aims to balance the conflicting requirements of communication and sensing [10]. To tackle the nonconvex nature of this problem, a successive convex approximation method is employed to iteratively refine the solution. The alternating direction method of multipliers (ADMM) is then used to solve the optimization problem, allowing for the joint optimization of waveform parameters and sequences. This approach ensures that the generated waveforms and sequences are not only optimal for communication but also meet the stringent requirements of radar sensing, such as low sidelobe levels and high resolution. The ADMM-RHB algorithm, which alternates between optimizing the communication sum-rate and the sensing beamforming gain, has been shown to achieve significant improvements over state-of-the-art 5G waveforms and sequences.

Practical design examples of AWSGNet demonstrate its effectiveness in various scenarios. For instance, the network can generate sequences with continuous amplitude and phase, unimodular sequences with continuous phase, and unimodular sequences with discrete phase, each tailored to specific application requirements. Numerical results indicate that AWSGNet can achieve a substantial reduction in PAPR, APSL, and CPSL, thereby enhancing the overall performance of ISAC systems. These improvements are crucial for enabling the seamless integration of communication and sensing functionalities in future 6G networks, where the coexistence of multiple services and the efficient use of spectrum resources are paramount [11].

### 3.2.3 Riemannian Geometry for Unimodular Waveform Optimization
Riemannian geometry plays a pivotal role in the optimization of unimodular waveforms for integrated sensing and communication (ISAC) systems [3]. The unimodular constraint, which requires the magnitude of each waveform component to be unity, imposes a non-convex and non-linear structure on the optimization problem. This constraint can be naturally modeled using the unit sphere in the complex plane, forming a Riemannian manifold. By leveraging the intrinsic geometric properties of this manifold, the optimization problem can be reformulated in a way that facilitates efficient and effective solutions.

The key step in this approach is the construction of the Riemannian manifold and its associated operators. The manifold is defined as the set of all complex vectors with unit magnitude, and the tangent space at each point on the manifold is characterized by the orthogonal complement of the vector itself. This geometric framework allows for the development of gradient descent methods that respect the unimodular constraint. Specifically, the Riemannian gradient descent (RGD) algorithm updates the waveform iteratively by moving along the geodesics of the manifold, ensuring that each update remains on the manifold. This method avoids the projection steps required in conventional constrained optimization, leading to faster convergence and reduced computational complexity.

To further enhance the performance of the optimization, a customized nonsmooth unimodulus manifold gradient descent (N-UMGD) algorithm is proposed [3]. This algorithm incorporates techniques from nonsmooth optimization to handle the non-differentiable nature of the objective function, particularly when dealing with metrics such as the peak-to-average power ratio (PAPR) and the auto-correlation function (ACF) sidelobe levels. The N-UMGD algorithm combines the benefits of Riemannian geometry with advanced optimization techniques, resulting in a robust and efficient solution for designing unimodular waveforms that meet the stringent requirements of ISAC systems. The effectiveness of this approach is demonstrated through extensive simulations, showing significant improvements in both communication and sensing performance.

## 3.3 System Integration and Performance Evaluation

### 3.3.1 OFDM Bistatic Micro-Doppler Models for Multi-Propeller Drones
In the context of integrated sensing and communication (ISAC) systems, the development of accurate and efficient bistatic micro-Doppler models for multi-propeller drones is essential for enhancing target recognition and classification [6]. The OFDM bistatic micro-Doppler model for multi-propeller drones builds upon the classical monostatic thin wire model, which has been extensively studied for its ability to capture the dynamic behavior of rotating propellers [6]. This model is extended to accommodate the complexities introduced by multiple propellers, each contributing unique micro-Doppler signatures due to their distinct rotational speeds and positions. The model accounts for the relative motion between the drone and the radar, as well as the interactions between the propellers and the surrounding environment, which can significantly affect the received signal.

The proposed model incorporates a detailed representation of the propeller blades, treating each blade as a series of discrete scatterers. This approach enables the simulation of the micro-Doppler effect caused by the periodic motion of the blades, which manifests as a series of frequency modulations in the received signal. By considering the contributions from all propellers, the model can accurately predict the composite micro-Doppler signature of the drone. The model also includes the effects of bistatic geometry, where the transmitter and receiver are spatially separated, leading to additional Doppler shifts and phase variations. These factors are crucial for developing robust ISAC systems that can operate in diverse environments and under varying conditions [11].

To validate the proposed model, extensive simulations and field measurements have been conducted. The simulations involve generating synthetic micro-Doppler signatures for a multi-propeller drone with varying numbers of propellers and rotational speeds, and comparing these with theoretical predictions [6]. Field measurements, on the other hand, involve deploying a bistatic radar system to capture the micro-Doppler signatures of actual drones in flight. The results show a strong agreement between the simulated and measured data, confirming the accuracy of the model. This validated model serves as a foundation for developing advanced ISAC algorithms that can effectively classify and track multi-propeller drones, contributing to the broader goal of integrating communication and sensing functionalities in next-generation wireless systems.

### 3.3.2 6DMA Technology for Enhanced System Capacity and Efficiency
6DMA (Sixth-Generation Multiple Access) technology represents a significant advancement in wireless communication systems, designed to enhance system capacity and efficiency by leveraging advanced signal processing techniques and innovative resource allocation strategies [5]. Unlike traditional multiple access methods, 6DMA is capable of supporting a vast number of users and devices with minimal interference, even in highly congested environments [7]. This is achieved through the integration of advanced modulation schemes, such as orthogonal time frequency space (OTFS) and discrete Fourier transform spread orthogonal frequency-division multiplexing (DFT-s-OFDM), which are specifically tailored to handle the challenges of high-frequency bands, including sub-THz and mmWave regimes. These waveforms not only improve spectral efficiency but also mitigate the effects of Doppler spread and delay, which are critical issues in fast-moving environments and dense urban areas.

One of the key features of 6DMA is its ability to dynamically allocate resources based on real-time network conditions and user demands. This dynamic resource allocation is facilitated by intelligent algorithms that can adapt to varying channel conditions and traffic patterns, ensuring optimal performance and resource utilization. For instance, in a scenario where multiple high-speed vehicles are communicating simultaneously, 6DMA can dynamically adjust the allocation of time, frequency, and spatial resources to maintain high data rates and low latency, even in the presence of significant Doppler shifts. Additionally, 6DMA supports the integration of artificial intelligence (AI) and machine learning (ML) techniques, which can further enhance the system's adaptability and efficiency by predicting and optimizing network behavior based on historical data and real-time analytics.

Moreover, 6DMA technology is designed to support a wide range of applications, from ultra-reliable low-latency communication (URLLC) for industrial automation and autonomous vehicles to massive machine-type communication (mMTC) for the Internet of Things (IoT) [7]. In urban environments, 6DMA can be deployed on various infrastructure elements, such as building rooftops, lamp posts, and advertising boards, to create a dense and flexible network that can support a diverse array of services [7]. The deployment of 6DMA in these settings not only enhances the overall network capacity but also improves the reliability and robustness of communication links, making it a crucial component in the transition to 6G networks [5]. By addressing the challenges of high-frequency band communications and dynamic resource management, 6DMA is poised to play a pivotal role in shaping the future of wireless communication systems.

### 3.3.3 Field Measurements and Validation of ISAC Channel Models
Field measurements play a crucial role in validating the accuracy and reliability of ISAC channel models, which are essential for the successful deployment of integrated sensing and communication (ISAC) systems [12]. In a bistatic ISAC setup, the transmitter (TX) and receiver (RX) are spatially separated, requiring precise control over the angles of departure (AoDs) and angles of arrival (AoAs). The propagation environment in such setups is often harsh, with multipath effects and non-line-of-sight (NLOS) conditions, which can significantly impact the performance of both communication and sensing functionalities. To address these challenges, extensive field measurements have been conducted to characterize the channel behavior under various conditions, including urban, rural, and indoor environments.

One of the key aspects of these measurements is the validation of the proposed ISAC channel models against real-world data [12]. For instance, a measurement campaign at 59.6 GHz in a vehicle-to-vehicle (V2V) communication scenario demonstrated the importance of accurate path loss (PL) models. The proposed PL model, parameterized to fit the specific scenario, showed a better fit to experimental data compared to other standard models, as evaluated using metrics such as root mean square error (RMSE) and Pearson correlation coefficient (PCC) [13]. These findings underscore the need for scenario-specific channel models that can accurately capture the statistical fading properties and dynamic changes in the environment.

Additionally, the validation process involves assessing the impact of hardware impairments, such as carrier frequency offset (CFO) and sampling time offset (STO), on the performance of ISAC systems. Field measurements with a 4 × 8 MIMO-OFDM setup at 27.5 GHz confirmed the consistency of CFO and SFO across all receive channels, with minor deviations attributed to limited estimation accuracy. The results also highlighted the importance of compensating for hardware non-idealities to ensure robust communication and radar sensing performance. Overall, these field measurements and validations provide valuable insights into the practical implementation of ISAC systems, guiding the development of more accurate and reliable channel models for future 6G networks [2].

# 4 AI and Machine Learning in Wireless Networks

## 4.1 Reinforcement Learning for Network Optimization

### 4.1.1 Actor-Critic DRL for UAV Path Optimization
In the realm of UAV path optimization, the Actor-Critic Deep Reinforcement Learning (AC-DRL) framework has emerged as a powerful approach to address the dynamic and complex decision-making processes involved in 5G mmWave communication networks [14]. This framework leverages two neural networks: the actor, which learns the policy for optimal actions, and the critic, which evaluates the actions taken by the actor. The actor network, parameterized by \( \theta \), outputs the probability distribution over actions given the current state, while the critic network, parameterized by \( \phi \), estimates the value function \( V(s) \) or the action-value function \( Q(s, a) \) to assess the expected future rewards. This dual-network structure enables the AC-DRL model to iteratively refine its policy and value function, leading to more efficient and adaptive path planning for UAVs.

The AC-DRL framework is particularly well-suited for UAV path optimization due to its ability to handle the non-stationary environments and the high-dimensional action spaces characteristic of 5G mmWave networks [14]. By training on channel data obtained from advanced ray tracing tools, the model can learn to anticipate and mitigate the effects of signal blockage, multipath fading, and other propagation challenges. The use of a model-free approach further enhances the adaptability of the AC-DRL model, as it does not require a priori knowledge of the environment, making it highly suitable for real-time path optimization in dynamic scenarios. The critic network provides a continuous feedback loop, allowing the actor to adjust its policy based on the immediate and long-term consequences of its actions, thus optimizing the UAV's trajectory for minimal handover and maximum signal-to-interference-plus-noise ratio (SINR).

Furthermore, the AC-DRL model's ability to jointly optimize multiple objectives, such as flight time, handover frequency, and connectivity, represents a significant advancement over previous approaches. This multi-objective optimization ensures that the UAV can navigate complex urban environments while maintaining high-quality communication links with ground base stations. The training process, which involves extensive simulations and real-world data, enables the model to develop a robust policy that can generalize well across different environmental conditions and network configurations. The results from our experiments demonstrate that the AC-DRL approach not only achieves superior performance in terms of path efficiency and connectivity but also provides a scalable and flexible solution for UAV-assisted 5G networks [14].

### 4.1.2 DRL for Adaptive Handover and Resource Allocation
Deep Reinforcement Learning (DRL) has emerged as a promising approach for addressing the complexities of adaptive handover (HO) and resource allocation in modern cellular networks. Traditional HO mechanisms, which rely on static thresholds and pre-defined rules, struggle to cope with the dynamic and unpredictable nature of network conditions, especially in high-mobility environments. DRL, by contrast, can learn from environmental feedback and adapt its policies over time, making it well-suited for optimizing HO decisions and resource allocation in real-time. DRL-based HO protocols can dynamically adjust the timing and target cells for handovers, reducing the probability of ping-ponging (frequent handovers between cells) and radio link failures (RLFs), while ensuring that Quality of Service (QoS) requirements are met.

In multi-tier networks, which consist of macro and micro base stations (BSs), the need for adaptive HO protocols is even more pronounced. These networks require sophisticated algorithms that can handle the heterogeneity of BSs and the varying density of user equipment (UE). DRL frameworks can model the interactions between UEs and BSs, taking into account factors such as signal strength, channel conditions, and network load. By training on historical data and continuously updating their policies, DRL agents can predict future network states and make proactive HO decisions. This not only improves the overall network efficiency but also enhances the user experience by minimizing disruptions during handovers.

Resource allocation in cellular networks is another critical area where DRL has shown significant potential. Conventional resource allocation methods, such as FDMA, TDMA, and CDMA, are based on fixed and static allocations, which can lead to suboptimal use of available resources [15]. DRL, on the other hand, can dynamically allocate resources such as frequency bands, time slots, and power levels based on the current network conditions and the specific needs of individual UEs. This adaptive approach ensures that resources are used more efficiently, leading to improved throughput, reduced latency, and better overall network performance. Multi-agent DRL systems, where multiple agents collaborate to optimize resource allocation across the network, have shown particular promise in handling the complexity of large-scale and heterogeneous networks. Despite the increased computational complexity, the benefits of DRL in terms of adaptability and performance make it a compelling solution for future wireless communication systems.

### 4.1.3 Game Theory for Distributed Decision Making
Game theory has emerged as a powerful framework for addressing the complexities of distributed decision-making in wireless networks, particularly in scenarios where multiple autonomous entities must make decisions that affect the overall system performance [16]. In the context of distributed decision-making, game theory models the interactions between nodes as strategic games, where each node is a player with its own objectives and strategies. The primary goal is to find a stable state, often referred to as a Nash equilibrium, where no player can unilaterally improve its payoff by changing its strategy. This approach is particularly useful in scenarios such as resource allocation, where nodes must decide how to use limited resources in a way that maximizes their individual or collective utility.

In the realm of wireless communications, game theory has been applied to various problems, including channel access, power control, and routing [16]. For instance, in channel access, nodes must decide when to transmit to avoid collisions and maximize throughput. Game-theoretic models can capture the strategic interactions between nodes, leading to efficient and fair resource allocation. Similarly, in power control, nodes must decide on their transmission power levels to balance between maximizing their own data rates and minimizing interference to other nodes. By modeling these interactions as games, researchers have developed algorithms that converge to Nash equilibria, ensuring stable and efficient network operation.

Moreover, game theory has been extended to dynamic and adaptive scenarios, where the network environment changes over time. In such settings, the traditional static game models are insufficient, and dynamic game models are required. These models account for the temporal evolution of the network and the adaptive behavior of the nodes. For example, in vehicular ad-hoc networks (VANETs), where vehicles move and the network topology changes rapidly, dynamic game models can predict the future states of the network and guide nodes to make optimal decisions. This is crucial for applications such as traffic management and autonomous driving, where real-time decision-making is essential. The integration of game theory with machine learning techniques, such as reinforcement learning, further enhances the adaptability of the nodes, enabling them to learn and optimize their strategies over time.

## 4.2 Machine Learning for Channel and Data Prediction

### 4.2.1 Neural Networks for CSI Prediction and Beamforming
Neural Networks (NNs) have emerged as a powerful tool for predicting Channel State Information (CSI) and optimizing beamforming in wireless communication systems. These models leverage historical CSI data to forecast future channel conditions, enabling proactive adjustments in beamforming and resource allocation. By treating the CSI prediction task as a regression problem, NNs can capture the complex temporal and spatial correlations in channel dynamics [17]. Early works utilized traditional feedforward and recurrent neural networks (RNNs) to predict CSI, achieving significant improvements over conventional methods. However, these models often struggled with the high computational complexity and data requirements for training.

Recent advancements have introduced more sophisticated architectures, such as transformers and reinforcement learning (RL), to enhance CSI prediction and beamforming. Transformers, with their ability to handle long-range dependencies and parallelize computations, have been particularly effective in CSI prediction tasks. A transformer-based parallel CSI prediction network has been proposed, which not only predicts future CSI but also facilitates rapid acquisition of the predicted CSI, crucial for real-time applications [17]. Additionally, RL-driven networks have been developed to jointly optimize CSI prediction and beamforming in multi-user scenarios. These RL models learn optimal beamforming policies by interacting with the environment, adapting to dynamic changes in channel conditions and user demands.

To further improve efficiency and reduce training costs, transfer learning has been integrated into CSI prediction and beamforming frameworks [17]. Transfer learning allows models to leverage pre-trained weights from related tasks, significantly reducing the amount of data and computational resources required for training. This approach has been particularly beneficial in unique communication scenarios, such as low Earth orbit (LEO) satellite and high-speed railway communications, where data collection is challenging. Moreover, the integration of reconfigurable intelligent surfaces (RIS) has opened new avenues for environment-aware active and passive beamforming, further enhancing the performance and robustness of wireless communication systems [1].

### 4.2.2 Deep Learning for Channel Estimation and Signal Detection
Deep learning (DL) has emerged as a powerful tool for channel estimation and signal detection in wireless communications, offering significant improvements over traditional methods [18]. By leveraging large datasets and complex neural network architectures, DL can capture intricate patterns and nonlinearities in channel data, leading to enhanced accuracy and robustness. Specifically, convolutional neural networks (CNNs) have been widely employed for channel estimation due to their ability to extract spatial and temporal features from channel state information (CSI). These networks can effectively handle the high-dimensional and noisy nature of CSI, providing more accurate predictions compared to conventional linear models. Moreover, recurrent neural networks (RNNs), particularly long short-term memory (LSTM) networks, have been utilized to model the temporal dynamics of channels, which is crucial for predicting future channel states and mitigating the effects of channel aging.

In addition to CNNs and RNNs, generative adversarial networks (GANs) have been explored for generating synthetic channel data, which can be used to augment training datasets and improve the generalization of channel estimation models. GANs consist of a generator and a discriminator, where the generator creates realistic channel data, and the discriminator evaluates its authenticity. This adversarial training process helps in creating diverse and realistic channel scenarios, enhancing the model's ability to generalize to unseen conditions. Furthermore, hybrid models combining multiple DL techniques, such as those integrating CNNs and RNNs, have been proposed to leverage the strengths of each architecture. These hybrid models can capture both spatial and temporal dependencies, leading to more comprehensive and accurate channel estimations.

Recent advances in DL have also focused on reducing the computational complexity and latency of channel estimation and signal detection algorithms. Techniques such as knowledge distillation and pruning have been applied to compress large neural networks, making them more suitable for real-time applications in resource-constrained environments. Additionally, federated learning has been explored to train DL models across multiple devices without centralized data aggregation, ensuring privacy and security [19]. This approach is particularly relevant for edge computing scenarios, where distributed devices can collaboratively learn and update channel estimation models. Overall, DL has revolutionized channel estimation and signal detection, offering promising solutions to the challenges posed by dynamic and complex wireless environments.

### 4.2.3 Transfer Learning for Reduced Training Times and Data Costs
Transfer learning has emerged as a powerful technique to reduce training times and data costs in various wireless communication tasks, particularly in the context of channel state information (CSI) prediction [17]. By leveraging pre-trained models, transfer learning allows for the adaptation of learned representations to new, related tasks, thereby significantly reducing the need for extensive labeled data and computational resources. For instance, in the prediction of downlink throughput in V2X networks, pre-training a model on a large, general dataset and then fine-tuning it on a smaller, task-specific dataset has been shown to achieve comparable or even superior performance to models trained solely on the smaller dataset. This approach not only accelerates the convergence of the model but also enhances its robustness to variations in the operational environment.

One of the key advantages of transfer learning in wireless communication is its ability to handle the dynamic and heterogeneous nature of wireless channels. Traditional machine learning models often struggle with concept drift, where the statistical properties of the data change over time, leading to a degradation in model performance. Transfer learning can mitigate this issue by continuously updating the model with new data, thereby maintaining its relevance and accuracy. For example, in the context of CSI prediction for high-speed railway communications, where the channel conditions can change rapidly, transfer learning has been used to adapt pre-trained models to new environments, reducing the need for extensive retraining [17]. This is particularly important in resource-constrained environments where computational and energy resources are limited.

Moreover, transfer learning techniques such as Low-Rank Adaptation (LoRA) have been applied to enable more efficient and fine-grained adaptation of pre-trained models. LoRA achieves this by introducing a small set of task-specific parameters while keeping the primary model parameters frozen, thus preserving the shared knowledge across tasks [20]. This approach not only reduces the computational overhead associated with fine-tuning but also enhances the model's ability to generalize to new tasks [20]. In the context of V2X networks, LoRA has been successfully used to adapt models for instantaneous downlink throughput prediction in urban environments, where the channel conditions are highly variable and the data is often limited. By leveraging these advanced transfer learning techniques, wireless communication systems can achieve significant improvements in efficiency and performance, making them more adaptable and robust in real-world scenarios.

## 4.3 AI-Driven System Design and Evaluation

### 4.3.1 Vision Transformers for Wireless Applications
Vision Transformers (ViTs) have emerged as a powerful tool in wireless communications, particularly for tasks requiring the processing of complex and high-dimensional data. Unlike traditional convolutional neural networks (CNNs), ViTs leverage self-attention mechanisms to capture long-range dependencies and global context, making them highly suitable for applications such as channel state information (CSI) prediction, beam management, and localization. For instance, in CSI prediction, ViTs can efficiently process large volumes of historical data to predict future channel conditions, thereby reducing the need for frequent and resource-intensive pilot transmissions. This capability is crucial in millimeter-wave (mmWave) communications, where the dynamic nature of the channel necessitates rapid and accurate predictions to maintain high data rates and reliability [21].

Moreover, ViTs have been successfully applied to beam management in wireless systems, where they can dynamically adapt to changing environmental conditions and user movements. By learning from past beamforming patterns and environmental data, ViTs can predict optimal beam directions and power allocations, enhancing the overall network performance and user experience. This is particularly beneficial in dense urban environments where multipath effects and blockages are common, and traditional methods struggle to maintain consistent connectivity. Additionally, ViTs can be integrated with reinforcement learning (RL) frameworks to further optimize beam management strategies, enabling real-time adjustments to network conditions and user demands.

In the realm of localization, ViTs have shown promise in improving the accuracy and robustness of positioning algorithms. By incorporating visual and environmental data, such as LiDAR and camera inputs, ViTs can create rich and detailed maps of the surroundings, which are essential for precise localization in indoor and outdoor settings. This is particularly relevant for applications like autonomous vehicles and augmented reality, where accurate and real-time localization is critical. Furthermore, the ability of ViTs to handle multimodal data makes them well-suited for integrating various sensor inputs, leading to more reliable and versatile localization solutions. Overall, the application of ViTs in wireless communications represents a significant step forward in addressing the complex and dynamic challenges of modern wireless networks.

### 4.3.2 Decentralized Mixture of Experts for GenAI Models
Decentralized Mixture of Experts (MoE) architectures represent a significant advancement in scaling Generative AI (GenAI) models while maintaining efficiency and performance. Unlike monolithic models, MoE frameworks distribute the computational load across a network of specialized sub-models, or experts, which are activated based on the input data's characteristics [22]. This selective activation mechanism ensures that only relevant experts are engaged, thereby reducing the overall computational overhead and improving the model's scalability. The gating mechanism, a critical component of MoE, dynamically determines which experts to activate, allowing for efficient resource utilization and enhanced performance, especially in resource-constrained environments such as edge computing and mobile devices.

State-of-the-art GenAI models, including DeepSeek, GLaM, and Switch Transformer, have successfully implemented MoE architectures to expand their capacity and handle complex tasks [22]. These models leverage the sparse activation of experts to manage the increased computational demands of GenAI, which often involve large-scale language generation, image synthesis, and multimodal reasoning. By dynamically selecting and activating only the necessary experts, these models can maintain high performance levels while significantly reducing the computational and energy costs associated with full model activation. This approach is particularly beneficial in wireless communication systems, where computational resources are limited and energy efficiency is crucial.

Moreover, the decentralized nature of MoE architectures enhances the robustness and flexibility of GenAI models. Each expert can be independently updated and optimized, allowing for continuous improvement and adaptation to new data and tasks without retraining the entire model. This modular design facilitates easier integration of new functionalities and supports the development of more versatile and adaptable GenAI systems. Additionally, the distributed computation across multiple experts can help mitigate the risk of overfitting and improve the model's generalization capabilities, making MoE a promising direction for the future of scalable and efficient GenAI models.

### 4.3.3 Predictive Routing for Vehicular Networks
Predictive routing in vehicular networks (VNs) represents a significant advancement in addressing the challenges posed by the dynamic and unpredictable nature of vehicular environments. Unlike traditional reactive routing methods, which make decisions based on current or near-real-time network conditions, predictive routing leverages advanced algorithms and models to forecast future network states. This approach is particularly crucial in VNs, where the topology can change rapidly due to vehicle movement, leading to frequent path disruptions and degraded Quality of Service (QoS). By anticipating these changes, predictive routing can proactively establish and maintain stable communication paths, thereby ensuring continuous and reliable connectivity.

One of the key enablers of predictive routing is the availability of accurate and timely data on vehicle mobility and network conditions. Modern VNs are equipped with sophisticated sensors and communication modules that can collect and share a wealth of information, including vehicle speed, direction, and proximity to other vehicles, as well as channel conditions and network load. Machine learning (ML) techniques, particularly deep learning (DL) models, have been extensively applied to process this data and generate accurate predictions. For instance, Random Forest (RF) and XGBoost models have shown promise in predicting the throughput of vehicular links, while deep neural networks (DNNs) like RadioUNet and RMEGAN have been used to construct channel knowledge maps (CKMs) that can predict channel conditions over a wide area. These predictions are then used to optimize routing decisions, such as selecting the best path or switching to an alternative route before a link failure occurs.

To implement predictive routing effectively, it is essential to design robust algorithms that can integrate various types of predictions into the routing process. One such approach is the top3 routing algorithm (TORA), which uses unified metrics to evaluate the strength, connectivity, and hop count of potential paths [23]. TORA selects the top three paths between a vehicle and a base station (BS) based on these metrics, ensuring that the network can quickly switch to an alternative path if the primary one fails. Additionally, the integration of cloud computing and edge computing resources can further enhance the capabilities of predictive routing by providing the necessary computational power to process large datasets and execute complex algorithms in real-time [23]. This combination of advanced prediction models and efficient routing algorithms holds the potential to significantly improve the reliability and performance of vehicular networks, making them more resilient to the dynamic changes in their operating environment.

# 5 Resource Management and Network Optimization

## 5.1 Federated Learning and Edge Computing

### 5.1.1 Federated Averaging over Real Networks
Federated Averaging (FedAvg) is a foundational algorithm in Federated Learning (FL) that enables decentralized training of machine learning models across multiple devices [19]. In real-world deployments, particularly over 5G networks, the performance of FedAvg is significantly influenced by the network's latency, bandwidth, and reliability [19]. The synchronous nature of FedAvg, where all participating devices must complete their local training rounds before the global model is updated, poses challenges in environments with varying network conditions. This section explores the practical implications of deploying FedAvg over real 5G networks, focusing on the impact of network dynamics on convergence and the strategies to mitigate these effects.

In a 5G network, the high variability in device connectivity and the potential for intermittent disconnections can disrupt the FedAvg process. The algorithm's reliance on consistent and timely updates from all clients can lead to increased training times and suboptimal model performance. To address these issues, researchers have proposed various techniques, such as adaptive client selection, where only the most reliable or well-connected devices are chosen for each round of training. This approach helps to reduce the impact of stragglers and ensures that the global model is updated more frequently with high-quality gradients. Additionally, the use of asynchronous variants of FedAvg, where updates are applied as soon as they are received, can further improve the robustness of the training process in dynamic network environments.

Another critical aspect of deploying FedAvg over 5G networks is the optimization of communication efficiency. The large volume of data exchanged during the training process can strain network resources, leading to increased latency and reduced throughput. Techniques such as model compression, quantization, and sparse updates have been explored to reduce the communication overhead. These methods aim to minimize the amount of data transmitted between devices and the central server, thereby improving the overall efficiency of the FL process. Furthermore, the integration of edge computing resources can help to preprocess and aggregate local updates, reducing the load on the 5G network and accelerating the convergence of the global model.

### 5.1.2 Edge Device Integration and Performance Metrics
Edge device integration within 5G and 6G networks is pivotal for achieving the stringent performance metrics required for ultra-reliable low-latency communications (URLLC) and massive machine-type communications (mMTC). The integration of edge devices, such as sensors, actuators, and IoT devices, necessitates a robust and flexible architecture that can support diverse communication needs. This section explores the technical challenges and solutions associated with integrating edge devices into 5G/6G networks, focusing on performance metrics such as latency, reliability, and throughput.

One of the primary challenges in edge device integration is ensuring low latency and high reliability, especially in mission-critical applications. The deployment of multi-access edge computing (MEC) plays a crucial role in addressing these challenges by bringing compute resources closer to the edge devices. MEC reduces the signal travel distance, thereby minimizing latency and improving responsiveness. However, current MEC implementations are often limited to operator-managed sites, which can restrict the scalability and flexibility of edge computing. To overcome this, researchers have proposed the use of device clouds, which leverage the computational capabilities of user devices to form a multi-level inference framework [24]. This approach not only enhances the computational capacity at the edge but also supports distributed inference and edge AI, which are essential for advanced applications like augmented reality (AR) and virtual reality (VR).

Another critical aspect of edge device integration is the development of efficient scheduling algorithms that can optimize resource allocation and task offloading. Traditional scheduling algorithms, such as Round Robin, often fail to meet the dynamic and heterogeneous demands of edge devices, leading to suboptimal performance. To address this, novel scheduling algorithms, such as the Intelligent Traffic Scheduling Preemptor (ITSP) algorithm, have been proposed [25]. These algorithms aim to minimize latency and maximize throughput by intelligently managing the allocation of network resources. Additionally, the integration of edge devices with 5G/6G networks requires the development of protocols and medium access control (MAC) mechanisms that can support both push- and pull-based traffic. These mechanisms must be designed to meet goal-oriented key performance indicators (KPIs), such as estimation error and control performance, in addition to traditional timing metrics. The successful implementation of these protocols and algorithms is essential for realizing the full potential of edge device integration in 5G/6G networks.

### 5.1.3 Reproducibility and Open-Source Contributions
Reproducibility and open-source contributions are pivotal for advancing the field of wireless communication, particularly in the development and deployment of 5G and 6G systems. Open-source platforms, such as OAI UE and srsUE, provide researchers and developers with the flexibility to customize and experiment with various layers of the protocol stack, which is crucial for exploring new communication paradigms and technologies [26]. These platforms enable the community to collaboratively address challenges and innovate, fostering a more transparent and collaborative research environment. However, achieving high levels of reliability and performance comparable to commercial RANs remains a significant challenge. Open-source solutions must not only match the performance of commercial systems but also maintain consistency under real-world conditions, including varying interference levels, diverse environmental scenarios, and high mobility [26].

To enhance reproducibility, open-source contributions must include detailed documentation, well-defined APIs, and comprehensive testing frameworks. This ensures that researchers and developers can easily replicate experiments and build upon existing work. For instance, the integration of AI/ML capabilities into O-RAN frameworks requires robust and standardized methods for training and deploying machine learning models. Open-source platforms that support these features can significantly accelerate the development and deployment of intelligent RAN solutions. Moreover, the availability of open datasets and simulation tools, such as ns3 with O-RAN (ns-O-RAN), facilitates the evaluation of new algorithms and protocols under realistic network conditions. By providing a common ground for experimentation, these tools help to validate theoretical findings and bridge the gap between academic research and practical implementation.

In addition to technical contributions, the open-source community plays a vital role in fostering innovation and collaboration. Open-source projects often benefit from a diverse and global community of contributors, each bringing unique perspectives and expertise. This collaborative environment accelerates the identification and resolution of issues, leading to more robust and reliable systems. Furthermore, open-source contributions can drive standardization efforts, ensuring that new technologies are interoperable and can be seamlessly integrated into existing networks. As the field of 5G and 6G continues to evolve, the importance of reproducibility and open-source contributions cannot be overstated, as they are essential for driving progress and ensuring that the benefits of these advanced technologies are widely accessible [26].

## 5.2 Network Scheduling and Resource Allocation

### 5.2.1 Dynamic Radio Scheduling with O-RAN Framework
Dynamic Radio Scheduling with O-RAN Framework represents a significant advancement in the realm of 5G and beyond (B5G) networks, addressing the inherent complexities and dynamic nature of modern radio environments [27]. The Open Radio Access Network (O-RAN) architecture introduces a disaggregated and software-defined approach to RAN, enabling greater flexibility, interoperability, and intelligence in network operations [27]. This framework facilitates the deployment of advanced scheduling algorithms that can dynamically adapt to varying network conditions, traffic demands, and application requirements.

In the O-RAN framework, the Radio Intelligent Controller (RIC) plays a pivotal role in orchestrating dynamic radio scheduling. The RIC leverages real-time data from various network elements, such as the Radio Unit (RU), Distributed Unit (DU), and Centralized Unit (CU), to make informed decisions about resource allocation. By integrating artificial intelligence (AI) and machine learning (ML) capabilities, the RIC can predict traffic patterns, optimize resource utilization, and minimize latency, thereby enhancing the overall network performance. This intelligent scheduling is particularly crucial for supporting diverse and demanding applications, such as ultra-reliable low-latency communications (URLLC) and massive machine-type communications (mMTC).

Moreover, the O-RAN framework supports the deployment of xApps (extended Applications) that can be tailored to specific use cases and network conditions. These xApps enable the implementation of flexible and adaptive scheduling strategies, allowing network operators to fine-tune resource allocation based on real-time feedback and analytics. For instance, an xApp for dynamic radio scheduling can adjust the allocation of time-frequency resources to prioritize critical traffic, manage interference, and ensure fair resource distribution among multiple users [27]. This level of customization and responsiveness is essential for meeting the stringent requirements of 5G and B5G networks, where the ability to dynamically adapt to changing conditions is paramount.

### 5.2.2 Mixed Integer Linear Programming for Slice Allocation
Mixed Integer Linear Programming (MILP) has emerged as a powerful tool for solving complex optimization problems in network resource allocation, particularly in the context of network slicing. In the realm of 5G and beyond, the efficient allocation of resources across multiple slices is critical to meeting the diverse Quality of Service (QoS) requirements of various applications [15]. MILP formulations allow for the precise modeling of discrete decisions, such as the assignment of resource blocks to specific slices, alongside continuous variables representing resource utilization levels. This dual capability makes MILP particularly suited for scenarios where both the allocation of discrete resources and the optimization of continuous performance metrics are necessary.

In the context of slice allocation, the MILP model typically aims to maximize the overall network utility while ensuring that each slice meets its specific QoS constraints. These constraints can include minimum bandwidth guarantees, latency bounds, and packet loss rates, which are essential for maintaining the performance of latency-sensitive applications such as URLLC (Ultra-Reliable Low Latency Communications). The formulation of the MILP problem involves defining decision variables that represent the allocation of resource blocks to slices, as well as constraints that enforce the QoS requirements and the physical limitations of the network infrastructure. The objective function often seeks to balance the trade-offs between maximizing throughput, minimizing latency, and ensuring fair resource distribution among slices.

To address the computational complexity of solving large-scale MILP problems, various heuristic and approximation algorithms have been developed. These algorithms leverage techniques such as branch-and-bound, cutting planes, and column generation to find near-optimal solutions in a reasonable amount of time. Additionally, the integration of machine learning models can further enhance the efficiency of the MILP-based slice allocation process by predicting future traffic patterns and dynamically adjusting resource allocations in real-time. This hybrid approach combines the precision of mathematical optimization with the adaptability of machine learning, making it a promising direction for future research in network slicing and resource management.

### 5.2.3 MAC Protocols for 6G Wireless Systems
Medium Access Control (MAC) protocols in 6G wireless systems are pivotal for achieving the stringent requirements of future communication networks, including ultra-reliable low-latency communication (URLLC) and massive machine type communication (mMTC) [28]. Unlike 5G, where the focus was primarily on enhancing mobile broadband (eMBB) capabilities, 6G aims to seamlessly integrate a diverse array of services, each with unique QoS requirements. This necessitates the development of MAC protocols that can dynamically adapt to varying network conditions and application demands. The design of these protocols must address the challenges posed by the heterogeneity of devices, the dynamic nature of wireless channels, and the need for efficient resource allocation.

To meet these challenges, 6G MAC protocols are expected to incorporate advanced techniques such as artificial intelligence (AI) and machine learning (ML) to enable intelligent and adaptive resource management [28]. These techniques can help in predicting channel conditions, optimizing scheduling decisions, and managing interference in a multi-user environment. Additionally, the integration of time-sensitive networking (TSN) principles into 6G MAC protocols is crucial for ensuring deterministic performance, which is essential for real-time applications in industrial automation and critical infrastructure [29]. TSN's time-aware shaping (TAS) mechanisms can be adapted to the wireless domain to provide guaranteed latency and jitter bounds, aligning with the 6G vision of a converged network that supports both wired and wireless communications [29].

Furthermore, 6G MAC protocols must support a flexible and scalable architecture that can accommodate the deployment of network slicing and multi-access edge computing (MEC) [28]. Network slicing allows for the creation of multiple virtual networks, each tailored to specific use cases, while MEC brings computation closer to the end-users, reducing latency and improving service delivery. The MAC layer will play a critical role in orchestrating these functionalities, ensuring that resources are allocated efficiently and that the performance of each slice is maintained. This holistic approach to MAC protocol design is essential for realizing the full potential of 6G networks, which are expected to serve a wide range of applications from augmented reality (AR) and virtual reality (VR) to autonomous vehicles and smart cities.

## 5.3 Performance Evaluation and Testbeds

### 5.3.1 5G Delay Testbed and CICV5G Dataset
The 5G Delay Testbed and CICV5G Dataset are pivotal contributions to the study of 5G communication delay, particularly in the context of Connected and Intelligent Connected Vehicles (CICVs) [30]. This testbed, established at the intelligent connected vehicle evaluation base at Tongji University, provides a modular and rapidly deployable platform for evaluating 5G communication performance under various real-world conditions. The testbed is designed to simulate and measure the impact of different traffic scenarios, driving velocities, data transmission frequencies, and network signal strengths on communication delay. By conducting extensive field tests, the testbed has collected over 300,000 records, forming the basis of the CICV5G dataset.

The CICV5G dataset is comprehensive, encompassing detailed metrics such as communication delay, reference signal received power (RSRP), and signal-to-noise ratio (SNR). These metrics are crucial for understanding the statistical characteristics of 5G communication delay in real driving environments [30]. The dataset reveals that under normal delay conditions, 5G communication can support the precise navigation and control (PnC) of CICVs with performance levels comparable to those of autonomous vehicles (AVs) [30]. However, under significant delays, the timing of delay occurrence significantly impacts control errors, highlighting the importance of delay management in CICV applications. This insight is valuable for designing robust control strategies and optimizing network configurations.

The 5G Delay Testbed and CICV5G dataset serve as essential tools for researchers and engineers working on the development and optimization of 5G-based CICV systems. The dataset's rich set of real-world data facilitates the validation of theoretical models and the testing of new algorithms aimed at reducing communication delay and improving overall system reliability. Furthermore, the testbed's modular design allows for easy adaptation and expansion, making it a versatile platform for ongoing research and development in the field of 5G and CICVs. Together, these resources contribute to advancing the integration of 5G technology in the automotive industry, paving the way for safer and more efficient transportation systems.

### 5.3.2 Simulation-Based Evaluations of LLM Inference Offloading
Simulation-based evaluations of LLM inference offloading are crucial for understanding the practical implications of deploying these models in real-world scenarios, particularly in environments with stringent latency and reliability requirements [24]. In this section, we explore the methodologies and findings from various simulation studies that assess the performance of LLM inference tasks when offloaded to different network components, such as edge clouds and MNO's networks [24]. These simulations typically incorporate detailed models of typical transformer architectures, GPU specifications, and the interplay between communication and computing latencies, providing a comprehensive view of the system's behavior under various conditions.

The simulations reveal that the performance of LLM inference offloading is highly dependent on the network architecture and the specific offloading strategy employed. For instance, a priority-based scheme that dynamically adjusts the allocation of computational resources based on the urgency of the inference tasks can significantly reduce latency and improve overall system efficiency. This approach is particularly effective in scenarios where multiple LLMs are deployed, and the network must manage a diverse set of inference requests with varying priorities. The simulations also highlight the importance of adaptive joint device selection and model partitioning, which can further optimize the offloading process by balancing the load across available resources and minimizing delays.

Moreover, the integration of computing capabilities within RAN nodes, as proposed in the concept of Integrated Communication and Computing (ICC), emerges as a promising solution for enhancing the performance of LLM inference offloading [24]. By enabling RAN nodes to perform initial processing of inference tasks, ICC can reduce the amount of data that needs to be transmitted over the network, thereby decreasing latency and improving the overall efficiency of the system. The simulation results demonstrate that ICC can achieve superior performance compared to traditional disjoint approaches, particularly in scenarios with high traffic loads and strict latency requirements. These findings underscore the potential of ICC in facilitating the seamless deployment of LLMs in future 5G and 6G networks.

### 5.3.3 Empirical Analysis of 5G-TSN Integration
Empirical analysis of 5G-TSN integration focuses on evaluating the performance and reliability of integrating Time-Sensitive Networking (TSN) functionalities within the 5G ecosystem. TSN, originally designed for wired networks, ensures deterministic communication with strict latency and jitter requirements, which are critical for industrial automation and other real-time applications [29]. The integration of TSN into 5G aims to extend these capabilities to wireless environments, thereby enabling a broader range of applications, such as factory automation, remote surgery, and autonomous vehicles. The empirical studies in this domain primarily assess the impact of 5G's inherent variability on TSN performance metrics, including latency, jitter, and packet loss [29].

Several key challenges emerge from the empirical analysis of 5G-TSN integration. One of the primary concerns is the variability in 5G network conditions, which can significantly affect the deterministic guarantees provided by TSN. For instance, the dynamic nature of 5G radio channels, influenced by factors such as mobility, interference, and environmental changes, can introduce unpredictable delays and jitter. Empirical studies have shown that while 5G can achieve low average latencies, the variance in latency can be substantial, particularly in dense and dynamic environments. This variability poses a significant challenge for TSN applications that require consistent and predictable performance.

To address these challenges, researchers have explored various techniques and algorithms to enhance the reliability and predictability of 5G-TSN integration. These include advanced scheduling algorithms, such as the Intelligent Traffic Scheduling Preemptor (ITSP) algorithm, which dynamically adjusts resource allocation based on real-time network conditions and traffic demands. Additionally, the use of network slicing in 5G allows for the creation of dedicated slices that can be optimized for specific TSN requirements, ensuring that critical traffic receives the necessary resources and prioritization [28]. Empirical evaluations have demonstrated that these approaches can significantly improve the performance of 5G-TSN integration, making it a viable solution for demanding industrial applications [29].

# 6 Future Directions


The current landscape of integrated sensing and communication (ISAC) systems for 5G-Advanced (5G-A) and 6G networks, while promising, is not without its limitations and gaps. One of the most significant challenges is the complexity of designing and optimizing systems that can efficiently integrate communication and sensing functionalities without compromising performance in either domain. The current approaches often struggle with the computational complexity of large-scale antenna arrays and the need for real-time processing, which can be particularly demanding in dynamic and high-frequency environments. Additionally, the integration of these systems into practical deployments remains a challenge, as real-world conditions such as multipath fading, non-line-of-sight (NLOS) propagation, and hardware impairments can significantly affect system performance. Furthermore, the lack of standardized and validated channel models for ISAC systems hinders the development of robust and reliable systems, particularly in bistatic and multistatic configurations.

To address these limitations, several directions for future research are proposed. First, there is a need for the development of more advanced and computationally efficient algorithms for beamforming and waveform optimization. This includes the exploration of hybrid beamforming techniques that can balance the benefits of analog and digital beamforming, as well as the design of adaptive waveforms that can dynamically adjust to varying channel conditions and application requirements. The integration of artificial intelligence (AI) and machine learning (ML) techniques, such as deep learning and reinforcement learning, can play a crucial role in this area by enabling real-time optimization and decision-making. For example, reinforcement learning can be used to dynamically adjust beamforming strategies and resource allocation in response to changing network conditions, while deep learning can be applied to predict channel conditions and optimize waveform parameters.

Second, the development of standardized and validated ISAC channel models is essential for the practical implementation of these systems. Field measurements and extensive simulations are required to capture the unique characteristics of ISAC channels, particularly in bistatic and multistatic configurations. These models should account for various environmental factors, such as urban and rural settings, and should be validated through real-world deployments to ensure their accuracy and reliability. Additionally, the integration of reconfigurable intelligent surfaces (RIS) and intelligent reflecting surfaces (IRS) into ISAC systems can enhance the control and manipulation of electromagnetic waves, leading to improved communication and sensing performance. Research in this area should focus on the design of RIS/IRS configurations and the optimization of their reflection coefficients to maximize system performance.

Finally, the potential impact of the proposed future work is significant. The development of more efficient and adaptive algorithms, standardized channel models, and the integration of RIS/IRS can lead to substantial improvements in the performance and reliability of ISAC systems. These advancements will not only enhance the capabilities of 5G-A and 6G networks but also open up new possibilities for applications such as autonomous vehicles, smart cities, and industrial automation. By addressing the current limitations and gaps, the proposed research directions can facilitate the seamless integration of communication and sensing functionalities, paving the way for the next generation of wireless communication systems.

# 7 Conclusion



This survey paper has provided a comprehensive overview of the recent advancements, challenges, and future directions in the design and optimization of integrated sensing and communication (ISAC) systems for 5G-Advanced (5G-A) and 6G networks. Key findings include the derivation of closed-form capacity formulas and the concept of ergodic saturation, which are essential for understanding the fundamental limits of communication systems. The paper also delved into efficient beamforming techniques and computational load reduction methods, multi-objective optimization for peak-to-average power ratio (PAPR) and ambiguity functions, and the integration of advanced antenna and waveform design, including dual-band reconfigurable antenna arrays and hybrid beamforming techniques. The role of artificial intelligence (AI) and machine learning (ML) in enhancing network performance, including reinforcement learning for network optimization, deep learning for channel and data prediction, and AI-driven system design and evaluation, was also extensively discussed. Additionally, the paper explored the practical implementation of ISAC systems through system integration and performance evaluation, emphasizing the importance of accurate modeling and real-world validation.

The significance of this survey lies in its comprehensive and structured analysis of the state-of-the-art in ISAC systems, which serves as a valuable resource for researchers and practitioners in the field. By synthesizing the existing literature, this survey identifies key challenges and open research questions, providing a roadmap for future research directions. The integration of sensing and communication functionalities is crucial for the development of 5G-A and 6G networks, where the coexistence and mutual enhancement of these functionalities are essential for achieving the desired performance and reliability. The insights and methodologies presented in this paper are expected to facilitate further advancements in the field and inspire innovative solutions for the design and optimization of ISAC systems.

In conclusion, the rapid evolution of wireless communication technologies and the integration of sensing and communication functionalities present both exciting opportunities and significant challenges. This survey highlights the importance of continued research and development in ISAC systems to address the growing demands of next-generation networks. We call upon the research community to build upon the findings presented here, explore new frontiers, and collaborate to advance the field. By doing so, we can pave the way for the realization of 5G-A and 6G networks that are not only faster and more reliable but also capable of supporting a wide array of integrated services and applications.

# References
[1] RIS-based Physical Layer Security for Integrated Sensing and  Communication  A Comprehensive Survey  
[2] Integrated Sensing and Communications Over the Years  An Evolution  Perspective  
[3] Unimodular Waveform Design for Integrated Sensing and Communication MIMO  System via Manifold Optimi  
[4] Channel Capacity Saturation Point and Beamforming Acceleration for  Near-Field XL-MIMO Multiuser Com  
[5] DBRAA  Sub-6 GHz and Millimeter Wave Dual-Band Reconfigurable Antenna  Array for ISAC  
[6] Modeling Micro-Doppler Signature of Multi-Propeller Drones in  Distributed ISAC  
[7] A Tutorial on Six-Dimensional Movable Antenna Enhanced Wireless  Networks  Synergizing Positionable  
[8] Optimal Azimuth Sampling and Interpolation for Bistatic ISAC Setups  
[9] Green Integration of Sensing, Communication, and Power Transfer via  STAR-RIS  
[10] The Optimal Tradeoff Between PAPR and Ambiguity Functions for  Generalized OFDM Waveform Set in ISAC  
[11] Research and Experimental Validation for 3GPP ISAC Channel Modeling  Standardization  
[12] Low-Complexity Channel Estimation for RIS-Assisted ISAC System  
[13] Vehicle to vehicle path loss modeling at millimeter wave band for  crossing cars  
[14] Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication  using DRL  
[15] An Efficient Reservation Protocol for Medium Access  When Tree Splitting  Meets Reinforcement Learni  
[16] Static and Repeated Cooperative Games for the Optimization of the AoI in  IoT Networks  
[17] AI for CSI Prediction in 5G-Advanced and Beyond  
[18] Joint Channel Estimation and Signal Detection for MIMO-OFDM  A Novel  Data-Aided Approach with Reduc  
[19] Federated Learning over 5G, WiFi, and Ethernet  Measurements and  Evaluation  
[20] 6G WavesFM  A Foundation Model for Sensing, Communication, and  Localization  
[21] CKMImageNet  A Dataset for AI-Based Channel Knowledge Map Towards  Environment-Aware Communication a  
[22] Decentralization of Generative AI via Mixture of Experts for Wireless  Networks  A Comprehensive Sur  
[23] Robust Predictive Routing for Internet of Vehicles Leveraging Both V2I  and V2V Links  
[24] 6G EdgeAI  Performance Evaluation and Analysis  
[25] Optimizing Resource Allocation and Scheduling towards FRMCS and GSM-R  networks coexistence in Railw  
[26] AraRACH  Enhancing NextG Random Access Reliability in Programmable  Wireless Living Labs  
[27] Performance Evaluation of Scheduling Scheme in O-RAN 5G Network using  NS-3  
[28] Medium Access for Push-Pull Data Transmission in 6G Wireless Systems  
[29] Empirical Analysis of the Impact of 5G Jitter on Time-Aware Shaper  Scheduling in a 5G-TSN Network  
[30] CICV5G  A 5G Communication Delay Dataset for PnC in Cloud-based  Intelligent Connected Vehicles  