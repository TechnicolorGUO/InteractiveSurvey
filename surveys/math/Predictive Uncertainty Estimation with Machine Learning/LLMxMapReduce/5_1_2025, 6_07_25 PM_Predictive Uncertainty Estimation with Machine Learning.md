# 5/1/2025, 6:07:25 PM_Predictive Uncertainty Estimation with Machine Learning  

0. Predictive Uncertainty Estimation with Machine Learning  

# 1. Introduction  

Predictive uncertainty quantifies the confidence or reliability associated with the output of a machine learning model [5]. Unlike traditional point predictions, which provide a single best estimate, predictive uncertainty aims to provide a measure of the potential variability or range of possible outcomes [3,5]. This is critically important in real-world machine learning applications, particularly in safety-critical domains such as medical diagnosis, automated driving, and autonomous systems [2,12,17,18,30]. In such scenarios, understanding when a model is likely to be incorrect or uncertain is paramount for safe and reliable decision-making [1,18]. Predictive uncertainty estimation enables models to “know what they do not know,” guiding users on how to effectively utilize predictions and recognize situations where errors are more likely [1,5].​  

![](images/8f1f6aed37bc703743feff5827960bd757f601063cc9d488d813b14e01a7281f.jpg)  

Predictive uncertainty can be broadly categorized into two main types: aleatoric uncertainty and epistemic uncertainty [2,8,9,10]. Aleatoric uncertainty arises from the inherent noise and randomness present in the data itself and cannot be reduced by simply collecting more data [2,10]. This type of uncertainty captures irreducible variability, such as sensor noise or the intrinsically probabilistic nature of the phenomenon being modeled [9]. Epistemic uncertainty, conversely, stems from a lack of knowledge or insufficient data about the model parameters [2,10]. It reflects the uncertainty in the model itself due to limited training data or model misspecification [9]. Epistemic uncertainty is reducible; it can typically be mitigated by acquiring more diverse and representative labeled data [10]. Distinguishing between these two types is important as they require different modeling approaches and strategies for reduction.​  

Standard machine learning models, particularly deep neural networks (DNNs), often struggle to provide reliable and robust uncertainty estimates [12,17]. These models typically output a single prediction or probability distribution (e.g., via a softmax layer) which often does not objectively reflect the true confidence or uncertainty, leading to models being overconfident, especially on inputs far from the training distribution or misclassified examples [12]. The lack of reliable uncertainty quantification poses significant risks, particularly when deploying these models in high-stakes applications, as it can lead to inappropriate or unsafe actions based on potentially inaccurate but confidently predicted outputs [12]. Addressing these limitations motivates the development of advanced methods for quantifying predictive uncertainty.  

This survey provides a comprehensive overview of predictive uncertainty estimation in machine learning. We begin by elaborating on the types of uncertainty (aleatoric and epistemic) and their sources. Subsequently, we delve into various methodologies developed to quantify uncertainty, ranging from Bayesian approaches like Bayesian Neural Networks [6,13,24,25] to non-Bayesian methods including ensemble techniques and test-time data augmentation [17]. We also discuss critical aspects related to evaluating the quality and reliability of uncertainty estimates [4,16,17]. Real-world applications benefiting from robust uncertainty quantification are highlighted [22,29,30]. Finally, we outline current challenges and potential future directions in this evolving field [29].  

# 2. Types of Predictive Uncertainty  

Predictive uncertainty is a critical aspect of machine learning models, reflecting the degree of confidence in their outputs.  

<html><body><table><tr><td>Type</td><td>Primary Source</td><td>Reducibility</td><td>Also Known As</td><td>Example Impact</td></tr><tr><td>Aleatoric</td><td>Inherent noise/randomn ess in data</td><td>Irreducible (given input)</td><td>Data Uncertainty, Random Uncertainty</td><td>Sensor noise, Label ambiguity, Stochastic process</td></tr><tr><td>Epistemic</td><td>Lack of model knowledge/data</td><td>Reducible (with more data)</td><td>Model Uncertainty, Cognitive Uncertainty</td><td>Input far from training data, Limited data</td></tr></table></body></html>  

Understanding the nature and sources of this uncertainty is paramount for reliable decision-making, especially in highstakes applications. Uncertainty in predictions can be broadly categorized into two fundamental types: aleatoric uncertainty and epistemic uncertainty [2,5,7,8,9,10,11,18,19,28]. These are sometimes referred to by alternative names such as data uncertainty (aleatoric) and model uncertainty (epistemic) [2,7,9,11,18,19] or accidental uncertainty (aleatoric) and perceptual/cognitive uncertainty (epistemic) [8,28].​  

Aleatoric uncertainty captures the inherent randomness or stochasticity within the data generation process itself [2,7,9,18]. Its sources include factors such as noise during data acquisition, measurement errors, label ambiguity, and the intrinsic variability of the system being modeled [2,7,8,9,10,18]. Crucially, aleatoric uncertainty reflects noise that is inherent to the observations and cannot be reduced, even with an infinite amount of data, given the same input features [10,18,19].  

In contrast, epistemic uncertainty arises from the model's lack of knowledge about the true underlying mapping function. This type of uncertainty is primarily a consequence of limited or biased training data, as well as potential limitations or misspecifications in the model architecture or training procedure. Epistemic uncertainty is particularly high for inputs that lie far from the training data distribution, representing regions where the model has not gained sufficient experience [19]. Unlike its aleatoric counterpart, epistemic uncertainty is theoretically reducible by increasing the size and diversity of the training dataset, which allows the model to learn the underlying data distribution more effectively and constrain the space of plausible functions [19].  

The ability to distinguish and quantify these two types of uncertainty is vital [8,30]. Aleatoric uncertainty dictates the fundamental limits of prediction accuracy given the inherent data noise, suggesting where improvements require better data collection or measurement systems. Epistemic uncertainty, on the other hand, highlights areas where the model is uncertain due to lack of training data, indicating where additional data collection or model improvements would be most beneficial. Understanding these sources of uncertainty thus directly impacts how model predictions are interpreted and how models should be evaluated and deployed, particularly in scenarios requiring rigorous uncertainty quantification [21]. The subsequent sections delve deeper into the characteristics, sources, and methods for estimating each type of uncertainty individually.​  

# 2.1 Aleatoric Uncertainty  

Aleatoric uncertainty, also referred to as data uncertainty or random uncertainty [2,11], represents the inherent randomness or noise within the data generation process or the underlying system being modeled [18,28]. Unlike epistemic uncertainty, which stems from a lack of knowledge about the model and can potentially be reduced by acquiring more data, aleatoric uncertainty is fundamentally irreducible [10,11]. It arises from factors such as inherent stochasticity akin to a coin flip, measurement errors, labeling noise, and variability in real-world situations. This includes information loss during data acquisition, such as low image resolution, or errors in labeling. Even small, imperceptible adversarial noise can introduce significant aleatoric uncertainty in deep neural networks. Sensor noise, if it reflects inherent randomness, is also a source of aleatoric uncertainty [22].  

Concrete examples of aleatoric uncertainty can be observed across various domains. In facial key point regression, for instance, significant labeling errors for visually similar data points highlight the presence of aleatoric uncertainty; a larger bias in the dataset implies greater aleatoric uncertainty. In language generation, models that produce a range of category distributions are better equipped to capture the inherent ambiguity in sentence completion, which is a form of aleatoric uncertainty. Autonomous driving scenarios inherently involve stochastic factors contributing to aleatoric uncertainty. The work by Caprio et al. emphasizes the relevance of disentangling aleatoric uncertainty when dealing with ambiguous ground truth in real-world applications [30].​  

Modeling aleatoric uncertainty typically involves predicting the parameters of a distribution over the output, rather than just a single point estimate. For regression tasks, this commonly involves predicting both the mean and variance of a conditional distribution $p ( \boldsymbol { y } | \boldsymbol { x } )$ . For example, the Social LSTM model captures aleatoric uncertainty by predicting the five parameters $( \mu _ { 1 } , \mu _ { 2 } , \sigma _ { 1 } , \sigma _ { 2 } , \rho )$ of a 2D position Gaussian distribution, reflecting inherent data variability. Common measures used to quantify aleatoric uncertainty include standard deviation, variance, Value at Risk (VaR), and entropy. Specialized loss functions are often employed to train models to predict these distribution parameters. Selecting output representations that accurately reflect aleatoric uncertainty is crucial during model training. One approach is to use heteroscedastic regression with deep neural networks, which can quantify irreducible observation noise [25]. Another technique mentioned involves using uninformative priors on sensor noise, allowing for optimal weighting among sensors even with uncertain noise levels, which relates to handling aleatoric uncertainty from measurement systems [22].  

Aleatoric uncertainty can be further categorized into homoscedastic and heteroscedastic types. Homoscedastic uncertainty refers to noise that is constant across all input values, meaning the level of uncertainty is the same regardless of the specific data point. In contrast, heteroscedastic uncertainty varies depending on the input data point; the noise level changes with the input features. Heteroscedastic regression models explicitly aim to capture this input-dependent noise variance [25]. While aleatoric uncertainty cannot be eliminated by gathering more data, understanding its sources and magnitude is essential for reliable predictions. Addressing it often involves focusing on improving data collection processes or enhancing measurement precision where possible.  

# 2.2 Epistemic Uncertainty  

Epistemic uncertainty originates from the model’s inherent lack of knowledge about the true underlying function [2,7,8,9,10,11,18]. Also referred to as cognitive, model, or perceptual uncertainty [2,8,11], this type of uncertainty primarily stems from insufficient or biased training data or limitations within the model architecture or training process itself [2,18,28]. A key characteristic is that epistemic uncertainty is high when the model encounters data points that are significantly different from the training distribution, representing areas of the input space where the model has limited or no experience [5,8,18,19]. For example, a model trained exclusively on images of lions and giraffes will exhibit high epistemic uncertainty when presented with an image of a zombie [18], or similarly, a model distinguishing human and chimpanzee faces will show low confidence for inputs like cat photos [8]. Epistemic uncertainty reflects the ignorance about the true model parameters or structure given the available data [7,11].​  

Quantifying epistemic uncertainty involves estimating the variability or range of possible models that could explain the training data. Key techniques for this include Bayesian approaches, ensemble methods, and dropout-based approximations [2,11]. Bayesian approaches model parameter distributions rather than relying on point estimates, allowing the quantification of uncertainty in the parameters themselves [13,15]. Bayesian Neural Networks (BNNs) are a direct implementation that treats network weights as probability distributions [6,13,24,25]. This approach inherently accounts for the uncertainty arising from limited data and can provide estimates of predictive uncertainty [13]. Ensemble methods quantify epistemic uncertainty by training multiple models and measuring the variance or disagreement among their predictions for a given input. The spread of predictions across the ensemble indicates the model’s uncertainty. Dropoutbased methods, such as Monte Carlo Dropout (MC Dropout), provide a computationally efficient approximation to Bayesian model averaging or ensembling by applying dropout during inference to sample different network configurations and estimating uncertainty from the variability of these samples. More broadly, Monte Carlo methods and the variance of multiple outputs are utilized to formulate and estimate this uncertainty.​  

A crucial aspect of epistemic uncertainty is its relationship with the amount and nature of training data [18]. Unlike aleatoric uncertainty, which is irreducible given the inputs, epistemic uncertainty is theoretically reducible as the model gains more knowledge [8,9,10]. Specifically, increasing the quantity and the diversity of the training data allows the model to better constrain its parameters and reduce the space of plausible functions, thereby decreasing epistemic uncertainty, particularly in regions well-covered by the data. Conversely, when training data is limited or does not adequately cover the input space, multiple models may fit the data equally well, leading to high epistemic uncertainty. While the primary lever for reducing epistemic uncertainty is data acquisition, model complexity also plays a role; complex models might require substantially more data to effectively constrain their parameters compared to simpler models. Improving the training procedure can also contribute to reducing model uncertainty [2].  

# 3. Methods for Predictive Uncertainty Estimation  

Quantifying predictive uncertainty is a critical aspect of developing reliable machine learning models [1,12,21,30]. A diverse range of methods has been developed to address this challenge, broadly categorized based on their underlying principles and approaches to modeling uncertainty.  

![](images/498e470ca8d4effdea90bc59cd0f9bf3c4cf0d32657699a61bc501c37cf3f041.jpg)  

The primary categories include Bayesian methods, ensemble approaches, and frequentist or other non-Bayesian techniques, with specific adaptations often developed for distinct model architectures like Graph Neural Networks (GNNs).  

Bayesian methods provide a principled probabilistic framework by treating model parameters as random variables with associated probability distributions. The core idea involves inferring the posterior distribution over parameters given the training data, from which a predictive distribution for new inputs is derived by marginalizing over all possible parameters: \ $( { \mathsf { p } } ( { \mathsf { y } } ^ { \wedge } | m i d x ^ { \wedge } , \mathsf { D } ) = \backslash { \mathsf { i n t } } { \mathsf { p } } ( { \mathsf { y } } ^ { \wedge } | m i d x ^ { \wedge } , \mathsf { w } ) { \mathsf { p } } ( { \mathsf { w } } { \backslash } { \mathsf { m i d } } \ \mathsf { D } ) { \mathsf { d } } { \mathsf { w } } { \backslash } )$ [15]. This framework inherently models epistemic uncertainty by capturing uncertainty in the model itself, and can capture aleatoric uncertainty through the choice of the likelihood function $\backslash ( \mathsf { p } ( \mathsf { y } ^ { \wedge } | m i d \times ^ { \wedge } , \mathsf { w } ) \backslash )$ . A significant challenge lies in the analytical intractability of the posterior distribution for complex models, necessitating the use of approximate inference techniques such as Markov Chain Monte Carlo (MCMC), including parallel-tempered MCMC [22], and Variational Inference (VI) [15]. Practical approximations like Monte Carlo Dropout (MCD) interpret dropout as approximate Bayesian inference, providing computationally tractable uncertainty estimates for deep networks by performing multiple forward passes at test time [4,6,17,19]. While theoretically appealing and capable of providing well-calibrated uncertainty, full Bayesian inference can be computationally expensive, and approximations may trade off some accuracy or calibration for scalability and speed.  

Ensemble methods quantify predictive uncertainty by combining predictions from multiple models [17]. The variability among the predictions of the ensemble members serves as a measure of uncertainty, particularly epistemic uncertainty reflecting the model's limitations in certain input regions. If the constituent models are probabilistic, ensembles can also capture aleatoric uncertainty. Key to their success is fostering diversity among models [12]. Common techniques include training models on bootstrapped data samples (Bootstrapped Ensembles), using Monte Carlo Dropout, or saving model snapshots during a single training run (Snapshot Ensembles) [6,15]. Bootstrapped ensembles offer strong performance but incur high training costs, while MCD and Snapshot Ensembles reduce training overhead at the expense of increased inference time compared to single deterministic models. Ensemble methods are generally effective at improving both predictive performance and the robustness of uncertainty estimates [20,32].​  

Frequentist and other non-Bayesian methods offer alternative perspectives. Classical frequentist approaches, prominent in time series analysis and traditional regression, provide uncertainty estimates typically in the form of prediction or confidence intervals based on statistical assumptions about the data or model errors [3]. Examples include methods applied to ARIMA and Exponential Smoothing models. In deep learning, a simple non-Bayesian approach is using softmax outputs as confidence scores, although these often require post-hoc calibration to be reliable [4]. Conformal Prediction stands out as a powerful non-Bayesian framework providing distribution-free prediction sets with theoretical coverage guarantees, requiring only data exchangeability [3,21,30]. It quantifies total predictive uncertainty without explicit separation of aleatoric and epistemic components [21]. Other non-Bayesian methods include integrating stochasticity into the model structure, such as SDE-Net which uses stochastic differential equations, and various learned methods in self-supervised learning for tasks like depth estimation, which optimize loss functions designed to predict uncertainty alongside the  

primary output. These include techniques based on image flipping, learned reprojection, log-likelihood maximization, and self-teaching approaches. Compared to Bayesian methods, frequentist approaches might offer different types of statistical guarantees (e.g., coverage) and vary widely in computational cost and the specific types of uncertainty they capture.  

Beyond these general categories, methods are often tailored to specific model types and data structures. Graph Neural Networks (GNNs), for instance, require specialized techniques that adapt Bayesian and ensemble principles or incorporate confidence and attention mechanisms to handle the unique dependencies and structures in graph data [20,26]. These include Bayesian GCNs, uncertainty-aware attention models, and methods focusing on calibrating low-confidence predictions [20]. Traditional models also employ specific methods; linear regression uses established statistical formulas for prediction intervals based on residuals, while ensemble tree methods like Random Forests derive uncertainty from the variability of predictions across trees [3]. The choice of method thus depends heavily on the desired properties of the uncertainty estimate, the computational budget, the scalability requirements, and the specific characteristics of the model architecture and data.​  

# 3.1 Bayesian Methods  

Bayesian methods offer a principled framework for quantifying uncertainty in machine learning models by treating model parameters not as fixed values, but as probability distributions [2,6,7,8,24,28]. The core idea revolves around inferring a posterior distribution over the model parameters, denoted as $\backslash ( \mathsf { p } ( \mathsf { w } \backslash \mathsf { m i d } \ \mathsf { D } ) \backslash )$ , given the training data $\left\backslash ( \mathsf { D } \backslash ) \right.$ . This posterior distribution encapsulates the model uncertainty [2]. For a new input $\backslash ( \times ^ { n } | ) ,$ thepredictivedistribution $| ( p ( y ^ { \boldsymbol { \wedge } } \backslash \mathsf { m i d } \times ^ { \boldsymbol { \wedge } ^ { \star } } , \mathsf { D } ) \backslash )$ is obtained by marginalizing over all possible model parameters $ ( \mathsf { w } \backslash ) $ :  

where $\backslash ( \mathsf { p } ( \mathsf { y } ^ { \wedge } | m i d \times ^ { \wedge } , \mathsf { w } ) \backslash )$ is the likelihood of the new data point given the parameters, and $\backslash ( \mathsf { p } ( \mathsf { w } \backslash \mathsf { m i d } \ \mathsf { D } ) \backslash )$ is the posterior distribution of the parameters given the training data [2,15]. This integration provides a probability distribution for the prediction, allowing for the estimation of predictive uncertainty, typically represented by the variance or entropy of the predictive distribution [11,16,17].  

![](images/d9c45f3e8be78e5fa0ba8721a6c6754b45f3a68a605c8e50a0038f5540e79188.jpg)  

Within the context of deep learning, Bayesian Neural Networks (BNNs) extend this concept by placing distributions over the   
network's weights and biases [6,15,24]. Each connection weight, for instance, might be represented by a Gaussian   
distribution with a mean and variance [6,24]. By inferring the posterior distribution over these parameters $\backslash ( \mathsf { p } ( \mathsf { w } \backslash \mathsf { m i d } \ \mathsf { D } ) \backslash )$ , a   
BNN effectively learns a distribution over functions, enabling the quantification of uncertainty [8]. The training of a BNN   
typically involves determining this posterior distribution using Bayes' theorem:​   
\​  

where $\backslash ( \mathsf { p } ( \mathsf { D } \backslash \mathsf { m i d } \ \mathsf { w } ) \backslash )$ is the likelihood of the data given the weights, $\mathsf { \backslash } ( \mathsf { p } ( \mathsf { w } ) \backslash )$ is the prior distribution over the weights, and \ $( \mathsf { p } ( \mathsf { D } ) \backslash )$ is the model evidence [2,9,15].  

A significant challenge arises because the integral in the denominator, \​  

is often analytically intractable, especially for complex models like deep neural networks with high-dimensional parameter spaces [2,9,15]. This intractability necessitates the use of approximate inference methods to estimate the posterior distribution or directly approximate the predictive distribution [9,15].  

Two primary classes of approximation techniques are commonly employed: Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) [15]. MCMC methods approximate the posterior distribution by drawing samples from it. Techniques like the No-U-Turn Sampler (NUTS) are used within BNN frameworks for uncertainty quantification [25]. For complex, high-dimensional problems, specialized MCMC methods such as parallel-tempered MCMC may be employed to handle multi-modal posteriors and improve sampling efficiency [22].  

VI, on the other hand, approximates the true posterior distribution $\backslash ( \mathsf { p } ( \mathsf { w } \backslash \mathsf { m i d } \ \mathsf { D } ) \backslash )$ with a simpler, tractable distribution \ $( \mathsf { q \_ w h i } ( \mathsf { w } ) \backslash )$ , parameterized by \(\phi\), by minimizing the divergence (e.g., KL divergence) between $\backslash ( { \mathsf { q } } _ { - } \backslash { \mathsf { p h i } } ( { \mathsf { w } } ) \backslash )$ and \ $( \mathsf { p } ( \boldsymbol { \mathsf { w } } \backslash \mathsf { m i d } \ \mathsf { D } ) \backslash )$ [14,15]. VI methods often involve optimizing an objective function, such as the Evidence Lower Bound (ELBO) for models like Variational Autoencoders (VAEs) [13]. Recent advancements in VI for BNNs focus on developing more efficient optimization algorithms, such as natural-gradient algorithms implementable within standard optimizers like Adam and RMSprop (e.g., Vprop) [19]. These methods aim to reduce computational effort and memory requirements compared to earlier VI approaches [19]. Comparing MCMC and VI, MCMC methods can theoretically provide asymptotically exact samples from the posterior given infinite time, but they are often computationally expensive and slow to converge in high dimensions. VI is typically faster and more scalable, framed as an optimization problem, but relies on the choice of the approximate distribution $\left. ( { \mathsf { q } } ) \right.$ , which might not fully capture the complexity of the true posterior, potentially leading to underestimated uncertainty.​  

A widely used and practical approximation to Bayesian inference in deep learning is Monte Carlo Dropout (MCD) [4,6,7,8,9,11,17]. It has been shown that dropout training in deep neural networks can be interpreted as approximate Bayesian inference in deep Gaussian processes or as an approximation to variational inference [9,19]. In MCD, dropout is applied not only during training but also kept active during prediction. By performing multiple forward passes $( \backslash ( { \mathsf { T } } \backslash ) )$ with dropout enabled, different subnetworks are sampled, effectively approximating sampling from the posterior distribution over weights [6,7,15]. The predictive mean and variance are then estimated from the distribution of outputs from these multiple forward passes [11,16]. For a prediction task, the mean can be estimated as:​  

where $\backslash ( \mathsf { p } ( \mathsf { y } \mathrm { = c } \backslash \mathsf { m i d } \times , \mathsf { \backslash h a t \{ \backslash o m e g a \} \_ t ) \backslash } )$ is the model output for class $\backslash ( \mathsf { c } \backslash )$ with parameters \(\hat{\omega}_t\) sampled during the \(t\)-th pass [17]. Predictive uncertainty can be estimated using metrics like predictive entropy [17] or variance [11,16]. MCD provides a computationally attractive way to obtain uncertainty estimates, particularly applicable to large networks where full BNN inference is challenging [8]. Variants like Concrete Dropout have been proposed to improve performance and calibration [19].​  

Beyond general BNNs and their approximations, Bayesian principles are integrated into various deep learning architectures and frameworks. Bayesian Deep Learning (BDL) frameworks can structure models into perception and task-specific components, employing specific Bayesian or deep learning models within these parts [13]. Examples include using Restricted Boltzmann Machines (RBMs), Variational Autoencoders (VAEs), and Natural-Parameter Networks (NPNs) in the perception layer, and Bayesian Networks or Stochastic Processes in the task-specific layer [13]. Deep generative models like VAEs and Generative Adversarial Networks (GANs) are also relevant as they model the data distribution, which is fundamental to Bayesian inference [6]. Specific BNN variants tailored for particular applications exist, such as Multi-Fidelity BNNs for incorporating information from models of varying fidelity or models integrating Gaussian Processes for spectral analysis [25]. Bayesian approaches are also explored in specialized domains like graph neural networks for robust graph contrastive learning, potentially involving uncertainty estimation in node representations or graph structures [20]. Hardware platforms leveraging novel transistor variability can even support stochastic synapses for BNN implementations, offering potential for ultra-low power designs [24]. Further inference techniques used within BNNs include Automatic Differentiation Variational Inference (ADVI) and Laplacian Approximation [25]. Despite their theoretical grounding and increasing  

practicality through approximations, the computational cost and complexity of implementing and training BNNs, especially exact methods or complex VI schemes, remain significant considerations compared to non-Bayesian approaches [8].  

# 3.2 Ensemble Methods  

Ensemble methods represent a prominent approach for quantifying predictive uncertainty by leveraging the combined insights from multiple distinct models [1,9,17,18]. This technique is grounded in the principle that a collection of models, potentially weak individually, can achieve superior performance and robustness when their predictions are aggregated, often surpassing single, highly complex models [23]. By aggregating predictions from diverse models, ensembles provide a mechanism to approximate the posterior distribution and improve the quality of uncertainty estimates [2,19].  

The core mechanism of ensemble methods involves training several models and then combining their individual predictions to yield a final prediction and an estimate of its uncertainty. The variability observed across the predictions of the ensemble members serves as a direct measure of epistemic uncertainty, reflecting the model's lack of confidence in regions of the input space where the trained models disagree [18]. If the constituent models themselves provide probabilistic outputs, the ensemble can also capture aleatoric uncertainty, which stems from inherent noise in the data [18]. Common aggregation techniques include averaging the predicted probabilities or regression outputs [2,17], or calculating the variance across ensemble members' outputs [16]. For instance, in Ensemble Bayesian Networks (EBN), the final predictive probability is obtained by averaging the probabilities from individual networks:​  

$\hat { p } ( y | x ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } p _ { \theta _ { i } } \left( y | x \right)$   
where $\left\backslash \left( \mathsf { N } \right\backslash \right)$ is the number of networks and \(\theta_i\) are the parameters of the \(i_{th}\) network [17]. Predictive Entropy,   
calculated as   
\​   
where $\mathsf { \backslash } ( { \mathsf { C } } \backslash )$ is the number of classes, quantifies the uncertainty; smaller PE values indicate high agreement among  

ensemble members [17].  

A critical factor for the success of ensemble methods is promoting diversity among the individual models [12]. Differen techniques exist to achieve this, influencing the specific type of ensemble method.  

![](images/debe675c47e8652e128cac42d7ee8e0d578887c15b7c9884d1616732b4ecd60b.jpg)  

Several notable ensemble techniques are employed for uncertainty estimation:  

1. Bootstrapped Ensemble (Boot): This traditional method involves training multiple models on different bootstrap samples (resampling with replacement) of the training data [3]. Additionally, diversity can be fostered by using different initialization parameters for each network [16]. While conceptually straightforward and often effective, training \(N\) independent networks incurs significant computational costs, requiring \(N\) separate training sessions [16].  

2. Monte Carlo Dropout (MCDropout): This method leverages dropout during the testing phase, effectively creating an ensemble of subnetworks by performing multiple forward passes with different dropout masks [6,11,15]. The mean and uncertainty are then estimated from these multiple inferences [6,16]. While requiring \(N\) inferences at test time, MCDropout avoids the expensive multiple training sessions of traditional Bootstrapped Ensembles, making it computationally less demanding during training [16].​  

3. Snapshot Ensemble (Snap): Inspired by training optimization techniques, this method trains a single network but saves multiple models (snapshots) at different points during training by periodically adjusting the learning rate using a cosine annealing schedule [16]. The learning rate at step \(t\) is given by:  

$$
\eta ( t ) = \eta _ { m i n } + \frac { \eta _ { m a x } - \eta _ { m i n } } { 2 } \left( 1 + \cos \left( \frac { t \pi } { T / C } \right) \right)
$$  

where $\left. { ( \mathsf { T } \backslash ) } \right.$ is the total steps and $\mathsf { \backslash } ( { \mathsf { C } } \backslash )$ is the number of cycles (and models saved) [16]. This yields an ensemble of models trained for varying durations or optimization states. Like MCDropout, it requires only one training phase, significantly reducing training costs compared to Bootstrapped Ensembles, but necessitates multiple inferences from the saved models [16].​  

1. Other Specialized Ensembles: Ensemble approaches have been applied across various domains and model architectures. Examples include Ensemble Multi-Relational Graph Neural Networks for improved robustness in handling complex graph relationships [20] and Ensemble Convolutional Neural Networks (ECNN) for tasks like driver drowsiness detection, combining multiple CNNs for more reliable predictions [32]. Parallel-tempered MCMC (PTMCMC) can also be viewed as an ensemble of Markov chains sampling at different temperatures to enhance posterior exploration, particularly in multi-modal landscapes [22]. Bayesian Neural Networks (BNNs) can be conceptualized as a form of ensemble model, effectively combining many ANNs while potentially using fewer parameters than a naive ensemble [24].  

Comparing these methods reveals distinct trade-offs. Bootstrapped Ensembles tend to be highly effective but are the most computationally expensive in terms of training time [16]. MCDropout offers a computationally lighter alternative during training but requires multiple forward passes during inference. Snapshot Ensembles provide a balance, requiring only one training run while still generating multiple distinct models for ensemble inference. More advanced sampling techniques like PTMCMC may offer better posterior exploration but come with increased computational complexity typical of MCMC methods [22]. Methods like SDE-Net propose an alternative sampling approach that achieves multiple output samples for uncertainty from a single trained model by leveraging Brownian motion, suggesting potential training cost advantages over multi-train ensembles [5]. Despite the computational overhead, ensemble methods are widely recognized for their ability to provide robust uncertainty estimates and often improve predictive performance [20,23,32].  

# 3.3 Frequentist and Other Non-Bayesian Methods  

Frequentist and other non-Bayesian approaches offer alternative paradigms for quantifying predictive uncertainty in machine learning models, distinct from Bayesian methods which model uncertainty over model parameters. These techniques often focus on providing guarantees on the outcomes of predictions rather than distributions over models.  

Classical frequentist methods, particularly prominent in time series analysis and traditional statistics, provide uncertainty estimates often in the form of confidence or prediction intervals. For example, the ARIMA model, a cornerstone frequentist approach, demonstrates the calculation and interpretation of confidence intervals for time series forecasts [27]. Other frequentist time series methods, such as Exponential Smoothing techniques like SES and HWES, also offer mechanisms for quantifying predictive uncertainty [27]. In regression, methods like LASSO or Ridge Regression, while primarily known for regularization and feature selection, are rooted in frequentist principles and can be extended to analyze parameter uncertainty, although the provided digests focus on their sparsification properties [23]. Techniques such as the Delta Method and the Mean-Variance Estimation Method, originating from nonlinear regression and classical statistics, are also employed for estimating prediction uncertainty by leveraging estimated statistics [3].  

In the context of modern machine learning, particularly deep learning, frequentist approaches manifest in various forms. A fundamental baseline for uncertainty estimation in classification tasks is the direct use of softmax outputs, where the output probability for a class is interpreted as a measure of confidence or certainty [4,9]. While simple, this approach often yields poorly calibrated uncertainty estimates [18]. Probabilistic Deep Learning extends this by explicitly adding probability prediction to the original task, applying not only to classification but also to tasks like object detection [9].  

A distinct and theoretically robust non-Bayesian framework is Conformal Prediction [3,21,30]. Conformal Prediction stands out due to its unique ability to provide distribution-free prediction sets or credal regions with theoretical coverage guarantees [21,30]. A key advantage is that these guarantees hold without making assumptions about the underlying data distribution, relying only on the assumption of data exchangeability [21]. This method can be applied post-hoc to predictions from any underlying algorithm, including complex models like neural networks, to produce confidence sets guaranteed to contain the ground truth with a user-specified probability [21]. Conformal Prediction methods are adaptive, yielding larger uncertainty intervals or sets for input examples that are more difficult for the model, thereby signaling potential model errors [21].​  

Other non-Bayesian approaches include incorporating stochasticity into the model structure. SDE-Net, for instance, integrates stochastic differential equations (SDEs) into neural networks to quantify uncertainty, presenting a non-Bayesian alternative to methods that rely on parameter distributions [5].  

Self-supervised learning methods also offer techniques for quantifying uncertainty, particularly in domains like computer vision [16]. Several non-Bayesian approaches within this paradigm have been explored for tasks like monocular depth estimation [16]:​  

• Uncertainty by image flipping (Post): This simple method estimates uncertainty by performing two forward passes one on the original image and one on a horizontally flipped version. The uncertainty  

$$
\sigma = | D - D ^ { \prime } |
$$  

is defined as the absolute difference between the original depth map $D$ and the re-flipped depth map ​D derived from the flipped image prediction. This technique requires no changes to the network architecture [16].  

• Learned Reprojection (Repr): This method learns an uncertainty map simultaneously with the primary task output, optimizing a loss function that penalizes large errors with low uncertainty and encourages uncertainty where the reprojection error is high:​  

$$
L = \sum _ { p } \frac { | I _ { p } - \hat { I } _ { p } | } { \sigma _ { p } } + \lambda \sigma _ { p }
$$  

where $I _ { p }$ ​ is the pixel intensity, $\hat { I } _ { p }$ is the reprojected intensity, and $\sigma _ { p }$ ​ is the predicted uncertainty at pixel $p$ . For numerical stability, only the uncertainty output network is typically optimized while the main network (e.g., DispNet) is fixed [16].​  

• Log-Likelihood Maximization (Log): This approach directly models the error distribution (e.g., Laplacian for L1 loss, Gaussian for L2 loss) and optimizes the negative log-likelihood, allowing the network to output uncertainty directly. The loss formulation for an L1-based error is:  

$$
L = \sum _ { p } \frac { | I _ { p } - \hat { I } _ { p } | } { \sigma _ { p } } + \log \sigma _ { p }
$$  

It has been noted that this joint optimization may not optimally disentangle different sources of uncertainty, such as depth and pose uncertainty [16].  

• Self-Teaching (Self): To address the entanglement issue, a teacher-student framework can be used, decoupling the training of the primary task network (teacher) and the uncertainty network (student). After training the teacher (e.g., DispNet) using traditional self-supervision, the uncertainty network (σ network) is trained using a log-likelihood loss on the primary task output errors (e.g., depth differences):​  

$$
L = \sum _ { p } \frac { | D _ { p } - \hat { D } _ { p } | } { \sigma _ { p } } + \log \sigma _ { p }
$$  

This decoupled approach has shown improved uncertainty accuracy [16].  

Beyond predictive uncertainty, some non-Bayesian methods address uncertainty related to data quality, such as label noise. Techniques like label propagation and meta-learning leverage structural information (e.g., graph structure) to smooth labels and mitigate the impact of noisy annotations, which can be viewed as a form of handling data uncertainty [11].  

Comparing these methods, their theoretical foundations vary significantly. Classical frequentist methods rely on statistical theory regarding parameter estimation and sampling distributions [27]. Conformal Prediction is grounded in the principles of exchangeability and provides exact finite-sample statistical coverage guarantees [21,30]. Learned non-Bayesian methods, including standard softmax and self-supervised approaches, typically rely on optimizing specific loss functions that encourage the model to output values interpretable as uncertainty alongside the primary prediction [16]. SDE-Net introduces stochastic process theory [5].​  

In terms of statistical guarantees, Conformal Prediction offers the strongest, providing reliable coverage probabilities for prediction sets regardless of the model or data distribution [21]. Frequentist confidence intervals also provide coverage guarantees, but these are often asymptotic and rely on specific model assumptions [27]. Learned methods like softmax outputs provide empirical confidence scores which may require post-hoc calibration [18] to achieve desirable coverage properties.​  

Computational requirements differ. Softmax outputs are computationally inexpensive, requiring only a single forward pass. Conformal Prediction can add computational cost, particularly during the calibration phase, and may require multiple model evaluations depending on the specific variant and underlying algorithm. SDE-Net involves training a modified network architecture [5]. Self-supervised vision methods for uncertainty quantification range from simple post-processing (image flipping requiring two forward passes) to modified network architectures and training objectives (Learned Reprojection, Log-Likelihood, Self-Teaching) [16].  

The types of uncertainty addressed also vary. Many learned methods based on output distributions, like softmax or the LogLikelihood approach in self-supervised learning, primarily capture aleatoric uncertainty inherent in the data. Conformal Prediction quantifies total predictive uncertainty by constructing sets that contain the true outcome, implicitly accounting for both aleatoric and epistemic uncertainty without explicitly separating them [21]. Methods targeting label noise address uncertainty stemming from data imperfections. Frequentist prediction intervals also aim to capture total predictive uncertainty under model assumptions [27]. SDE-Net's use of stochasticity may help capture aspects of epistemic uncertainty [5].​  

Other non-Bayesian language modeling methods like PPM, PAQ, and LSTMcomp are mentioned, though the digests do not detail specific uncertainty quantification techniques applied within these models [33].  

In summary, non-Bayesian methods offer a diverse toolkit for uncertainty quantification. Conformal Prediction provides robust, distribution-free guarantees on prediction sets. Classical frequentist methods offer model-specific interval estimates, while learned methods in deep learning leverage output distributions and specialized loss functions, often focusing on empirical performance and requiring careful calibration. Self-supervised learning techniques adapt these learned approaches to specific tasks and data modalities, sometimes through novel training strategies or architectural modifications.​  

# 3.4 Uncertainty Estimation with Specific Model Types (e.g., GNNs)  

Uncertainty estimation techniques are often intricately linked to the specific architecture of the machine learning model employed and the inherent structure of the data it processes. This is particularly evident in fields utilizing complex data structures, such as graphs, which necessitate methods tailored to Graph Neural Networks (GNNs) [20,26].  

Quantifying uncertainty in GNNs presents unique challenges due to the dependencies between nodes and edges, as well as the varying neighborhood structures. Research in this area seeks to adapt general uncertainty quantification principles— including the distinction between aleatoric (inherent data noise) and epistemic (model parameter uncertainty) uncertainty— to the graph domain [20].  

<html><body><table><tr><td>Model Type</td><td>Uncertainty Approach Examples</td><td>Notes</td></tr><tr><td>Graph Neural Networks (GNNs)</td><td>Bayesian GCNs, Uncertainty- aware Attention, Calibration Techniques, Anomaly Detection</td><td>Tailored to graph structure</td></tr><tr><td>Linear Regression</td><td>Prediction Intervals (based on residuals)</td><td>Statistical formulas</td></tr><tr><td>Tree-Based (e.g., Random Forest)</td><td>Variabilityacross ensemble tree predictions</td><td>Non-parametric estimation</td></tr></table></body></html>  

Various approaches have been proposed, often building upon Bayesian or ensemble techniques [20].  

Several GNN-specific methods for uncertainty estimation and robustness have been developed. Bayesian methods adapted for graphs include Bayesian Graph Convolutional Neural Networks (BGCNs), which treat the observed graph as a random graph family and infer the joint posterior of parameters and node labels. Another Bayesian approach, Bayesian Graph Neural Networks with Adaptive Connection Sampling, utilizes an adaptive sampling framework to handle uncertainty and mitigate overfitting. Beyond Bayesian methods, confidence or attention mechanisms are integrated—for instance, in Confidence-based Graph Convolutional Networks (ConfGCNs), which estimate neighboring node confidence to weight the central node, and Uncertainty-aware Attention Graph Neural Networks (UAGs), which address data uncertainty by considering the class diversity of k-hop neighbors. Probabilistic modeling has also been applied, for example in Probabilistic Graph Neural Networks for Traffic Signal Control, where a probabilistic attention model and variational inference are employed, and in Uncertain Graph Neural Networks (UGNs) for facial action unit detection, which introduce a random variable to capture noise uncertainty. Furthermore, robustness techniques against adversarial attacks or structural perturbations are closely related to uncertainty, with methods such as representing node embeddings as Gaussian distributions to reduce the weight of uncertain nodes or using randomized smoothing for certified robustness. Calibration techniques specifically for GCNs have also been explored to improve reliability [20]. Additionally, anomaly detection approaches using GNNs—such as Deep Graph-level Anomaly Detection by Glocal Knowledge Distillation—can provide anomaly scores that serve as a proxy for uncertainty in graph classification tasks [32].  

While deep learning models like GNNs receive significant attention, traditional models also employ specific methods for uncertainty quantification that differ from simple variance prediction. For linear regression, uncertainty is commonly expressed through prediction intervals [3]. These intervals are calculated based on the predicted value and the estimated standard deviation of the predicted distribution. The standard deviation is estimated using the residual sum of squares, providing a statistical measure of uncertainty around the point prediction [3]. The formula for a prediction interval is given by:​  

$$
\hat { y } \pm z \cdot \sigma
$$  

where $\hat { y }$ is the predicted value, $z$ is the number of standard deviations corresponding to the desired confidence level, and $\sigma$ is the standard deviation of the predicted distribution [3]. An unbiased estimate of the standard deviation $( \sigma )$ can be calculated as:​  

$$
\sigma = \sqrt { \frac { 1 } { N - 2 } \sum _ { i = 1 } ^ { N } { e _ { i } ^ { 2 } } }
$$  

where $N$ is the number of observations and $e _ { i }$ is the residual for the $\mathbf { \chi } _ { i }$ -th observation [3]. Tree-based methods like Regression Trees partition the feature space and assign the mean of the target values in each partition as the prediction [23]. Random Forests improve upon single trees by averaging predictions from multiple trees trained on bootstrapped samples and random feature subsets [23]. While a single tree doesn't naturally output variance, the ensemble nature of Random Forests allows for estimating uncertainty by examining the distribution of predictions across the individual trees in the forest, providing a method distinct from the parametric variance estimation in linear models [23].​  

In summary, uncertainty estimation approaches are diverse and depend significantly on the underlying model architecture and data type. GNNs require specialized methods that adapt Bayesian and ensemble techniques to graph structures, alongside confidence, attention, and robustness mechanisms. Traditional models like linear regression rely on established statistical formulas for prediction intervals, while ensemble tree methods leverage their structure to provide non-parametric uncertainty estimates through the variability of predictions across the ensemble.​  

# 4. Evaluation and Calibration  

Ensuring the reliability and utility of predictive uncertainty estimates is paramount, particularly in applications where model decisions have significant consequences [2,4,16,17,18,27]. This necessity underscores the critical role of rigorous evaluation and calibration methodologies. Evaluating the quality of uncertainty estimates involves assessing various attributes, such as calibration, sharpness, and their effectiveness in supporting downstream tasks like misclassification or out-of-distribution detection [4,16,17,27].  

A key property of reliable uncertainty is calibration, which refers to the statistical consistency between predicted probabilities (or confidence scores) and the observed frequencies of correctness [1]. A well-calibrated model is one where, for example, predictions made with $8 0 \%$ confidence are indeed correct approximately $8 0 \%$ of the time. Evaluating this property involves metrics such as the Expected Calibration Error (ECE) [4,17], which quantifies the discrepancy between confidence and accuracy across different confidence levels. Other relevant evaluation metrics include proper scoring rules like the Negative Log-Likelihood (NLL) and Brier Score, which assess the overall quality of the predictive distribution, encompassing both calibration and sharpness. Sharpness measures the concentration of the predictive distribution; narrower prediction intervals in regression, for instance, indicate sharper estimates, often assessed alongside coverage probability metrics. Furthermore, metrics evaluating the discriminative power or practical utility, such as the Area Under the  

Receiver Operating Characteristic Curve (AUROC) for misclassification or OOD detection [4], task-specific metrics like those based on sparsification plots, or metrics derived from uncertainty confusion matrices [17], are employed to understand how well uncertainty estimates serve specific purposes. Standard error metrics like Forecast Error, Mean Forecast Error, and Mean Absolute Error provide a baseline understanding of the point prediction accuracy, which is related but distinct from uncertainty evaluation. Other metrics from model evaluation, such as AIC for model selection [33], F1-score for classification tasks [32], and metrics for assessing sampling efficiency and convergence in probabilistic models [22], also contribute to evaluating the broader performance context.​  

Given the importance of calibration, various techniques have been developed to improve the alignment between confidence and accuracy. Post-hoc methods, such as Temperature Scaling and Platt Scaling, are commonly applied after model training to adjust the model's outputs and enhance calibration. Other approaches involve techniques integrated during the training process or specific model architectures designed to produce better-calibrated outputs [12,20]. Conformal Prediction offers a framework with theoretical calibration guarantees, providing prediction sets or intervals that cover the true value with a specified probability [30]. Despite advancements, developing comprehensive benchmarks and metrics that capture all facets of uncertainty quality remains an active research challenge, highlighting the ongoing need for improved evaluation and calibration methods.  

# 4.1 Evaluation Metrics  

Evaluating the quality of predictive uncertainty estimates is crucial for determining their reliability and utility across various machine learning tasks.  

<html><body><table><tr><td>Metric Category</td><td>Examples</td><td>Purpose /What it Measures</td></tr><tr><td>Calibration</td><td>Expected Calibration Error (ECE), Brier Score</td><td>Alignment between confidence and accuracy</td></tr><tr><td>Overall Predictive Quality</td><td>Negative Log-Likelihood (NLL)</td><td>Quality of the entire predictive distribution (calibration + sharpness)</td></tr><tr><td>Sharpness</td><td>Mean Prediction Interval Width (MPIW)</td><td>Concentration/Precision of estimates</td></tr><tr><td>Utility/Discrimination</td><td>AUROC (Misclassification/OOD), Sparsification Plots (AUSE, AURG)</td><td>Effectiveness for detecting errors or OOD samples</td></tr><tr><td>Task-Specific (Classification)</td><td>USen, USpe, UPre, UAcc</td><td>How well uncertainty signals correct/incorrect predictions</td></tr></table></body></html>  

A diverse set of metrics has been developed to assess different facets of uncertainty, including calibration, sharpness, and effectiveness in downstream tasks like detecting misclassifications or out-of-distribution (OOD) samples [4,16,17,27].  

Calibration is a fundamental property of uncertainty estimates, indicating whether the predicted probabilities (or confidence scores) align with the observed frequencies of correctness. A well-calibrated model should predict an outcome with $8 0 \%$ confidence approximately $8 0 \%$ of the time. The Expected Calibration Error (ECE) is a widely used metric for assessing calibration, particularly in classification tasks [4,17]. ECE quantifies the difference between accuracy and confidence across different bins of confidence scores [17]. The formula for ECE is given by:​  

$$
E C E = \sum _ { m = 1 } ^ { M } \frac { | B _ { m } | } { n } \left| a c c ( B _ { m } ) - c o n f ( B _ { m } ) \right|
$$  

where $\mathsf { \backslash } ( \mathsf { B \_ m } )$ represents the set of samples whose confidence falls into the m-th bin, $\mathsf { \backslash } ( | \mathsf { B } _ { - } \mathsf { m } | \mathsf { \backslash } )$ is the number of samples in bin $\mathsf { \backslash } ( \mathsf { m } \backslash ) , \backslash ( \mathsf { n } \backslash )$ is the total number of samples, $\mathsf { \backslash } ( \mathsf { a c c } ( \mathsf { B \_ m } ) \backslash )$ is the accuracy within bin $\left\backslash \left( \mathsf { m } \right\backslash \right)$ , and $\mathsf { \backslash } ( \mathsf { c o n f } ( \mathsf { B \_ m } ) \backslash )$ is the average confidence within bin $\mathsf { \backslash } ( \mathsf { m } \backslash )$ [17]. A lower ECE indicates better calibration.  

Another metric related to calibration and overall predictive performance is the Brier score, which is a proper scoring rule used to measure the accuracy of probabilistic predictions [18]. For binary outcomes, it is calculated as the mean squared difference between the predicted probability and the actual outcome (0 or 1). The Negative Log-Likelihood (NLL), often equivalent to the softmax cross-entropy loss in multi-class classification [18], also serves as an evaluation metric for the entire predictive distribution. Lower NLL indicates a better model, reflecting both good calibration and sharpness (concentration of probability mass on the correct outcome). While ECE specifically targets calibration errors, NLL is a proper scoring rule that evaluates the entire predictive distribution's quality.​  

Sharpness refers to the concentration of the predictive distribution. Sharper uncertainty estimates are preferred when they are well-calibrated, as they provide more precise information. For regression tasks, sharpness is often evaluated in terms of the width of prediction intervals. A key goal is to achieve "smaller prediction sets" or narrower prediction intervals while maintaining a desired coverage level [27,30]. Metrics like Mean Prediction Interval Width (MPIW) quantify the average width, while Prediction Interval Coverage Probability (PICP) assesses the percentage of true values falling within the predicted intervals. The goal is typically to maximize PICP up to a target level (e.g., $9 5 \%$ ) while minimizing MPIW [27]. Standard regression error metrics such as Forecast Error, Mean Forecast Error (Bias), and Mean Absolute Error (MAE) [27] provide context for the quality of the point prediction itself, which is complementary to the uncertainty quantification.  

Beyond calibration and sharpness, the practical utility of uncertainty estimates is frequently evaluated by their ability to flag unreliable predictions. Metrics like the Area Under the Receiver Operating Characteristic Curve (AUROC) are commonly employed to assess how well uncertainty can distinguish between different classes of samples, such as correctly classified vs. misclassified instances [4], or in-distribution vs. out-of-distribution samples [5]. A higher AUROC indicates that uncertainty serves as a better ranking criterion for identifying samples where the model is likely wrong or facing novel data.  

Task-specific metrics have also been developed. In the context of monocular depth estimation, sparsification plots are used to visualize the relationship between uncertainty and error [16]. By sorting predictions by descending uncertainty and iteratively removing the most uncertain ones, the change in error on the remaining data can be observed [16]. Metrics derived from these plots include Area Under the Sparsification Error (AUSE), which measures the area between the uncertainty-based error curve and an "oracle" curve (sorted by actual error), and Area Under the Random Gain (AURG), which measures the gain over random removal [16]. A lower AUSE and a higher AURG indicate more effective uncertainty estimates for error prediction.​  

For classification tasks, particularly in medical imaging, metrics like Uncertainty Sensitivity (USen), Uncertainty Specificity (USpe), Uncertainty Precision (UPre), and Uncertainty Accuracy (UAcc) are proposed to evaluate how effectively the model communicates its confidence regarding correct and incorrect predictions [17]. USen measures the ability to identify incorrect predictions as uncertain, while USpe assesses the ability to identify correct predictions as certain [17]. UAcc quantifies the overall agreement between uncertainty and prediction correctness [17]. The formulas are defined as:  

$$
\begin{array} { l } { { U S e n = \displaystyle \frac { T U } { T U + F C } } } \\ { { U S p e = \displaystyle \frac { T C } { T C + F U } } } \\ { { U P r e = \displaystyle \frac { T U } { T U + F U } } } \\ { { U A c c = \displaystyle \frac { T U + T C } { T U + T C + F U + F C } } } \end{array}
$$  

where TU denotes incorrect and uncertain predictions, FC incorrect and certain, TC correct and certain, and FU correct and uncertain [17].  

Furthermore, in applications requiring selective prediction, metrics such as Area Under Risk Coverage (AURC) and coverage for a specific selective accuracy constraint are utilized [4]. These metrics evaluate the performance of a model when it is allowed to abstain from making predictions on the most uncertain inputs.  

In summary, evaluating uncertainty estimates requires considering multiple aspects. Calibration metrics like ECE and Brier Score assess the statistical consistency of predicted probabilities. Proper scoring rules such as NLL evaluate the entire predictive distribution, encompassing both calibration and sharpness. Sharpness metrics, particularly prediction interval width in regression, quantify the precision of the estimates. Utility metrics such as AUROC, AUSE/AURG, USen/USpe/UPre/UAcc, and AURC/Coverage evaluate the practical usefulness of uncertainty for tasks like error detection,  

OOD identification, or selective prediction. Each metric offers a different perspective, and a comprehensive evaluation often involves using a combination tailored to the specific task and type of uncertainty being assessed.  

# 4.2 Calibration Techniques  

Calibration techniques are fundamental for ensuring that a machine learning model’s confidence scores accurately reflect the probability of its predictions being correct [18]. A well-calibrated model provides uncertainty estimates that align with the observed frequency of errors, which is crucial for reliable decision‐making in downstream tasks.​  

A common approach to improving calibration involves post-hoc methods applied after the model has been trained [18]. Among these, Temperature Scaling and Platt Scaling are prominent examples [18]. Platt Scaling, originally developed for calibrating support vector machine outputs, fits a logistic regression model to the raw outputs (logits) to map them to calibrated probabilities. Temperature Scaling is a simpler variant often used for neural networks, where the logits are divided by a single scalar parameter (the “temperature”) before applying the softmax function. This temperature parameter is typically tuned on a validation set to minimize a calibration loss, such as the Expected Calibration Error. These methods are computationally efficient and do not require retraining the base model, although they only adjust the final layer’s outputs and cannot correct for poorly distributed uncertainty in earlier layers.​  

Beyond post-hoc adjustments, calibration can also be promoted through techniques applied during the training process or via specific architectural choices [12,18]. Training-time regularization methods such as label smoothing and MixUp have been shown to implicitly improve calibration by discouraging overconfident predictions [18]. Architectural approaches, including ensemble learning and Bayesian neural networks, inherently provide uncertainty estimates and can yield better calibrated models compared to single deterministic networks, although they often incur higher computational costs [18]. The development of methods providing well-calibrated uncertainty remains an active area of research [12].  

Certain techniques are specifically designed to provide calibration guarantees. Conformal Prediction methods, for instance, offer a rigorous framework to ensure a calibration property, typically guaranteeing that prediction intervals or sets cover the true value or class with a user-specified probability, assuming exchangeability of data [30]. Furthermore, calibration techniques can be tailored for specific model architectures, such as Graph Convolutional Networks (GCNs), where specialized methods like “Calibrate and Debias Layer-wise Sampling” and “Select and Calibrate the Low-confidence: Dual-Channel Consistency based Graph Convolutional Networks” have been proposed [20].​  

While specific calibration techniques like Temperature Scaling or Platt Scaling may not be universally applied or detailed in all uncertainty estimation contexts, the concept of improving the accuracy of confidence or prediction intervals is a shared goal [27]. Evaluating calibration can sometimes be approached indirectly, for example, by assessing the distribution of residuals between model predictions and observed data, which can offer insights into how well the model’s stated uncertainty aligns with its errors [22]. The choice of calibration technique depends on factors such as the model architecture, the computational budget, and the desired level of calibration rigor and guarantees.​  

# 5. Applications of Predictive Uncertainty Estimation  

Predictive uncertainty estimation is paramount across a diverse spectrum of domains, significantly enhancing the reliability, safety, and efficacy of machine learning applications. As machine learning models are increasingly deployed in critical realworld scenarios, understanding and quantifying the confidence of their predictions becomes indispensable for robust decision-making and risk management [1,12,30].  

![](images/63d970881a5adc691f5fe6ec8727ac70edf841b62d2173f84724cf8d02c1503f.jpg)  

This section provides an overview of the practical utility and impact of predictive uncertainty estimation by showcasing its applications in key areas, detailing specific problems where uncertainty is critical, and explaining how its quantification improves outcomes, enhances safety, aids decision-making, and manages risk [5,9,12,21,30].  

In high-stakes fields such as healthcare and medical diagnosis, reliable predictions are crucial to avoid potentially catastrophic outcomes like misdiagnosis, which can have severe consequences [14,18,21,30]. Quantifying uncertainty here allows models to flag predictions with low confidence, enabling review by medical experts and thus directly contributing to patient safety [17]. Similarly, in autonomous systems and robotics, operating in complex and dynamic environments (e.g., self-driving cars and robot control) demands robust safety mechanisms [1,12,14,18,30]. Uncertainty estimation informs safer decision-making, enables risk-aware control strategies, and improves robustness to unforeseen situations or distribution shifts, allowing systems to appropriately react, such as requesting human intervention [1,30].​  

The finance sector, characterized by inherent market volatility, relies heavily on predictive uncertainty estimation for reliable probabilistic forecasting and risk assessment [30,31]. Confidence measures derived from uncertainty quantification aid in setting risk tolerances, optimizing portfolio allocation, managing inventory under uncertain demand, and prioritizing investigations in fraud detection systems [30,31]. Beyond these critical areas, uncertainty estimation finds widespread application across numerous other domains, including various Computer Vision tasks like image classification, object detection, and anomaly detection [1,4,8,19,29,32], Natural Language Processing tasks such as authorship verification and biomedical data-to-text generation [21,26], and Time Series Forecasting, including applications in climate change problems like weather prediction [12,27]. Furthermore, quantifying uncertainty is essential for detecting out-of-distribution samples, misclassifications, and adversarial attacks, as well as for improving data efficiency through active learning strategies [1,2,5,9,18]. Its utility extends to recommender systems, engineering predictions, geophysical inversions, supply chain management, and analysis of heterogeneous data [13,22,23,25,31]. Across these diverse applications, predictive uncertainty estimation empowers users with a deeper understanding of model limitations and confidence, enabling more informed, safer, and robust deployment of machine learning technologies.  

# 5.1 Healthcare and Medical Diagnosis  

In high-stakes domains such as healthcare and medical diagnosis, where the reliability of predictions is paramount, predictive uncertainty estimation plays a vital role [21,30]. The deployment of black-box machine learning methods in these critical settings necessitates robust uncertainty quantification to prevent potentially catastrophic model failures [21]. Misdiagnosis or erroneous predictions can have severe, even fatal, consequences for patients [14,18]. Consequently, it is imperative that diagnostic models are designed to explicitly acknowledge and quantify their uncertainty [30].  

Incorporating uncertainty information significantly enhances diagnostic systems and improves clinical decision support by providing clinicians with a measure of confidence in the model's output. A primary benefit is the ability to flag predictions associated with high uncertainty, directing these cases for further review by medical experts [17]. This contributes directly to patient safety by ensuring that decisions in ambiguous or complex cases are made with human oversight.  

Uncertainty quantification is increasingly applied in various medical contexts. Medical image analysis is a particularly crucial area where accurate uncertainty estimation is essential for making informed clinical decisions [12,18]. Examples include the application of uncertainty estimation to improve diagnostic accuracy in tasks like detecting COVID-19 from chest X-ray images, where identifying potentially erroneous predictions facilitates obtaining second opinions from medical experts [17]. Similarly, efforts in lung nodule detection from chest radiographs, while perhaps not always explicitly quantifying uncertainty, rely on concepts like confidence levels which are intrinsically related to prediction reliability [32]. Research in cancer diagnosis, including melanoma detection, further highlights the critical need for uncertainty estimation in medical image-based tasks [12,19]. Techniques aimed at improving the reliability of medical image analysis, such as detecting anomalies or enhancing robustness, are also indirectly relevant to managing uncertainty [14,29].  

Beyond image analysis, uncertainty quantification is valuable in identifying and managing risk in various high-stakes medical scenarios. For instance, prediction-driven surge planning in emergency departments requires accounting for uncertainty in patient arrival rates and service times to ensure efficient resource allocation and preparedness [31]. The application of Bayesian Neural Networks (BNNs), which inherently provide uncertainty estimates, has been explored in diagnostic tasks like diabetes prediction using standard datasets, demonstrating their relevance for common medical conditions [24]. Furthermore, improving the reliability of biomedical data-to-text generation could also benefit from methods that understand and convey uncertainty in the generated information [26]. The ability of methods like BNNs to be robust to adversarial attacks is also important in medical diagnosis applications to ensure reliability and avoid fatal consequences [14]. Overall, integrating predictive uncertainty into machine learning models is fundamental for their safe and effective deployment in the complex and high-stakes environment of healthcare and medical diagnosis.  

# 5.2 Autonomous Systems and Robotics  

The deployment of autonomous systems and robots in complex, uncertain, and dynamic environments necessitates robust mechanisms for ensuring safety and reliability [12,30]. Predictive uncertainty estimation plays a critical role in addressing this challenge, particularly in safety-critical applications such as automated driving, self-driving cars, and robot control [1,12,14,18]. These systems operate in environments where unexpected events, sensor noise, and dynamic changes are commonplace. The inherent risk associated with potential failures in such high-stakes scenarios, where incorrect decisions can lead to fatal outcomes, underscores the vital importance of reliable system behavior [14]. Challenges such as distribution shift, exemplified by training a self-driving car in one city (San Francisco) and deploying it in another (Boston), highlight the limitations of models without adequate uncertainty awareness when encountering novel conditions [1]. Furthermore, applications like optimizing on-demand ride-sourcing services also rely on handling uncertainty in predictions of demand and traffic conditions [31].  

Quantifying uncertainty across the various modules of an autonomous system—including perception, prediction, and planning—enables safer decision-making, facilitates risk-aware control strategies, and improves overall robustness to unforeseen situations [30]. In the perception module, uncertainty estimation is crucial for tasks like object detection and semantic segmentation, allowing the system to understand the confidence in its interpretation of the environment. For instance, it is vital for safe navigation and decision-making when the model is uncertain about the presence of pedestrians or other hazards [18]. Research demonstrates the scalability of methods like loss-calibrated models for per-pixel semantic segmentation on autonomous driving datasets [19] and highlights its relevance in object detection in challenging scenarios such as drone imagery operating in dynamic environments [32].​  

Beyond perception, quantifying the uncertainties of component outputs and propagating them forward through the entire system pipeline can significantly enhance safety [19]. For prediction and planning, real-time extraction and evaluation of quantitative measures of uncertainty within end-to-end controllers for self-driving cars have been demonstrated [19]. Such measures can serve as valuable indicators; for example, mutual information has been identified as a promising metric for predicting forthcoming crashes [19]. By providing estimates of confidence alongside predictions, uncertainty quantification allows the autonomous system to identify situations where its understanding is poor, enabling it to request human intervention, adopt a more conservative strategy, or explicitly reason about potential risks, thereby improving decisionmaking in uncertain environments [30].  

# 5.3 Finance and Risk Assessment  

The application of predictive uncertainty estimation is paramount in the domain of finance and risk assessment, particularly given the inherent volatility of financial markets. Achieving reliable probabilistic forecasting is crucial for informed decisionmaking within this sector [30]. Uncertainty quantification provides confidence measures for predictions, thereby significantly enhancing various financial operations. For instance, in risk management, understanding the potential range of outcomes and the confidence associated with forecasts is vital for setting appropriate risk tolerances and hedging strategies. The context of managing inventory under uncertain demand and yield, such as explored in the newsvendor model, directly illustrates the relevance of quantifying randomness for effective risk assessment in financial supply chains or related areas [31]. Beyond direct risk exposure, uncertainty quantification is also instrumental in optimizing portfolio allocation by providing a clearer picture of potential risks and returns associated with different asset combinations. Furthermore, in fraud detection systems, confidence measures derived from uncertainty estimation can help prioritize suspicious transactions, allowing analysts to focus on cases where the prediction of fraudulent activity is less certain and requires further investigation. By quantifying the uncertainty associated with machine learning predictions, financial institutions can make more robust decisions, manage risks more effectively, and build more resilient systems.​  

# 5.4 Other Applications (Computer Vision, NLP, etc.)  

Predictive uncertainty estimation extends its utility across a diverse array of machine learning applications beyond established domains like medical diagnostics or autonomous systems.  

<html><body><table><tr><td>Domain</td><td>Example Tasks /Use Cases</td><td>Benefits of UQ</td></tr><tr><td>Computer Vision</td><td>Image Classification, Object Detection, Anomaly Detection, Depth Estimation, Multimodal Reasoning</td><td>Flag low-confidence predictions,Improve robustness</td></tr><tr><td>Natural Language Processing</td><td>Authorship Verification, Biomedical Data-to-Text Generation</td><td>Understand confidence in text outputs</td></tr><tr><td>Time Series Forecasting</td><td>Weather Prediction, Financial Forecasting</td><td>Provide confidence intervals,Risk awareness</td></tr><tr><td>General ML Utility</td><td>Out-of-Distribution Detection, Misclassification Detection, Active Learning, Recommender Systems</td><td>Identify model limitations, Improve data efficiency</td></tr><tr><td>Engineering/Science</td><td>Fatigue Life Prediction, Spectroscopic Analysis, Aerodynamic Loads</td><td>Quantify reliability of scientific/engineering models</td></tr></table></body></html>  

In Computer Vision, uncertainty quantification plays a crucial role in tasks such as image classification [1,4]. For example, models evaluated on datasets like ImageNet benefit from calibrated confidence measures, which can inform applications requiring high accuracy such as image-based search or content moderation [4].  

Beyond classification, uncertainty is pertinent in various object detection tasks [32], including floor‐plan analysis, face mask detection, person identification, and salient object detection [32]. It is also applied in simpler CV tasks like reading postal codes using image classifiers [1]. Furthermore, uncertainty can be utilized in multimodal AI reasoning, such as video question answering, potentially informed by metrics like the Modality Importance Score to quantify the contribution of different data streams [30].  

Techniques involving generative models have also shown potential in extracting interpretable attributes from images and decoupling temporal features in video data [6]. Uncertainty estimation methods have been explored for tasks like conditional image generation, few‐shot learning, and anomaly detection in computer vision contexts [19].​  

In Natural Language Processing (NLP), uncertainty quantification contributes to tasks like authorship verification, leveraging methods such as graph neural networks and language models [26]. General applications of Deep Neural Networks in NLP can also benefit from uncertainty quantification [14].  

Time Series Forecasting is another domain where estimating predictive uncertainty is vital [27]. Providing confidence intervals alongside point predictions allows for a more comprehensive understanding of future trends and potential variability, as demonstrated in practical examples like forecasting daily female births [27]. Applications related to climate change problems, such as weather prediction, are also relevant here [12].  

Uncertainty is particularly valuable for leveraging machine learning models in critical safety and reliability contexts. It enables the detection of Out-of-Distribution (OOD) samples, misclassification detection, and the identification of adversarial samples [5,9]. By providing calibrated confidence measures, models can indicate situations where additional processing or expert review is required, effectively excluding potential errors [18].​  

Beyond detection tasks, uncertainty estimation is a key component in improving data efficiency through Active Learning [5]. Models can prioritize data labeling by selecting samples exhibiting high epistemic uncertainty coupled with low aleatoric noise, leading to faster improvements in performance metrics like Root Mean Squared Error (RMSE) [5].  

Predictive uncertainty also finds application in systems designed for user interaction and engineering predictions. In Recommender Systems, uncertainty can enhance performance in tasks like matrix completion, exemplified by problems such as the Netflix challenge [13,23]. Collaborative Deep Learning methods integrate content information and rating data, benefiting from uncertainty modeling [13]. In engineering and material science, uncertainty estimation is crucial for tasks like multiaxial fatigue life prediction [25]. Other diverse applications include spectroscopic analysis (Raman and CARS spectroscopies) and predicting aerodynamic loads on transonic airfoils [25].  

Furthermore, uncertainty has been explored in topic modeling using techniques like Relational Stacked Denoising Autoencoders [13], control problems [13], supply chain management [31], online retailing [31], and data analysis on heterogeneous data [31]. The ability to quantify uncertainty thus provides valuable insights and enables more robust decision-making across a wide spectrum of applications.​  

# 6. Challenges and Future Directions  

Despite significant advancements, the widespread adoption and improvement of predictive uncertainty estimation methods in machine learning face several substantial challenges [1,2,11,13,21,25,30].  

![](images/5b724e7ccffe4ea30b3470232812e401a5337e3a1959b9db8b6fefd192810122.jpg)  

A primary hurdle lies in the computational cost and scalability of many UQ techniques, particularly for complex models like deep neural networks and large datasets [1,2,30]. Methods offering theoretically robust uncertainty, such as Bayesian Neural Networks (BNNs) and traditional Markov Chain Monte Carlo (MCMC) sampling, often incur significant computational overhead, limiting their practical applicability [8,13,22]. Similarly, ensemble methods, including Bootstrapped Ensembles, demand training multiple model instances, leading to considerable expense and challenging real-time performance, especially for high-resolution data [3,11,16,29]. This highlights a critical trade-off between the reliability of uncertainty estimates and computational efficiency. Addressing this requires developing more scalable approximate inference techniques and exploring hardware acceleration [12,24].  

Another crucial challenge is the evaluation, calibration, and benchmarking of predictive uncertainty. Objectively comparing diverse UQ methods and ensuring the reliability and unbiasedness of estimates remains difficult [2,4,16,27]. Standard deep neural networks often produce uncalibrated probabilities, particularly when encountering data different from their training distribution [1,17]. There is a pressing need for standardized evaluation protocols, comprehensive benchmarking datasets, and metrics that provide clearer performance indicators across various tasks and datasets [4,30]. Improving calibration techniques and developing methods optimized for both point prediction accuracy and uncertainty reliability are vital future directions [12,17].​  

Ensuring the robustness, interpretability, and trustworthiness of uncertainty estimates is paramount, particularly in safety-critical applications [1,12,30]. Machine learning models, including neural networks and GNNs, are vulnerable to adversarial attacks and backdoors, which can compromise the reliability of their uncertainty outputs [11,14,19,26]. Developing effective defenses and understanding failure modes of existing UQ methods are active research areas [19]. Furthermore, the interpretability of uncertainty estimates is crucial for user trust; understanding whya model is uncertain i as important as the estimate itself [6,20]. Future work should focus on methods that provide robust estimates and clear, understandable insights into the sources of uncertainty [6,20].  

Quantifying uncertainty in multimodal and complex data presents specific challenges due to the need to integrate information and uncertainty across disparate data types, each with potentially different characteristics [21]. Handling multimodal posterior distributions arising from diverse data sources and addressing potential biases inherent in individual modalities are significant hurdles in multimodal AI reasoning and applications like camouflaged object detection [22,29,30].  

In light of these challenges, key open research questions and promising future directions emerge. Developing more efficient and scalable methods is critical, exploring not only algorithmic improvements like Monte Carlo Dropout [6] but also novel hardware architectures [24]. Improving calibration techniques and developing comprehensive evaluation metrics and benchmarks are essential for reliable progress [4,17]. Future research should focus on developing more robust methods against adversarial attacks [14] and enhancing the interpretability of uncertainty estimates. Furthermore, developing new methods and architectures specifically tailored for uncertainty quantification is a vital direction, including creating models with explicit uncertainty components or fewer restrictive assumptions [5,30]. Exploring uncertainty in complex data types like multimodal data and integrating uncertainty into complex decision-making processes and new learning paradigms such as multi-task learning and reinforcement learning represent significant avenues for future work [21,30]. Applying uncertainty estimation to new areas and further refining techniques for separating different types of uncertainty (e.g., aleatoric vs. epistemic) are also crucial for advancing the field and enabling more trustworthy and reliable AI systems [1].  

# 6.1 Scalability and Computational Cost  

A significant challenge for many uncertainty estimation methods lies in their computational cost and scalability, particularly when applied to large deep learning models and extensive datasets [1,2]. Methods offering more robust uncertainty quantification often incur substantial computational overhead. For instance, Bayesian Neural Networks (BNNs), while theoretically appealing, face limitations in training speed and computational complexity when scaled to large networks [8,13]. Similarly, traditional Markov Chain Monte Carlo (MCMC) methods are inherently computationally intensive [22], although strategies like using a distributed computing architecture can allow individual chains to run in parallel to mitigate this [22]. However, even parallel approaches like Parallel Tempering MCMC (PTMCMC) can remain extremely costly, especially when dealing with deep, well-separated modes, as the time for information propagation between chains increases with the number of inverse temperatures [22].​  

Ensemble methods also present considerable computational demands. Techniques such as Bootstrapped Ensembles require training the model multiple times (e.g., N times), leading to significant expense [16]. General bootstrap resampling methods, often used for constructing prediction intervals, are also computationally expensive [3]. The computational cost of methods like Bayesian Graph Neural Networks (GNNs) and other ensemble approaches is widely acknowledged. The cumulative effect of these costs often makes achieving real-time performance challenging, particularly for tasks involving high-resolution data like images or videos [29].​  

This prevalent issue highlights a critical trade-off between the accuracy and reliability of uncertainty estimates and computational efficiency. Methods that strive for more complete or accurate representations of the posterior distribution or rely on multiple model instances tend to be more computationally demanding, while computationally cheaper methods might offer less precise uncertainty estimates.  

Addressing these challenges is an active area of research, with efforts focused on developing more scalable solutions [12]. One approach involves developing more efficient approximate inference techniques that can provide reasonable uncertainty estimates at a lower computational cost. In contrast to the high cost of full BNN training, methods like Monte Carlo Dropout (MCDropout) offer better scalability, having the same training cost as a standard neural network and only requiring additional forward passes during prediction [6]. Beyond algorithmic improvements, leveraging hardware acceleration is explored as a means to improve scalability [24]. For instance, research into using 2D materials aims to address the energy and area inefficiency of traditional BNN hardware implementations, offering potential improvements in computational cost and scalability [24]. Furthermore, research directions include exploring model compression and optimization techniques specifically tailored for uncertainty-aware models to reduce their computational footprint while preserving the quality of uncertainty estimates. Ultimately, improving the scalability and reducing the computational burden of uncertainty estimation methods are crucial steps towards their wider adoption in practical applications, especially those requiring deployment on resource-constrained devices or demanding real-time performance.  

# 6.2 Evaluation, Calibration, and Benchmarking  

Evaluating the quality of predictive uncertainty estimates in machine learning remains a significant challenge, characterized by inherent difficulties in objectively comparing diverse methods and the limitations of existing evaluation metrics [2,4]. A critical need exists for careful evaluation using appropriate metrics to ensure that models provide reliable and unbiased uncertainty estimates [27]. It is imperative to assess whether uncertainty estimations are genuinely reliable indicators rather than merely superficial additions [16]. Evaluation approaches may involve assessing aspects such as the convergence of sampling-based methods using statistics like the $\frac { \hat { V } _ { k } } { W _ { k } }$ statistic or analyzing the distribution of residuals to identify potential model inadequacies [22].​  

The field necessitates comprehensive benchmarking and the development of methods that yield desirable properties such as smaller prediction sets with provable coverage guarantees [30]. Research highlights the importance of understanding the relationships between different evaluation metrics, suggesting that a multifaceted evaluation approach is crucial [4]. To facilitate fair comparison, accelerate progress, and provide clearer performance indicators across various tasks and datasets, there is a pressing need for standardized evaluation protocols and diverse benchmark datasets.  

Beyond evaluating predictive uncertainty, a key objective is the calibration of uncertainty estimates. A known problem is that probabilities generated by deep neural network models are frequently uncalibrated, often leading models to overconfidently classify samples and produce misleading outcomes [17]. Improving the accuracy and calibration of uncertainty quantification results is an explicit area requiring further development [12]. This challenge is particularly pronounced for complex models and becomes significantly more difficult when models encounter data that differs considerably from their training distribution, making it challenging to maintain well-calibrated estimates [1].  

# 6.3 Robustness, Interpretability, and Trustworthiness  

Ensuring the robustness, interpretability, and trustworthiness of predictive uncertainty estimates is paramount, particularly when deploying machine learning systems in safety-critical and high-stakes domains [12,30]. Trustworthy AI systems require the ability to reliably identify their limitations and provide reliable uncertainty information, which is crucial for safe realworld deployment [1].​  

A significant challenge to the robustness of uncertainty estimates stems from the susceptibility of machine learning models, including neural networks and Graph Neural Networks (GNNs), to adversarial attacks and backdoors [11,26]. These attacks can manipulate inputs to cause models to make incorrect predictions, potentially leading to unreliable uncertainty estimates. Consequently, there is a pressing need for effective defensive approaches to mitigate the impact of adversarial examples and enhance the safety and reliability of applications like computer vision [14]. Research has highlighted specific vulnerabilities in widely used uncertainty estimation methods; for instance, failure modes have been identified for MC dropout, a common approach for estimating uncertainty in deep models [19]. Addressing these vulnerabilities involves developing new attacks to probe weaknesses and corresponding defense mechanisms [19]. Beyond adversarial examples, other factors, such as the choice of prior knowledge and the problem setup, can also influence the robustness of model inversions and thus the resulting uncertainty estimates. For example, utilizing uninformative priors on sensor noise can facilitate optimal weighting among multiple sensors even when noise levels are uncertain, thereby contributing to robustness [22].​  

In addition to robustness, the interpretability of uncertainty estimates is vital. Users need to understand whya model is uncertain to effectively utilize the provided information and build trust in the system. While Bayesian Neural Networks (BNNs) are recognized for their ability to provide high uncertainty estimates when encountering abnormal, rare, or noisy data—which can help users avoid costly incorrect predictions [6]—the mechanisms behind these estimates may not always be transparent. Methods that can automatically learn interpretable and easily understandable features, such as GraphicalGAN within the Bayesian networks framework, contribute significantly to improving the interpretability of uncertainty sources [6]. For specific model types like GNNs, ensuring consistency in interpretations is crucial for building trust in their explanations and, by extension, their uncertainty outputs [20].  

Ultimately, the development of methods that provide both robust and reliable uncertainty information, coupled with clear interpretability, is essential for building trustworthy machine learning systems. Trustworthiness in domains like graph learning, for instance, is closely linked to the reliability and explainability of the models, underscoring the interconnectedness of these properties [20]. Achieving reliability, explainability, and privacy protection are highlighted as essential for the widespread adoption of technologies, especially in sensitive applications [20].​  

# 6.4 Multimodal and Complex Data Uncertainty  

Quantifying uncertainty in complex data scenarios, particularly within multimodal learning, presents unique challenges. This is primarily due to the necessity of integrating uncertainty across disparate data types, each potentially carrying different noise characteristics, levels of reliability, and inherent ambiguities [21].  

The growing interest in multimodal applications, such as multimodal conditionality for natural language generation, underscores the increasing relevance of this problem [26]. Similarly, extending tasks like camouflaged object detection to incorporate additional modalities such as infrared or depth data highlights the need for uncertainty estimation methods capable of handling information from diverse sensor inputs [29].​  

Addressing complex data often involves integrating information from multiple sensors, as seen in applications utilizing data from various geophysical sensors [22]. In such contexts, a key challenge is handling multi-modal posterior distributions, which can arise when different data sources or interpretations lead to multiple plausible explanations or predictions.  

Techniques like parallel-tempered Markov Chain Monte Carlo (MCMC) are explored to effectively navigate and explore these multi-modal posteriors, aiming to provide a more comprehensive picture of the underlying uncertainty [22].  

Beyond merely integrating data, challenges in multimodal AI reasoning include preventing models from relying on biases inherent in individual modalities. Concepts such as the Modality Importance Score (MIS) are introduced to assess the contribution and reliability of different modalities, aiming to foster true multimodal understanding and potentially inform the quantification of uncertainty by weighting or contextualizing the information from each source [30].  

Effectively integrating uncertainty across these varied data streams, accounting for their individual characteristics and potential interdependencies, remains a significant area of research.  

# 6.5 Developing New Methods and Architectures  

Future advancements in predictive uncertainty estimation necessitate the development of entirely new methodologies and the seamless integration of uncertainty quantification (UQ) into emerging machine learning architectures and paradigms [30]. A primary objective is to address current limitations and enhance the granularity and interpretability of uncertainty estimates.  

<html><body><table><tr><td>Direction</td><td>Example Approaches /Goals</td><td>Impact/ Benefit</td></tr><tr><td>Novel Model Design</td><td>SDE-Net (separate drift/diffusion),Less restrictive assumptions</td><td>Explicit UQ components, More flexible methods</td></tr><tr><td>Integration into Architectures</td><td>Hardware acceleration (BNN accelerators), Interpretable generative models</td><td>Improved efficiency, Enhanced interpretability</td></tr><tr><td>Improved Inference/Optimization</td><td>Efficient sampling (pCN), Optimizing UQ metrics in loss function</td><td>Better posterior approximation, Directly trained UQ</td></tr><tr><td>UQ in New Paradigms</td><td>Multi-task learning, Reinforcement Learning</td><td>Inform decision-making under uncertainty</td></tr><tr><td>Granular Uncertainty</td><td>Better separation of Aleatoric/Epistemic</td><td>Differentiate uncertainty sources, Targeted action</td></tr></table></body></html>  

Promising directions include the design of novel models specifically tailored for UQ. For instance, the SDE-Net architecture proposes the use of separate networks—a drift network for predicting the output and a diffusion network specifically dedicated to estimating uncertainty [5]. This structured approach explicitly links components of the model to different aspects of the prediction process, potentially offering a clearer path towards understanding and improving uncertainty estimation. Another avenue involves developing methods that are less reliant on restrictive assumptions, such as the approach by Caprio et al. which removes the consonance assumption to handle ambiguous ground truth effectively [30]. The goal is to achieve more flexible and assumption-free UQ techniques [30]. Furthermore, research is exploring methods that promote diversity in model predictions to enhance uncertainty estimates [12]. It is also crucial for systems to gain the capability to identify situations where they are likely to fail, a key aspect of reliable UQ [1]. Improving the core predictive models themselves, as suggested for time series forecasting, can also directly lead to more reliable uncertainty estimations [27]. In the realm of probabilistic methods like MCMC, improving the efficiency and robustness of sampling—for example, through adaptive and efficient proposal distributions like the preconditioned Crank–Nicolson (pCN) proposal—is critical for tackling high-dimensional and complex problems [22].  

Integrating UQ into novel deep learning architectures is another significant research frontier. Beyond dedicated UQ architectures like SDE-Net [5], this includes exploring hardware acceleration for UQ methods, such as the development of BNN accelerators utilizing 2D materials that provide both synaptic functions and programmable randomness [24]. There is also potential in incorporating more interpretable structures into deep generative models, like the Graphical-GAN approach which combines Bayesian networks for interpretable data parts and neural networks for fitting the remainder [6]. For graph neural networks (GNNs), future directions involve developing more robust GNN architectures and exploring new UQ methods tailored for graph data [11]. Research suggests significant potential for developing even more powerful GNN designs and learning paradigms, potentially through techniques like edge-featured graph neural architecture search [20,26]. A direct way to embed UQ into the training process is by including uncertainty evaluation metrics within the loss function, optimizing networks not only for point predictions but also for their uncertainty estimates [17].  

The role of UQ in emerging learning paradigms is also a crucial area for exploration [30]. While the provided digests primarily touch upon multi-task learning in the context of adversarial robustness for Bayesian neural networks [14], the broader challenge lies in seamlessly integrating UQ into frameworks like multi-task learning and reinforcement learning [30] to inform decision-making under uncertainty. Although not explicitly detailed in the digests, developing methods that better separate aleatoric (inherent data noise) and epistemic (model uncertainty) components of uncertainty or provide more granular uncertainty information for specific inputs or regions of the input space remains a vital goal for improving trust and reliability in machine learning predictions. This granular understanding is essential for applications requiring differential treatment of various uncertainty sources. Addressing data scarcity, for example in areas like product re-identification, also requires developing new algorithms that can provide reliable predictions and uncertainty estimates with minimal training data [32]. Techniques like Boosting, while a general method for improving model performance, highlight the potential for leveraging simpler models to achieve higher predictive accuracy without significantly increasing computational load [23].  

# 7. Conclusion  

This survey has reviewed the critical and burgeoning field of predictive uncertainty estimation in machine learning. As machine learning systems are increasingly deployed in safety-critical and high-stakes applications, the ability to quantify the reliability and trustworthiness of their predictions becomes paramount [1,9,12,18,30]. Knowing the certainty associated with an AI's output allows for more informed decision-making and safer system operation [9].​  

A fundamental concept in this domain is the distinction between the two primary types of uncertainty: aleatoric and epistemic [2,9,10,11,28]. Aleatoric uncertainty arises from inherent randomness or noise within the data itself and is generally considered irreducible with more data. In contrast, epistemic uncertainty reflects the model's lack of knowledge about the underlying data distribution, often stemming from limited data or regions of the input space far from the training data [10]. Crucially, epistemic uncertainty can potentially be reduced by acquiring more data or improving the model architecture [10].​  

Various methodological approaches have been developed to estimate these uncertainties. Bayesian methods, particularly Bayesian Neural Networks (BNNs) and the broader framework of Bayesian Deep Learning (BDL), represent a significant direction, extending traditional neural networks by treating weights as distributions to quantify model uncertainty [6,7,13,15,25,28]. BDL combines the strengths of neural networks for high-dimensional data processing with probabilistic graphical models for inference, offering advantages in robustness and interpretability [6,13]. However, challenges remain in achieving efficient and accurate inference with BNNs and ensuring interpretability in related deep generative models [6]. Ensemble methods, including Monte Carlo dropout, are also widely used, particularly for estimating epistemic uncertainty [8,9]. Studies have demonstrated the effectiveness of ensemble techniques in capturing uncertainties, for instance, in medical imaging tasks like COVID-19 detection [17], or through variations like image flipping in monocular depth estimation [16]. Beyond these, newer approaches such as SDE-Net leverage stochastic differential equations for uncertainty estimation, presenting a promising direction for risk-aware applications and tasks like out-of-distribution detection [5]. Methods like Conformal Prediction also contribute to providing rigorous uncertainty guarantees [30].  

Estimating uncertainty is only part of the challenge; proper evaluation and calibration of these estimates are equally vital to ensure they are reliable and meaningful [2,12,16,17,22,30]. The development of specific metrics and evaluation frameworks, such as novel confusion matrices and performance indicators for uncertainty estimates, is crucial for assessing their quality [17]. Well-calibrated uncertainty estimates are particularly significant in high-risk applications [2]. The challenges in evaluating the convergence and adequacy of uncertainty estimates, particularly in complex, high-dimensional problems like geophysical inversion, underscore the ongoing need for robust evaluation methodologies [22]. While some methods show high sensitivity and specificity, improving the precision of uncertainty estimates remains an area for development [17].  

Looking ahead, the field of predictive uncertainty estimation is dynamic and critical for the future trajectory of AI. A core challenge lies in the inherent inability of models to anticipate and robustly handle every conceivable situation they might encounter in complex, real-world environments [1]. Machine learning systems need to be able to recognize when they are operating outside their training distribution or encountering novel scenarios where predictions might be unreliable [1]. Research into enhancing the robustness of UQ methods, for example, against adversarial attacks using techniques like multitask adversarial training for BNNs, is vital [14]. Advancements in hardware accelerators tailored for probabilistic models, such as BNNs leveraging memory transistors, could address computational efficiency challenges [24]. The continued exploration of innovative approaches, such as SDE-based models [5] and methods focusing on diverse model predictions [12], along with ongoing efforts in improving calibration and evaluation [30], are essential for pushing the boundaries of reliable AI. Ultimately, progress in uncertainty quantification will significantly broaden the safe and effective deployment of machine learning across diverse applications, from healthcare and autonomous systems to climate science and time series forecasting [12,18,27,30].​  

# References  

[1] 人工智能安全：机器学习中可靠的不确定性量化 https://baijiahao.baidu.com/s? id=1804284994554023919&wfr=spider&for=pc  

[2] 深度神经网络不确定性综述：来源、类型与估计方法 https://blog.csdn.net/Rad1ant_up/article/details/139114754​   
[3] Prediction Intervals for Machine Learning: A Pract https://machinelearningmastery.com/prediction-intervals-for  
machine-learning/​   
[4] ImageNet Classifier Uncertainty: A Comprehensive P https://paperswithcode.com/paper/what-can-we-learn-from-the  
selective/review/   
[5] SDE-Net：利用随机微分方程量化神经网络不确定性 https://cloud.tencent.com/developer/article/1746536​   
[6] 贝叶斯深度学习：融合深度学习与贝叶斯学习框架 https://book.qq.com/book-read/44819429/19​   
[7] 贝叶斯深度学习：原理、应用与不确定性处理 https://baijiahao.baidu.com/s?id $\mid =$ 1698906817577942018&wfr=spider&for=pc   
[8] 深度学习中的不确定性：偶然与认知 https://blog.csdn.net/djfjkj52/article/details/130559019​   
[9] 计算机视觉中的不确定性估计：重要性、建模与计算 https://zhidao.baidu.com/question/701411618380601084.html​   
[10] Aleatoric 与 Epistemic 不确定性：预测模型中的不确定性分解 https://blog.csdn.net/DeniuHe/article/details/121544899   
[11] GNN 和深度学习中的不确定性估计：论文集与简要笔记 https://blog.csdn.net/qq_16763983/article/details/119997093   
[12] PhD Student Awarded Funding for Machine Learning R https://www.eng.cam.ac.uk/news/funding-awarded-harness  
power-machine-learning-healthcare-and-safety-critical-applications​   
[13] 贝叶斯深度学习：原理、框架与应用 https://www.elecfans.com/d/2302007.html   
[14] Multi-Task Adversarial Training for Robust Bayesia   
https://www.sciencedirect.com/science/article/abs/pii/S0020025522000809   
[15] 贝叶斯深度学习简介 https://www.cnblogs.com/wuliytTaotao/p/10281766.html​   
[16] CVPR2020单目深度估计：不确定度建模与评估研究 https://blog.csdn.net/weixin_39967598/article/details/112829998   
[17] COVID-19 Detection: Objective Evaluation of Deep U https://www.nature.com/articles/s41598-022-05052-x​   
[18] 深度学习：不确定性估计与模型校准 https://blog.csdn.net/xys430381_1/article/details/119531335   
[19] Yarin Gal's Publications https://www.cs.ox.ac.uk/people/yarin.gal/website/publications.html​   
[20] 2022年5月图神经网络最新研究进展速递 https://www.cnblogs.com/BlairGrowing/articles/16323763.html   
[21] 人工智能学术速递[7.16]: 最新论文速览 https://www.cloud.tencent.com/developer/article/1852778​   
[22] Obsidian v0.1.2: Efficiency and Robustness in 3-D  http://dx.doi.org/10.5194/gmd-12-2941-2019   
[23] Athey & Imbens 合著：经济学家应知的机器学习方法 https://mp.weixin.qq.com/s?   
__biz $: =$ Mzg3NzU5NjMyNg==&mid=2247733280&idx $\underline { { \underline { { \mathbf { \Pi } } } } }$ 1&sn=6dd2ca8e5af085c87115cbf774271b4f&chksm $\mid =$ cede387b1b713204   
65c533380ee1701f3d61a5c9f63674bcd028e757406522b57a589f78abf1&scene=27   
[24] 二维材料记忆晶体管助力贝叶斯神经网络发展 https://mp.weixin.qq.com/s?   
_biz $: =$ MzA4NDk3ODEwNQ $\scriptstyle 1 = =$ &mid=2698867953&idx $\vartriangle { \underline { { \mathbf { \Pi } } } }$ 4&sn $=$ 6df431cf3710bf1e7c5ecb78fba354b9&chksm=baf724278d80ad31   
40ec034e47ac003b80d4004e24a0646bb74858b51a8ea0c9c024f86f8409&scene=27   
[25] 贝叶斯神经网络：深度学习创新新思路 https://www.bilibili.com/opus/1004466844828958729   
[26] 机器学习速递[9.6]: 图学习、Transformer、GAN等最新研究 https://cloud.tencent.com/developer/article/1878366   
[27] 时间序列预测：不确定性、经典方法与性能评估 https://juejin.cn/post/7414734289130471443   
[28] 贝叶斯网络：原理、应用与模型不确定性处理 https://blog.csdn.net/qq_38556984/article/details/110431547   
[29] Deep Learning-Based Camouflaged Object Detection:  https://www.sciengine.com/doi/10.3778/j.issn.1673-   
9418.2206078​   
[30] PRECISE Center News: AI Innovations and Recognitio https://precise.seas.upenn.edu/about/news   
[31] 浙江大学“商学+”青年学者论坛：7场国际化研讨会助力学术交流 http://baijiahao.baidu.com/s?   
id=1747690805576856020&wfr=spider&for=pc   
[32] 计算机视觉与模式识别学术速递[12.21]：Transformer与目标检测进展   
https://cloud.tencent.com/developer/article/1924112   
[33] Larger Speaker Populations Linked to Increased Mac https://www.nature.com/articles/s41598-023-45373-z​  