# A Survey of Vectorization Methods in Topological Data Analysis  

Dashti Ali, Aras Asaad, Maria Jose Jimenez, Vidit Nanda, Eduardo Paluzo-Hidalgo, and Manuel Soriano-Trigueros  

Abstract. Attempts to incorporate topological information in supervised learning tasks have resulted in the creation of several techniques for vectorizing persistent homology barcodes. In this paper, we study thirteen such methods. Besides describing an organizational framework for these methods, we comprehensively benchmark them against three well-known classification tasks. Surprisingly, we discover that the best-performing method is a simple vectorization, which consists only of a few elementary summary statistics. Finally, we provide a convenient web application which has been designed to facilitate exploration and experimentation with various vectorization methods.  

# Introduction  

Propelled by deep theoretical foundations and a host of computational breakthroughs, topological data analysis emerged roughly three decades ago as a promising method for extracting insights from unstructured data [32, 14, 42, 44]. The principal instrument of the enterprise is persistent homology; this consists of three basic steps, each relying on a different branch of mathematics.  

(1) Metric geometry: construct an increasing family $\{ X _ { t } \}$ of cell complexes around the input dataset $X _ { \cdot }$ , where the indexing $t$ is a scale parameter in $\mathbb { R } _ { \geq 0 }$ .   
(2) Algebraic topology: compute the $d$ -th homology vector spaces $H _ { d } ( X _ { t } )$ for scales $t$ in $\mathbb { R } _ { \geq 0 }$ and dimensions $d$ in $\mathbb { Z } _ { \geq 0 }$ .   
(3) Representation theory: decompose each family of vector spaces $\{ H _ { d } ( X _ { t } ) \mid t \geq 0 \}$ into irreducible summands, thus producing a barcode.  

The resulting barcodes are finite multisets of real intervals $[ p , q ] \subset \mathbb { R } ,$ which admit concrete geometric interpretations in low dimensions — see Figure 1. The ultimate goal is to infer the coarse geometry of $X$ across various scales by examining the longer intervals in its barcodes. Crucially, once the method for constructing $\{ X _ { \epsilon } \}$ from $X$ has been fixed, the entire persistent homology pipeline is unsupervised: one requires neither labelled data nor hyperparameter tuning to produce barcodes from $X$ .  

At the other end of the data analysis spectrum lies supervised machine learning using contemporary neural networks, which are replete with billions of tunable parameters and gargantuan training datasets [3]. The practical aspects of deep neural networks appear to be light years ahead of the underlying theory. It nevertheless remains the case that machine learning has driven astonishing progress in the systematic automation of several important classification tasks. One direct consequence of these success stories is the irresistible urge to combine topological methods with machine learning. The most common avenue for doing so is to turn barcodes into vectors (lying in a convenient Euclidean space) which then become input for suitably-trained neural networks.  

![](images/27c58c2b4dd908656c4b0ef31d4d93e43d080de26dc167fa739b5bd2cff0d393.jpg)  
Figure 1. An increasing family of cell complexes built around a point cloud dataset; the associated barcode in dimensions 0 (blue) and 1 (red) catalogues the connected components and cycles respectively.  

The good news, at least from an engineering perspective, is that barcodes are inherently combinatorial objects, and as such, they are remarkably easy to vectorize. Several dozen vectorization methods have been proposed across the last decade, and new ones continue to appear with alarming frequency and increasing complexity — the reader will encounter thirteen of them here. The bad news, on the other hand, comes in the form of three serious challenges which must be confronted by those who build or use such vectorizations:  

(1) Given the large number of options, even established practitioners are not aware of all the vectorization techniques; similarly, knowledge of which vectorizations are suitable for which types of data is difficult – if not impossible – to glean from the published literature.   
(2) There is a natural metric between barcodes called the bottleneck distance; when it is endowed with this metric, the space of barcodes becomes infinite-dimensional and highly nonlinear. As such, it does not admit any faithful embeddings into finite-dimensional vector spaces.   
(3) Even the stable vectorizations, which preserve distances by mapping barcodes into infinite-dimensional vector spaces, may suffer from a lack of discriminative power  

in practice: by design, they are poor at distinguishing between datasets whose coarse structures are similar and whose differences reside in finer scales.  

In This Paper. Here we seek to comprehensively describe, catalogue and benchmark vectorization methods for persistent homology barcodes. The first contribution of this paper is the following taxonomy of the known methods, which we hope will serve as a convenient organizational framework for beginners and experts alike —  

(1) Statistical vectorizations: these summaries consist of basic statistical quantities;   
(2) Algebraic vectorizations: these are generated from polynomials;   
(3) Curve vectorizations: these come from maps $\mathbb { R } \to H ,$ , where $H$ is a vector space;   
(4) Functional vectorizations: these are maps of the form $X \to H$ for $X \neq \mathbb { R } ;$ ;   
(5) Ensemble vectorizations: these are generated from collections of training barcodes.  

There are unavoidable overlaps between these five categories. When such an overlap occurs, we have placed the given vectorization technique in the earliest relevant category among those in the list above; thus, an algebraic vectorization given by polynomial functions of basic statistical quantities will be placed in category (1) rather than category (2). The reader might claim, quite reasonably, that category (3) should be subsumed into category (4). However, the sheer number of curve-based vectorizations compelled us to set them apart.  

The second contribution of this paper is a comprehensive benchmarking of thirteen vectorization techniques across these five categories on three well-known image classification datasets. These datasets were selected to simultaneously (a) provide an increasing level of difficulty for topological methods, and (b) to be instantly recognizable for the broader machine learning community. These are: the Outex texture database [43], the SHREC14 shape retrieval dataset [47], and the Fashion-MNIST database [59]. Surprisingly, the bestperforming vectorization in all three cases is a rather na¨ıve one obtained by collecting basic statistical quantities associated to (the multiset of) intervals in a given barcode.  

Our third contribution is a companion web application which computes and visualizes all thirteen vectorization techniques which have been investigated in this paper. In addition to running online1, this web app can also be downloaded2 and run locally on more challenging datasets.  

Not In This Paper. Vectorization methods form but a small part of the ever expanding interface between topological data analysis and machine learning. As such, there are several related techniques which are not benchmarked here. The precise inclusion criteria for our study in this paper are as follows.  

(1) We restrict our attention to those methods which produce genuine vectors from barcodes. In particular, kernel methods [50, 17] are beyond the scope of this paper.   
(2) We only consider those vectorizations that are either straightforward for us to implement, or have an easily accessible and trusted implementation. For instance, path signature based vectorizations [21, 33] are excluded.   
(3) We do not compare machine learning architectures designed for the explicit purpose of inferring (persistent) homology [16, 37, 40].   
(4) We do not touch upon various attempts to design or study neural networks using tools from topological data analysis [41, 15].   
(5) Finally, even among methods which satisfy the first four criteria, we have discarded techniques which regularly obtained a classification accuracy below fifty percent.  

Similar Efforts. The authors of [49] have summarised – but not compared – several vectorization and kernel methods for barcodes. Another summary (sans comparison) may be found in [53], with emphasis on metric aspects of the chosen vectorizations. The work of [23] describes a common overarching framework for what we have called curve vectorizations here. More recently, [7] and [24] have described and compared five and four vectorization methods respectively.  

Outline. Notation and preliminaries involving barcodes are established in Section 1. In Sections 2 and 3 we introduce the thirteen vectorizations (suitably organised into our taxonomy) and the three datasets. Section 4 contains the results of our experiments whose finer details have been relegated to Appendices A and B. We provide a description of the web app in Section 5 and some brief concluding remarks in Section 6.  

# 1. Persistence Barcodes from Data  

At its core, persistent homology studies sequences of finite-dimensional vector spaces $V = \{ V _ { i } \mid 0 \leq i \stackrel {  } { \leq } n \}$ and linear maps $a = \{ a _ { i } : \bar { V } _ { i - 1 } \to V _ { i } \mid 1 \leq i \leq n \}$ :  

$$
V _ { 0 } \xrightarrow { { } \quad a _ { 1 } \quad } V _ { 1 } \xrightarrow { { } \quad a _ { 2 } \quad } \cdots \xrightarrow { { } \quad a _ { n } \quad } V _ { n } .
$$  

Such sequences $( V , a )$ are called persistence modules. Among the simplest examples are interval modules — for each pair of integers $\boldsymbol { p } \le \boldsymbol { q }$ with $[ p , q ] \subset [ 0 , n ] ,$ , the corresponding interval module $( I ^ { [ p , q ] } , c ^ { [ p , q ] } )$ has  

$$
\dim I _ { i } ^ { [ p , q ] } = { \left\{ \begin{array} { l l } { 1 } & { { \mathrm { i f ~ } } p \leq i \leq q } \\ { 0 } & { { \mathrm { o t h e r w i s e } } ; } \end{array} \right. }
$$  

similarly, the map $c _ { i } ^ { [ p , q ] }$ is the identity whenever $p + 1 \leq i \leq q$ and zero otherwise.  

1.1. Structure and Stability. Every persistence module decomposes into a direct sum of interval modules. In particular, we have the following structure theorem [61, 18].  

Theorem 1.1. For every persistence module $( V , a )$ , there exists a unique set $\mathbf { B a r } ( V , a )$ of subintervals of $[ 0 , n ]$ along with a unique function ${ \bf B a r } ( V , a ) \to \mathbb { Z } _ { > 0 }$ denoted $[ p , q ] \mapsto \mu _ { p , q }$ for which we have an isomorphism  

$$
( V , a ) \simeq \bigoplus _ { [ p , q ] \in \mathbf { B a r } ( V , a ) } \left( I ^ { [ p , q ] } , c ^ { [ p , q ] } \right) ^ { \mu _ { p , q } } .
$$  

Thus, the algebraic object $( V , a )$ may be fully recovered (up to isomorphism) from purely combinatorial data consisting of the set of intervals $\mathbf { B a r } ( V , a )$ and the multiplicity function $\mu$ . Alternately, one may view $\mathbf { B a r } ( V , a )$ itself as a multiset with $\mu _ { p , q }$ copies of each interval $\left[ p , q \right]$ . This multiset is called the barcode of $( V , a )$ . It is often useful in applications to let the vector spaces $V _ { i }$ be indexed by real numbers rather than integers. With this modification in place, $\mathbf { B a r } ( V )$ becomes a collection of real intervals $[ p , q ] \subset \mathbb { R }$ .  

The most important property of persistence modules, beyond the structure theorem, is their stability [18]. There is a natural metric on the set of persistence modules called the interleaving distance and a metric on the set of barcodes called the bottleneck distance  

Theorem 1.2. The assignment $( V , a ) \mapsto \mathbf { B a r } ( V , a )$ is an isometry from the space of persistence modules (with interleaving distance) to the space of barcodes (with bottleneck distance).  

The advantage of this theorem is that barcodes remain robust to (certain types of) perturbations of the original dataset, thus conferring upon the topological data analysis pipeline a degree of noise-tolerance. The significant difficulty from a statistical perspective, however, is that the metric space of persistence barcodes with bottleneck distance is nonlinear — even averages can not be defined for arbitrary collections of barcodes [56, 26, 11].  

1.2. Barcodes from Data. Persistence modules arise naturally from a wide class of datasets. The first step in topological data analysis involves imposing the structure of a filtered cell complex – either simplicial [4, Chapter 8] or cubical [38] – from the data [32, 14, 42]. The two most prominent examples of filtered cell complex structures arising from data are as follows.  

(1) Given a finite point cloud $X \subset \mathbb { R } ^ { n }$ , one constructs a family of increasing simplicial complexes $\{ S _ { \epsilon } \mid \epsilon \geq 0 \}$ defined as follows. A collection $\{ x _ { 0 } , \ldots , x _ { k } \}$ forms a $k \mathrm { . }$ -simplex in $S _ { \epsilon }$ if and only if the (Euclidean) distance between $x _ { i }$ and $x _ { j }$ is no larger than $\epsilon$ for all $i , j$ in $\{ 0 , \ldots , k \}$ . Since there are only finitely many $\epsilon$ values at which new simplices are introduced, the filtration is indexed by a subset of the natural numbers. The collection $S _ { \epsilon }$ is called the Vietoris-Rips filtration of $X$ . These filtrations can be defined for any metric space in a similar fashion.   
(2) Consider a grayscale image $I ,$ given in terms of $m \times n$ pixels with intensity values in the set $\{ 0 , 1 , \ldots , 2 5 5 \}$ . This naturally forms a two-dimensional cubical complex, which can be endowed with the upper-star filtration by intensity values. In particular, each elementary cube of dimension $< 2$ appears at the smallest intensity encountered among the 2-dimensional cubes in its immediate neighbourhood. Higher-dimensional cubical filtrations may be similarly generated from higherdimensional pixel grids.  

Once the given dataset has been suitably modeled by a filtered cell complex, persistence modules are obtained by computing homology groups with coeffiecients in a field. The reader who is interested in the definition and computation of homology is urged to either consult standard algebraic topology references such as [35, Ch 2] or see the more recent [44, 30, 42].  

A substantial difficulty in topological data analysis is that although persistent homology barcodes can be readily associated with a large class of datasets, the space of all such barcodes is notoriously unpleasant to encounter from a statistical perspective. Fortunately, barcodes are combinatorial objects which can be mapped to Hilbert spaces in a plethora of reasonable ways. Indeed, across the last decade, such vectorization methods have been proposed by various authors, and our main purpose in this work is to benchmark many of these methods against standard classification tasks.  

# 2. Vectorization Methods for Barcodes  

Throughout this section, we assume knowledge of the barcode $B : = \mathbf { B a r } ( V , a )$ of an $\mathbb { R } ^ { _ { - } }$ indexed persistence module along with its multiplicity function $\mu : B \to \mathbb { Z } _ { > 0 }$ . We note that for each interval $[ p , q ]$ in $B$ the numbers $p$ and $q$ are called its birth and death respectively, and the length $q - p$ is called its lifespan.  

2.1. Statistical Vectorizations. The first and simplest category of vectorizations considered in this paper are generated from basic statistical quantities associated to the given barcode. Variants of the following vectorization have been defined and used on several occasions — see for instance [5, sec 2.3] , [23, Sec 6.2.1] and [49, Sec 4.1.1].  

Definition 2.1. The persistence statistics vector of $\mu : B \to \mathbb { Z } _ { > 0 }$ consists of:  

(1) the mean, the standard deviation, the median, the interquartile range, the full range, the $1 0 ^ { \mathrm { t h } } , 2 5 ^ { \mathrm { t h } } , 7 5 ^ { \mathrm { t h } }$ and $9 0 ^ { \mathrm { t h } } $ percentiles of the births $p _ { \cdot }$ , the deaths $q ,$ the midpoints $\frac { p + q } { 2 }$ and the lifespans $q - p$ for all intervals $\left[ p , q \right]$ in $B$ counted with multiplicity;   
(2) the total number of bars (again counted with multiplicity), and   
(3) the entropy of $\mu$ , defined as the real number  

$$
E _ { \mu } : = - \sum _ { [ p , q ] \in B } \mu _ { p , q } \cdot \left( \frac { q - p } { L _ { \mu } } \right) \cdot \log \left( \frac { q - p } { L _ { \mu } } \right) ,
$$  

where $L _ { \mu }$ is the weighted sum  

$$
L _ { \mu } : = \sum _ { [ p , q ] \in B } \mu _ { p , q } \cdot ( q - p ) .
$$  

The entropy from Definition 2.1(3) was introduced in [22, 52]. Our second statistical vectorization is from [6], where entropy has been upgraded to a real-valued piecewise constant function rather than a single number.  

Definition 2.2. The entropy summary function of $\mu : B \to \mathbb { Z } _ { > 0 }$ is the map $S _ { \mu } : \mathbb { R } \to \mathbb { R }$ given by  

$$
S _ { \mu } ( t ) = - \sum _ { [ p , q ] \in { \cal B } } \mathbb { 1 } _ { p \leq t < q } \cdot \mu _ { p , q } \cdot \left( \frac { q - p } { L _ { \mu } } \right) \cdot \log \left( \frac { q - p } { L _ { \mu } } \right) .
$$  

Here $\mathbb { 1 } _ { \bullet }$ is the indicator function — it equals 1 when the conditional $\bullet$ is true and it equals 0 otherwise. The number $L _ { \mu }$ appearing in the expression above is defined in (1).  

The entropy summary function has also been called the life entropy curve, e.g., in [23].  

2.2. Algebraic Vectorizations. The vectorizations in this category are generated using polynomial maps constructed from the barcode $\mu : B \to \mathbb { Z } _ { > 0 }$ .  

The first example considered here is from [2]. It becomes convenient, for the purpose of defining it, to arbitrarily order the intervals in $B$ as $\{ [ p _ { i } , q _ { i } ] \mid 1 \leq i \leq n \}$ with the understanding that each $\left[ p , q \right]$ occurs $\mu _ { p , q }$ times in this ordered list.  

Definition 2.3. The ring of algebraic functions on $\mu : B \to \mathbb { Z } _ { > 0 }$ consists of all those R-polynomials $f$ in variables $\left\{ x _ { 1 } , y _ { 1 } , \ldots , x _ { n } , y _ { n } \right\}$ for which the following property holds: there exist polynomials $\{ g _ { i } \mid 1 \leq i \leq n \}$ satisfying  

$$
{ \frac { \partial f } { \partial x _ { i } } } + { \frac { \partial f } { \partial y _ { i } } } = ( x _ { i } - y _ { i } ) \cdot g _ { i } .
$$  

(Here $\partial f / \partial x _ { i }$ indicates the partial derivative of $f$ with respect to $\textstyle { x _ { i } , }$ and so forth).  

The desired vectorization is obtained by selecting finitely many algebraic functions from this ring and evaluating them at $x _ { i } = p _ { i }$ and $y _ { i } = q _ { i }$ for all $i$ . The feature maps generated by making such choices are sometimes called Adcock-Carlsson coordinates — see for instance [46]. Letting $q _ { \mathrm { m a x } }$ be the maximum death-value encountered among the intervals in $B ,$ four of the most widely-used algebraic functions are:  

$$
\begin{array} { l c l } { { f _ { 1 } = \displaystyle \sum _ { i } p _ { i } ( q _ { i } - p _ { i } ) \quad } } & { { f _ { 2 } = \displaystyle \sum _ { i } ( q _ { \mathrm { m a x } } - q _ { i } ) ( q _ { i } - p _ { i } ) } } \\ { { f _ { 3 } = \displaystyle \sum _ { i } p _ { i } ^ { 2 } ( q _ { i } - p _ { i } ) ^ { 4 } \quad } } & { { f _ { 4 } = \displaystyle \sum _ { i } ( q _ { \mathrm { m a x } } - q _ { i } ) ^ { 2 } ( q _ { i } - p _ { i } ) ^ { 4 } } } \end{array}
$$  

Small changes in the barcode (in terms of bottleneck distance) are liable to create large fluctuations in the associated algebraic functions. The methods of tropical geometry were used in [39] to address the bottleneck instability of algebraic functions. In this setting, the standard polynomial operations $( + , \times )$ are systematically replaced by $\left( \operatorname* { m a x } , + \right)$ . To define the resulting vectorization, we once again use an ordering $\{ [ p _ { i } , q _ { i } ] \mid 1 \leq i \leq n \}$ of the intervals in $B$ .  

Definition 2.4. A tropical coordinate function for $\mu : B \to \mathbb { Z } _ { > 0 }$ is a function $F$ of variables $\left\{ x _ { 1 } , y _ { 1 } , \ldots , x _ { n } , y _ { n } \right\}$ which is both tropical and symmetric as described below.  

(1) Tropical: there is an expression for $F$ which uses only the operations max, min, $+$ and on the variables $\{ x _ { i } \}$ and $\{ y _ { i } \}$ .   
(2) Symmetric: any permutation of $\{ 1 , \ldots , n \}$ , when applied to both $\left\{ x _ { i } \right\}$ and $\{ y _ { i } \}$ , leaves $F$ unchanged.  

Let $\lambda _ { i }$ be the lifespan $q _ { i } - p _ { i }$ of the $i \cdot$ -th interval in $B$ . To generate feature maps from the tropical coordinate functions described above, one simply evaluates them at $x _ { i } = \lambda _ { i }$ and $y _ { i }$ equal to either $\boldsymbol { \mathrm { m a x } } ( \boldsymbol { r } \lambda _ { i } , \boldsymbol { p } _ { i } )$ or $\operatorname* { m i n } ( r \lambda _ { i } , p _ { i } )$ for a positive integer parameter $r$ . Examples of such tropical coordinate features include:  

$$
\begin{array} { l l } { F _ { 1 } = \underset { i } { \operatorname* { m a x } } ~ \lambda _ { i } } & { F _ { 2 } = \underset { i < j } { \operatorname* { m a x } } ( \lambda _ { i } + \lambda _ { j } ) } \\ { F _ { 3 } = \underset { i < j < k } { \operatorname* { m a x } } ( \lambda _ { i } + \lambda _ { j } + \lambda _ { k } ) } & { F _ { 4 } = \underset { i < j < k < l } { \operatorname* { m a x } } ( \lambda _ { i } + \lambda _ { j } + \lambda _ { k } + \lambda _ { l } ) } \\ { F _ { 5 } = \sum _ { i } \lambda _ { i } } & { F _ { 6 } = \underset { i } { \overset { \mathrm { m i n } } { \operatorname* { m i n } } } ( r \lambda _ { i } , p _ { i } ) , } \end{array}
$$  

along with the somewhat more complicated  

$$
F _ { 7 } = \sum _ { j } \left[ \operatorname* { m a x } _ { i } \left( \operatorname* { m i n } ( r \lambda _ { i } , p _ { i } ) + \lambda _ { i } \right) - ( \operatorname* { m i n } ( r \lambda _ { j } , p _ { j } ) + \lambda _ { j } ) \right] .
$$  

These seven tropical coordinates were used in [39] for performing classification on the MNIST database, with $r = 2 8$ .  

The third and final algebraic vectorization considered here is generated by extracting complex polynomials from barcodes [31, 27]. In what follows, the symbol $i$ should be interpreted as $\sqrt { - 1 }$ (and not as an index for the intervals in $B$ ). Consider the three continuous maps $R , S , T : \mathbb { R } ^ { 2 }  \mathbb { C }$ defined as follows:  

$$
\begin{array} { l } { \displaystyle R ( x , y ) = x + i y } \\ { \displaystyle S ( x , y ) = \left\{ \frac { y - x } { \alpha \sqrt { 2 } } \cdot ( x + i y ) \quad \mathrm { i f ~ } ( x , y ) \neq ( 0 , 0 ) \right. } \\ { \displaystyle 0 \qquad \mathrm { o t h e r w i s e } } \\ { \displaystyle T ( x , y ) = \frac { y - x } { 2 } \cdot \left[ ( \cos \alpha - \sin \alpha ) + i ( \cos \alpha + \sin \alpha ) \right] , } \end{array}
$$  

where $\alpha$ is the norm $\sqrt { x ^ { 2 } + y ^ { 2 } }$ .  

Definition 2.5. Given a barcode $\mu : B \to \mathbb { Z } _ { > 0 } ,$ let $X : \mathbb { R } ^ { 2 }  \mathbb { C }$ be any one of the three functions $R , S , T$ defined above. The complex polynomial vectorization of $\mu$ of type $X$ is the sequence of coefficients of the complex polynomial in one variable $z$ given by  

$$
C _ { X } ( z ) : = \prod _ { [ p , q ] \in B } [ z - X ( p , q ) ] ^ { \mu _ { p , q } } .
$$  

In practice, it is customary to either take only the first few highest degree coefficients of $C _ { X } ( z )$ or to multiply it by a suitable power of $z$ . This is done to guarantee that the feature vectors assigned to a collection of barcodes all have the same dimension.  

Other Algebraic Vectorizations: In the subsequent section, we describe how to extract vectorizations by using barcode data to build curves which take values in a vector space. Once such a curve has been extracted, one can compute its path signature via iterated integrals [20]. The path signature resides in the tensor algebra of the target vector space; elements of the tensor algebra are equivalent to coefficients of non-commuting polynomials, and hence constitute algebraic vectorizations of barcodes — see [21, 33] for examples of this approach.  

2.3. Curve Vectorizations. There are several interesting ways of turning barcodes into one or more curves, which for our purposes here mean (piecewise) continuous maps from R to a convenient vector space. Feature vectors can then be constructed by sampling the given curve at finite subsets of R. Perhaps the simplest and most widely used curve-based vectorization is the following.  

Definition 2.6. The Betti curve of $\mu : B \to \mathbb { Z } _ { > 0 }$ is the curve $\beta _ { \mu } : \mathbb { R } \to \mathbb { R }$ given by  

$$
\beta _ { \mu } ( t ) = \sum _ { [ p , q ] \in B } \mathbb { 1 } _ { p \leq t < q } \cdot \mu _ { p , q } .
$$  

Here 1 is the indicator function as described in Definition 2.2, so this function counts the number of intervals (with multiplicity) in $B$ which contain $t$ . Very similar in spirit (and formula) to the Betti curve is the following vectorization from [23].  

Definition 2.7. The lifespan curve of $\mu : B \to \mathbb { Z } _ { > 0 }$ is the map $L _ { \mu } : \mathbb { R } \to \mathbb { R }$ given by  

$$
L _ { \mu } ( t ) = \sum _ { [ p , q ] \in B } \mathbb { 1 } _ { p \leq t < q } \cdot \mu _ { p , q } \cdot ( q - p ) .
$$  

It is not difficult to create very different-looking Betti and lifespan curves from two barcodes which have arbitrarily small bottleneck distance — we can always add lots of very small intervals to a given barcode without changing its bottleneck distance by a significant amount. One way to rectify the bottleneck instability of Betti and lifespan curves is to test the containment not only of $t$ in each interval $[ p , q ] \in B ,$ , but rather of the largest subinterval of the form $\left[ t - s , t + s \right]$ . This modification leads to one of the oldest and best-known stable curve vectorizations [10, 12], as defined below.  

Definition 2.8. The persistence landscape of the barcode $\mu : B \to \mathbb { Z } _ { > 0 }$ is a sequence of curves $\left\{ \Lambda _ { i } ^ { \mu } : \mathbb { R } \to \left[ - \infty , \infty \right] | i \in \mathbb { Z } _ { > 0 } \right\}$ given by  

$$
\Lambda _ { i } ^ { \mu } ( t ) : = \operatorname* { s u p } \left\{ s \geq 0 \Big | \left( \sum _ { [ p , q ] \in B } \mathbb { 1 } _ { [ t - s , t + s ] \subset [ p , q ] } \cdot \mu _ { p , q } \right) \geq i \right\} .
$$  

By convention, the supremum over the empty set is zero. Moreover, since our barcode $B$ is assumed to be finite, the landscape functions ${ \dot { \Lambda } } _ { i } ^ { \mu }$ become identically zero for sufficiently large $i$ . An alternate approach to defining persistence landscapes comes from the function $\Delta : B \times \mathbb { R }  \mathbb { R } ,$ given by  

$$
\Delta ( [ p , q ] , t ) : = \operatorname* { m a x } \left( \operatorname* { m i n } ( t - p , q - t ) , 0 \right) .
$$  

For each $i \in \mathbb { Z } _ { > 0 } ,$ the curve $\Lambda _ { i } ^ { \mu }$ from Definition 2.8 equals the $i$ -th largest number in the multiset that contains $\mu _ { p , q }$ copies of $\Delta ( [ p , q ] , t )$ for each interval $\left[ p , q \right]$ in $B$ . The fourth and final curve vectorization that we consider here was introduced in [19], and it is also defined in terms of the functions $\Delta$ from (2).  

Definition 2.9. Let $w : B \to { \mathbb { R } } _ { > 0 }$ be any function, which we will denote $[ p , q ] \mapsto w _ { p , q }$ . The $w$ -weighted persistence silhouette of $\mu : B \to \mathbb { Z } _ { > 0 }$ is the map $\phi _ { \mu } ^ { w } : \mathbb { R } \to \mathbb { R }$ defined as the weighted average  

$$
\phi _ { \mu } ^ { w } ( t ) : = \frac { \sum w _ { p , q } \cdot \mu _ { p , q } \cdot \Delta ( [ p , q ] , t ) } { \sum w _ { p , q } \cdot \mu _ { p , q } } .
$$  

Here both sums on the right are indexed over all $[ p , q ] \in B ,$ , and $\Delta$ is defined in (2).  

Reasonable choices of weight functions are provided by setting $w _ { p , q } = ( q - p ) ^ { \alpha }$ for a real-valued scale parameter $\alpha \geq 0$ . For small $\alpha ,$ the shorter intervals dominate the value of the silhouette curve, whereas for large $\alpha$ it is the longer intervals which play a more substantial role — see [19, Sec 4] for details.  

Other Curve Vectorizations: See the envelope embedding from [21], the accumulated persistence function in [9], and the persistent Betti function of [57]. In [29], the persistent Betti function is decomposed along the Haar basis to produce a vectorization. More recently, [23] provides a general framework for constructing several different curve vectorizations.  

2.4. Functional Vectorizations. Here we catalogue those barcode vectorizations which are given by maps from spaces other than R. The first, and perhaps most prominent member of this category is the following vectorization from [1]. Its definition below makes use of two auxiliary components besides the given barcode $\mu : B \to \mathbb { Z } _ { > 0 }$ . The first is a continuous, piecewise-differentiable function $f : \mathbb { R } ^ { 2 } \to \mathbb { R } _ { \geq 0 }$ satisfying $f ( x , 0 ) = 0$ for all $x \in \mathbb { R }$ . And the second is a collection of smooth probability distributions $\Psi : = \bigl \{ \psi _ { p , q } \mid [ p , q ] \in B \bigr \}$ where $\psi _ { p , q }$ has mean $( p , q - p )$ .  

Definition 2.10. The persistence surface of $\mu : B \to \mathbb { Z } _ { > 0 }$ with respect to $f$ and $\Psi$ (as described above) is the function $\mathbb { R } ^ { 2 } \to \mathbb { R }$ given by  

$$
\rho _ { f , \Psi } ^ { \mu } ( x , y ) = \sum _ { [ p , q ] \in B } \mu _ { p , q } \cdot f ( p , q - p ) \cdot \psi _ { p , q } ( x , y ) .
$$  

The persistence image $\pmb { I } _ { f , \Phi } ^ { \mu }$ of $\mu$ with respect to $( f , \Phi )$ assigns a real number to every subset $\boldsymbol { Z } \subset \mathbb { R } ^ { 2 } .$ ; this number is given by integrating the persistence surface over $Z$ :  

$$
I _ { f , \Psi } ^ { \mu } ( Z ) = \iint _ { Z } \rho _ { f , \Psi } ^ { \mu } ( x , y ) d x d y .
$$  

In order to obtain a vector from the persistence image, one lets $Z$ range over grid pixels in a rectangular subset of $\mathbb { R } ^ { 2 }$ and renormalizes the resulting array of numbers, thus producing a grayscale image. Standard choices of $f$ and $\Psi = \{ \psi _ { p , q } \}$ are:  

$$
\begin{array} { r l } & { f ( x , y ) = \left\{ \begin{array} { l l } { 0 } & { t \leq 0 } \\ { t / \lambda _ { \operatorname* { m a x } } } & { 0 < t < \lambda _ { \operatorname* { m a x } } } \\ { 1 } & { t > \lambda _ { \operatorname* { m a x } } } \end{array} \right. } \\ & { \psi _ { p , q } ( x , y ) = \cfrac { 1 } { 2 \pi \sigma ^ { 2 } } \cdot \exp \left( - \frac { \left( x - p \right) ^ { 2 } + \left( y - ( q - p ) \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right) . } \end{array}
$$  

Here $\lambda _ { \operatorname* { m a x } }$ is the largest lifespan $\mathrm { m a x } _ { [ p , q ] \in B } ( q - p )$ encountered among the intervals in $B _ { . }$ , and $\sigma$ is a user-defined parameter which forms the common standard deviation of every $\psi _ { p , q }$ in sight.  

The second and final functional vectorization which we will examine was introduced in the paper [46]. Set $\mathbb { W } : = \{ ( x , y ) \in \mathbb { R } ^ { 2 } \ | \ 0 \le x < y \} .$ , and note that points $( x , y ) \in \mathbb { W }$ parameterize intervals $[ x , y ] \subset \mathbb { R }$ with strictly positive length that could possibly lie in a given barcode. Let $C _ { c } ( \mathbb { W } )$ be the set of all continuous functions $f : \mathbb { W } \to \bar { \mathbb { R } }$ with compact support3. The given barcode $\mu : B \to \mathbb { Z } _ { > 0 }$ induces a function $V _ { \mu } : C _ { c } ( \mathbb { W } ) \to \mathbb { R }$ via  

$$
V _ { \mu } ( f ) = \sum _ { [ p , q ] \in B } \mu _ { p , q } \cdot f ( p , q - p ) .
$$  

A subset $T$ of $C _ { c } ( \mathbb { W } )$ is called a template system if for any distinct pair $\mu _ { 1 } : B _ { 1 } \to \mathbb { Z } _ { > 0 }$ and $\mu _ { 2 } : B _ { 2 } \to \mathbb { Z } _ { > 0 }$ of barcodes, there exists at least one $f \in T$ so that $\bar { V _ { \mu _ { 1 } } } ( f ) \ne V _ { \mu _ { 2 } } ( f )$ .  

Definition 2.11. Fix an integer $n > 0$ and let $\operatorname { S u b } _ { n } ( T )$ be the collection of all size $n$ subsets of a template system $T$ as described above. The template function vectorization of $\mu : B \to \mathbb { Z } _ { > 0 }$ with respect to $T$ is the map $\tau : \mathsf { S u b } _ { n } ( T ) \to \mathsf { \bar { R } } ^ { n }$ defined as follows. Given $f = \{ f _ { 1 } , \ldots , f _ { n } \}$ in $\operatorname { S u b } _ { n } ( T )$ , the associated vector in $\mathbb { R } ^ { n }$ is  

$$
\tau ^ { \mu } ( f ) : = \left( V _ { \mu } ( f _ { 1 } ) , \dots , V _ { \mu } ( f _ { n } ) \right) ,
$$  

where $V _ { \mu } ( f _ { i } )$ is as defined in (3).  

Two convenient choices of $T _ { \ast }$ , called tent functions and interpolating polynomials, have been highlighted in [46]. Tent functions are indexed by points $( u , v ) \in \mathbb { R } ^ { 2 }$ and require an additional parameter $\delta > 0$ ; they have the form  

$$
g _ { u , v } ^ { \delta } ( x , y ) = \operatorname* { m a x } \left( 1 - \frac { 1 } { \delta } \cdot \operatorname* { m a x } ( | x - u | , | y - v | ) , 0 \right)
$$  

By construction, each such function is supported on the square of side length $2 \delta$ around the point $\scriptstyle ( u , v )$ in the birth-lifespan plane. The normal pipeline for selecting finitely many template functions requires covering a sufficiently large bounded subset of $\mathbb { W }$ with a square grid and then selecting the appropriate tent functions supported on grid cells. We direct interested readers to [46, Sections 6 and 7] for details on interpolating polynomials and for suggestions on how one might select suitable $n$ and $f \in \operatorname { S u b } _ { n } ( T )$ for a given classification task.  

Other Functional Vectorizations: See the generalised persistence landscape in [8] and the crocker stacks of [58].  

2.5. Ensemble Vectorizations. Our last category contains two methods which require access to a sufficiently large collection of training barcodes $\mu _ { i } : B _ { i } \to Z _ { > 0 }$ in order to generate a vectorization. The first of these methods, introduced in [48], is a modification of the template system vectorization from Definition 2.11. We recall that $\mathbb { W } \subset \mathbb { R } ^ { 2 }$ is defined as $\{ ( x , { \bar { y } } ) \mid 0 \leq x < y \}$ and that every barcode $B$ is identified with a subset $P ( B ) \subset \mathbb { W }$ via the map that sends intervals $\left[ p , q \right]$ of positive length to points $( p , q )$ .  

Definition 2.12. The adaptive template system induced by a collection of barcodes $\{ \mu _ { i } : B _ { i } \to \mathbb { Z } _ { > 0 } \}$ is obtained via the following two steps. Letting $P \subset \mathbb { W }$ be the union $\textstyle \bigcup _ { i } P ( B _ { i } ) .$ , one  

(1) identifies finitely many ellipses $E _ { j } \subset \mathbb { W }$ which tightly contain $P _ { \cdot }$ , and then (2) constructs suitable functions $g _ { j }$ supported on $E _ { j } ,$ as described in (5) below.  

The desired vectorization of a new barcode $\mu : B \to \mathbb { Z } _ { > 0 }$ is now obtained by using these $g _ { j } ,$ rather than tent functions, as template functions in Definition 2.11. Three different methods for finding the $E _ { j }$ can be found in [48, Sec 3]. Let $v ^ { * }$ denote the transpose of a given vector $v$ in $\mathbb { R } ^ { 2 }$ . Now each ellipse $E$ with centre $x = ( x _ { 1 } , x _ { 2 } ) ^ { * }$ corresponds to a symmetric 2 2 matrix A satisfying  

$$
E = \left\{ z \in \mathbb { R } ^ { 2 } \mid ( z - x ) ^ { * } A ( z - x ) = 1 \right\} .
$$  

Setting $h ( z ) : = ( z - x ) ^ { * } A ( z - x ) .$ , the adaptive template function $g$ supported on $E$ is  

$$
g ( z ) = { \left\{ \begin{array} { l l } { 1 - h ( z ) } & { h ( z ) < 1 } \\ { 0 } & { { \mathrm { o t h e r w i s e } } . } \end{array} \right. }
$$  

The second instance of an ensemble vectorization framework which we benchmark in this paper is from [51]. Let $\mu _ { i } : B _ { i } \to \mathbb { Z } _ { > 0 }$ be a collection of training barcodes as before, and fix a dimension parameter $b \in \mathbb { Z } _ { > 0 }$ . Much like the adaptive template systems of Definition 2.12, the automatic topology-oriented learning (ATOL) vectorization is a two-step process for mapping each $B _ { i }$ to a vector space, which in this instance is always $\mathbb { R } ^ { b }$ .  

Definition 2.13. The ATOL contrast functions corresponding to the collection of barcodes $\{ \mu _ { i } : B _ { i } \to \mathbb { Z } _ { > 0 } \}$ and parameter $b \in \mathbb { Z } _ { > 0 }$ are obtained as follows:  

(1) Treating the point clouds  

$$
P _ { i } : = \Big \{ ( p , q ) \in \mathbb { R } ^ { 2 } \ | \ [ p , q ] \in B _ { i } \mathrm { ~ a n d ~ } q > p \Big \}
$$  

as discrete measures on $\mathbb { R } ^ { 2 }$ , one estimates their average measure $E$ .  

(2) Let $z : = ( z _ { 1 } , z _ { 2 } , \dots , z _ { b } )$ be a point sample in $\mathbb { R } ^ { 2 }$ drawn (in independent, identically distributed function) along $E$ . Define the real numbers $\sigma _ { i } ( z )$ for $1 \leq i \leq b$ by  

$$
\sigma _ { i } ( z ) : = \frac { 1 } { 2 } \operatorname* { m a x } _ { j \neq i } \| z _ { j } - z _ { i } \| _ { 2 } ,
$$  

where ${ \big \| } \bullet { \big \| } _ { 2 }$ denotes the usual Euclidean norm on $\mathbb { R } ^ { 2 }$ .  

The contrast functions $\{ \Omega _ { i } : \mathbb { R } ^ { 2 }  \mathbb { R } \mid 1 \leq i \leq b \}$ are now given by  

$$
\Omega _ { i } ( x ) = \exp \left( - \frac { \| x - z _ { i } \| } { \sigma _ { i } ( z ) } \right) .
$$  

The reader is directed to [51, Algorithm 1] for further details. Once the contrast functions have been produced in the manner described above, the corresponding ATOL vectorization of a given barcode $\mu : B \to \mathbb { Z } _ { > 0 }$ equals $\left( \Omega _ { 1 } ^ { \mu } , \ldots , \Omega _ { b } ^ { \mu } \right) _ { \textstyle \mathrm { { \cdot \cdot \cdot } } }$ , where  

$$
\Omega _ { i } ^ { \mu } : = \sum _ { [ p , q ] \in B } \mu _ { p , q } \cdot \Omega _ { i } ( p , q ) .
$$  

Other Ensemble Vectorizations: The persistence codebooks approach from [60] proposes three different types of barcode vectorizations; these are based on bag-of-word embeddings, VLAD (vector of locally aggregated descriptors), and Fisher Vectors respectively.  

# 3. Datasets  

The vectorization methods described in the preceding section have been benchmarked against three standard datasets; these are described below and arranged in increasing order of difficulty for topological methods. All three of them have been used in the past for comparing vectorizations (or kernels) for persistence barcodes [46, 48, 50, 17, 34, 21].  

3.1. Outex. Outex is a database of images developed for the assessment of texture classification algorithms [43] — see Fig. 2, right-bottom, for some samples of textures from the 68 categories. Each texture class contains 20 images of size $1 2 8 \times 1 2 8$ pixels, which results in 1, 360 images in total. We designed a reduced version of the experiment by randomly selecting 10 of the total 68 classes in the dataset, which we refer to as Outex10 below. The full classification is referred to as Outex68. In both cases, a train/test split of 70/30 has been applied.  

We treat each image as a cubical complex; the filtration is induced by considering the pixel intensity on the 2-dimensional cells, which is inherited by other cells via the lowerstar and upper-star filtrations. Persistent homology barcodes are computed in dimensions 0 and 1 using the GUDHI library [28]. No pre-processing has been applied to the images.  

![](images/3a948eee161040959273f03ca28497685e20916d7eaf58334dd6cb4c3ae59f0a.jpg)  
Figure 2. Samples from datasets used in our experiments  

3.2. SHREC14. The Shape Retrieval of a non-rigid 3D Human Models dataset, usually abbreviated SHREC14 [47], is designed to test shape classification and retrieval algorithms. It contains real and synthetic human shapes and poses stored as 3D meshes (which are already simplicial complexes). We use the synthetic part of the dataset; this constitutes a classification task with 15 classes (5 men, 5 women and 5 children), each one with 20 different poses — see the upper-right corner of Fig. 2.  

We apply the Heat Kernel Signature (HKS) to obtain filtrations [54, 50]. For a fixed real parameter $t > 0$ , this filtration assigns to each mesh point $x$ the value  

$$
\mathrm { H K S } _ { t } ( x ) = \sum _ { i = 0 } ^ { \infty } e ^ { - \lambda _ { i } t } \cdot \phi _ { i } ( x ) ^ { 2 }
$$  

Here $\lambda _ { i }$ and $\phi _ { i }$ are eigenvalues and corresponding eigenfunctions of (a discrete approximation to) the Laplace-Beltrami operator of the given mesh. Every simplex of dimension $> 0$ is assigned the largest value of $\mathrm { H K } S _ { t }$ encountered among its vertices. We used the pre-computed barcodes (for such filtrations across a range of $t$ -values) which have been provided in the repository4 accompanying [7]. Of the 300 samples, $7 0 \%$ were used for training and the other $3 0 \%$ for testing.  

3.3. FMNIST. The Fashion-MNIST database contains $2 8 \times 2 8$ grayscale images (7, 000 images per class, with 10 classes) — see the left side of Fig. 2 for some sample images. We split this dataset into 60, 000 training and 10, 000 testing images.  

The filtration used for generating barcodes is as follows: we performed padding, median filter, and shallow thresholding before computing canny edges [13]. Then each pixel is given a filtration value equalling its distance from the edge-pixels. Finally, all other cells inherit filtration values from the top pixels via the lower star filtration rule.  

# 4. Results  

Here we report the classification accuracy of the thirteen vectorization methods from Section 2 on each of the three datasets from Section 3. Implementation details and parameter choices are provided in Appendix A. The source code is available at the following GitHub repository: https://github.com/Cimagroup/vectorization-maps.  

4.1. Outex. Table 1 displays the classification accuracy for the smaller (and easier) experiment on 10 classes. As one might expect, all techniques perform rather well, with Persistence Statistics and Algebraic Functions sharing the best performance with $9 9 . 2 \%$ accuracy each, followed closely by Persistent Silhouettes with $9 8 . 3 \%$ each.  

Results from the full experiment with 68 classes are contained in Table 2; as one might expect, the performance of every single vectorization degrades in the passage from Outex10 to Outex68. Here Persistence Statistics is the clear winner by a significant margin, earning $9 3 . 4 \%$ accuracy. Tropical Coordinates ranks second with $8 8 . 7 \%$ . Setting aside the outstanding performance of Persistence Statistics, it appears clear from these results that the algebraic vectorizations perform far better on Outex68 than the vectorizations from the other categories.  

<html><body><table><tr><td>Vectorization Method</td><td>Accuracy</td><td>Parameters</td><td>Estimator</td></tr><tr><td>Persistence Statistics</td><td>0.992</td><td></td><td>SVM, rbf kernel, C1, Y1</td></tr><tr><td>Entropy Summary</td><td>0.975</td><td>100</td><td>SVM, rbf kernel,C1, Y1</td></tr><tr><td>Algebraic Functions</td><td>0.992</td><td></td><td>SVM, linear kernel, C3</td></tr><tr><td>Tropical Coordinates</td><td>0.975</td><td>250</td><td>SVM, linear kernel, C4</td></tr><tr><td>Complex Polynomial</td><td>0.950</td><td>5,R</td><td>SVM, rbf kernel, C1, Y1</td></tr><tr><td>Betti Curve</td><td>0.908</td><td>200</td><td>SVM, rbf kernel,C1, Y1</td></tr><tr><td>Lifespan Curve</td><td>0.975</td><td>100</td><td>SVM, rbf kernel,C1, Y1</td></tr><tr><td>Persistence Landscape</td><td>0.975</td><td>50,20</td><td>SVM, rbf kernel, C2, Y2</td></tr><tr><td>Persistence Silhouette</td><td>0.983</td><td>100,0</td><td>SVM, rbf kernel, C1, Y1</td></tr><tr><td>Persistence Image</td><td>0.938</td><td>1, 25</td><td>RF，n=500</td></tr><tr><td>TemplateFunction</td><td>0.958</td><td>35,20</td><td>SVM, rbf kernel,C1 Y1</td></tr><tr><td>Adaptive Template System</td><td>0.975</td><td>GMM, 40</td><td>SVM, rbf kernel, C1, Y1</td></tr><tr><td>ATOL</td><td>0.967</td><td>32</td><td>SVM, linear kernel, C4</td></tr></table></body></html>  

Table 1. Outex10 results. The relevant parameter values are $C _ { 1 } = 9 3 6 . 5 3 9 1 ,$ , $\gamma _ { 1 } ~ = ~ 0 . 0 1 8 7 ,$ $C _ { 2 } = 9 1 4 . 9 6 2 0 \$ $\gamma _ { 2 } = 0 . 0 0 6 1$ , $C _ { 3 } = 8 6 . 0 4 4 2 ,$ and $C _ { 4 } = 9 9 8 . 1 8 4 8$ .  

We note that the authors of [23] have also used Outex to compare the performance of various curve vectorizations, with Persistence Statistics being used as a baseline. They also obtained their best results with Persistence Statistics.  

4.2. SHREC14. We used 10 different $t { \cdot }$ -values $t _ { 1 } < t _ { 2 } < \cdots < t _ { 1 0 } ,$ as in [50, 46, 48], for generating filtrations via the heat kernel from (6). At $t _ { 1 0 }$ we found several sparse or empty barcodes, which led us to discard that classification problem. Table 3 collects the best performance for each method across the first 9 values of $t ;$ it also contains values of the optimal parameters (see Appendix A) and the optimal values of $t$ .  

<html><body><table><tr><td>Vectorization Method</td><td>Accuracy</td><td>Parameters</td><td>Estimator （204号</td></tr><tr><td>Persistence Statistics</td><td>0.934</td><td></td><td>SVM, rbf kernel, C1 Y1</td></tr><tr><td>Entropy Summary</td><td>0.859</td><td>100</td><td>SVM, poly kernel, C2, Y2, deg=2</td></tr><tr><td>Algebraic Functions</td><td>0.875</td><td></td><td>SVM, linear kernel, C4</td></tr><tr><td>Tropical Coordinates</td><td>0.887</td><td>50</td><td>SVM, linear kernel, C5</td></tr><tr><td>Complex Polynomial</td><td>0.846</td><td>10, R</td><td> SVM, linear kernel, C4</td></tr><tr><td>Betti Curve</td><td>0.804</td><td>200</td><td>SVM, rbf kernel, C1, Y1</td></tr><tr><td>Lifespan Curve</td><td>0.842</td><td>100</td><td>SVM, rbf kernel, C1 Y1</td></tr><tr><td>Persistence Landscape</td><td>0.822</td><td>50,20</td><td>SVM, rbf kernel, C3, Y3</td></tr><tr><td>Persistence Silhouette</td><td>0.844</td><td>100, 1</td><td>SVM, linear kernel, C4</td></tr><tr><td>Persistence Image</td><td>0.762</td><td>1, 150</td><td>RF, n=500</td></tr><tr><td>Template Function</td><td>0.831</td><td>35,20</td><td>RF, n=200</td></tr><tr><td>Adaptive Template Sys.</td><td>0.819</td><td>GMM,50</td><td>SVM, linear kernel, C6</td></tr><tr><td>ATOL</td><td>0.854</td><td>16</td><td>SVM, linear kernel, C7</td></tr></table></body></html>  

Table 2. Outex68 results. The optimal parameter values are $C _ { 1 } = 9 3 6 . 5 3 9 1$ , $\gamma _ { 1 } = 0 . 0 1 8 7 _ { \cdot }$ , $C _ { 2 } =$ 957.5357, $\gamma _ { 2 } = 0 . 0 1 2 0$ , $C _ { 3 } = 9 1 4 . 9 6 2 0 \$ , $\gamma _ { 3 } = 0 . 0 0 6 1$ , $C _ { 4 } = 9 9 8 . 1 8 4 8$ , $C _ { 5 } = 8 8 4 . 1 2 5 5$ , $C _ { 6 } = 1 4 3 . 1 2 0 1$ and $C _ { 7 } = 4 9 4 . 0 5 9 6 .$ .  

<html><body><table><tr><td>Vectorization Method</td><td>Accuracy</td><td>Parameters</td><td>Estimator</td></tr><tr><td>Persistence Statistics</td><td>0.947</td><td>t5</td><td>RF, n=100</td></tr><tr><td>Entropy Summary</td><td>0.723</td><td>t6,200</td><td>RF, n=300</td></tr><tr><td>Algebraic Functions</td><td>0.909</td><td>t5</td><td>RF, n=500</td></tr><tr><td>Tropical Coordinates</td><td>0.844</td><td>t6,50</td><td>SVM, linear kernel, C5</td></tr><tr><td>Complex Polynomial</td><td>0.889</td><td>t6,20,S</td><td>SVM, linear kernel, C6</td></tr><tr><td>Betti Curve</td><td>0.728</td><td>t5,200</td><td>RF, n=100</td></tr><tr><td>Lifespan Curve</td><td>0.878</td><td>t7,200</td><td>SVM, linear kernel, C7</td></tr><tr><td>Persistence Landscape</td><td>0.889</td><td>t6,50,10</td><td>SVM, rbf kernel, C1 Y1</td></tr><tr><td>Persistence Silhouette</td><td>0.867</td><td>t6,200,2</td><td>SVM, rbf kernel, C2, Y2</td></tr><tr><td>Persistence Image</td><td>0.916</td><td>t5, 1,10</td><td>RF, n=100</td></tr><tr><td>Template Function</td><td>0.944</td><td>t5, 14, 0.7</td><td>SVM, rbf kernel, C3, Y3</td></tr><tr><td>Adaptive Template Sys.</td><td>0.889</td><td>t5, GMM, 15</td><td>SVM, linear kernel, C8</td></tr><tr><td>ATOL</td><td>0.933</td><td>t8，16</td><td>SVM, rbf kernel, C4, Y4</td></tr></table></body></html>  

Table 3. Best performance of each method on SHREC14. The parameters are $C _ { 1 } = 8 3 5 . 6 2 5 7$ , $\gamma _ { 1 } = 0 . 0 0 0 2$ , $\bar { C } _ { 2 } = 2 1 2 . 6 2 8 1 .$ , $\gamma _ { 2 } = 0 . 0 0 3 1$ , $C _ { 3 } = 8 7 9 . 1 4 2 5 _ { \cdot }$ , $\gamma _ { 3 } = 0 . 0 0 1 0$ , $C _ { 4 } = 9 3 6 . 5 3 9 1 _ { . }$ , $\gamma _ { 4 } =$ 0.0187, $C _ { 5 } = 1 4 1 . 3 8 6 9 _ { \cdot }$ , $C _ { 6 } = 6 2 5 . 0 3 0 0$ , $C _ { 7 } = 9 9 8 . 1 8 4 8 .$ , $C _ { 8 } = 2 7 4 . 5 0 0$ .  

Persistence Statistics yielded the best classification accuracy of $9 4 . 7 \%$ , followed closely by Template Functions at $9 4 . 4 \%$ . One remarkable feature of these results is that the dataset does not appear to favour any one category of vectorizations over the other — it is possible to achieve over $8 8 \%$ accuracy by using a suitable statistical, algebraic, curve, functional or ensemble vectorization. In fact, only the curve-based vectorizations failed to achieve over  

$9 0 \%$ accuracy on this dataset. The variation of classification accuracy with the heat kernel parameter $t$ is discussed in Appendix B.  

4.3. FMNIST. The results of our experiments on FMNIST are recorded in Table 4. We note that these experiments only used information contained in the 0-dimensional barcodes and that the SVM classifier was not used. The classification accuracy of all the methods is much lower than the corresponding figures for the two preceding datasets. Once more, the Persistence Statistics vectorization takes the top spot with $7 4 . 9 \%$ and Template Functions are slightly behind at $7 4 . 7 \%$  

<html><body><table><tr><td>Vectorization Method</td><td>Accuracy</td><td>Parameters</td></tr><tr><td>Persistence Statistics</td><td>0.749</td><td></td></tr><tr><td>Entropy Summary</td><td>0.696</td><td>30</td></tr><tr><td></td><td></td><td></td></tr><tr><td>Algebraic Functions Tropical Coordinates</td><td>0.710</td><td></td></tr><tr><td></td><td>0.696</td><td>10</td></tr><tr><td>Complex Polynomial</td><td>0.661</td><td>10, R</td></tr><tr><td>Betti Curve</td><td>0.618</td><td>50</td></tr><tr><td>Lifespan Curve</td><td>0.692</td><td>30</td></tr><tr><td>Persistence Landscape Persistence Silhouette</td><td>0.694</td><td>30,5</td></tr><tr><td></td><td>0.670</td><td>30,0</td></tr><tr><td>Persistence Image</td><td>0.698</td><td>1, 12</td></tr><tr><td>Template Functions</td><td>0.747</td><td>10, 2</td></tr><tr><td> Adaptive Template System</td><td>0.602</td><td>GMM,5</td></tr><tr><td>ATOL</td><td>0.730</td><td>16</td></tr></table></body></html>  

Table 4. FMNIST results. All the scores have been achieved for Random Forest classifier with 100 trees.  

One rather surprising aspect of these results is the fact that Adaptive Template Systems performed far worse than ordinary Template Functions despite having recourse to 60, 000 training barcodes. We do not have a clear explanation for this phenomenon, particularly in light of a fairly competitive performance from ATOL (which was also exposed to the same training data).  

# 5. Web Application  

In order to illustrate and visualize the vectorization methods described here, we have built an interactive web application that runs on any modern browser; it is available at  

# https://persistent-homology.streamlit.app/  

The app has been built in Python using the Streamlit library together and makes use of several existing Python libraries. The sidebar contains options for selecting different types of input data and displays several options for data visualization. One sample image/pointcloud from each of the three datasets used in this paper has been pre-loaded, but the user is free to upload their own data. Specifications, formatting guidelines, and downloading  

# instructions are available in our GitHub repository:  

https://github.com/dashtiali/vectorisation-app  

# BRAVA  

# Barcode Representation And Vectorization App  

![](images/1978a5a9479f9f18c74997b58b1166e96660d8738a102d4cee39edf2b5206e76.jpg)  
Figure 3. A screenshot of the web app  

![](images/add4ca7dd862ca2688912a15ffa278c5474b13e6f7c629019dccf0dcd48e45a0.jpg)  

All of the barcode vectorization methods considered in this paper can be computed and visualized in different formats (tables, bar graphs, scatter plots), depending on the type of vectorization being invoked. Barcodes are computed by default in dimensions 0 and 1, and depicted as in Figure 4.  

# Persistence Barcodes  

![](images/0ad55da9b47e6e6650a745770dff0febd39edc7c385d8f1bb192fd27290e6327.jpg)  
Figure 4. Intervals in barcodes of dimensions 0 and 1 as displayed by the web app.  

The Persistence Statistics vectorization is purely numerical, so we show its values in a table, as in Figure 5.  

# Persistence Statistics  

$= 1$   


<html><body><table><tr><td></td><td>Mean</td><td>STD</td><td>Median</td><td>IQR</td><td>Range</td><td>P10</td><td>P25</td><td>P75</td><td>P90</td></tr><tr><td>Births</td><td>73.8873</td><td>11.5833</td><td>76.0000</td><td>15.5000</td><td>64.0000</td><td>58.0000</td><td>66.0000</td><td>81.5000</td><td>87.0000</td></tr><tr><td>Deaths</td><td>87.5718</td><td>12.2165</td><td>87.0000</td><td>10.0000</td><td>189.0000</td><td>77.0000</td><td>82.0000</td><td>92.0000</td><td>99.6000</td></tr><tr><td>Midpoints</td><td>80.7296</td><td>9.1496</td><td>80.0000</td><td>11.0000</td><td>91.0000</td><td>63.0000</td><td>74.0000</td><td>88.0000</td><td>94.0000</td></tr><tr><td>Lifespans</td><td>13.6845</td><td>15.2305</td><td>10.0000</td><td>15.0000</td><td>209.0000</td><td>2.0000</td><td>4.0000</td><td>19.0000</td><td>29.0000</td></tr></table></body></html>  

![](images/47be5bd35f3e4c847fe9cfec60ae14b39bb150c491f62b9628edda5d677da582.jpg)  
Figure 5. The Persistence Statistics vectorization as shown in the web app.  

Algebraic vectorizations are illustrated as bar graphs. In Figure 6, for instance, one finds bars whose heights correspond to values attained by the 7 chosen tropical coordinate polynomials on the input barcodes.  

# Tropical Coordinates  

![](images/dd10a9c99ae77ea51ac5ae5e219f77ed25ece87d9d36d7868fc9afe45e5d6215.jpg)  
Figure 6. A visualization of the Tropical Coordinates vectorization from the web app.  

Curve vectorizations, such as persistence landscapes, are depicted via piecewise-linear graphs (see Figure 7). Sliders have been provided to set the resolution parameter.  

# PersistenceLandscapes  

![](images/46a8f0ab33f38f7d4f8c931edfb085e9528c1d95ebfabada6b71a701bbec848c.jpg)  
Figure 7. Persistence landscapes in the web app  

Persistence images are displayed as heat maps — see Figure 8.  

# Persistence lmage  

$\mathbf { \sigma } = \mathbf { \sigma }$ $\mathbf { \Sigma } = \mathbf { \Sigma }$  

Resolution  

![](images/ee4a78fb88aeb4a800c031d1308356383b33b6bf8b3d5a3880f83072fd7d5435.jpg)  
Figure 8. Persistence images as shown in the web app  

Template Functions, their adaptive version, and ATOL are all displayed as bar graphs with heights of bars indicating the values of the selected functions. Figure 9, for instance, depicts Template Functions.  

TemplateFunction  

![](images/b3eca1a6dc2b13f2354ed3e9eb85b8136eb6a49f493dd278a6f79445e686de61.jpg)  
  
Figure 9. The web app visualization of template functions  

It is our hope that users will benefit from the ability to generate these visualizations without having to write any code of their own. In order to facilitate downstream analysis, the web app also provides the ability to download the vectors generated by each vectorization method.  

# 6. Concluding Remarks  

At the time of writing, it remains difficult to accurately pinpoint those attributes which might make a given vectorization method a good choice for a particular classification problem. There are no powerful theorems or immutable doctrines available to guide scientists who wish to incorporate topological information into machine learning pipelines. In the absence of such theoretical foundations, the best that one can expect are principled heuristics supported by reproducible empirical evidence. This paper is an outcome of our efforts to provide such evidence. En route, we have organized thirteen available vectorization methods into five categories in Section 2 and provided a web application which will allow others to conduct their own experiments involving these methods.  

One possible conclusion that may be drawn from the results of Section 4 is that we can dispense with sophisticated vectorization techniques and only use (some variant of) Persistence Statistics. We do not necessarily suggest such a course of action. While it is certainly true that Persistence Statistics earned top honors in all of our experiments and is much faster to compute than the alternatives, there are other factors to consider. In particular, no comparative study such as ours can be truly exhaustive. There is always the chance that making different choices – for instance, using another dataset for classification, or adding some new polynomials to one of the algebraic vectorizations – could dramatically update our priors about which methods perform best.  

# Acknowledgments  

M.J. Jimenez, E. Paluzo-Hidalgo and M. Soriano-Trigueros are funded by the Spanish grants Ministerio de Ciencia e Innovacion - Agencia Estatal de Investigacion/10.13039/501100011033, PID2019-107339GB-I00 and Agencia Andaluza del Conocimiento, PAIDI-2020 P20-01145. M.J. Jimenez is also funded by a grant of Convocatoria de la Universidad de Sevilla para la recualificacion del sistema universitario espan˜ ol, 2021-23, funded by the European Union, NextGenerationEU.  

V. Nanda is supported by EPSRC grant EP/R018472/1 and by US AFOSR grant FA9550-22-1-0462.  

We are grateful to the team of GUDHI and TEASPOON developers, for their work and their support. We are also grateful to Streamlit for providing extra resources to deploy the web app online on Streamlit community cloud.  

# References  

[1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, S. Chepushtanova, E. Hanson, F. Motta, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology. J. Mach. Learn. Res., 18(1):218–252, 2017.   
[2] A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. Homology, Homotopy Appl., 18:381–402, 2016.   
[3] C. C. Aggarwal. Neural Networks and Deep Learning. Springer, 2018.   
[4] M. A. Armstrong. Basic Topology. Springer, 1983.   
[5] A. Asaad, D. Ali, T. Majeed, and R. Rashid. Persistent homology for breast tumor classification using mammogram scans. Mathematics, 10(21), 2022.   
[6] N. Atienza, R. Gonzalez-Diaz, and M. Soriano-Trigueros. On the stability of persistent entropy and new summary functions for topological data analysis. Pattern Recognition, 107:107509, 2020.   
[7] D. Barnes, L. Polanco, and J. A. Perea. A comparative study of machine learning methods for persistence diagrams. Frontiers in Artificial Intelligence, 4, 2021.   
[8] E. Berry, Y.-C. Chen, J. Cisewski-Kehe, and B. T. Fasy. Functional summaries of persistence diagrams. Journal of Applied and Computational Topology, 4(2):211–262, 2020.   
[9] C. A. Biscio and J. Møller. The accumulated persistence function, a new useful functional summary statistic for topological data analysis, with a view to brain artery trees and spatial point process applications. Journal of Computational and Graphical Statistics, 28(3):671–681, 2019.   
[10] P. Bubenik. Statistical topological data analysis using persistence landscapes. J. Mach. Learn. Res., 16(1):77– 102, Jan. 2015.   
[11] P. Bubenik, V. de Silva, and V. Nanda. Higher interpolation and extension for persistence modules. SIAM Journal on Applied Algebra and Geometry, 1(1):272–284, 2017.   
[12] P. Bubenik and P. Dłotko. A persistence landscapes toolbox for topological statistics. Journal of Symbolic Computation, 78:91–114, 2017.   
[13] J. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8(6):679–698, 1986.   
[14] G. Carlsson. Topology and data. Bull. Amer. Math. Soc. (N.S.), 46(2):255–308, 2009.   
[15] G. Carlsson and R. B. Gabrielsson. Topological approaches to deep learning. In Topological data analysis, pages 119–146. Springer, 2020.   
[16] M. Carri\`ere, F. Chazal, Y. Ike, T. Lacombe, M. Royer, and Y. Umeda. Perslay: A neural network layer for persistence diagrams and new graph topological signatures. In International Conference on Artificial Intelligence and Statistics, pages 2786–2796. PMLR, 2020.   
[17] M. Carriere, M. Cuturi, and S. Oudot. Sliced wasserstein kernel for persistence diagrams. In International conference on machine learning, pages 664–673. PMLR, 2017.   
[18] F. Chazal, V. de Silva, M. Glisse, and S. Oudot. The Structure and Stability of Persistence Modules. Springer, 2016.   
[19] F. Chazal, B. T. Fasy, F. Lecci, A. Rinaldo, and L. Wasserman. Stochastic convergence of persistence landscapes and silhouettes. In Proceedings of the Thirtieth Annual Symposium on Computational Geometry, SOCG’14, page 474–483, New York, NY, USA, 2014. Association for Computing Machinery.   
[20] I. Chevyrev and A. Kormilitzin. A primer on the signature method in machine learning. arXiv:1603.03788 [stat.ML], 2016.   
[21] I. Chevyrev, V. Nanda, and H. Oberhauser. Persistence paths and signature features in topological data analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(1):192–202, 2020.   
[22] H. Chintakunta, T. Gentimis, R. Gonzalez-Diaz, M. Jimenez, and H. Krim. An entropy-based persistence barcode. Pattern Recognit., 48(2):391–401, Feb. 2015.   
[23] Y. Chung and A. Lawson. Persistence curves: A canonical framework for summarizing persistence diagrams. Adv. Comput. Math., 48(1):6, 2022.   
[24] F. Conti, D. Moroni, and M. A. Pascali. A topological machine learning pipeline for classification. Mathematics, 10(17):3086, 2022.   
[25] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.   
[26] V. de Silva and V. Nanda. Geometry in the space of persistence modules. In Proceedings of the 29th Annual Symposuim on Computational Geometry, ACM, pages 397–404, 2013.   
[27] B. Di Fabio and M. Ferri. Comparing persistence diagrams through complex vectors. In V. Murino and E. Puppo, editors, Image Analysis and Processing — ICIAP 2015, pages 294–305, Cham, 2015. Springer International Publishing.   
[28] P. Dlotko. Cubical complex. In GUDHI User and Reference Manual. GUDHI Editorial Board, 3.6.0 edition, 2022.   
[29] Z. Dong, C. Hu, C. Zhou, and H. Lin. Vectorization of persistence barcode with applications in pattern classification of porous structures. Computers & Graphics, 90:182–192, 2020.   
[30] H. Edelsbrunner and J. Harer. Computational Topology - an Introduction. American Mathematical Society, 2010.   
[31] M. Ferri and C. Landi. Representing size functions by complex polynomials. Proc. Math. Met. in Pattern Recognition, 9:16–19, 1999.   
[32] R. Ghrist. Barcodes: the persistent topology of data. Bull. Amer. Math. Soc. (N.S.), 45(1):61–75, 2008.   
[33] C. Giusti and D. Lee. Signatures, lipschitz-free spaces, and paths of persistence diagrams. arXiv preprint arXiv:2108.02727, 2021.   
[34] W. Guo, K. Manohar, S. L. Brunton, and A. G. Banerjee. Sparse-tda: Sparse realization of topological data analysis for multi-way classification. IEEE Transactions on Knowledge and Data Engineering, 30(7):1403– 1408, 2018.   
[35] A. Hatcher. Algebraic topology. Cambridge University Press, 2002.   
[36] T. K. Ho. Random decision forests. In Proceedings of 3rd international conference on document analysis and recognition, volume 1, pages 278–282. IEEE, 1995.   
[37] C. D. Hofer, R. Kwitt, and M. Niethammer. Learning representations of persistence barcodes. J. Mach. Learn. Res., 20(126):1–45, 2019.   
[38] T. Kaczynski, K. M. Mischaikow, M. Mrozek, and K. Mischaikow. Computational homology. Applied mathematical sciences (Springer-Verlag New York Inc.); v. 157. Springer, New York, 2004.   
[39] S. Kaliˇsnik. Tropical coordinates on the space of persistence barcodes. Foundations of Computational Mathematics, 19(1):101–129, 2019.   
[40] A. Keros, V. Nanda, and K. Subr. Dist2Cycle: a simplicial neural network for homology localization. In Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence, 2022.   
[41] M. Moor, M. Horn, B. Rieck, and K. Borgwardt. Topological autoencoders. In International conference on machine learning, pages 7045–7054. PMLR, 2020.   
[42] V. Nanda and R. Sazdanovic. Simplicial models and topological inference in biological systems. In Discrete and Topological Models in Molecular Biology, pages 109–141. Springer, 2014.   
[43] T. Ojala, T. Maenpaa, M. Pietikainen, J. Viertola, J. Kyllonen, and S. Huovinen. Outex-new framework for empirical evaluation of texture analysis algorithms. In 2002 International Conference on Pattern Recognition, volume 1, pages 701–706. IEEE, 2002.   
[44] S. Y. Oudot. Persistence theory: from quiver representations to data analysis, volume 209. American Mathematical Soc., 2017.   
[45] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.   
[46] J. A. Perea, E. Munch, and F. A. Khasawneh. Approximating continuous functions on persistence diagrams using template functions. Foundations of Computational Mathematics, 2022.   
[47] D. Pickup, X. Sun, P. L. Rosin, R. R. Martin, Z. Cheng, Z. Lian, M. Aono, A. Ben Hamza, A. Bronstein, M. Bronstein, S. Bu, U. Castellani, S. Cheng, V. Garro, A. Giachetti, A. Godil, J. Han, H. Johan, L. Lai, B. Li, C. Li, H. Li, R. Litman, X. Liu, Z. Liu, Y. Lu, A. Tatsuma, and J. Ye. SHREC’14 track: Shape retrieval of non-rigid 3d human models. In Proceedings of the 7th Eurographics workshop on 3D Object Retrieval, EG 3DOR’14. Eurographics Association, 2014.   
[48] L. Polanco and J. A. Perea. Adaptive template systems: Data-driven feature selection for learning with persistence diagrams. In 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), pages 1115–1121. IEEE, 2019.   
[49] C. S. Pun, K. Xia, and S. X. Lee. Persistent-homology-based machine learning and its applications–a survey. arXiv preprint arXiv:1811.00252, 2018.   
[50] J. Reininghaus, S. Huber, U. Bauer, and R. Kwitt. A stable multi-scale kernel for topological machine learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4741–4748, 2015.   
[51] M. Royer, F. Chazal, C. Levrard, Y. Umeda, and Y. Ike. Atol: Measure vectorization for automatic topologically-oriented learning. In A. Banerjee and K. Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1000–1008. PMLR, 4 2021.   
[52] M. Rucco, F. Castiglione, E. Merelli, and M. Pettini. Characterisation of the idiotypic immune network through persistent entropy. In Proceedings of ECCS 2014, pages 117–128. Springer International Publishing, 2016.   
[53] A. Som, K. N. Ramamurthy, and P. Turaga. Geometric metrics for topological representations. In Handbook of Variational Methods for Nonlinear Geometric Data, pages 415–441. Springer, 2020.   
[54] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and provably informative multi-scale signature based on heat diffusion. In Computer graphics forum, volume 28, pages 1383–1392. Wiley Online Library, 2009.   
[55] The GUDHI Project. GUDHI User and Reference Manual. GUDHI Editorial Board, 3.6.0 edition, 2022.   
[56] K. Turner, Y. Mileyko, S. Mukherjee, and J. Harer. Fr´echet means for distributions of persistence diagrams. Discrete & Computational Geometry, 52:44–70, 2014.   
[57] K. Xia. Persistent similarity for biomolecular structure comparison. Communications in Information and Systems, 18(4):269–298, 2018.   
[58] L. Xian, H. Adams, C. M. Topaz, and L. Ziegelmeier. Capturing dynamics of time-varying data via topology. Foundations of Data Science, 4(1), 2022.   
[59] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   
[60] B. Zieli ´nski, M. Lipin´ ski, M. Juda, M. Zeppelzauer, and P. Dłotko. Persistence codebooks for topological data analysis. Artificial Intelligence Review, 54(3):1969–2009, 2021.   
[61] A. Zomorodian and G. Carlsson. Computing persistent homology. Discrete & Computational Geometry, 33(2):249–274, 2005.  

# Appendix A. Implementation and Parameter Details  

We have made use of several existing software packages, such as GUDHI [55], Teaspoon5 or Scikit-learn [45], as well as our own implementations in some cases. Salient information regarding each method has been provided in the list below. Full details can be found in the GitHub repository accompanying this paper6.  

A.1. Persistence Statistics. The persistence statistics vectorization from Definition 2.1 requires no additional parameters. We have implemented this method ourselves.  

A.2. Entropy Summary Function. We have used the GUDHI implementation of the entropy summary function from Definition 2.2. There is a single resolution parameter which selects the grid points on which the entropy summary function is sampled.  

A.3. Algebraic Functions. The algebraic functions of Definition 2.3 are implemented in the Teaspoon package. For reasons which remain unclear to us, this implementation includes a fifth tropical polynomial $f _ { 5 } = \operatorname* { m a x } _ { i } \{ ( q _ { i } - p _ { i } ) \}$ beyond the four ordinary polynomials $f _ { 1 } , \ldots , f _ { 4 }$ which were described after Definition 2.3. We do not expect that removing this function will improve the results described below.  

A.4. Tropical Coordinates. We have implemented the tropical polynomials $F _ { 1 } , \ldots , F _ { 7 }$ described after Definition 2.4. The parameter $r$ has been optimized over the set $\{ 1 0 , 5 0 , 2 5 0 ,$ $5 0 0 , 8 0 0 \}$ for Outex and SHREC14, and over $\{ 1 0 , 5 0 , 2 5 0 \}$ for FMNIST.  

A.5. Complex Polynomials. We have used the GUDHI implementation of complex polynomials, which have been described in Definition 2.5. We generated the polynomials with respect to all three of the transformations $R , S , T : \mathbb { R } ^ { 2 }  \mathbb { C }$ . The number of coefficients used was chosen from $\{ 5 , 1 0 , 2 0 \}$ for Outex and SHREC14 and $\{ 3 , 5 , 1 0 \}$ for FMNIST.  

A.6. Betti Curve. The Betti curve vectorization from Definition 2.6 has been implemented in GUDHI, and it only requires a resolution parameter. This parameter was chosen from $\{ 5 0 , 1 0 0 , 2 0 0 \}$ for Outex and SHREC14 and $\{ 1 5 , 3 0 , 5 0 \}$ for FMNIST.  

A.7. Lifespan Curve. We implemented the lifespan curve ourselves, with a resolution parameter optimised across the set $\{ 5 0 , 1 0 0 , 2 0 0 \}$ for Outex and SHREC14 and across the set $\{ 1 5 , 3 0 , 5 0 \}$ for FMNIST.  

A.8. Persistence Landscapes. We have used the GUDHI implementation of persistence landscapes (see Definition 2.8). The are two parameters to consider: the resolution (to identify the grid points where each landscape is sampled) and the total number of landscapes used. The resolution was optimized over $\{ 5 0 , 1 0 0 , 2 0 0 \}$ for Outex and SHREC14, and over $\{ 1 5 , 3 0 , 5 0 \}$ for FMNIST; the number of landscapes ranged over $\{ 2 , 5 , 1 0 , 2 0 \}$ for Outex and SHREC14 and over $\{ 1 , 2 , 3 , 5 \}$ for FMNIST.  

A.9. Persistence Silhouette. We have used the GUDHI implementation of persistence silhouettes (see Definition 2.9). The resolution parameter was optimized over $\{ 5 \bar { 0 } , 1 0 0 , 2 0 0 \}$ for Outex and SHREC14 and over $\{ 1 5 , 3 0 , 5 0 \}$ for FMNIST; the weight $w$ ranged over $\{ 0 , 1 , 2 , 5 , 1 0 , 2 0 \}$ for Outex and SHREC14 and $\{ 0 , 1 , 2 , 5 \}$ for FMNIST.  

A.10. Persistence Images. Persistence images (from Definition 2.10) have been implemented in GUDHI. The resolution parameter $r$ , which results in images of size $\boldsymbol { r } \times \boldsymbol { r } ,$ , ranged over $\{ 2 5 , 7 5 , 1 5 0 \}$ for Outex, over $\{ 1 0 , 2 0 , 4 0 \}$ for SHREC14, and over $\{ 3 , 6 , 1 2 , 2 0 \}$ for FMNIST. Bandwidth values of the Gaussian kernel ( $\dot { } \sigma$ in Definition 2.10) were chosen from $\{ 0 . 0 5 , 1 \}$ for Outex and from $\{ 0 . 0 5 , 0 . 5 , 1 \}$ for both, SHREC14 and FMNIST.  

A.11. Template Functions. We have used code from the repository7 provided with the paper [46] for computing template functions (see Definition 2.11). We use tent functions as described in (2), which require two parameters: a grid resolution $\delta$ and a padding parameter $\pi$ (for enlarging the area covered by the square grid). We optimized over  

$\delta$ in $\{ 3 5 , 5 0 , 6 5 \}$ and $\pi$ in $\{ 2 0 , 2 5 , 3 0 \}$ for Outex;   
$\delta$ in $\{ 3 , 4 , \dots , 1 4 , 1 5 \}$ and $\pi$ in $\{ 0 . 5 , 0 . 6 , \dots , 1 . 1 , 1 . 2 \}$ for SHREC14;   
$\delta$ in $\{ 2 , 3 , 5 , 1 0 \}$ and $\pi$ in $\{ 0 . 5 , 1 , 2 \}$ for FMNIST.  

A.12. Adaptive Template Systems. The implementation of adaptive template systems (Definition 2.12) has also been sourced from the same repository as template functions. We have used the Gaussian mixture model for generating ellipsoidal domains, and require only one parameter: the number of clusters. This has been optimized over  

$\{ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 \}$ for Outex, $\bullet \ \{ 5 , 1 0 , 1 5 , 2 0 , 2 5 , 3 0 , 3 5 , 4 0 , 4 5 \}$ for SHREC14, and $\{ 3 , 4 , 5 , 1 0 , 1 5 \}$ for FMNIST.  

A.13. ATOL. The ATOL vectorization from Definition 2.13 has been implemented in GUDHI, and it also requires the number of functions $b$ as a parameter. We have optimized this over $\{ 2 , 4 , 8 , 1 6 , 3 2 , 6 4 \}$ for Outex and over $\{ 2 , 4 , 8 , 1 6 \}$ for both SHREC14 and FMNIST.  

A.14. Dimensions, Classifiers and Hyperparameters. In the case of Outex, we have concatenated vectors arising from barcodes of dimensions 0 and 1; for SHREC14, the vectors computed from only dimension 1 barcodes performed better, so the results are only reported for them. Finally, only dimension 0 barcodes were taken to build vectors for FMNIST. We considered both Support Vector Machine (SVM) [25] and Random Forest (RF) [36] classifiers. Due to convergence issues, only RF has been performed for FMNIST.  

For each parameter of each vectorization method, we accomplished a hyperparameter optimization process based on random search (when optimizing SVM and RF jointly) or grid search (for optimizing RF), with 5-fold cross-validation on the training data, to find the best (hyper)parameters for both the machine learning models and the vectorization methods; then, we assigned to each method the parameters with the best average score among all the 5-fold cross-validation scheme; finally the vectorization methods were evaluated on the test dataset 100 times, to report the average accuracy.  

# Appendix B. Heat Kernel Parameter Dependence  

As mentioned in Section 3, the filtration for SHREC14 is generated using the Heat Kernel Signature (6) which depends on a single parameter $t$ . In Table 5 we depict the best classification accuracy of each vectorization method across all 9 values of $t$ which were used in our experiments.  

<html><body><table><tr><td>Method</td><td>t1</td><td>t2</td><td>t3</td><td>t4</td><td>t5</td><td>t6</td><td>t7</td><td>t8</td><td>t9</td></tr><tr><td>Pers Stat</td><td>0.729</td><td>0.785</td><td>0.662</td><td>0.704</td><td>0.947</td><td>0.910</td><td>0.915</td><td>0.915</td><td>0.908</td></tr><tr><td>Ent Sum</td><td>0.378</td><td>0.333</td><td>0.522</td><td>0.536</td><td>0.656</td><td>0.723</td><td>0.633</td><td>0.656</td><td>0.530</td></tr><tr><td>Alg Fun</td><td>0.467</td><td>0.456</td><td>0.556</td><td>0.567</td><td>0.909</td><td>0.878</td><td>0.863</td><td>0.833</td><td>0.711</td></tr><tr><td>Trop Coord</td><td>0.505</td><td>0.556</td><td>0.522</td><td>0.612</td><td>0.822</td><td>0.844</td><td>0.833</td><td>0.767</td><td>0.800</td></tr><tr><td>Com Poly</td><td>0.322</td><td>0.456</td><td>0.400</td><td>0.467</td><td>0.856</td><td>0.889</td><td>0.844</td><td>0.850</td><td>0.790</td></tr><tr><td>Bet Cur</td><td>0.511</td><td>0.467</td><td>0.628</td><td>0.660</td><td>0.728</td><td>0.633</td><td>0.611</td><td>0.644</td><td>0.536</td></tr><tr><td>Lif Cur</td><td>0.456</td><td>0.411</td><td>0.593</td><td>0.639</td><td>0.789</td><td>0.833</td><td>0.878</td><td>0.833</td><td>0.798</td></tr><tr><td>Pers Land</td><td>0.700</td><td>0.511</td><td>0.789</td><td>0.778</td><td>0.878</td><td>0.889</td><td>0.857</td><td>0.833</td><td>0.789</td></tr><tr><td>Pers Sil</td><td>0.400</td><td>0.378</td><td>0.556</td><td>0.589</td><td>0.811</td><td>0.867</td><td>0.856</td><td>0.856</td><td>0.656</td></tr><tr><td>Pers Img</td><td>0.644</td><td>0.691</td><td>0.795</td><td>0.856</td><td>0.916</td><td>0.794</td><td>0.871</td><td>0.811</td><td>0.718</td></tr><tr><td>Temp Func</td><td>0.778</td><td>0.735</td><td>0.933</td><td>0.789</td><td>0.944</td><td>0.919</td><td>0.908</td><td>0.932</td><td>0.922</td></tr><tr><td>Ad Temp Sys</td><td>0.802</td><td>0.872</td><td>0.833</td><td>0.727</td><td>0.889</td><td>0.856</td><td>0.889</td><td>0.844</td><td>0.633</td></tr><tr><td>ATOL</td><td>0.828</td><td>0.786</td><td>0.911</td><td>0.833</td><td>0.906</td><td>0.867</td><td>0.900</td><td>0.933</td><td>0.867</td></tr></table></body></html>  

Table 5. Best Results for SHREC14 for various vectorization methods across nine $t$ -values.  

We note that for small values of $t ,$ the ensemble vectorizations perform best, whereas for intermediate and larger values both ensemble and functional vectorizations achieve good performance. The algebraic and curve based vectorizations perform quite poorly for low $t$ -values, but tend to become more competitive between $t _ { 5 }$ and $t _ { 8 }$ .  