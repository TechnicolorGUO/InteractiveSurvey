# A Survey of Vectorization Techniques in Topological Data Analysis

# 1 Abstract


Topological Data Analysis (TDA) has emerged as a powerful framework for extracting meaningful insights from complex and high-dimensional datasets by leveraging concepts from algebraic topology. This survey paper focuses on the vectorization techniques in TDA, specifically those that enable the effective integration of persistence diagrams (PDs) with machine learning models. The paper provides a comprehensive overview of the current state-of-the-art methods, including embedding and transformation techniques, binning and discrete integral transforms, batch and mini-batch quantization, and clustering and approximation methods. These techniques are essential for converting PDs into vector representations that can be readily processed by machine learning algorithms, thereby enhancing their applicability in various domains such as image analysis, genomics, and material science. The main findings include the development of robust and computationally efficient methods for vectorizing PDs, the integration of these methods with deep learning architectures, and the demonstration of their effectiveness in enhancing the performance of machine learning models. By bridging the gap between topological data analysis and machine learning, this survey aims to advance the field and facilitate the development of more robust and interpretable data analysis tools.

# 2 Introduction
Topological Data Analysis (TDA) has emerged as a powerful framework for extracting meaningful insights from complex and high-dimensional datasets [1]. By leveraging concepts from algebraic topology, TDA provides a robust and interpretable approach to understanding the underlying structure and patterns in data [2]. Central to TDA is the concept of persistence diagrams (PDs), which capture the birth and death of topological features across different scales [3]. PDs have found applications in a wide range of fields, including image analysis, genomics, and material science. However, the non-Euclidean nature of PDs poses significant challenges for their integration into standard machine learning pipelines, necessitating the development of vectorization techniques that can transform PDs into a format suitable for machine learning algorithms [1].

This survey paper focuses on the vectorization techniques in Topological Data Analysis, specifically those that enable the effective integration of PDs with machine learning models [4]. The paper provides a comprehensive overview of the current state-of-the-art methods, including embedding and transformation techniques, binning and discrete integral transforms, batch and mini-batch quantization, and clustering and approximation methods. These techniques are essential for converting PDs into vector representations that can be readily processed by machine learning algorithms, thereby enhancing their applicability in various domains [5].

The paper begins by exploring the Kuratowski embedding and quadratic programming, which provide a powerful tool for transforming PDs into a functional form that can be analyzed using standard machine learning techniques. The Kuratowski embedding preserves the metric structure of PDs, enabling the application of optimization methods such as quadratic programming. This approach is particularly useful for tasks that require a vector representation of PDs, such as classification and clustering [5]. The quadratic programming formulation allows for the efficient computation of the optimal function, which can then be used to classify or cluster PDs in a high-dimensional space.

Next, the paper delves into binning and discrete integral transforms, which offer a computationally efficient and theoretically sound framework for converting PDs into vector representations. The Qupid method, which involves a two-step process of binning and discrete integral transforms, emphasizes regions of the PD with a high density of points, thereby preserving the topological information while enhancing computational efficiency. The flexibility of the Qupid method in adjusting the grid resolution and the choice of integral transform further enhances its applicability across a wide range of data types and analysis tasks.

The paper also discusses batch and mini-batch quantization techniques, which are designed to handle the computational challenges associated with large-scale PDs. These methods extend classical quantization techniques, such as k-means clustering, to the context of PDs, enabling the efficient approximation of the mean measure of a distribution of PDs. The batch and mini-batch algorithms are designed to converge to a minimax optimal estimator of the best possible k-point approximation of the expected measure, ensuring that the resulting quantized representations are both stable and efficient [6].

Furthermore, the paper examines clustering and approximation methods, including k-means clustering in non-Euclidean spaces, mean measure approximation, and minimax estimation [2]. These methods address the challenges of defining centroids and means in the space of PDs, which is equipped with metrics like the Wasserstein or bottleneck distances [3]. The paper explores various adaptations of the k-means algorithm for non-Euclidean spaces, such as the use of Fréchet means and kernel methods, which transform PDs into feature vectors that can be used as inputs for standard k-means clustering in Euclidean space [2].

Finally, the paper highlights the contributions of this survey, which include a comprehensive review of the latest vectorization techniques in TDA, a detailed exploration of their theoretical foundations, and a discussion of their practical applications in machine learning [7]. The survey provides a valuable resource for researchers and practitioners interested in the integration of TDA with machine learning, offering insights into the strengths and limitations of various vectorization methods and suggesting directions for future research [7]. By bridging the gap between topological data analysis and machine learning, this survey aims to advance the field and facilitate the development of more robust and interpretable data analysis tools [8].

# 3 Vectorization of Persistence Diagrams

## 3.1 Embedding and Transformation Techniques

### 3.1.1 Kuratowski Embedding and Quadratic Programming
The Kuratowski embedding is a fundamental concept that allows the embedding of a metric space into a Banach space, specifically the space of bounded continuous functions. In the context of persistence diagrams (PDs), this embedding provides a powerful tool for transforming PDs into a functional form that can be more readily analyzed using standard machine learning techniques [9]. By mapping each point in a PD to a function in the Banach space, the Kuratowski embedding preserves the metric structure of the PD, enabling the application of optimization methods such as quadratic programming. This transformation is particularly useful for tasks that require a vector representation of PDs, such as classification and clustering.

The quadratic programming problem arises naturally from the application of the Kuratowski embedding to PDs. Specifically, the embedding of a PD into a Banach space can be formulated as a quadratic optimization problem, where the objective is to find a function that maximizes the margin between different classes of PDs [10]. This is achieved by defining a kernel function that measures the similarity between PDs in the embedded space. The kernel function, in turn, can be derived from the metric structure of the PDs, such as the bottleneck or Wasserstein distance. The quadratic programming formulation allows for the efficient computation of the optimal function, which can then be used to classify or cluster PDs in a high-dimensional space.

In practice, the Kuratowski embedding and quadratic programming approach offer several advantages over traditional methods for analyzing PDs. First, the embedding provides a natural way to handle the non-Euclidean nature of PDs, which are typically represented as multi-sets of points in a plane. Second, the quadratic programming formulation allows for the incorporation of prior knowledge or constraints, such as the requirement for a maximal margin classifier. Finally, the method is computationally efficient and can be scaled to handle large datasets, making it a valuable tool for applications in topological data analysis (TDA) and machine learning [1].

### 3.1.2 Binning and Discrete Integral Transforms
Binning and discrete integral transforms constitute a powerful framework for converting persistence diagrams (PDs) into vector representations that are both computationally efficient and theoretically sound [8]. This section introduces the Qupid method, which involves a two-step process: first, PDs are transformed into finite measures through a binning procedure, and second, discrete integral transforms are applied to these measures to extract and enhance valuable structural information. The binning procedure is designed to emphasize regions of the PD with a high density of points, which are often associated with fine geometric details of the underlying data. This approach not only preserves the topological information but also enhances the computational efficiency of the vectorization process.

The binning step involves partitioning the domain of the PD into a grid of cells, where the size and resolution of the grid can be adaptively adjusted to balance between computational efficiency and the preservation of topological features. Each cell in the grid is assigned a weight based on the number of points in the PD that fall within it. This weighted grid is then used to construct a finite measure, which serves as the input for the discrete integral transform. The choice of the integral transform, such as the Fourier or wavelet transform, is crucial as it can capture different aspects of the PD's structure, such as frequency components or localized features. The resulting transformed values are arranged into a vector, forming the final vectorized representation of the PD.

The proposed method not only addresses the computational challenges associated with the direct computation of Wasserstein distances but also provides a stable and interpretable representation of PDs [11]. The stability of the vectorization is ensured by the continuity of the integral transform with respect to the Wasserstein distance, which allows for robustness to input noise. Additionally, the method's efficiency is demonstrated through its ability to handle large datasets with a sparse grid of scale values, making it particularly suitable for applications in machine learning and data analysis. The flexibility of the Qupid method in adjusting the grid resolution and the choice of integral transform further enhances its applicability across a wide range of data types and analysis tasks.

### 3.1.3 Batch and Mini-batch Quantization
Batch and mini-batch quantization techniques represent a significant advancement in the field of topological data analysis (TDA), particularly when dealing with the computational challenges associated with large-scale persistence diagrams (PDs) [8]. These methods extend classical quantization techniques, such as k-means clustering, to the context of PDs, enabling the efficient approximation of the mean measure of a distribution of PDs. The primary goal of batch and mini-batch quantization is to construct a k-point approximation of the expected value of a random PD, \(E(X)\), using a finite sample of PDs, \(X_1, \ldots, X_n\) [6]. This is achieved by iteratively refining a set of centroids that best represent the distribution of PDs, thereby reducing the computational complexity associated with direct PD comparison.

The batch quantization algorithm processes the entire dataset at each iteration, making it suitable for smaller datasets where memory and computational resources are not limiting factors. In contrast, the mini-batch quantization algorithm updates the centroids using a subset (mini-batch) of the data at each iteration, which significantly reduces the computational burden and makes it feasible to handle large-scale datasets. Both algorithms are designed to converge to a minimax optimal estimator of the best possible k-point approximation of \(E(X)\), provided that \(E(X)\) satisfies certain structural assumptions. This optimality ensures that the resulting quantized representations are both stable and efficient, making them suitable for various machine learning tasks, such as classification and regression.

Moreover, the flexibility of batch and mini-batch quantization allows for the adjustment of the grid resolution used in the binning process, which can be tailored to the specific characteristics of the PDs being analyzed. This adaptability is crucial for capturing the fine geometric information contained in regions of high point density within the PDs. By emphasizing these regions, the quantization techniques enhance the discriminative power of the resulting vectorized representations, leading to improved performance in downstream machine learning tasks. Additionally, the use of discrete integral transforms, such as Fourier or wavelet transforms, further enhances the ability to extract and highlight valuable structural information from the quantized PDs, contributing to the overall effectiveness of the method.

## 3.2 Clustering and Approximation Methods

### 3.2.1 K-means Clustering in Non-Euclidean Spaces
K-means clustering, a cornerstone of unsupervised learning, traditionally operates in Euclidean spaces where the concept of a centroid is well-defined and computationally tractable [2]. However, when dealing with non-Euclidean spaces, such as the space of persistence diagrams (PDs) in topological data analysis (TDA), the application of k-means becomes significantly more challenging [9]. The primary issue arises from the lack of a well-defined mean in non-Euclidean spaces, which complicates the centroid update step in the k-means algorithm. Additionally, the space of PDs, equipped with metrics like the Wasserstein or bottleneck distances, is not a Hilbert space, leading to non-uniqueness of means and geodesics, and thus complicating the convergence properties of the algorithm.

To address these challenges, researchers have proposed various adaptations of the k-means algorithm for non-Euclidean spaces [2]. One approach involves defining a generalized notion of the mean, such as the Fréchet mean, which minimizes the sum of squared distances to the points in the cluster. However, computing the Fréchet mean in the space of PDs is computationally expensive and often requires iterative optimization techniques. Another approach is to map PDs into a Hilbert space using kernel methods or vectorization techniques, such as persistence landscapes, persistence images, or persistence scale-space kernels [1]. These methods transform PDs into feature vectors that can be used as inputs for standard k-means clustering in Euclidean space, thereby leveraging the well-established theory and computational efficiency of the algorithm.

Despite these advancements, the choice of vectorization or kernel method can significantly impact the performance of k-means clustering in non-Euclidean spaces [2]. For instance, while persistence landscapes and persistence images provide stable and computationally efficient representations, they may lose some topological information present in the original PDs. On the other hand, more sophisticated methods like the persistence scale-space kernel can capture finer topological details but at the cost of increased computational complexity [12]. Therefore, the selection of an appropriate method depends on the specific application and the trade-off between computational efficiency and the preservation of topological information.

### 3.2.2 Mean Measure Approximation and Minimax Estimation
In the context of mean measure approximation and minimax estimation, the primary objective is to develop robust and computationally efficient methods for estimating the central tendency of a distribution of measures, particularly in the space of persistence diagrams. The central measure, defined as the Wasserstein barycenter, serves as a natural extension of the mean concept to the non-Euclidean setting of persistence diagrams [6]. This measure is obtained by minimizing the expected squared Wasserstein distance between the distribution of measures and a candidate measure. The theoretical foundation of this approach is rooted in the Karush–Kuhn–Tucker (KKT) conditions, which provide a characterization of the optimal solution to the minimization problem. The KKT conditions ensure that the solution is a partial optimal point, a KKT point, and a critical point, thereby establishing the convergence of the k-means algorithm in this context.

To address the computational challenges associated with the estimation of the central measure, two algorithms—batch and mini-batch—are proposed, extending classical quantization techniques [6]. These algorithms are designed to approximate the central measure by iteratively refining a set of representative points (or centroids) that best capture the distribution of the measures. The batch algorithm updates the centroids using the entire dataset, while the mini-batch variant uses a subset of the data at each iteration, significantly reducing the computational burden. Theoretical analysis of these algorithms demonstrates that they provide minimax optimal estimators of the best possible k-points approximation of the expected measure, under certain structural assumptions [6]. This optimality is crucial for ensuring that the estimated central measure is both accurate and stable, even in the presence of noisy or complex data.

The practical utility of these methods is further validated through extensive simulation experiments and real-world applications. The results show that the proposed algorithms achieve competitive accuracy in supervised classification tasks while maintaining a significant advantage in computational efficiency compared to other state-of-the-art methods, such as persistence images and kernel methods [8]. The ability to handle large datasets and high-dimensional persistence diagrams without sacrificing accuracy or computational speed makes these methods particularly appealing for applications in topological data analysis (TDA) and machine learning [8]. Moreover, the simplicity of the algorithms facilitates their integration into existing data analysis pipelines, making them accessible to a broader audience of researchers and practitioners.

### 3.2.3 Template Functions and Topological Vector Spaces
Template functions play a crucial role in the transformation of persistence diagrams (PDs) into a format suitable for machine learning algorithms [9]. These functions, which are continuous and compactly supported on \(\mathbb{R}^2\), serve as a bridge between the topological features captured by PDs and the vector space required by many machine learning models. By evaluating a set of template functions on a PD, we can generate a feature vector that retains the topological information while being amenable to standard machine learning techniques [4]. This process not only stabilizes the representation of PDs with respect to the distance metrics but also allows for the application of a wide range of statistical and machine learning methods [3].

One of the key challenges in using template functions is the selection and parameterization of these functions to effectively capture the topological features of interest. To address this, we propose an adaptive method that partitions the PD into localized regions and modifies the parameters of the template functions to better fit these regions. This adaptive approach reduces the number of features needed compared to the original method, thereby improving computational efficiency while maintaining or even enhancing the discriminative power of the resulting feature vectors. The adaptive method leverages the intrinsic structure of the PD to optimize the template functions, ensuring that the most relevant topological information is preserved.

In this section, we also explore the theoretical underpinnings of using template functions in the context of topological vector spaces. Specifically, we demonstrate that the space of continuous, compactly supported functions on \(\mathbb{R}^2\) (denoted \(C_c(\mathbb{R}^2)\)) can be used to construct a dense subset of the space of continuous functions on the PD (denoted \(C(D, \mathbb{R})\)). This theoretical foundation supports the use of template functions as a robust and versatile tool for vectorizing PDs. We further discuss the practical implications of this approach, including its application in regression and classification tasks, and provide empirical evidence of its effectiveness through a series of experiments.

## 3.3 Integration with Machine Learning

### 3.3.1 Persistence Curves and Graph Neural Networks
Persistence curves (PCs) represent a significant advancement in the vectorization of persistence diagrams (PDs), enabling their integration with machine learning algorithms [9]. Unlike traditional methods that directly manipulate PDs, which are inherently complex and non-linear, PCs provide a structured and interpretable representation. The construction of a PC involves mapping each point in a PD to a continuous function, typically a Gaussian or a B-spline, and then integrating these functions over a predefined domain. This process results in a vector that captures the topological features of the data while preserving the stability and robustness of the original PD [4]. The flexibility of PCs lies in the choice of the mapping function and the integration domain, which can be tailored to specific applications, making them a versatile tool in topological data analysis (TDA) [13].

Graph Neural Networks (GNNs) have emerged as a powerful framework for learning from graph-structured data, and their integration with PCs offers a novel approach to leveraging topological information in graph-based machine learning tasks [14]. By representing the topological features of a graph as a PC, GNNs can incorporate this information into their learning process [14]. This combination addresses the limitations of traditional GNNs, which often struggle to capture the global structure of graphs. The integration of PCs into GNNs is achieved by using the vectorized representation of the PC as an additional feature vector for each node or the entire graph. This enriched feature set allows GNNs to better understand the topological context of the graph, leading to improved performance in tasks such as node classification, link prediction, and graph classification.

The synergy between PCs and GNNs is particularly evident in applications where the data has a complex topological structure, such as molecular graphs in chemistry, social networks, and biological networks. Experimental results have shown that the use of PCs in conjunction with GNNs can lead to significant improvements in model accuracy and robustness. For instance, in the classification of molecular graphs, the topological features captured by PCs can help distinguish between different molecular structures that may have similar chemical compositions but different topological arrangements. Similarly, in social network analysis, the topological context provided by PCs can enhance the ability of GNNs to identify influential nodes and community structures. This integration not only enriches the feature space but also provides a more comprehensive understanding of the data, making it a promising direction for future research in TDA and machine learning [7].

### 3.3.2 Persistence Images and B-spline Grids
Persistence Images (PIs) represent a powerful technique for converting persistence diagrams (PDs) into a format that is more amenable to machine learning [15]. PIs are constructed by mapping each point in a PD to a Gaussian distribution centered at the point's coordinates in the birth-persistence plane [5]. These distributions are then integrated over a grid, resulting in a matrix of pixel values that captures the topological structure of the original data. The choice of the grid is crucial, as it balances the trade-off between resolution and computational efficiency. A finer grid provides higher resolution but increases computational cost, while a coarser grid reduces computation but may lose important topological details.

To address the limitations of traditional grid-based methods, the use of B-spline grids has been proposed. B-splines offer a flexible and adaptive way to partition the domain of the persistence diagram, allowing for a more precise representation of the underlying topological features. Unlike fixed grids, B-spline grids can be adjusted to better capture the density and distribution of points in the PD. This adaptivity is particularly useful for PDs with varying point densities, as it ensures that regions with high topological significance are adequately represented. The B-spline basis functions are smooth and continuous, which helps in maintaining the integrity of the topological information during the discretization process [5].

The construction of a B-spline grid involves defining a set of control points and a knot vector, which together determine the shape and spacing of the B-spline basis functions. Once the grid is established, the persistence B-spline function (PB f(D)) is computed by evaluating the B-spline basis functions at the points in the PD. The resulting function is then integrated over each grid cell to produce a vectorized representation. This vectorization method preserves the entire information of the persistence B-spline function, thereby enhancing both the effectiveness and efficiency of the topological analysis [5]. The stability of the vectorized representations with respect to the 1-Wasserstein distance further ensures that the method is robust to small perturbations in the input data.

### 3.3.3 Adaptive Partitioning and Template Function Featurization
Adaptive partitioning and template function featurization represent a significant advancement in the vectorization of persistence diagrams (PDs) for machine learning applications [9]. Traditional methods, such as the persistence bag-of-words (PBoW), rely on a fixed codebook generated through clustering of PD points, which can be limiting in capturing the nuanced topological features of complex datasets [5]. In contrast, adaptive partitioning dynamically adjusts the partitioning of the PD space based on the distribution of points, allowing for more localized and context-specific feature extraction. This approach leverages the inherent structure of the data to optimize the featurization process, thereby enhancing the representational power of the resulting feature vectors.

The core of this adaptive method involves the use of template functions, which are continuous and compactly supported functions defined on \(\mathbb{R}^2\). These functions are evaluated at the points within each partition of the PD, generating a set of feature values that collectively form a vector representation of the PD. By adaptively partitioning the PD space, the method can focus on regions of high topological significance, leading to a more efficient and informative featurization. This is particularly useful for datasets with varying densities and structures, where a fixed partitioning scheme might fail to capture important features.

Moreover, the adaptive partitioning and template function featurization approach significantly reduces the number of features required for an accurate representation, compared to traditional methods [9]. This reduction in feature dimensionality not only improves computational efficiency but also enhances the interpretability of the resulting vectors. The method is designed to maintain the stability properties of the original PDs, ensuring that the vectorized representations remain robust to small perturbations in the input data [5]. This makes the approach particularly suitable for real-world applications where data may be noisy or incomplete. The combination of adaptive partitioning and template function featurization thus provides a powerful tool for integrating topological data analysis into machine learning workflows, enabling more effective and efficient data analysis [8].

# 4 Integration of TDA with Machine Learning Models

## 4.1 Enhancing Image Analysis

### 4.1.1 Persistent Homology with Support Vector Machines
Persistent homology, a cornerstone of topological data analysis (TDA), provides a robust framework for capturing the topological features of complex datasets, which are essential for understanding the underlying structure and patterns [4]. However, the space of persistence diagrams (PDs), which encapsulate the topological information, cannot be isometrically embedded into a Hilbert space, posing a significant challenge for integration with traditional machine learning algorithms, particularly support vector machines (SVMs) [16]. SVMs, which rely on the inner product structure of a Hilbert space, require the input data to be represented as vectors. This incompatibility necessitates the development of vectorization methods that can transform PDs into a format suitable for SVMs while preserving the topological information.

To address this challenge, several vectorization techniques have been proposed, including persistence landscapes and kernel methods [12]. Persistence landscapes, introduced by Bubenik, map PDs into a space of functions, effectively converting the topological information into a sequence of vectors. This transformation allows PDs to be used in SVMs by leveraging the functional representation, which can be readily processed by the algorithm [5]. Alternatively, kernel methods, such as the persistence weighted Gaussian kernel, provide a direct way to compute the similarity between PDs without explicitly vectorizing them. These kernels are designed to capture the intrinsic geometry of the PD space and can be seamlessly integrated into SVMs, enabling the classification of topological features.

The application of persistent homology with SVMs has shown promising results in various domains, including image analysis, bioinformatics, and material science [5]. For instance, in image classification tasks, the combination of persistent homology and SVMs has demonstrated superior performance compared to traditional feature extraction methods. The ability of PDs to capture both local and global topological features, combined with the discriminative power of SVMs, allows for a more nuanced and accurate classification [4]. Moreover, the use of saliency maps has revealed that "small bars" in PDs, often dismissed as noise, actually play a crucial role in detecting subtle curvature and texture variations, further enhancing the model's performance. These findings underscore the importance of considering all topological features, regardless of their scale, in topological machine learning tasks.

### 4.1.2 Generalized Rank Invariant Landscape for Graphs
The Generalized Rank Invariant Landscape (GRIL) is a novel vector representation that extends the concept of persistence landscapes to 2-parameter persistence modules, providing a richer and more discriminative topological descriptor [14]. Unlike traditional persistence landscapes, which are based on 1-parameter persistence and encode the persistence of topological features as a sequence of piecewise-linear functions, GRIL leverages the generalized rank invariant to capture the intricate relationships between features across multiple scales. This approach not only enhances the discriminative power of the representation but also ensures 1-Lipschitz stability and differentiability with respect to the filtration function, making it suitable for integration into machine learning models.

GRIL is constructed by first computing the generalized rank invariant, which measures the rank of the homology groups of a 2-parameter filtration at various points in the parameter space. This invariant is then transformed into a landscape-like structure, where each "mountain" represents the persistence of a topological feature across the two parameters. The height of each mountain in the landscape corresponds to the persistence of the feature, and the overall shape of the landscape captures the complex interactions between features. The construction of GRIL can be viewed as a generalization of the persistence landscape, offering a more nuanced and detailed representation of the topological structure of graphs.

To demonstrate the utility of GRIL, we apply it to both synthetic and benchmark graph datasets, comparing its performance with other vector representations of 1-parameter and 2-parameter persistence modules [14]. The results indicate that GRIL significantly enhances the performance of graph neural networks (GNNs) in graph classification tasks [14]. Specifically, when used as an additional feature, GRIL provides GNNs with a more robust and informative topological descriptor, leading to improved classification accuracy and stability. This suggests that GRIL has the potential to become a valuable tool in the topological analysis of complex graph structures, particularly in applications where the topological features of graphs play a crucial role in the classification task.

### 4.1.3 Topology-based GAN Evaluation Metrics
Topology-based GAN evaluation metrics leverage the principles of topological data analysis (TDA) to assess the quality and diversity of generated images [17]. Unlike traditional metrics such as the Fréchet Inception Distance (FID) or Inception Score, which primarily focus on statistical similarities between real and generated data, topology-based metrics aim to capture the underlying structural and connectivity properties of the data. By doing so, these metrics provide a more nuanced understanding of the generative model's ability to replicate the topological features of the training dataset.

One prominent approach in this domain involves the use of persistent homology, a key tool in TDA that quantifies the shape and connectivity of data across different scales [18]. Specifically, the persistence diagram, a compact representation of topological features, is used to compare the topological structures of real and generated images [8]. The discrepancy between these diagrams can be quantified using various distance measures, such as the bottleneck or Wasserstein distances, which provide a robust measure of how well the GAN captures the topological characteristics of the data. This method is particularly useful in scenarios where the data exhibits complex topological structures, such as in medical imaging or natural scenes [8].

Another significant development in topology-based GAN evaluation is the introduction of hyperparameter-free metrics that can be applied directly to GAN-generated images [17]. These metrics often involve constructing a bipartite graph from the images and computing topological descriptors, such as barcodes, which represent the persistence of topological features over different scales. The resulting topological descriptors are then used to calculate the discrepancy between the real and generated distributions. This approach not only provides a quantitative measure of GAN performance but also offers insights into the types of topological features that the GAN is capable of generating, thereby facilitating a deeper understanding of the model's strengths and limitations [17].

## 4.2 Deep Learning Architectures

### 4.2.1 RipsNet for Rips Persistence Diagrams
RipsNet is a novel deep learning architecture specifically designed to handle Rips persistence diagrams (PDs) derived from point clouds, addressing the inherent challenges of working with persistence diagrams in machine learning pipelines [8]. Unlike traditional vector spaces, the space of persistence diagrams lacks a well-defined structure, making it difficult to apply standard machine learning techniques directly [9]. RipsNet overcomes this by employing a DeepSets-like architecture, which is inherently permutation-invariant and can efficiently learn finite-dimensional embeddings of Rips PDs [1]. This architecture ensures that the learned representations are robust to the order of points in the input PD, a critical property given the set-like nature of persistence diagrams [8].

One of the key advantages of RipsNet is its robustness to perturbations in the input point cloud. Specifically, RipsNet demonstrates that perturbing a proportion \(\lambda\) of the points in a point cloud can only change the output of the network by \(O(\lambda)\). This is a significant improvement over the exact persistence diagrams, which can exhibit substantial changes even with minor perturbations [2]. This robustness makes RipsNet particularly suitable for practical applications where data may be noisy or subject to variations. Experimentally, RipsNet has been shown to produce fast, accurate, and useful estimations of topological descriptors, often outperforming the use of exact PDs in classification tasks [1]. This is particularly evident in scenarios where the topological properties of the data play a crucial role in the task at hand.

Despite its advantages, RipsNet also faces computational challenges, especially when dealing with large point clouds. The computation of Rips PDs can be computationally expensive, and the sensitivity of these diagrams to small perturbations remains a concern. However, RipsNet mitigates these issues by providing a computationally efficient and stable alternative to exact PDs. By leveraging the power of deep learning, RipsNet not only accelerates the process of topological feature extraction but also enhances the robustness and reliability of the resulting representations [1]. This makes RipsNet a promising tool for integrating topological data analysis into machine learning workflows, particularly in applications requiring real-time or large-scale data processing [8].

### 4.2.2 Indefinite Topological Kernels for Classification
Indefinite topological kernels represent a significant advancement in the integration of topological data analysis (TDA) with machine learning, particularly for classification tasks [1]. Unlike traditional kernels, which are positive definite and thus can be embedded into a Hilbert space, indefinite kernels do not satisfy the positive definiteness condition. This characteristic is crucial for handling the complex and often non-metric spaces that arise in TDA, such as the space of persistence diagrams [16]. Persistence diagrams, which capture the birth and death of topological features across different scales, are inherently non-Euclidean and cannot be isometrically embedded into a Hilbert space [16]. Therefore, the use of indefinite kernels allows for a more flexible and accurate representation of these topological features in machine learning models.

One of the key challenges in using indefinite kernels for classification is the lack of a well-defined optimization landscape, which can lead to issues such as non-convexity and the presence of multiple local optima. To address these challenges, recent research has focused on developing algorithms that can effectively optimize over indefinite kernels. For instance, multiple kernel learning (MKL) approaches have been proposed to combine various indefinite kernels, each capturing different aspects of the topological information. By learning a weighted combination of these kernels, MKL can adapt to the specific characteristics of the data, thereby improving classification performance. This adaptability is particularly important in TDA, where the topological features can vary significantly across different datasets and applications [19].

Another important aspect of indefinite topological kernels is their ability to capture the nuances of "small bars" in persistence diagrams, which are often dismissed as noise in traditional TDA approaches. Recent studies have shown that these "small bars" can provide valuable information about the curvature and fine-scale structure of the data. By incorporating this information into the kernel design, indefinite topological kernels can enhance the discriminative power of the classification models. For example, the persistence weighted Gaussian kernel (PWGK) and the persistence Fisher kernel (PFK) are two such kernels that have been successfully applied in various classification tasks [12]. These kernels not only capture the topological features at different scales but also provide a principled way to weight these features based on their significance, thereby improving the robustness and accuracy of the classification results.

### 4.2.3 TopoResNet for Skin Lesion Classification
TopoResNet is a novel architecture that integrates topological data analysis (TDA) with deep learning to enhance the classification of skin lesions [20]. This approach leverages the topological features extracted from persistence diagrams (PDs) to complement the traditional convolutional features [1]. The PDs, which capture the topological structure of the input images, are generated using persistent homology, a method that tracks the evolution of topological features across different scales [21]. By incorporating these PDs into the ResNet architecture, TopoResNet aims to improve the robustness and generalization of the model, particularly in scenarios where the training data is limited or noisy.

In TopoResNet, the input images are first processed through a series of convolutional layers to extract low-level and mid-level features. These features are then concatenated with the topological features derived from the PDs. The PDs are vectorized using methods such as persistence landscapes or Betti curves, which transform the topological information into a format that can be effectively integrated into the neural network. This fusion of topological and convolutional features is designed to provide a more comprehensive representation of the input data, enabling the model to capture both the geometric and topological characteristics of skin lesions.

The effectiveness of TopoResNet is demonstrated through its application to the ISIC 2018 challenge dataset, which consists of a large number of dermatoscopic images of various skin lesions [20]. The model achieves competitive performance, highlighting the potential of combining TDA with deep learning for medical image classification. The integration of topological features not only enhances the model's ability to distinguish between different types of skin lesions but also provides insights into the topological structures that are most relevant for classification. This approach opens new avenues for the development of more robust and interpretable deep learning models in dermatology and other medical imaging applications.

## 4.3 Topological Features in Neural Networks

### 4.3.1 PersLay for Graph Analysis
PersLay, introduced as a neural network architecture for handling persistence diagrams, represents a significant advancement in the integration of topological data analysis (TDA) with machine learning [16]. Unlike traditional methods that rely on handcrafted vectorizations of persistence diagrams, PersLay utilizes a permutation-invariant layer to directly process the raw persistence diagram data [2]. This approach not only preserves the intrinsic topological information but also leverages the expressive power of neural networks to capture complex patterns within the data [20]. The architecture of PersLay is designed to be flexible and adaptable, allowing it to be integrated seamlessly with various machine learning frameworks and datasets.

One of the key strengths of PersLay lies in its ability to handle the permutation invariance of persistence diagrams, a critical property that ensures the robustness of the model against the order of points in the diagram. This is achieved through the use of permutation-invariant operators such as sum, mean, or max, which aggregate the features extracted from individual points in the diagram. By doing so, PersLay can effectively summarize the topological structure of the data while maintaining the necessary invariance properties. This capability is particularly important in graph analysis, where the topological features of the graph, such as connected components, loops, and voids, provide valuable insights into the underlying structure and connectivity of the graph.

In practical applications, PersLay has demonstrated superior performance in graph classification tasks compared to traditional methods. When applied to benchmark datasets from various fields, including biology, chemistry, and social sciences, PersLay consistently outperforms other neural network architectures that handle persistence diagrams [22]. This success can be attributed to its ability to capture both local and global topological features of the graph, thereby providing a more comprehensive and nuanced representation of the data. Moreover, the interpretability of PersLay, facilitated by the direct processing of persistence diagrams, allows researchers to gain deeper insights into the topological characteristics that drive the classification outcomes [8].

### 4.3.2 Vector Stitching for Image Classification
Vector Stitching for Image Classification represents a novel approach that integrates topological data analysis (TDA) with convolutional neural networks (CNNs) to enhance the classification of images, particularly in complex datasets like ImageNet [23]. This method addresses the limitations of traditional vectorization techniques by combining the topological features extracted from persistence diagrams (PDs) with the raw pixel data processed by CNNs [4]. The integration is achieved through a process known as Vector Stitching, which involves the strategic concatenation of topological feature vectors with the output of the convolutional layers, thereby enriching the feature space with additional topological information.

The key innovation of Vector Stitching lies in its ability to leverage the topological structure of data, which is often overlooked in conventional machine learning approaches. By incorporating topological features, such as persistence landscapes and Betti curves, the method can capture the intrinsic geometric and topological properties of images, which are crucial for distinguishing between different classes. This is particularly beneficial in tasks where the images have complex structures or where subtle differences in topology can significantly affect the classification outcome. For instance, in the context of drone image classification, Vector Stitching has been shown to improve the discriminator accuracy by 4%, demonstrating its effectiveness in real-world applications.

Moreover, the Vector Stitching approach is designed to be flexible and adaptable to various neural network architectures. It can be seamlessly integrated into existing CNN frameworks, allowing for the enhancement of both the training and inference phases. The method also addresses the challenge of selecting the most appropriate vectorization technique for a given task by providing a framework that can be fine-tuned through the training process [10]. This adaptability, combined with the robustness of topological features, makes Vector Stitching a promising technique for advancing the state-of-the-art in image classification, particularly in scenarios where traditional methods may struggle to capture the essential characteristics of the data.

### 4.3.3 Transformer-based Architectures for Topological Data
Transformer-based architectures have emerged as a powerful tool for handling complex and structured data, including topological data. In the context of topological data analysis (TDA), transformers offer a unique advantage by enabling the effective processing of persistence diagrams, which are key descriptors of topological features in datasets [16]. Persistence diagrams, represented as sets of points in \( \mathbb{R}^2 \), capture the birth and death of topological features across different scales [21]. Traditional methods for integrating persistence diagrams into machine learning pipelines often involve vectorization techniques, which can lead to loss of topological information [9]. Transformers, however, can directly process these diagrams as sequences of points, preserving the rich topological structure.

The Persformer, a transformer architecture specifically designed for persistence diagrams, leverages self-attention mechanisms to capture the relationships between different topological features. By treating each point in the persistence diagram as a token, the transformer can dynamically weigh the importance of each feature, allowing for a more nuanced understanding of the underlying topology [24]. This approach not only enhances the model's ability to generalize across different datasets but also improves its interpretability, as the attention weights can provide insights into which topological features are most relevant for specific tasks. Compared to earlier neural network architectures like PersLay and PLLAy, which rely on fixed vectorization schemes, Persformer demonstrates superior performance in tasks such as classification and regression, particularly in high-dimensional and noisy datasets.

Moreover, the flexibility of transformer architectures allows for the integration of additional topological information beyond persistence diagrams. For example, multi-persistence diagrams, which capture topological features across multiple parameters, can be seamlessly incorporated into the transformer framework. This capability is crucial for applications where the data exhibits complex, multi-scale structures, such as in materials science and biological systems. The transformer's ability to handle variable-length inputs and its scalability to large datasets make it an ideal choice for advancing the field of topological machine learning. Future work in this area could explore the development of specialized transformer layers that are optimized for specific topological descriptors, further enhancing the model's performance and applicability.

# 5 Topological Analysis of Complex Data Structures

## 5.1 Advanced Algebraic Structures

### 5.1.1 Persistent Cohomology with A∞-Algebras
Persistent cohomology, an extension of persistent homology, plays a crucial role in the study of topological features in data across varying scales [5]. Unlike persistent homology, which focuses on cycles and boundaries, persistent cohomology emphasizes cocycles and coboundaries, offering a dual perspective that can sometimes provide more efficient computational methods and richer algebraic structures [25]. In the context of temporal hypergraphs, persistent cohomology can capture the evolution of topological features over time, such as the appearance and disappearance of connected components, loops, and higher-dimensional voids [22]. This dual approach is particularly useful when dealing with large datasets, as it often reduces the computational complexity associated with homology calculations.

In this section, we explore the integration of A∞-algebras with persistent cohomology [26]. A∞-algebras are algebraic structures that generalize associative algebras by allowing for higher-order multiplications, which satisfy a set of coherence conditions known as the A∞-relations. These structures are particularly well-suited for capturing the intricate interactions and higher-order relationships within topological data [13]. By incorporating A∞-algebras into the framework of persistent cohomology, we can encode not only the presence and persistence of topological features but also the ways in which these features interact with each other over time [25]. This enriched algebraic structure provides a more nuanced understanding of the underlying data, enabling the detection of subtle patterns and anomalies that might be missed by traditional methods.

The application of A∞-algebras to persistent cohomology involves constructing a sequence of cochain complexes, each equipped with an A∞-algebra structure, and studying how these structures evolve along the filtration [26]. This approach allows us to track not only the birth and death of cohomological features but also the higher-order interactions that occur during the filtration process. The resulting persistent cohomology with A∞-algebras can be visualized using generalized persistence diagrams, which extend the traditional persistence diagrams by including information about the higher-order multiplications [2]. This extension enhances the ability to analyze complex dynamical systems, such as those represented by temporal hypergraphs, by providing a more comprehensive topological summary of the data [27].

### 5.1.2 Bigraded Betti Numbers and Möbius Inversion
Bigraded Betti numbers are a fundamental invariant in the study of multi-parameter persistence modules, offering a way to capture the structure of these modules in a combinatorial manner [28]. For a finitely generated interval decomposable module \( M : \mathbb{Z}^2 \to \text{vec} \), the bigraded Betti numbers can be determined from the indecomposable summands of \( M \) [28]. Specifically, each indecomposable summand corresponds to a birth and death of a homological feature, and the bigraded Betti numbers count the number of such births and deaths at each bigrade. This process is visually intuitive and can be illustrated through a series of steps, as shown in Fig. 1 (A)-(C).

The computation of bigraded Betti numbers for a general finitely generated module \( N : \mathbb{Z}^2 \to \text{vec} \) that may not be interval decomposable is more complex. However, a similar process can be applied to approximate these numbers. The key idea is to use the indecomposable summands of a related interval decomposable module to infer the bigraded Betti numbers of \( N \). While this method provides a theoretical framework, it is not necessarily computationally efficient. Instead, efficient algorithms for computing bigraded Betti numbers can be used to approximate the generalized persistence diagram, which is a more practical approach.

Möbius inversion plays a crucial role in the computation of bigraded Betti numbers by providing a way to relate the rank invariant of a multi-parameter persistence module to its bigraded Betti numbers. The rank invariant, which counts the number of linearly independent homology classes at each pair of values in the parameter space, can be reconstructed using a small set of critical values determined by the discrete gradient vector field. By applying Möbius inversion, one can derive the bigraded Betti numbers from the rank invariant. This relationship is particularly useful for understanding the structure of multi-parameter persistence modules and for developing algorithms that efficiently compute these invariants [28].

### 5.1.3 Discrete Morse Theory for Rank Invariants
Discrete Morse theory provides a powerful framework for simplifying the computational complexity involved in the analysis of multi-parameter persistence modules, particularly in the context of rank invariant computations [29]. By focusing on critical cells, which are the minimal subset of cells that retain the essential topological information of the original complex, discrete Morse theory enables a significant reduction in the size of the input data without losing the topological features necessary for persistence analysis. This reduction is achieved through the construction of a Morse complex, which is homotopy equivalent to the original simplicial complex but contains only the critical cells.

The application of discrete Morse theory to rank invariant computation involves a series of steps that leverage the properties of critical cells to simplify the persistence module [29]. First, a discrete Morse function is defined on the simplicial complex, which assigns a value to each cell and determines the gradient vector field. This gradient vector field guides the pairing of cells, where non-critical cells are paired with adjacent cells, effectively collapsing them out of the complex. The remaining critical cells form the Morse complex, which is then used to compute the rank invariant. This approach not only reduces the computational burden but also enhances the geometric interpretability of the rank invariant, as the critical cells directly correspond to the topological features of interest.

Moreover, the integration of discrete Morse theory with rank invariant computation offers a robust method for handling the challenges posed by multi-parameter persistence, such as the lack of a direct generalization of the barcode representation and the increased computational complexity [29]. By focusing on the critical cells, the method ensures that the rank invariant is computed efficiently and accurately, even for large and complex datasets. This approach also facilitates the visualization and interpretation of the rank invariant, making it a valuable tool for both theoretical analysis and practical applications in topological data analysis.

## 5.2 Application to Medical Imaging

### 5.2.1 Persistent Homology for Breast Tumor Classification
Persistent homology, a core tool in Topological Data Analysis (TDA), has gained significant traction in the field of medical imaging, particularly for the classification of breast tumors [12]. By constructing a sequence of nested simplicial complexes from mammographic images, persistent homology captures the topological features of the data at various scales. This approach allows for the identification of topological invariants such as connected components, loops, and cavities, which are robust to noise and can provide insights into the underlying structure of the tumor. The filtration process, which involves varying a scale parameter, enables the tracking of the birth and death of these topological features, resulting in a persistence diagram or barcode that succinctly summarizes the topological information [30].

In the context of breast tumor classification, the persistence diagram serves as a powerful feature representation. Each point in the diagram corresponds to a topological feature, with its coordinates indicating the scale at which the feature appears and disappears. This multi-scale representation is particularly useful for distinguishing between benign and malignant tumors, as it captures both local and global structural properties. Machine learning algorithms, such as support vector machines and deep neural networks, can then be applied to these persistence diagrams to classify tumors with high accuracy. The stability of persistence diagrams under small perturbations ensures that the classification is robust to variations in imaging data, making it a reliable tool in clinical settings.

To enhance the utility of persistent homology in breast tumor classification, several techniques have been developed to transform persistence diagrams into a format compatible with machine learning algorithms [2]. These include methods such as persistence images, persistence landscapes, and persistence curves, which convert the topological information into vectorized forms [13]. These transformations not only preserve the topological features but also enable the use of advanced machine learning techniques for feature extraction and classification [31]. For instance, persistence images, which represent the persistence diagram as a 2D image, can be fed into convolutional neural networks to leverage their ability to capture spatial hierarchies. This integration of TDA with deep learning has shown promising results in improving the accuracy and interpretability of breast tumor classification, highlighting the potential of persistent homology as a valuable tool in medical imaging.

### 5.2.2 Harmonic Chain Barcodes for Geometric Information
Harmonic chain barcodes offer a novel approach to capturing geometric information from filtrations, distinct from traditional persistence barcodes [32]. By focusing on the evolution of harmonic chains, these barcodes provide a more direct and interpretable representation of the underlying data's geometric features. Specifically, each bar in a harmonic chain barcode corresponds to a unique harmonic chain, which is a distinguished representative of a homology class [32]. This uniqueness eliminates the ambiguity inherent in selecting cycle representatives, thereby simplifying the interpretation of the barcode.

The construction of harmonic chain barcodes involves a global analysis of the filtration, ensuring that each bar is associated with a specific geometric feature [32]. This method leverages the properties of harmonic chains, which are uniquely determined within each homology class, to track the birth and death of these features throughout the filtration [33]. As a result, harmonic chain barcodes not only capture topological changes but also retain geometric information, making them particularly useful for applications requiring a deeper understanding of the data's structure. The stability of harmonic chain barcodes under small perturbations further enhances their reliability and applicability in various domains.

To demonstrate the utility of harmonic chain barcodes, we apply them to a range of datasets, including synthetic and real-world examples [33]. Our experiments show that harmonic chain barcodes can effectively distinguish between different geometric configurations, providing insights that are not readily apparent from traditional persistence barcodes. This enhanced interpretability and robustness make harmonic chain barcodes a valuable addition to the toolkit of topological data analysis, particularly for tasks involving feature vectorization and machine learning [33]. The canonical nature of harmonic chain representatives ensures that the resulting barcodes are both meaningful and computationally tractable, opening new avenues for the analysis of complex data structures.

### 5.2.3 Stability and Interpretability in Medical Data
Stability and interpretability are paramount in the application of topological data analysis (TDA) to medical data, particularly in the context of diagnosing and monitoring diseases [13]. Medical datasets often contain high-dimensional, noisy, and heterogeneous information, making it challenging to extract meaningful insights. Persistent homology, a key tool in TDA, provides a robust framework for analyzing the topological features of such data [2]. By tracking the birth and death of topological features across different scales, persistent homology can reveal the underlying structure and patterns that might be obscured in traditional statistical analyses [21]. However, the stability of these topological features under small perturbations is crucial for ensuring that the insights derived are reliable and reproducible.

To enhance the interpretability of topological features in medical data, various methods have been developed to transform persistence diagrams into more accessible formats [1]. One such method involves converting persistence diagrams into vector representations, such as persistence landscapes or Betti curves, which can be readily integrated into machine learning pipelines [9]. These vectorized representations not only facilitate the application of standard machine learning algorithms but also provide a more intuitive visualization of the topological features [4]. For instance, Betti curves, which represent the number of topological features (e.g., connected components, loops, voids) at different scales, can be plotted as functions and analyzed alongside clinical data to identify correlations and trends.

Despite these advancements, the interpretability of topological features remains a significant challenge, especially in the context of complex medical datasets [8]. The inherent complexity of persistence diagrams and the abstract nature of topological features require careful consideration of domain-specific knowledge [8]. For example, in the analysis of medical imaging data, such as MRI or CT scans, topological features can be linked to specific anatomical structures or pathologies [8]. By combining topological analysis with expert knowledge, researchers can develop more accurate and interpretable models for disease diagnosis and prognosis. Furthermore, the development of interactive visualization tools and user-friendly software can help bridge the gap between topological insights and clinical practice, making TDA a more accessible and valuable tool for medical professionals.

## 5.3 Analysis of Large-scale Data

### 5.3.1 Cosmic Web Analysis with Cubical Complexes
In the analysis of the cosmic web, cubical complexes serve as a powerful tool for capturing and analyzing the intricate topological and geometric features of the large-scale structure of the universe [24]. Unlike simplicial complexes, which are often used in topological data analysis (TDA), cubical complexes are particularly suited to handle the regular grid-like structures that are common in astronomical data, such as galaxy surveys and N-body simulations. By constructing a cubical complex from a dataset, researchers can efficiently track the evolution of topological features, such as voids, filaments, and clusters, across different scales. This is achieved through a filtration process where the complex is built incrementally, starting from the smallest cubes and gradually incorporating larger ones as the scale increases.

The application of cubical complexes in cosmic web analysis leverages the robustness and interpretability of persistent homology, a key technique in TDA. Persistent homology allows for the identification and quantification of topological features that persist across multiple scales, providing a comprehensive view of the cosmic web's structure [5]. In the context of cubical complexes, the filtration is typically constructed by thresholding the density field, which is often estimated using methods like the Delaunay Tessellation Field Estimator (DTFE). As the threshold varies, the cubical complex evolves, and the birth and death of topological features are recorded in a persistence diagram or barcode [34]. This diagram not only captures the topological invariants but also provides insights into the geometric properties of the cosmic web, such as the sizes and shapes of voids and filaments [24].

Moreover, the use of cubical complexes in cosmic web analysis offers several advantages over traditional methods [24]. The regular structure of cubical complexes simplifies the computational complexity of the filtration process, making it feasible to analyze large-scale datasets [23]. Additionally, the geometric nature of cubical complexes allows for a more direct interpretation of the topological features, facilitating the identification of physically meaningful structures in the cosmic web. This approach has been instrumental in advancing our understanding of the large-scale structure of the universe, providing a bridge between observational data and theoretical models of cosmic evolution.

### 5.3.2 Probabilistic Analysis and Generalized Persistence
Probabilistic analysis in the context of generalized persistence involves the study of probabilistic perturbations and their effects on the persistence diagrams and barcodes [25]. Unlike traditional stability results that focus on deterministic small perturbations, probabilistic analysis considers perturbations that are rare but can be large. This is particularly relevant in real-world data analysis, where noise and outliers are common. The probabilistic framework allows for the assessment of the robustness of topological features under such perturbations, providing a more comprehensive understanding of the data's underlying structure. For instance, the law of large numbers and central limit theorems have been established for various representations of persistence diagrams, such as persistence landscapes, which are useful for statistical inference and hypothesis testing [9].

Generalized persistence extends the concept of persistence beyond the one-dimensional filtration to multi-parameter settings. This extension is crucial for capturing the complex and multi-faceted nature of real-world data. However, the computational and theoretical challenges associated with multi-parameter persistence are significant. The rank invariant, a key concept in generalized persistence, captures the evolution of topological features across multiple parameters, but its computation is often infeasible for high-dimensional data [5]. To address this, researchers have explored various approximations and summarization techniques, such as persistence curves and persistence images, which provide computationally efficient and interpretable representations of the rank invariant. These methods enable the integration of topological information into machine learning pipelines, facilitating the analysis of large and complex datasets [8].

The integration of probabilistic analysis with generalized persistence offers a powerful tool for understanding the stability and robustness of topological features in dynamic and noisy environments. By combining the probabilistic framework with advanced summarization techniques, researchers can develop more reliable and interpretable models for data analysis. For example, the persistence energy, a measure derived from the persistence diagram, can be used to quantify the significance of topological features across different scales and parameters [21]. This approach not only enhances the robustness of topological data analysis but also provides a bridge between topological methods and modern machine learning techniques, enabling the application of topological insights to a wide range of data-driven problems [8].

### 5.3.3 Time Series Analysis with CROCKER Plots
Time Series Analysis with CROCKER Plots leverages the principles of Topological Data Analysis (TDA) to provide a robust framework for understanding the dynamic evolution of topological features in time series data [1]. CROCKER plots, a specialized visualization tool, are particularly adept at capturing the temporal changes in the topological structure of data [35]. These plots are constructed by computing persistence diagrams at various points in time and then encoding these diagrams into Betti curves [35]. Each Betti curve represents the number of topological features (such as connected components, loops, and voids) at different scales, providing a concise summary of the topological structure at each time step.

The key advantage of CROCKER plots lies in their ability to visualize the evolution of topological features over time, which is particularly useful for detecting and analyzing dynamic changes in complex systems. For instance, in the context of cyber log data, CROCKER plots can help identify anomalies or shifts in network behavior by highlighting sudden changes in the topological structure of the log data. This is achieved by tracking the appearance and disappearance of topological features, which can indicate the onset of malicious activities or system failures. The simplicity and interpretability of CROCKER plots make them a valuable tool for both theoretical analysis and practical applications in time series analysis.

Moreover, the stability of CROCKER plots under small perturbations in the data is a critical property that enhances their reliability in real-world applications. This stability ensures that minor noise or variations in the time series do not significantly alter the topological features captured by the plots, thereby providing a consistent and robust representation of the underlying system's dynamics. Additionally, the computational efficiency of generating CROCKER plots, combined with their geometric interpretability, makes them an attractive choice for large-scale time series analysis. By integrating CROCKER plots into the TDA pipeline, researchers and practitioners can gain deeper insights into the temporal behavior of complex systems, facilitating more informed decision-making and anomaly detection.

# 6 Future Directions


The current landscape of vectorization techniques in Topological Data Analysis (TDA) has made significant strides in integrating persistence diagrams (PDs) with machine learning models. However, several limitations and gaps remain. The non-Euclidean nature of PDs continues to pose challenges for their seamless integration into standard machine learning pipelines, particularly in terms of computational efficiency and stability. Existing vectorization methods, while effective in many cases, often struggle with large-scale datasets and high-dimensional PDs, leading to increased computational costs and potential loss of topological information. Additionally, the interpretability of the resulting vectorized representations remains a critical issue, as the abstract nature of topological features can make it difficult to translate these insights into actionable knowledge.

To address these limitations, several directions for future research are proposed. First, the development of more efficient and scalable vectorization techniques is essential. This could involve exploring novel methods for compressing PDs while preserving their topological information, such as advanced clustering algorithms or dimensionality reduction techniques tailored to PDs. Additionally, the integration of parallel computing and distributed systems could significantly reduce the computational burden, making it feasible to handle large-scale datasets in real-time applications.

Second, there is a need for more robust and interpretable vectorization methods. This could be achieved by developing hybrid approaches that combine topological features with traditional statistical and machine learning features. For example, integrating topological features with domain-specific knowledge, such as anatomical landmarks in medical imaging, could enhance the interpretability of the resulting models. Moreover, the development of visualization tools that can effectively communicate the topological insights to domain experts would be invaluable in translating these complex features into practical applications.

Third, the exploration of new theoretical frameworks for handling PDs in non-Euclidean spaces is crucial. This could involve extending the current methods to more general metric spaces or developing new distance metrics that better capture the intrinsic geometry of PDs. Additionally, the study of the stability and robustness of these methods under various perturbations, such as noise and outliers, is essential for ensuring their reliability in real-world applications.

The potential impact of the proposed future work is significant. More efficient and scalable vectorization techniques could revolutionize the application of TDA in fields such as genomics, material science, and medical imaging, where large-scale and high-dimensional data are common. Enhanced interpretability and robustness would make TDA more accessible and valuable to domain experts, leading to more informed decision-making and innovative solutions. Finally, the development of new theoretical frameworks could open up entirely new avenues for research, pushing the boundaries of what is possible with topological data analysis and machine learning.

# 7 Conclusion



This survey has provided a comprehensive overview of the vectorization techniques in Topological Data Analysis (TDA) that enable the effective integration of persistence diagrams (PDs) with machine learning models. The paper explored a wide range of methods, including embedding and transformation techniques, binning and discrete integral transforms, batch and mini-batch quantization, and clustering and approximation methods. Each of these techniques addresses the non-Euclidean nature of PDs, transforming them into vector representations that can be readily processed by machine learning algorithms. The Kuratowski embedding and quadratic programming, for instance, preserve the metric structure of PDs, enabling the application of optimization methods. Binning and discrete integral transforms, such as the Qupid method, emphasize regions of high point density, enhancing computational efficiency while preserving topological information. Batch and mini-batch quantization techniques extend classical quantization methods to handle large-scale PDs, ensuring stable and efficient approximations. Clustering and approximation methods, including k-means clustering in non-Euclidean spaces and mean measure approximation, address the challenges of defining centroids and means in the space of PDs, providing robust and interpretable representations.

The significance of this survey lies in its contribution to the field of TDA and its practical applications. By bridging the gap between topological data analysis and machine learning, the survey offers a valuable resource for researchers and practitioners. The comprehensive review of vectorization techniques, along with their theoretical foundations and practical applications, provides insights into the strengths and limitations of various methods. This knowledge is crucial for advancing the field and developing more robust and interpretable data analysis tools. The survey highlights the potential of TDA to enhance the performance of machine learning models in a wide range of domains, including image analysis, genomics, and material science. The integration of topological features into machine learning pipelines not only improves the accuracy and robustness of models but also provides deeper insights into the underlying data structures.

In conclusion, this survey underscores the importance of continuing research in the vectorization of persistence diagrams and the integration of TDA with machine learning. Future work should focus on developing more efficient and scalable methods, exploring the application of TDA in new domains, and enhancing the interpretability of topological features. The growing interest in TDA and its potential to address complex data analysis challenges make it an exciting area for ongoing research and development. By fostering collaboration between topological data analysts and machine learning experts, we can further advance the field and unlock new possibilities for data-driven discovery and innovation.

# References
[1] RipsNet  a general architecture for fast and robust estimation of the  persistent homology of point  
[2] $k$-Means Clustering for Persistent Homology  
[3] Vector Summaries of Persistence Diagrams for Permutation-based  Hypothesis Testing  
[4] Image Classification using Combination of Topological Features and  Neural Networks  
[5] Persistence B-Spline Grids  Stable Vector Representation of Persistence  Diagrams Based on Data Fitt  
[6] Optimal quantization of the mean measure and applications to statistical  learning  
[7] TDAvec  Computing Vector Summaries of Persistence Diagrams for  Topological Data Analysis in R and P  
[8] Discrete transforms of quantized persistence diagrams  
[9] Adaptive Partitioning for Template Functions on Persistence Diagrams  
[10] A Vectorization Method Induced By Maximal Margin Classification For  Persistent Diagrams  
[11] A Class of Topological Pseudodistances for Fast Comparison of  Persistence Diagrams  
[12] Instability of the Betti Sequence for Persistent Homology and a  Stabilized Version of the Betti Seq  
[13] Persistence Curves  A canonical framework for summarizing persistence  diagrams  
[14] GRIL  A $2$-parameter Persistence Based Vectorization for Machine  Learning  
[15] A computationally efficient framework for vector representation of  persistence diagrams  
[16] Persformer  A Transformer Architecture for Topological Machine Learning  
[17] Barcode Method for Generative Model Evaluation driven by Topological  Data Analysis  
[18] Hypothesis Testing for Shapes using Vectorized Persistence Diagrams  
[19] A fast topological approach for predicting anomalies in time-varying  graphs  
[20] TopoResNet  A hybrid deep learning architecture and its application to  skin lesion classification  
[21] Persistence Images  A Stable Vector Representation of Persistent  Homology  
[22] PersLay  A Neural Network Layer for Persistence Diagrams and New Graph  Topological Signatures  
[23] Preserving Information  How does Topological Data Analysis improve  Neural Network performance   
[24] Hierarchical Clustering in $Λ$CDM Cosmologies via Persistence  Energy  
[25] Persistent Homology and Applied Homotopy Theory  
[26] A higher homotopic extension of persistent (co)homology  
[27] Malicious Cyber Activity Detection Using Zigzag Persistence  
[28] Bigraded Betti numbers and Generalized Persistence Diagrams  
[29] Morse-based Fibering of the Persistence Rank Invariant  
[30] The density of expected persistence diagrams and its kernel based  estimation  
[31] Topological Bayesian Optimization with Persistence Diagrams  
[32] Harmonic Chain Barcode and Stability  
[33] Tracking the Persistence of Harmonic Chains  Barcode and Stability  
[34] Persistent Homology for Breast Tumor Classification using Mammogram  Scans  
[35] Detecting bifurcations in dynamical systems with CROCKER plots  