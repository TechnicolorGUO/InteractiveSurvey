# A Survey of Optimization Methods for Wireless Communications

# 1 Abstract


Optimization methods are essential for advancing wireless communication systems, addressing challenges such as resource allocation, interference mitigation, and spectral efficiency. This survey paper provides a comprehensive overview of recent advancements in optimization techniques for wireless communications, with a focus on beamforming and phase optimization in reconfigurable intelligent surface (RIS)-aided systems, adaptive polynomial chaos expansion for uncertainty quantification, non-convex optimization, and the integration of traditional optimization with neural networks. The paper highlights key findings, including the effectiveness of generalized Benders decomposition and weighted mean square error minimization in RIS-aided systems, the computational efficiency of sparse polynomial chaos expansion, and the robustness of deep reinforcement learning in UAV-assisted networks. The survey also explores the practical implications and potential advancements that can be achieved through the integration of advanced optimization techniques in wireless communication systems, providing a roadmap for future research and development.

# 2 Introduction
Optimization methods play a pivotal role in the advancement of wireless communication systems, enabling the efficient allocation of resources, the enhancement of system performance, and the mitigation of interference. As the demand for high-speed, reliable, and low-latency communication continues to grow, the need for sophisticated optimization techniques becomes increasingly apparent. Traditional methods, while effective in many scenarios, often struggle with the non-convexity and high dimensionality of modern wireless systems, necessitating the development of new and innovative approaches. This survey paper aims to provide a comprehensive overview of the recent advancements in optimization methods for wireless communications, focusing on key areas such as beamforming and phase optimization in reconfigurable intelligent surface (RIS)-aided systems, adaptive polynomial chaos expansion for uncertainty quantification, non-convex optimization for spectral efficiency, and the integration of traditional optimization with neural networks.

The research topic of this survey paper is the exploration of advanced optimization methods and their applications in wireless communication systems, with a particular emphasis on RIS-aided systems, resource allocation, and network optimization [1]. The paper delves into various optimization techniques, including generalized Benders decomposition, weighted mean square error minimization, sparse polynomial chaos expansion, and deep reinforcement learning, among others. These methods are crucial for addressing the complex challenges posed by modern wireless systems, such as the optimization of beamforming and phase shifts in RIS-aided systems, the efficient management of resources in UAV-assisted networks, and the enhancement of spectral efficiency in multi-user scenarios.

The paper begins with an in-depth examination of beamforming and phase optimization in RIS-aided systems [2]. This section covers joint power allocation and beamforming optimization, including the use of generalized Benders decomposition for single-user systems and weighted mean square error minimization for multi-user systems [3]. The discussion also extends to scalable and robust beamforming with deep learning, highlighting the advantages of using deep neural networks to handle the computational complexity and non-convexity of the optimization problems [4]. Additionally, the paper explores the use of adaptive polynomial chaos expansion for uncertainty quantification, focusing on sparse polynomial chaos expansion and orthogonal matching pursuit for coefficient estimation. The role of full-wave EM solvers in generating accurate samples for optimization is also discussed.

The survey then shifts to non-convex optimization for spectral efficiency, detailing methods such as inner approximation and successive convex approximation, as well as block coordinate descent for iterative optimization. The importance of Monte Carlo simulations in validating the performance of these methods is emphasized, with a focus on their ability to handle imperfect channel state information and multi-RIS scalability. The next section addresses resource allocation and network optimization, including the application of deep reinforcement learning for UAV beamforming, trajectory and power allocation optimization, and the systematic analysis of IRS deployments. The paper also explores the integration of traditional optimization with neural networks, discussing multi-tier computing with advanced wireless technologies, determinantal point process-based learning, and hybrid model-based and data-driven systems.

The contributions of this survey paper are multifaceted. First, it provides a comprehensive and up-to-date overview of the latest optimization methods and their applications in wireless communication systems, making it a valuable resource for researchers and practitioners in the field. Second, it highlights the key challenges and open research questions in the domain, providing a roadmap for future research. Finally, the paper synthesizes the findings from various studies, offering insights into the practical implications and potential advancements that can be achieved through the integration of advanced optimization techniques in wireless communication systems.

# 3 Beamforming and Phase Optimization in RIS-Aided Systems

## 3.1 Joint Power Allocation and Beamforming Optimization

### 3.1.1 Generalized Benders Decomposition for Single-User Systems
Generalized Benders Decomposition (GBD) is a powerful optimization technique that has been widely applied to solve complex, non-convex problems in various fields, including wireless communication systems. In the context of single-user systems, GBD is particularly useful for optimizing the beamforming and power allocation strategies in IRS-assisted systems. The key idea behind GBD is to decompose the original problem into a master problem and a series of subproblems, where the master problem manages the global variables, and the subproblems handle the local variables. This decomposition allows for an efficient handling of the non-convex constraints, such as the unit modulus constraint on the IRS reflection coefficients, which are critical for maintaining the physical feasibility of the solution.

For single-user systems, the GBD-based approach begins by formulating the master problem, which involves optimizing the transmit power and beamforming vectors at the base station (BS). The subproblems, on the other hand, focus on optimizing the phase shifts of the IRS elements, given the current solution of the master problem. The interaction between the master and subproblems is facilitated through the generation of Benders cuts, which are constraints added to the master problem to ensure that the solution remains feasible and optimal. These cuts are derived from the dual information of the subproblems and help to refine the solution iteratively until convergence is achieved.

The effectiveness of the GBD approach in single-user systems lies in its ability to handle the non-convex nature of the problem while maintaining computational efficiency. By leveraging the structure of the problem, GBD can significantly reduce the complexity of the optimization process, making it suitable for real-time applications. Furthermore, the method provides a systematic way to balance the trade-offs between the transmit power, beamforming, and IRS phase shifts, leading to improved system performance [3]. Empirical results have shown that GBD-based methods outperform traditional approaches in terms of both convergence speed and solution quality, making them a promising tool for the design of IRS-assisted single-user systems.

### 3.1.2 Weighted Mean Square Error Minimization for Multi-User Systems
In the context of multi-user systems, the weighted mean square error (WMSE) minimization approach has emerged as a powerful and flexible framework for optimizing system performance. This approach is particularly relevant in scenarios where multiple users are simultaneously served by a base station (BS), and the objective is to balance the quality of service (QoS) requirements across all users. The WMSE formulation allows for the incorporation of individual user weights, reflecting the relative importance or priority of each user in the system. By minimizing the WMSE, the system can achieve a more equitable distribution of resources, thereby enhancing the overall user experience and system efficiency.

The WMSE minimization problem in multi-user systems is typically formulated as a non-convex optimization problem due to the presence of constraints such as power limits and unit modulus constraints on the phase shifts of reconfigurable intelligent surfaces (RISs). To address this challenge, various techniques have been developed to transform the original non-convex problem into a more tractable form. One common approach is to use the method of successive convex approximation (SCA), which iteratively approximates the non-convex objective function with a sequence of convex functions. This method has been shown to converge to a stationary point of the original problem, making it a practical choice for real-world implementations. Additionally, the use of Lagrangian dual decomposition and block coordinate descent (BCD) methods has been proposed to further simplify the optimization process and reduce computational complexity.

Another key aspect of WMSE minimization in multi-user systems is the joint optimization of active and passive beamforming. In RIS-assisted systems, this involves optimizing the transmit beamforming at the BS and the phase shifts at the RIS to minimize the WMSE [5]. The joint optimization problem is often decomposed into subproblems, where the BS beamforming and RIS phase shifts are updated alternately. This alternating optimization approach has been shown to be effective in achieving near-optimal solutions with a reasonable computational burden. Furthermore, the introduction of dummy users and the use of closed-form expressions for updating the beamforming matrices have been proposed to handle the constraints and improve the convergence properties of the optimization algorithm [2]. These techniques collectively contribute to the robustness and efficiency of the WMSE minimization framework in multi-user systems.

### 3.1.3 Scalable and Robust Beamforming with Deep Learning
Scalable and robust beamforming in multi-RIS-aided systems presents significant challenges, particularly in terms of computational complexity and performance under imperfect channel state information (CSI) [2]. Traditional beamforming techniques often suffer from high complexity, scaling poorly with the number of RIS elements and users, making them impractical for large-scale systems [2]. Deep learning (DL) has emerged as a promising solution, offering a scalable and efficient approach to beamforming optimization [4]. DL models, particularly deep neural networks (DNNs), can learn to map CSI to optimal beamforming solutions, significantly reducing the computational burden while maintaining or even improving performance [4].

One of the key advantages of DL-based beamforming is its ability to handle the non-convex and high-dimensional nature of the optimization problem. By training on a large dataset of channel realizations, DL models can generalize well to new scenarios, making them robust to variations in the channel conditions. This is particularly important in multi-RIS systems, where the channel is highly dynamic and the number of parameters to optimize can be very large. Moreover, DL models can be designed to operate in real-time, enabling rapid adaptation to changing environments. This is crucial for practical deployment in dynamic wireless networks, where quick and accurate beamforming decisions are essential.

Recent advancements in DL for beamforming have focused on integrating DL with traditional optimization techniques to leverage the strengths of both approaches. For example, hybrid methods that combine DL with alternating optimization (AO) or block coordinate descent (BCD) have shown promising results. These methods use DL to provide an initial, fast approximation of the beamforming solution, which is then refined using optimization techniques to achieve higher accuracy. This hybrid approach not only reduces the computational complexity but also enhances the robustness of the beamforming solution, making it suitable for large-scale and multi-RIS-aided systems [2]. Additionally, the use of reinforcement learning (RL) has been explored to dynamically adjust beamforming parameters based on feedback from the environment, further improving the adaptability and performance of the system.

## 3.2 Adaptive Polynomial Chaos Expansion for Uncertainty Quantification

### 3.2.1 Sparse Polynomial Chaos Expansion for Efficient Optimization
Sparse Polynomial Chaos Expansion (PCE) has emerged as a powerful tool for efficient optimization in complex systems, particularly in scenarios where high-fidelity simulations are computationally expensive. The key idea behind sparse PCE is to represent the output of a system as a linear combination of orthogonal polynomials, where the coefficients are determined using a small number of model evaluations. This approach leverages the sparsity of the polynomial coefficients, meaning that only a few significant terms contribute to the overall response, thus reducing the computational burden significantly. By employing compressed sensing techniques, such as the Orthogonal Matching Pursuit (OMP) algorithm, the sparse PCE can accurately approximate the system behavior with minimal data, making it highly suitable for optimization tasks [6].

In the context of optimization, sparse PCE provides a computationally efficient surrogate model that can be used to replace the original, computationally intensive model. This surrogate model is then used to guide the optimization process, allowing for rapid exploration of the design space and identification of near-optimal solutions. The adaptivity of sparse PCE methods, such as the adaptive PCE proposed in recent studies, further enhances their utility by dynamically adjusting the polynomial basis to capture the most relevant features of the system [6]. This adaptivity ensures that the surrogate model remains accurate and efficient, even as the complexity of the optimization problem increases. For instance, in the optimization of antenna arrays or reflector shapes, sparse PCE has been successfully applied to reduce the number of full-wave electromagnetic simulations required, thereby accelerating the design process.

Moreover, the robustness of sparse PCE in handling uncertainties and noise in the input data is another critical advantage. In many real-world applications, such as communication systems or structural mechanics, the input parameters are often subject to uncertainties. Sparse PCE can effectively propagate these uncertainties through the model, providing a probabilistic assessment of the system performance. This capability is crucial for robust design optimization, where the goal is to find solutions that perform well across a range of possible operating conditions. By integrating sparse PCE with optimization algorithms, researchers have demonstrated significant improvements in both computational efficiency and solution quality, making it a valuable tool in the arsenal of modern optimization techniques.

### 3.2.2 Orthogonal Matching Pursuit for Coefficient Estimation
Orthogonal Matching Pursuit (OMP) is a greedy algorithm widely used for sparse signal recovery, which is particularly relevant in the context of coefficient estimation for channel models in wireless communications. In the OMP framework, the algorithm iteratively selects the most correlated atom from a dictionary to the residual, thereby constructing a sparse representation of the signal. This process is repeated until a stopping criterion is met, such as a fixed number of iterations or a threshold on the residual energy. The key advantage of OMP lies in its computational efficiency and simplicity, making it suitable for real-time applications where the channel conditions are rapidly changing.

In the context of channel estimation, OMP can be applied to recover the sparse channel coefficients from a limited number of measurements. The channel is often modeled as a sparse vector in a certain domain, such as the frequency domain, where only a few taps are non-zero. By leveraging the sparsity of the channel, OMP can accurately estimate the channel coefficients even when the number of measurements is much smaller than the dimension of the channel vector. This is particularly useful in scenarios with high-dimensional channels, such as those encountered in millimeter-wave (mmWave) communications, where the number of channel taps can be very large.

The effectiveness of OMP in coefficient estimation is further enhanced by its ability to handle noisy measurements. By iteratively refining the estimate and updating the residual, OMP can mitigate the impact of noise and improve the accuracy of the estimated coefficients. Additionally, the algorithm can be adapted to incorporate prior knowledge about the channel structure, such as the sparsity pattern or the distribution of non-zero coefficients, which can further improve the estimation performance. Despite its simplicity, OMP has been shown to achieve near-optimal performance in many practical scenarios, making it a popular choice for coefficient estimation in wireless communication systems.

### 3.2.3 Full-Wave EM Solver for Sample Generation
In the realm of electromagnetic (EM) optimization, the full-wave EM solver plays a crucial role in generating accurate and reliable samples for the optimization process. This solver is essential for capturing the intricate details of EM interactions, particularly in complex environments such as those involving reconfigurable intelligent surfaces (RISs) and other advanced antenna systems [7]. The full-wave EM solver is typically based on methods like the finite-difference time-domain (FDTD) technique, which allows for the precise simulation of EM fields and their interactions with materials and structures. By leveraging the FDTD method, the solver can compute the derivatives required for optimization algorithms, such as the adjoint-field method, which significantly enhances the efficiency and accuracy of the optimization process.

The integration of the full-wave EM solver into the sample generation process is critical for ensuring that the optimization algorithms operate on realistic and physically consistent data. This is particularly important in scenarios where the optimization involves a large number of design variables, such as the phase shifts of RIS elements or the geometry of antenna arrays. The solver's ability to handle complex boundary conditions and material properties makes it indispensable for optimizing systems with high-dimensional and non-linear characteristics. Moreover, the use of the full-wave EM solver in conjunction with gradient-based optimization methods, such as the projected gradient method, enables the simultaneous optimization of multiple parameters, leading to more robust and efficient designs.

To further enhance the scalability and efficiency of the optimization process, the full-wave EM solver can be combined with surrogate modeling techniques. For instance, a Bayesian neural network can be used to create a surrogate model that approximates the behavior of the full-wave EM solver, thereby reducing the computational burden of generating new samples. This approach has been successfully applied in various optimization problems, such as the design of circularly polarized omnidirectional antennas and the optimization of horn antennas with multiple geometric variables. The combination of the full-wave EM solver and surrogate modeling not only accelerates the optimization process but also improves the robustness of the solutions, making it a powerful tool for advanced EM design and optimization.

## 3.3 Non-Convex Optimization for Spectral Efficiency

### 3.3.1 Inner Approximation and Successive Convex Approximation
In the realm of optimization for wireless communication systems, particularly those involving Reconfigurable Intelligent Surfaces (RISs), the challenges posed by non-convex problems necessitate innovative solution methods. One such method is the inner approximation technique, which involves constructing a sequence of convex sets that progressively approximate the feasible region of the original non-convex problem. This approach is particularly useful in scenarios where the feasible set is non-convex but can be approximated by a series of convex sets, thereby enabling the application of efficient convex optimization algorithms. By iteratively refining these approximations, the method can converge to a solution that is close to the global optimum, even in the presence of complex constraints.

Successive Convex Approximation (SCA) is another powerful technique that addresses the non-convexity of the objective function and constraints. SCA works by iteratively approximating the non-convex problem with a sequence of convex subproblems, each of which is easier to solve. At each iteration, the current solution is used to construct a local convex approximation of the original problem, and the solution to this subproblem is then used as the starting point for the next iteration. This process continues until convergence is achieved. SCA is particularly effective in problems where the objective function or constraints are smooth and differentiable, allowing for the construction of accurate local approximations. The method has been successfully applied to various aspects of RIS-aided systems, including power allocation, precoding, and phase shift optimization, leading to significant improvements in system performance.

The combination of inner approximation and SCA techniques offers a robust framework for solving complex optimization problems in RIS-aided systems. By leveraging the strengths of both methods, it is possible to handle the non-convex nature of the problem while maintaining computational efficiency. For instance, in the context of sum-rate maximization, the inner approximation can be used to simplify the feasible set, while SCA can be employed to iteratively refine the solution. This hybrid approach not only enhances the convergence properties of the optimization algorithm but also ensures that the solution remains feasible throughout the iterative process. Numerical simulations have shown that this combined approach can achieve near-optimal performance with a significantly reduced computational burden, making it a promising technique for practical implementation in large-scale RIS-aided communication systems [5].

### 3.3.2 Block Coordinate Descent for Iterative Optimization
Block Coordinate Descent (BCD) is a powerful iterative optimization technique that has gained significant attention in the realm of large-scale optimization problems, particularly in the context of Reconfigurable Intelligent Surfaces (RIS)-assisted communication systems. The core idea of BCD is to decompose a complex optimization problem into simpler subproblems by optimizing over a subset of variables while keeping the others fixed. This approach is particularly useful when the original problem is non-convex or computationally intractable. In the context of RIS, BCD is often employed to iteratively optimize the phase shifts of the RIS elements and the precoding matrices at the base station (BS), leading to significant performance improvements in terms of sum rate and energy efficiency.

The BCD framework is particularly advantageous in scenarios where the optimization variables are naturally grouped, such as in RIS-aided systems where the phase shifts and precoding matrices can be treated as separate blocks. By alternating between optimizing the RIS phase shifts and the BS precoding matrices, BCD can converge to a locally optimal solution that is often superior to those obtained by traditional methods. The key to the effectiveness of BCD lies in its ability to handle the unit-modulus constraint on the RIS phase shifts, which is a critical requirement for practical RIS implementations. This is typically achieved by incorporating specialized algorithms, such as the projected gradient method, which ensures that the phase shifts remain on the unit circle while optimizing the overall system performance.

Moreover, the BCD method can be further enhanced by integrating it with other optimization techniques, such as the majorization-minimization (MM) method, to derive closed-form solutions for each subproblem. This combination not only reduces the computational complexity but also accelerates the convergence of the algorithm. The iterative nature of BCD makes it well-suited for dynamic environments where the channel conditions and system parameters change over time. Simulation results have consistently demonstrated the superiority of BCD-based approaches in achieving higher sum rates and better energy efficiency, making it a preferred choice for optimizing RIS-aided communication systems.

### 3.3.3 Monte Carlo Simulations for Validation
Monte Carlo (MC) simulations are a crucial tool for validating the performance and robustness of the proposed method in various scenarios. These simulations involve generating a large number of random samples to approximate the behavior of the system under different conditions, thereby providing a comprehensive understanding of its performance. In our study, we employed MC simulations to evaluate the convergence, total data rate, and robustness of the proposed method under imperfect channel state information (CSI) and multi-RIS scalability. The simulations were designed to mimic real-world conditions, including varying user densities, channel conditions, and system configurations.

The MC simulations were conducted by running the proposed method and several baseline methods, such as simulated annealing (SA), exhaustive search (ES), and conventional NOMA (C-NOMA), across a wide range of scenarios. Each scenario was run multiple times to gather sufficient statistical data. The results demonstrated that the proposed method consistently outperformed the baselines in terms of total data rate and convergence speed. Specifically, the proposed method achieved superior local solutions and nearly satisfied the unit-modulus constraint, which is critical for maintaining the performance of the system. The simulations also revealed that the proposed method exhibited high multi-RIS scalability, achieving the highest sum spectral efficiency (SE) even as the number of RISs increased.

To further validate the robustness of the proposed method, we introduced various levels of CSI estimation errors and evaluated the performance degradation. The MC simulations showed that the proposed method maintained its performance with a softer degradation compared to traditional approaches as the CSI noise variance increased. This robustness is particularly important in practical systems where perfect CSI is often unattainable. Additionally, the simulations provided valuable insights into the impact of different precoding schemes on multi-user interference, highlighting the importance of carefully designed precoding strategies in large-scale systems. The results from these MC simulations not only confirmed the effectiveness of the proposed method but also provided a solid foundation for its practical implementation.

# 4 Resource Allocation and Network Optimization

## 4.1 Deep Reinforcement Learning for UAV Beamforming

### 4.1.1 Double-Loop Structured Optimization-Driven DRL
Double-Loop Structured Optimization-Driven Deep Reinforcement Learning (DRL) represents a sophisticated approach to address the complex, non-convex optimization problems inherent in dynamic and resource-constrained environments, such as those encountered in UAV-assisted wireless networks [8]. This method leverages the hierarchical structure of optimization, where the inner loop focuses on the rapid adaptation of specific parameters, such as UAV receive beamforming, to immediate environmental changes. The outer loop, in contrast, is responsible for the strategic learning and optimization of higher-level variables, such as UAV trajectories and resource allocation, which are more stable and less frequently updated. This separation of concerns allows for a more efficient and scalable solution to the optimization problem, as it reduces the computational burden by breaking down the problem into more manageable sub-tasks.

The double-loop structure in DRL is particularly well-suited for scenarios where the optimization landscape is highly dynamic and requires real-time adjustments. For instance, in UAV-aided data collection, the inner loop can quickly adapt to changes in the communication channel or the positions of ground nodes, while the outer loop can optimize the UAV's flight path to maximize data collection efficiency over a longer period. This approach not only enhances the responsiveness of the system but also improves its overall performance by ensuring that both short-term and long-term objectives are met. The use of advanced DRL algorithms, such as the Rainbow algorithm, further enhances the system's ability to learn from its environment and make optimal decisions under uncertainty.

Moreover, the double-loop structured optimization-driven DRL framework is designed to handle the intricate coupling between multiple optimization variables, which is a common challenge in multi-agent systems. By iteratively refining the inner-loop parameters and periodically updating the outer-loop variables, the framework can converge to near-optimal solutions even in highly non-convex and multi-dimensional optimization spaces. This is particularly important in scenarios where the optimization variables are interdependent, such as in the joint optimization of UAV trajectories and communication resources. The effectiveness of this approach has been demonstrated through extensive simulations, which show significant performance improvements over traditional optimization methods and baseline DRL algorithms, highlighting its potential for practical applications in next-generation wireless networks.

### 4.1.2 Fully DRL-Based Approach for Data Collection
In the realm of data collection, particularly in dynamic environments such as those encountered in Internet of Things (IoT) and sensor networks, the integration of Deep Reinforcement Learning (DRL) has emerged as a powerful paradigm to optimize the data collection process [9]. A fully DRL-based approach leverages the ability of DRL to learn optimal policies through interaction with the environment, making it particularly suitable for scenarios where the network topology and data sources are subject to frequent changes. This approach addresses the limitations of traditional methods, which often rely on pre-defined rules or static optimization techniques that fail to adapt to dynamic conditions.

The core of the fully DRL-based approach lies in its capability to jointly optimize multiple aspects of the data collection process, including UAV trajectory planning, receive beamforming, ground node (GN) scheduling, and power allocation [8]. By formulating the data collection problem as a Markov Decision Process (MDP), DRL algorithms can dynamically adjust these parameters to maximize the total data collected while minimizing energy consumption and ensuring timely data delivery. This is achieved through a reward function that balances the trade-offs between data collection efficiency and resource utilization, allowing the system to adapt to varying network conditions and data demands.

Numerical simulations and real-world experiments have demonstrated the effectiveness of the fully DRL-based approach in enhancing the data collection capabilities of UAVs. Compared to conventional methods, DRL-based systems exhibit superior performance in terms of data throughput, energy efficiency, and robustness to environmental changes. The ability of DRL to continuously learn and improve over time ensures that the system remains optimized even as new data sources are added or existing ones become unavailable, making it a promising solution for future IoT and sensor network applications.

### 4.1.3 Trajectory and Power Allocation Optimization
In the realm of UAV-assisted wireless networks, trajectory and power allocation optimization have emerged as pivotal strategies to enhance system performance and efficiency [10]. This section delves into the methodologies and techniques employed to optimize UAV trajectories and power allocation, which are critical for maximizing communication throughput, minimizing energy consumption, and ensuring robust and secure communication links [10]. The optimization of UAV trajectories involves designing the flight paths of UAVs to achieve specific objectives, such as minimizing latency, maximizing coverage, or enhancing security. This is often formulated as a non-convex optimization problem due to the complex interplay between UAV dynamics, communication constraints, and environmental factors.

One of the primary approaches to trajectory optimization is the use of model predictive control (MPC), which allows for real-time adjustments to the UAV's path based on current and predicted system states. MPC is particularly useful in dynamic environments where the UAV must adapt to changing conditions, such as varying user demands or the presence of obstacles. Another notable technique is the rapidly exploring random tree (RRT) algorithm, which is effective in planning energy-efficient paths that consider the UAV's maneuverability and energy consumption. These methods are often combined with other optimization techniques, such as genetic algorithms or particle swarm optimization, to find near-optimal solutions to the trajectory planning problem.

Power allocation optimization, on the other hand, focuses on determining the optimal transmit power levels for both the UAVs and the ground nodes to achieve desired communication objectives. This is crucial for maintaining link quality, especially in the presence of interference and channel variations. Joint optimization of trajectory and power allocation is a common approach, as the two are inherently coupled. For instance, the UAV's trajectory can be designed to minimize the required transmit power by leveraging favorable channel conditions, while the power allocation can be adjusted to ensure reliable communication along the optimized path [10]. Techniques such as alternating optimization and sequential parametric convex approximation (SPCA) are often employed to solve the resulting non-convex optimization problems. These methods iteratively refine the trajectory and power allocation until a satisfactory solution is reached, balancing the trade-offs between performance and computational complexity.

## 4.2 Systematic Analysis of IRS Deployments

### 4.2.1 Theoretical Performance Evaluation of IRS Reflection Links
The theoretical performance evaluation of IRS reflection links is a critical aspect of understanding the capabilities and limitations of IRS technology in enhancing wireless communication systems [11]. In single-IRS reflection links, the primary focus is on optimizing the phase shifts of the IRS elements to maximize the received signal strength at the intended receiver. This involves joint optimization of the IRS phase shifts and the base station (BS) active beamforming, which can significantly improve the link quality and system throughput. However, the complexity of this optimization increases with the number of IRS elements and the dynamic nature of the wireless environment, necessitating the development of efficient algorithms that can handle real-time adjustments.

For double-IRS and multi-IRS reflection links, the theoretical performance evaluation becomes even more intricate due to the increased number of reflection paths and the interactions between multiple IRSs [11]. In these scenarios, the optimization problem extends beyond simple phase shift adjustments to include the coordination of multiple IRSs to ensure constructive interference at the receiver. This requires a comprehensive understanding of the channel state information (CSI) and the development of advanced algorithms capable of managing the complex interplay between multiple IRSs and the BS. The performance gains from multi-IRS systems can be substantial, particularly in environments with high levels of multipath fading and shadowing, where traditional communication systems struggle to maintain reliable links.

Despite the theoretical advancements, practical implementation of IRS reflection links faces several challenges, including the need for precise CSI acquisition, the computational complexity of optimization algorithms, and the physical constraints of IRS deployment [11]. Theoretical models often assume ideal conditions, such as perfect CSI and negligible hardware impairments, which may not hold in real-world scenarios. Therefore, future research should focus on developing robust and adaptive algorithms that can operate under imperfect CSI and varying environmental conditions, ensuring that the theoretical performance gains of IRS reflection links can be realized in practical deployments.

### 4.2.2 Optimization Techniques for IRS Placement and Beam Routing
Optimization techniques for IRS placement and beam routing are critical for enhancing the performance of wireless communication systems, particularly in terms of energy efficiency and throughput. These techniques aim to maximize the constructive interference and minimize the destructive interference in the communication link, thereby improving the overall system performance. The placement of IRSs is a non-trivial problem due to the complex interplay between the IRS, the base station (BS), and the user equipment (UE). Various optimization methods, including both conventional and machine learning (ML) approaches, have been proposed to address this challenge. Conventional methods often rely on mathematical formulations and optimization algorithms, such as gradient descent and alternating optimization, to find the optimal positions of IRSs. These methods are effective in scenarios where the channel state information (CSI) is known and the problem can be formulated as a convex optimization problem [12].

In contrast, ML-based approaches leverage the ability of neural networks and reinforcement learning (RL) to handle non-convex and dynamic environments. For instance, deep reinforcement learning (DRL) has been used to optimize the IRS placement and beam routing in real-time, adapting to changes in the environment and user mobility. These methods can learn the optimal policies for IRS placement and beamforming without requiring explicit knowledge of the channel conditions, making them suitable for dynamic and complex scenarios. However, the computational complexity and training time of ML models can be significant, which is a trade-off that needs to be considered in practical implementations.

Another important aspect of IRS placement and beam routing is the optimization of the beamforming vectors to maximize the channel gain and minimize interference. This involves the design of both active and passive beamforming schemes, where the IRS is used to steer the reflected beams towards the desired UEs while nulling out the interference to other users. Techniques such as semidefinite relaxation (SDR) and successive convex approximation (SCA) have been employed to solve the non-convex optimization problems associated with beamforming design [1]. Additionally, the integration of IRS with other technologies, such as multiple-input multiple-output (MIMO) and non-orthogonal multiple access (NOMA), further complicates the optimization process but offers significant performance gains [9]. The joint optimization of IRS placement, beam routing, and beamforming is a promising direction for future research, as it can lead to more efficient and robust wireless communication systems [11].

### 4.2.3 Efficient Channel Acquisition Methods
Efficient channel acquisition methods are crucial for the successful deployment of intelligent reflecting surface (IRS) technology in wireless communication systems, especially in scenarios involving multiple IRSs [13]. The complexity of channel estimation increases significantly with the number of IRSs, as it involves estimating the cascaded channels between the base station (BS), IRSs, and users. Traditional methods, such as least squares (LS) and minimum mean square error (MMSE), become computationally infeasible due to the high dimensionality and the need for frequent updates. To address these challenges, recent research has focused on developing low-complexity and high-accuracy channel acquisition techniques. One such approach is the use of compressed sensing (CS), which exploits the sparsity of the IRS channels to reduce the number of required pilots and improve estimation accuracy. CS-based methods leverage the fact that the IRS channels are often sparse in the angular domain, allowing for efficient channel estimation with fewer measurements.

Another promising direction is the application of machine learning (ML) techniques, particularly deep learning (DL), to channel acquisition. DL models, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), can learn the underlying patterns in the channel data and predict the channel state information (CSI) with high accuracy. These models can be trained using a combination of synthetic and real-world channel data, enabling them to generalize well across different environments and scenarios. Furthermore, DL-based methods can adapt to dynamic changes in the channel, making them suitable for real-time applications. For instance, a DL-based channel estimator can be integrated into the BS to continuously update the CSI, thereby optimizing the IRS configurations in real-time. This approach not only reduces the computational burden but also enhances the overall system performance by enabling more accurate and timely channel feedback.

In addition to CS and DL, hybrid methods that combine conventional signal processing techniques with ML have also been proposed. These hybrid approaches aim to balance the trade-off between estimation accuracy and computational complexity. For example, a hybrid method might use a low-complexity initial channel estimation technique, such as a pilot-based method, followed by a refinement step using a DL model. This two-stage process can significantly reduce the pilot overhead while maintaining high estimation accuracy. Another hybrid approach involves using unsupervised learning techniques, such as clustering and autoencoders, to preprocess the channel data and extract relevant features before applying a supervised learning model for final estimation. These hybrid methods are particularly useful in scenarios where the channel conditions are highly dynamic and require frequent updates, such as in mobile environments or dense urban areas. Overall, the development of efficient channel acquisition methods is a critical component in realizing the full potential of IRS-aided wireless communication systems [11].

## 4.3 Sparse Reconfigurable Intelligent Surfaces

### 4.3.1 Maximization of Coverage and Throughput
Maximizing coverage and throughput in UAV-assisted wireless communication networks is a critical challenge that has garnered significant attention in recent years [14]. The primary objective is to ensure that the network can support a wide area of operation while maintaining high data rates and reliability. Traditional approaches often rely on fixed infrastructure and static optimization techniques, which may not be sufficient to handle the dynamic nature of UAV operations and varying environmental conditions. To address these limitations, recent research has focused on integrating advanced optimization methods, particularly those leveraging machine learning (ML) and reconfigurable intelligent surfaces (RIS).

One of the key strategies for maximizing coverage and throughput involves the strategic placement and movement of UAVs. By dynamically adjusting the UAV's trajectory and altitude, it is possible to optimize the coverage area and reduce interference. ML techniques, such as reinforcement learning and deep learning, have been employed to predict optimal flight paths and adjust them in real-time based on current network conditions and user demands. These methods can significantly enhance the network's adaptability and responsiveness, leading to improved throughput and reduced latency. Additionally, the use of multiple UAVs in a coordinated manner can further expand coverage and increase data rates through multi-connectivity and cooperative communication [14].

Another innovative approach is the deployment of RIS, which can be strategically placed in the environment to reflect and redirect wireless signals [15]. RIS can create constructive interference patterns that enhance the signal strength and coverage in targeted areas, thereby improving the overall network performance. By optimizing the phase shifts of the RIS elements, it is possible to steer the reflected signals to specific users, reducing the need for additional infrastructure and minimizing energy consumption [7]. The integration of RIS with UAVs can lead to a highly flexible and efficient network architecture, capable of adapting to changing conditions and user needs, thus maximizing both coverage and throughput [16].

### 4.3.2 Power Consumption Minimization
Power consumption minimization is a critical aspect of UAV-Base Station (UAV-BS) design and operation, primarily due to the direct impact on flight duration, environmental sustainability, and operational costs [14]. The primary energy consumers in a UAV-BS include the propulsion system and the communication equipment, both of which must be optimized to achieve efficient energy utilization [14]. Propulsion energy, which is the dominant energy consumer, can be minimized through optimal trajectory planning and efficient flight patterns that reduce unnecessary maneuvers and maintain optimal speeds. This optimization often involves sophisticated algorithms that consider environmental factors such as wind conditions and terrain, as well as the dynamic nature of the network traffic demands.

On the communication side, energy consumption can be significantly reduced by optimizing the transmission power and resource allocation. Techniques such as adaptive modulation and coding, beamforming, and dynamic spectrum access are employed to ensure that the communication energy is used efficiently. For instance, beamforming can focus the transmitted energy directly towards the intended users, reducing the overall power required for communication. Additionally, the integration of Reconfigurable Intelligent Surfaces (RIS) can further enhance energy efficiency by creating favorable propagation conditions and reducing the need for high transmission power [7]. The use of RIS in UAV-BS systems not only optimizes the communication link but also contributes to the overall energy savings by reducing the complexity and power consumption of the onboard communication equipment [15].

Furthermore, the deployment of machine learning (ML) algorithms has emerged as a powerful tool for power consumption minimization. ML techniques, particularly reinforcement learning (RL), can adapt to changing network conditions and optimize the UAV-BS's operation in real-time. These algorithms can predict traffic patterns, adjust flight paths, and dynamically allocate resources to minimize energy usage while maintaining high service quality. The application of ML in UAV-BS systems not only enhances energy efficiency but also improves the robustness and adaptability of the network, making it more resilient to environmental and operational uncertainties.

### 4.3.3 Simulations and Theoretical Analysis
In the realm of simulations and theoretical analysis, a rigorous approach is adopted to validate the efficacy of the proposed methodologies and algorithms. This section delves into the numerical simulations and analytical frameworks employed to assess the performance of various techniques, particularly in the context of reconfigurable intelligent surfaces (RISs) and multi-antenna systems [7]. The simulations are designed to mimic real-world scenarios, incorporating factors such as channel impairments, user mobility, and environmental conditions, to provide a comprehensive evaluation of the system's robustness and efficiency.

The theoretical analysis focuses on deriving the fundamental performance limits and optimal solutions for the considered problems. For instance, in the context of RIS-assisted communication systems, the analysis often involves formulating the problem as an optimization task, where the objective is to maximize metrics such as sum rate, energy efficiency, or coverage area. The use of advanced mathematical tools, including convex optimization, stochastic geometry, and game theory, enables the derivation of closed-form expressions and insights into the system's behavior under different conditions. These theoretical findings serve as a foundation for the development of practical algorithms and provide a benchmark for evaluating the performance of simulation results.

Simulation results are presented to corroborate the theoretical findings and demonstrate the practical applicability of the proposed methods. Key performance indicators, such as sum rate, bit error rate (BER), and energy efficiency, are evaluated under various system parameters, including the number of RIS elements, transmit power, and user density. The results highlight the significant improvements achieved by the proposed techniques compared to traditional approaches, validating their potential for enhancing the performance of next-generation wireless communication systems. Additionally, sensitivity analyses are conducted to explore the impact of critical parameters on system performance, offering valuable insights for system design and optimization.

# 5 Integration of Traditional Optimization with Neural Networks

## 5.1 Multi-Tier Computing with Advanced Wireless Technologies

### 5.1.1 Supervised Learning for User Location Estimation
Supervised learning has emerged as a powerful tool for user location estimation in wireless networks, leveraging the rich datasets available from network signals and user interactions. Traditional methods for location estimation often rely on complex mathematical models and extensive computational resources, which can be impractical in real-time applications. In contrast, supervised learning approaches, particularly those utilizing neural networks (NNs), offer a balance between accuracy and computational efficiency. These models can be trained on large datasets of network signals and corresponding user locations, allowing them to learn intricate patterns and relationships that are difficult to capture with traditional methods.

One of the key advantages of supervised learning in user location estimation is its ability to adapt to dynamic environments. As network conditions and user behavior change over time, NNs can be retrained or fine-tuned to maintain high accuracy. This adaptability is crucial in scenarios such as indoor positioning, where multipath effects and environmental changes can significantly impact signal propagation. Moreover, the use of deep learning architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), has shown promise in handling the temporal and spatial dependencies inherent in location data. These models can effectively capture the spatial distribution of users and the temporal dynamics of their movements, leading to more accurate and robust location estimates.

Despite the advantages of supervised learning, there are several challenges that must be addressed. One major challenge is the requirement for large, labeled datasets, which can be costly and time-consuming to collect. Additionally, the performance of these models can degrade if the training data does not adequately represent the operational environment. To mitigate these issues, researchers are exploring techniques such as transfer learning and semi-supervised learning, which can leverage pre-trained models and unlabeled data to improve generalization. Furthermore, the integration of domain-specific knowledge, such as channel state information (CSI) and environmental maps, can enhance the performance of supervised learning models, making them more suitable for practical deployment in diverse wireless networks.

### 5.1.2 Fronthaul-Constrained Distributed Massive MIMO
Fronthaul-constrained distributed Massive MIMO systems present a unique set of challenges and opportunities in the realm of 5G and beyond networks. The primary challenge lies in the limited capacity of the fronthaul links, which connect the remote radio heads (RRHs) to the central processing unit (CPU). These constraints can significantly degrade the performance of the system, particularly in terms of data rate and latency. To mitigate these effects, various techniques have been proposed, including compressive sensing, sparse signal processing, and advanced coding schemes. These methods aim to reduce the amount of data that needs to be transmitted over the fronthaul links without compromising the overall system performance.

One of the key approaches to addressing fronthaul constraints is the use of distributed processing techniques, where some of the signal processing tasks are offloaded to the RRHs. This approach can significantly reduce the amount of data that needs to be transmitted to the CPU, thereby alleviating the fronthaul bottleneck. For instance, RRHs can perform tasks such as channel estimation, precoding, and data compression, which can then be aggregated at the CPU. This distributed processing paradigm not only reduces the fronthaul load but also enhances the robustness and scalability of the system. However, it introduces additional complexity in terms of coordination and synchronization among the RRHs, which must be carefully managed to ensure optimal performance.

Another important aspect of fronthaul-constrained distributed Massive MIMO systems is the optimization of resource allocation and signal processing algorithms. Techniques such as dynamic resource allocation, adaptive modulation and coding, and interference management play a crucial role in maximizing the system's spectral efficiency and capacity. Machine learning (ML) and artificial intelligence (AI) have emerged as powerful tools for optimizing these processes, enabling real-time adaptation to varying network conditions and user demands. For example, ML algorithms can be used to predict channel conditions and optimize the allocation of fronthaul resources, thereby improving the overall efficiency and reliability of the system. Despite these advancements, further research is needed to fully realize the potential of fronthaul-constrained distributed Massive MIMO systems in future wireless networks.

### 5.1.3 Dense Fog Massive MIMO Architectures
Dense fog massive MIMO architectures represent a significant advancement in the realm of high-throughput, low-latency wireless communications, particularly in environments characterized by high user density and dynamic mobility [17]. These architectures leverage the deployment of a large number of remote radio heads (RRHs) equipped with multiple antennas, creating a dense fog of wireless access points. Each RRH operates in coordination with others to form a seamless network, enabling users to maintain high-quality connections as they move through the environment. The key advantage of this approach lies in its ability to dynamically adapt to user movement and traffic patterns, thereby optimizing resource utilization and minimizing latency.

In dense fog massive MIMO systems, the integration of multi-tier computing and advanced signal processing techniques plays a crucial role in enhancing overall system performance [17]. Multi-tier computing allows for the distribution of computational tasks across a hierarchy of devices, from edge nodes to cloud servers, ensuring that processing is performed as close to the user as possible. This proximity reduces latency and improves the responsiveness of the network. Additionally, the use of massive MIMO technology, which involves deploying a large number of antennas at each RRH, provides significant gains in terms of spatial multiplexing, diversity, and array gain [4]. These gains are essential for achieving high data rates and robust communication links, especially in challenging environments with high levels of interference and multipath fading.

To fully realize the potential of dense fog massive MIMO architectures, several technical challenges must be addressed. One of the primary challenges is the efficient management and coordination of a large number of RRHs, which requires sophisticated algorithms for resource allocation, scheduling, and interference mitigation. Techniques such as zero-forcing beamforming (ZFBF) and coordinated multipoint (CoMP) transmission are critical for optimizing the performance of these systems. Furthermore, the deployment of dense fog massive MIMO architectures necessitates the development of cost-effective and energy-efficient hardware solutions, as well as the integration of advanced machine learning algorithms to enable real-time decision-making and adaptive network configurations. Despite these challenges, the potential benefits of dense fog massive MIMO architectures in terms of improved user experience, higher data rates, and enhanced network reliability make them a promising direction for future wireless communication systems [17].

## 5.2 Determinantal Point Process-Based Learning

### 5.2.1 DPP for Subset Selection in Large-Scale Networks
Determinantal Point Processes (DPPs) have emerged as a powerful tool for subset selection in large-scale networks, offering a principled approach to balance quality and diversity. Unlike traditional methods that often struggle with the exponential complexity of subset selection, DPPs provide a probabilistic framework that naturally captures the trade-off between selecting high-quality items and maintaining diversity among them [18]. By representing the optimal subset as a realization of a DPP, the problem is reduced to sampling from a DPP, which can be efficiently performed using algorithms that scale well with the size of the network [18].

The key advantage of the DPP-based learning (DPPL) framework lies in its ability to learn the parameters of the DPP that best fit the given problem. These parameters, often encapsulated in a similarity matrix, are trained to maximize the likelihood of selecting subsets that are both high-quality and diverse. This learning process is particularly beneficial in dynamic network environments, where the optimal subset can change rapidly due to varying network conditions. The DPPL framework can adapt to these changes by continuously updating the parameters based on new data, ensuring that the selected subsets remain optimal over time.

To further enhance the applicability of DPPs in large-scale networks, recent research has focused on designing more effective similarity matrices that can better capture the intricate relationships between network elements. These matrices are crucial for the performance of the DPP, as they determine the probability of selecting specific subsets. By incorporating domain-specific knowledge and leveraging advanced optimization techniques, these new approaches have shown significant improvements in both the accuracy and scalability of subset selection. For instance, in the context of link scheduling in wireless communications, the DPPL framework has been successfully applied to solve the NP-hard problem of selecting a subset of links that maximizes the overall network throughput while minimizing interference.

### 5.2.2 Training DPP Parameters for Quality and Similarity
Training Determinantal Point Process (DPP) parameters for quality and similarity involves leveraging the inherent properties of DPPs to optimize the selection of subsets that balance both criteria. DPPs are probabilistic models that capture the diversity and quality of elements within a set, making them particularly suitable for tasks where the selection of diverse yet high-quality items is crucial. In the context of training neural networks (NNs) for Signal-to-Interference-plus-Noise Ratio (SINR) optimization, DPPs can be used to guide the selection of training samples that are both representative of the underlying distribution and diverse enough to cover a wide range of scenarios.

The training process for DPP parameters typically involves defining a kernel matrix \( K \) that encodes the similarities and qualities of the elements in the ground set. Each element in the kernel matrix represents the pairwise similarity between two items, while the diagonal elements reflect the individual quality of each item. The goal is to learn the parameters of this kernel matrix such that the resulting DPP selects subsets that maximize the desired trade-off between quality and diversity. This can be achieved through various optimization techniques, including gradient-based methods and maximum likelihood estimation. By optimizing the DPP parameters, the model can effectively capture the structure of the data and ensure that the selected subsets are both high-quality and diverse [18].

In practice, the integration of DPPs into the training of NNs for SINR optimization can significantly enhance the performance and robustness of the model [19]. For instance, by using DPPs to select training samples, the NN can be exposed to a more diverse set of scenarios, leading to better generalization and faster convergence. Additionally, the use of DPPs can help mitigate the risk of overfitting by ensuring that the model is trained on a balanced and representative dataset. Simulation results have shown that this approach can lead to substantial improvements in SINR performance and convergence speed, making it a promising direction for future research in the field of wireless communication and machine learning [19].

### 5.2.3 Application to Resource Allocation and Interference Management
In the realm of resource allocation and interference management, the integration of advanced machine learning techniques, particularly deep learning (DL), has emerged as a promising solution to address the complexities of modern wireless networks. Traditional methods, such as water-filling and game theory, have provided robust solutions but often struggle with the dynamic and heterogeneous nature of contemporary networks, especially in scenarios with high user density and varying channel conditions. DL, on the other hand, offers a flexible framework that can adapt to changing environments and learn from historical data to predict and mitigate interference, thereby optimizing resource allocation. For instance, neural networks (NNs) can be trained to dynamically adjust transmission powers, allocate subcarriers, and manage beamforming vectors, leading to improved spectral efficiency and reduced interference.

One of the key challenges in applying DL to resource allocation and interference management is the need for real-time decision-making, which is critical in high-speed wireless communications, such as 5G and beyond. Traditional DL models, while powerful, often suffer from high computational complexity and latency, making them less suitable for low-power and low-latency applications, such as those found in Internet of Things (IoT) devices and millimeter-wave (mm-wave) systems. To overcome this, researchers have explored lightweight and efficient DL architectures, such as convolutional neural networks (CNNs) and graph neural networks (GNNs), which can be deployed on edge devices with limited computational resources. These models are designed to balance the trade-off between accuracy and computational efficiency, enabling real-time resource allocation and interference management in resource-constrained environments.

Moreover, the integration of DL with classical optimization techniques has shown significant potential in enhancing the performance of resource allocation and interference management. By leveraging the optimality guarantees of traditional methods, DL models can be guided to converge to high-quality solutions more efficiently. For example, reinforcement learning (RL) algorithms can be combined with convex optimization to dynamically adjust resource allocation policies based on real-time network conditions, while ensuring that the solutions remain within the bounds of known optimal solutions. This hybrid approach not only improves the robustness of the system but also enhances its adaptability to evolving network dynamics, making it a viable solution for future wireless networks.

## 5.3 Hybrid Model-Based and Data-Driven Systems

### 5.3.1 LoRD-Net for Blind Symbol Detection
LoRD-Net, or Low Resolution Detection Network, represents a significant advancement in blind symbol detection, particularly in scenarios where only one-bit measurements are available. This architecture innovatively merges the robustness of model-based maximum-likelihood estimation (MLE) with the efficiency and adaptability of deep learning techniques. By employing deep unfolding, a method that translates iterative algorithms into the layers of a deep neural network, LoRD-Net effectively leverages the strengths of both model-based and data-driven approaches. This hybrid design not only enhances the network's ability to learn complex mappings from noisy one-bit measurements to symbol decisions but also ensures that the learned mappings are grounded in the theoretical foundations of signal processing.

One of the key advantages of LoRD-Net is its ability to bypass the need for separate channel estimation, a critical and often computationally intensive step in traditional MLE-based detectors [20]. By integrating channel estimation and symbol detection into a single, optimized process, LoRD-Net significantly reduces the computational overhead and latency associated with these tasks [20]. This streamlined approach is particularly beneficial in high-speed communication systems where rapid and accurate symbol detection is crucial. Moreover, the low computational cost and high scalability of LoRD-Net make it an attractive solution for real-time applications, such as those in 5G and beyond, where resource constraints are a significant concern.

The architecture of LoRD-Net is designed to be highly flexible and adaptable, allowing it to handle a wide range of channel conditions and modulation schemes. This flexibility is achieved through the use of deep neural network layers that are specifically tailored to the problem of blind symbol detection. These layers are trained to capture the intricate relationships between the one-bit measurements and the underlying symbols, thereby improving the overall detection performance. Additionally, the hybrid nature of LoRD-Net ensures that the network can maintain high accuracy even in the presence of noise and interference, making it a robust solution for practical communication systems. The combination of these features positions LoRD-Net as a promising approach for advancing the state-of-the-art in blind symbol detection.

### 5.3.2 Deep Unfolding Technique for MLE Computation
Deep unfolding is a powerful technique that bridges the gap between traditional model-based optimization methods and modern data-driven approaches, particularly in the context of maximum likelihood estimation (MLE) computation. In the realm of signal processing and communications, the MLE is a fundamental approach for parameter estimation, but its computational complexity and sensitivity to initialization often limit its practical applicability. By unfolding the iterations of the MLE optimization process into the layers of a deep neural network (DNN), deep unfolding transforms the iterative optimization problem into a learnable architecture. This transformation not only accelerates the convergence to the optimal solution but also leverages the representational power of DNNs to handle non-linearities and high-dimensional data more effectively.

The deep unfolding technique for MLE computation involves carefully designing each layer of the DNN to mimic a single iteration of the MLE algorithm. For instance, in the context of LoRD-Net, the first-order gradient iterations of the MLE are unfolded, where each layer corresponds to a gradient update step. This design choice ensures that the DNN retains the theoretical guarantees of the MLE while benefiting from the data-driven adaptability of neural networks. Moreover, the hybrid nature of deep unfolding allows for the integration of domain-specific knowledge, such as channel models and noise characteristics, into the training process. This integration is crucial for tasks like blind symbol detection, where the model must operate under limited or no prior information about the communication channel.

Another significant advantage of deep unfolding in MLE computation is its ability to reduce the computational burden and latency associated with traditional MLE methods. By training the DNN to approximate the MLE solution, the need for computationally intensive iterative algorithms is mitigated, leading to faster inference times and lower power consumption. This efficiency is particularly important in real-time applications, such as wireless communication systems, where rapid and accurate decision-making is critical. Furthermore, the scalability of deep unfolding makes it well-suited for deployment in resource-constrained environments, such as edge devices and IoT systems, where computational resources are limited. Overall, the deep unfolding technique represents a promising direction for enhancing the performance and practicality of MLE-based methods in a wide range of applications.

### 5.3.3 Two-Stage Training Procedure for Performance Improvement
The two-stage training procedure is designed to enhance the performance and convergence speed of neural network (NN) models in the context of signal-to-interference-plus-noise ratio (SINR) optimization. This approach leverages the strengths of traditional optimization methods, which provide robust and theoretically grounded solutions, while harnessing the flexibility and adaptability of NNs. In the first stage, the training process focuses on learning the optimization dynamics that underpin the traditional methods. By unrolling the iterative steps of these methods, the NN is trained to mimic the behavior of the optimization algorithm, effectively capturing the underlying mathematical structure and the sequence of operations that lead to optimal solutions.

This initial stage is crucial as it ensures that the NN learns not just the final optimized state but also the intermediate steps, which are essential for understanding the problem's landscape and the path to the solution. Once the NN has been trained to accurately replicate the optimization process, the second stage of training commences. Here, the focus shifts to fine-tuning the NN through end-to-end training, where the entire network is optimized to minimize the error between the predicted and actual SINR values [19]. This stage allows the NN to refine its performance, adapting to the nuances of the specific application and potentially discovering more efficient paths to the optimal solution than those provided by the traditional methods.

Simulation results have consistently shown that this two-stage training procedure leads to significant improvements in both the accuracy and convergence speed of the NN models. The integration of traditional optimization insights during the first stage provides a strong foundation, while the end-to-end training in the second stage enables the NN to achieve superior performance [19]. This hybrid approach not only accelerates the training process but also enhances the robustness and reliability of the NN in real-world scenarios, making it a promising technique for optimizing SINR in complex communication systems [19].

# 6 Future Directions


Despite the significant advancements in optimization methods for wireless communication systems, several limitations and gaps remain. Current methods often struggle with the high computational complexity and non-convexity of problems in large-scale systems, particularly in scenarios involving multiple reconfigurable intelligent surfaces (RISs) and dynamic environments. The reliance on accurate channel state information (CSI) is another challenge, as real-world systems frequently encounter imperfect or delayed CSI, which can degrade the performance of optimization algorithms. Additionally, the integration of traditional optimization techniques with machine learning (ML) and deep learning (DL) methods, while promising, is still in its early stages, and more research is needed to develop robust and efficient hybrid approaches. There is also a need for more comprehensive theoretical analysis to understand the fundamental limits and trade-offs in these systems, especially in terms of energy efficiency and spectral efficiency.

To address these limitations, several directions for future research are proposed. First, the development of more scalable and computationally efficient algorithms is essential. This could involve the use of advanced approximation techniques, such as stochastic gradient methods and distributed optimization, to handle the high dimensionality and complexity of large-scale systems. Additionally, exploring the potential of quantum computing for solving non-convex optimization problems could open new avenues for research. Second, the robustness of optimization methods under imperfect CSI conditions should be a focus. Techniques such as robust optimization and Bayesian inference can be integrated to account for uncertainties and improve the reliability of solutions. Third, the hybridization of traditional optimization with ML and DL methods should be further explored. This includes developing end-to-end trainable models that can jointly optimize multiple aspects of the system, such as beamforming, power allocation, and trajectory planning. Finally, the application of reinforcement learning (RL) to dynamically adapt optimization strategies in real-time is a promising area, particularly for UAV-assisted networks and RIS-aided systems.

The potential impact of the proposed future work is substantial. By developing more efficient and robust optimization methods, the performance of wireless communication systems can be significantly enhanced, leading to higher data rates, lower latency, and improved reliability. The integration of advanced computational techniques and ML can enable real-time adaptation to dynamic environments, making the systems more resilient to changes in network conditions and user demands. Furthermore, the theoretical insights gained from this research can inform the design of future wireless standards and protocols, contributing to the broader goal of achieving ubiquitous, high-quality communication in a wide range of applications, from 5G and beyond to the Internet of Things (IoT) and smart cities.

# 7 Conclusion



This survey paper has provided a comprehensive overview of the recent advancements in optimization methods for wireless communication systems, with a focus on key areas such as beamforming and phase optimization in reconfigurable intelligent surface (RIS)-aided systems, adaptive polynomial chaos expansion for uncertainty quantification, non-convex optimization for spectral efficiency, and the integration of traditional optimization with neural networks. The paper has highlighted the use of advanced techniques such as generalized Benders decomposition, weighted mean square error minimization, sparse polynomial chaos expansion, and deep reinforcement learning. These methods address the complex challenges posed by modern wireless systems, including the optimization of beamforming and phase shifts in RIS-aided systems, the efficient management of resources in UAV-assisted networks, and the enhancement of spectral efficiency in multi-user scenarios. The integration of deep learning and reinforcement learning has been shown to significantly improve the computational efficiency and robustness of these optimization methods, making them suitable for real-time applications and large-scale systems.

The significance of this survey lies in its comprehensive and up-to-date review of the latest optimization methods and their applications in wireless communication systems. By synthesizing the findings from various studies, the paper provides a valuable resource for researchers and practitioners in the field, offering insights into the practical implications and potential advancements that can be achieved through the integration of advanced optimization techniques. The survey not only highlights the key challenges and open research questions in the domain but also serves as a roadmap for future research, guiding the development of new methods and applications. The detailed examination of techniques such as inner approximation, block coordinate descent, and Monte Carlo simulations underscores the importance of these methods in validating the performance and robustness of optimization algorithms under various conditions.

In conclusion, the continued advancement of optimization methods is crucial for the evolution of wireless communication systems. The integration of machine learning and traditional optimization techniques holds great promise for addressing the increasing complexity and dynamic nature of these systems. Researchers and practitioners are encouraged to explore the synergies between these methods to develop more efficient, robust, and adaptive solutions. Future work should focus on the practical implementation of these methods, the development of real-time algorithms, and the exploration of new applications in emerging technologies such as 6G and beyond. By building on the foundations laid out in this survey, the wireless communication community can drive innovation and push the boundaries of what is possible in the realm of advanced optimization.

# References
[1] Integration of NOMA with Reflecting Intelligent Surfaces  A Multi-cell  Optimization with SIC Decodi  
[2] Scalable Beamforming Design for Multi-RIS-Aided MU-MIMO Systems with  Imperfect CSIT  
[3] Beamforming for PIN Diode-Based IRS-Assisted Systems Under a Phase  Shift-Dependent Power Consumptio  
[4] Rapid and Power-Aware Learned Optimization for Modular Receive  Beamforming  
[5] Manifold-Based Optimizations for RIS-Aided Massive MIMO Systems  
[6] Adaptive Polynomial Chaos Expansion for Uncertainty Quantification and  Optimization of Horn Antenna  
[7] Investigating Sparse Reconfigurable Intelligent Surfaces (SRIS) via  Maximum Power Transfer Efficien  
[8] UAV-Enabled Data Collection for IoT Networks via Rainbow Learning  
[9] Two-Agent DRL for Power Allocation and IRS Orientation in Dynamic  NOMA-based OWC Networks  
[10] Robust Trajectory and Transmit Power Design for Secure UAV  Communications  
[11] Intelligent Reflecting Surface Aided Wireless Networks  From  Single-Reflection to Multi-Reflection  
[12] Joint SIM Configuration and Power Allocation for Stacked Intelligent  Metasurface-assisted MU-MISO S  
[13] Movable Antennas Meet Intelligent Reflecting Surface  Friends or Foes   
[14] A Survey on Energy Optimization Techniques in UAV-Based Cellular  Networks  From Conventional to Mac  
[15] Beamforming Optimization for Active RIS-Aided Multiuser Communications  With Hardware Impairments  
[16] Navigation of a UAV Equipped with a Reconfigurable Intelligent Surface  for LoS Wireless Communicati  
[17] Task Offloading with Multi-Tier Computing Resources in Next Generation  Wireless Networks  
[18] Determinantal Learning for Subset Selection in Wireless Networks  
[19] Distilling Knowledge from Resource Management Algorithms to Neural  Networks  A Unified Training Ass  
[20] LoRD-Net  Unfolded Deep Detection Network with Low-Resolution Receivers  